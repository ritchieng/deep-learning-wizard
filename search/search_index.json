{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["trimmer","stopWordFilter"]},"docs":[{"location":"about/","title":"About Us","text":"<p>We deploy a top-down approach that enables you to grasp deep learning and deep reinforcement learning theories and code easily and quickly. We have open-sourced all our materials through our Deep Learning Wizard Tutorials. For visual learners, feel free to sign up for our video course and join over 6000 deep learning wizards.</p> <p>To this date, we have taught thousands of students across more than 120+ countries from students in high school to postgraduates and professionals in leading MNCs and research institutions around the world.</p>"},{"location":"about/#experienced-research-and-applied-core-team","title":"Experienced Research and Applied Core Team","text":"<p>Ritchie Ng</p> <p>Industry: As the Group Head of Data &amp; AI at Eastspring Investments, I spearhead engineering, governance, and strategy across data and machine learning in 13 countries with Assets Under Management (AUM) exceeding USD 220 billion.</p> <p>Academia: Serving in the capacity of a Visiting Research Scholar at NExT++ Lab, jointly setup between NUS and Tsinghua University, I focus on applied deep learning research for Asian systematic trading strategies and the development of financial Large Language Models (LLM) using multimodal and multilingual data.</p> <p>Education: Master's degree from Imperial College London and Bachelor's degree from NUS.</p> <p>About: ritchieng.live</p> <p>Notes: ritchieng.com</p> <p>Guides: deeplearningwizard.com</p> <p>Jie Fu</p> <p>Jie Fu (\u4ed8\u6770) is a happy and funny machine learning researcher (principal investigator), at Beijing Academy of Artificial Intelligence2 (BAAI, \u5317\u4eac\u667a\u6e90\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u9662), with his human-friendly big AI dream.</p> <p>He worked as a postdoc with Yoshua Bengio at University of Montreal, Quebec AI Institute (Mila), funded by Microsoft Research Montreal. He was an IVADO postdoc fellow working with Chris Pal at Polytechnique Montreal, Quebec AI Institute (Mila). He obtained his PhD from National University of Singapore under the supervision of Tat-Seng Chua. He received ICLR 2021 Outstanding Paper Award.</p> <p>He is currently working towards system-2 deep learning and its adaption to various real-world tasks, including AI for Science (drug discovery in particular) and AI for Social Good that can benefit all of society. He is also broadly interested in general deep learning, reinforcement learning, and language processing.</p> <p>Check out my profile link at bigaidream.github.io</p>"},{"location":"about/#supporters","title":"Supporters","text":"<p>Alfredo Canziani</p> <p>Alfredo is the main person who inspired Ritchie to open-source Deep Learning Wizard's materials. We are very grateful for his advice particular in the space of open-source projects, deep learning and PyTorch.</p> <p>He is currently working as an Assistant Professor at Courant Institute of Mathematical Sciences, under the guidance of professor Yann LeCun.</p> <p>Do check out his latest mini-course on PyTorch that was held in Princeton University and was delivered with Ritchie Ng at AMMI (AIMS) Kigali, Rwanda supported by Google and Facebook.</p> <p>Marek Bardonski</p> <p>Since graduation, Marek has been working on state-of-the-art research. He was involved in inventing the system that diagnoses problems with Ubuntu packages using Machine Learning and in improving the detection accuracy of breast cancer using Deep Learning, each time leading 40 highly skilled Machine Learning/Deep Learning engineers for over 6 months.</p> <p>NASA noticed his unique skills and asked him to manage a project aiming to improve the International Space Station rotation algorithm that would allow them to collect enough power from solar panels while preventing them from overheating. The project was successful and generated a huge ROI for NASA. Marek has been honored by multiple Harvard professors and the Director of Advanced Exploration Systems at NASA, who signed an official Letter of Recommendation.</p> <p>Since his first summer of university coursework, Marek has participated in four internships at NVIDIA HQ and Microsoft HQ. He also spent one year in NVIDIA Switzerland optimizing Deep Learning inference for self-driving cars and, by implementing very efficient Deep Learning algorithms in the GPU assembly language, improving the speed of Tesla\u2019s cars self-driving engine inference. He's currently the Head of AI at Sigmoidal.</p> <p>\"NVIDIA Inception Partner</p> <p></p> <p>Deep Learning Wizard is proudly an NVIDIA Inception Partner, empowering people across the world to leverage on deep learning via open-source programming languages and frameworks to solve problems.</p> <p>Amazon AWS Activate Programme Startup</p> <p></p> <p>Deep Learning Wizard is also supported by the Amazon AWS Activate Programme, Portfolio Plus, allowing us to have the infrastructure to scale and grow.   </p>"},{"location":"about/#citation","title":"Citation","text":"<p>If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.</p> <p></p>"},{"location":"consultancy/","title":"Training, Consultancy &amp; Deployment","text":"<p>We've empowered hundreds of clients from large start-ups, to MNCs, to educational institutions, and to government organizations with deep learning.</p> <p>If you require on-site training, consultancy or deployment on deep learning, deep reinforcement learning, machine learning, and/or general data science projects, please feel free to contact us at ritchie@deeplearningwizard.com.</p>"},{"location":"home/","title":"Learn The Whole AI Pipeline With","text":"<p>You will the learn the suite of tools to build an end-to-end deep learning pipeline.</p> Tools You Will Learn To Use <ul> <li>PyTorch (CPU/GPU)</li> <li>Scikit-learn (CPU)</li> <li>RAPIDS cuML (GPU)</li> <li>Gym (CPU)</li> <li>Python (scripting)</li> <li>C++ (programming)</li> <li>Bash (scripting)</li> <li>NumPy (CPU)</li> <li>CuPy (GPU)</li> <li>Pandas (CPU)</li> <li>RAPIDS cuDF (GPU)</li> <li>Matplotlib (Plot)</li> <li>Plotly (Plot)</li> <li>Streamlit (Dashboard)</li> <li>CassandraDB (CPU DB)</li> <li>BlazingSQL (GPU DB)</li> </ul> <p>We deploy a top-down approach that enables you to grasp deep learning and deep reinforcement learning theories and code easily and quickly. We have open-sourced all our materials through our Deep Learning Wizard Tutorials. For visual learners, feel free to sign up for our video course and join over 6000 deep learning wizards.</p> <p>To this date, we have taught thousands of students across more than 120+ countries from students in high school to postgraduates and professionals in leading MNCs and research institutions around the world.</p> <p></p> <ol> <li> <p>Simulation of deep reinforcement learning agent mastering games like Super Mario Bros, Flappy Bird and PacMan. These games have APIs for algorithms to interact with the environment, and they are created by talented people so feel free to check out their respective repositories with the links given.\u00a0\u21a9</p> </li> </ol>"},{"location":"pipeline/","title":"CPU to GPU Production-level Pipeline for AI","text":"<p>At Deep Learning Wizard, we cover the basics of some parts of the whole tech stack for production-level CPU/GPU-powered AI.</p> <p>This AI pipeline is entirely based on open-source distributions.</p> <p>This stack would get you started, and enable you to adjust the stack according to your needs.</p> <p></p>"},{"location":"review/","title":"Reviews","text":"<p>To this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world.</p> <p>These are just some of the hundreds of reviews we've had and will be updated when we have more time or contributors.</p> <p>Roberto Trevi\u00f1o Cervantes</p> <p>Congratulations! You have achieved the unachievable, teaching a 15 year old boy how to get a machine to estimate a function, just what he had been trying to do for almost a year.</p> <p>Muktabh Mayank</p> <p>This course helped me understand idiomatic pytorch and avoiding translating theano-to-torch.</p> <p>Charles Neiswender</p> <p>I really feel like I can take the techniques I learned and apply them to deep learning projects of my choosing.</p> <p>Ian Lipton</p> <p>This was a good overview of the different types of neural networks. I like that the instructor started with linear and logistic regression before diving into the neural networks. That background helped make sense of the neural network functionality. I like that this gave enough detail on the mathematics so that you can understand the basic functioning of the models but didn't overwhelm you with the nitty gritty math.</p> <p>And check out hundreds of more reviews for our video course! </p>"},{"location":"data_engineering/nosql/cassandra/intro/","title":"Apache Cassandra","text":"Tools You Will Learn To Use <ul> <li>Apache Cassandra</li> </ul>"},{"location":"data_engineering/nosql/cassandra/intro/#reasons-for-using-apache-cassandra","title":"Reasons for Using Apache Cassandra","text":"<ul> <li>It is is an open-source NoSQL database management system (DBMS) that is designed to handle large amounts of data across many servers, providing high availability with no single point of failure. <ul> <li>Meaning if one server fails, everything goes as per normal as data is shared amongst nodes (servers).</li> <li>Compared to other master/slave models like MongoDB, Cassandra has a master-less model.</li> </ul> </li> <li>It has extremely fast read/write speeds.<ul> <li>You read/write millions of rows of data easily.</li> </ul> </li> </ul> <p>Work in progress</p> <p>This open-source portion is still a work in progress. Stay tuned while we gradually upload our tutorials and notes. Feel free to contact Ritchie Ng if you would like to contribute via our Facebook page.</p>"},{"location":"data_engineering/nosql/cassandra/setting_up_cluster/","title":"Setting up Cassandra Multi-node Cluster","text":""},{"location":"data_engineering/nosql/cassandra/setting_up_cluster/#install-apache-cassandra","title":"Install Apache Cassandra","text":"<p>Patience is key initially</p> <p>It will be quite tedious to many people in setting up a multi-node Cassandra cluster. You are likely to face a lot of problems. But with a strong community and this tried-and-proven guide, you should be ok! Just be patient.</p> <p>Remember when you're asking for help on StackOverflow, here or anywhere else, do paste the logs to make everyone's life easier to help you debug quickly. </p> <p>All you need to do is to run this base command and paste the logs.</p> <pre><code>cat /var/log/cassandra/system.log\n</code></pre> <p>Here, we will install all of the required Debian packages. To ensure you've the latest version of Cassandra, please use the official link at cassandra.apache.org/download.</p> <p>Bash Commands <pre><code>echo \"deb http://www.apache.org/dist/cassandra/debian 311x main\" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list\n\ncurl https://www.apache.org/dist/cassandra/KEYS | sudo apt-key add -\n\nsudo apt-get update\n\nsudo apt-get install cassandra\n</code></pre></p> <p>This would get you up and running with Cassandra v3.11.</p>"},{"location":"data_engineering/nosql/cassandra/setting_up_cluster/#install-datastax-python-cassandra-driver","title":"Install DataStax Python Cassandra Driver","text":"<p>We need to install the Python Cassandra Driver so we can easily interact with our Cassandra Database without using the CQLSH (this is Cassandra's shell to run CQL commands to perform CRUD operations) commandline directly.</p> <p>What is CRUD?</p> <p>CRUD is the acronym of CREATE, READ, UPDATE, and DELETE. For databases, we typically have these 4 categories of operations so we can create new data points, update, read or delete them in our database.</p> <p>Bash Commands <pre><code>pip install cassandra-driver\n</code></pre></p> <p>This is strictly for installation on Linux platforms, refer to the official website for more details.</p>"},{"location":"data_engineering/nosql/cassandra/setting_up_cluster/#single-node-cassandra-cluster","title":"Single Node Cassandra Cluster","text":"<p>Before we venture into the cool world of multi-node cluster requiring many servers, we will start with a single node cluster that only requires a single server (desktop/laptop).</p> <p>Once you've installed everything so far, you should run the following.</p> <p>Bash commands</p> <pre><code>sudo service cassandra start\nsudo nodetool status\n</code></pre> <p>And this will print out something like that:</p> <pre><code>Datacenter: datacenter1\n=============================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address       Load       Tokens       Owns (effective)  Host ID                               Rack\nUN  127.0.0.1     318.78 KiB  256          100.0%            g5ac4c9-99b7-65d-24cfd82524f9      rack1\n</code></pre> <p>That's it, we've built our single node Cassandra database.</p>"},{"location":"data_engineering/nosql/cassandra/setting_up_cluster/#multi-node-cassandra-cluster","title":"Multi-node Cassandra Cluster","text":"<p>Remember it's standard to have at least 3 nodes, and in a basic 3 separate server configuration (3 separate desktops for non-enterprise users). Because of this, we will base this tutorial on setting up a 3-node cluster. But the same steps apply if you want to even do 10000 nodes.</p> <p>Installing Sublime critical</p> <p>Sublime is a text editor, and it would help if you're not familiar with VIM which I use frequently. Honestly using Sublime to edit all the following files we will edit is much easier, trust me! </p> <p>To install Sublime run the following bash commands in sequence: <pre><code>wget -qO - https://download.sublimetext.com/sublimehq-pub.gpg | sudo apt-key add -\n\necho \"deb https://download.sublimetext.com/ apt/stable/\" | sudo tee /etc/apt/sources.list.d/sublime-text.list\n\nsudo apt-get update\n\nsudo apt-get install sublime-text\n</code></pre></p> <p>Now check if it works by running this command in your bash: <pre><code>subl\n</code></pre></p> <p>This should open Sublime text editor! You're now ready.</p> <p>Before we move on to the steps to set up each node, we need the IP address of all 3 servers. This is simple, just run the following bash command:</p> <pre><code>ifconfig | sed -En 's/127.0.0.1//;s/.*inet (addr:)?(([0-9]*\\.){3}[0-9]*).*/\\2/p'\n</code></pre> <p>This would return the server's IP address that we need for example:</p> <pre><code>server_1_ip: 112.522.6.61\nserver_2_ip: 112.522.6.62\nserver_3_ip: 112.522.6.63\n</code></pre> <p>We will be using these IPs as a base for configuration in the subsequent sections. Take note yours would differ and you need to change accordingly.</p>"},{"location":"data_engineering/nosql/cassandra/setting_up_cluster/#steps-per-node","title":"Steps Per Node","text":"<p>Critical Section You Need To Repeat</p> <p>This is a critcal section. On EVERY server, you need to repeat all the steps shown in this section. In our case of a 3 node cluster, using 3 servers, we need to repeat this 3 times.</p>"},{"location":"data_engineering/nosql/cassandra/setting_up_cluster/#step-1-modify-cassandra-configuration-settings","title":"Step 1: Modify Cassandra Configuration Settings","text":"<p>Run the following bash to edit the configuration file for server 1 (112.522.6.61):</p> <pre><code>cd /etc/cassandra\nsubl cassandra.yaml\n</code></pre> <p>Now you need to find the following fields and change them accordingly.</p> <pre><code>cluster_name: 'CassandraDBCluster'\n\n\nseed_provider:\n  - class_name: org.apache.cassandra.locator.SimpleSeedProvider\n    parameters:\n         - seeds: \"112.522.6.61, 112.522.6.62, 112.522.6.63\"\n\n\nlisten_address: 112.522.6.61\n\n\nrpc_address: 112.522.6.61\n\n\nendpoint_snitch: GossipingPropertyFileSnitch\n\nauto_bootstrap: true\n</code></pre>"},{"location":"data_engineering/nosql/cassandra/setting_up_cluster/#step-2-modify-rack-details","title":"Step 2: Modify Rack Details","text":"<p>Run the following bash command to edit the rack details:</p> <pre><code>cd /etc/cassandra\nsubl cassandra-rackdc.properties\n</code></pre> <p>And change the following fields to this: <pre><code>dc=datacenter1\n</code></pre></p>"},{"location":"data_engineering/nosql/cassandra/setting_up_cluster/#step-3-check-status","title":"Step 3: Check Status","text":"<p>Restart service with the following bash commands:</p> <pre><code>sudo rm -rf /var/lib/cassandra/data/system/*\nsudo service cassandra restart\nsudo nodetool status\n</code></pre> <p>Here you would see an output like this:</p> <pre><code>Datacenter: datacenter\n=============================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address       Load       Tokens       Owns (effective)  Host ID                               Rack\nUN  112.522.6.61  318.78 KiB  256          100.0%            f5c84c9-99b7-45d-8856-24cfd82523f9  rack1\nUN  112.522.6.62  206.42 KiB  256          100.0%            ac2f24da-1b2c-4e3-8a75-0e28ec366a4  rack1\nUN  112.522.6.63  338.34 KiB  256          100.0%            55a227d-ffbe-45d5-8698-60c374b3a6b  rack1\n</code></pre>"},{"location":"data_engineering/nosql/cassandra/setting_up_cluster/#step-4-configure-firewall-ip-settings","title":"Step 4: Configure Firewall IP Settings","text":"<p>Run the following bash commands</p> <pre><code>sudo apt-get install iptables-persistent\n\nsudo iptables -A INPUT -p tcp -s 112.522.6.62 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT\nsudo iptables -A INPUT -p tcp -s 112.522.6.63 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT\n\nsudo bash -c \"iptables-save &gt; /etc/iptables/rules.v4\"\n\nsudo netfilter-persistent reload\n\nsudo nodetool status\n</code></pre>"},{"location":"data_engineering/nosql/cassandra/setting_up_cluster/#repeat-steps-1-to-4","title":"Repeat Steps 1 to 4","text":"<p>You have to repeat all 4 steps for the other 2 servers but step 1 and step 4 requires slightly different settings. For the other servers, you need to basically change the IP to the server's IP. And for the IP firewall settings, you need to allow entry from the other 2 servers instead of itself.</p>"},{"location":"data_engineering/nosql/cassandra/setting_up_cluster/#server-2-example-for-step-2","title":"Server 2 Example for Step 2","text":"<pre><code>cluster_name: 'CassandraDBCluster'\n\n\nseed_provider:\n  - class_name: org.apache.cassandra.locator.SimpleSeedProvider\n    parameters:\n         - seeds: \"112.522.6.61, 112.522.6.62, 112.522.6.63\"\n\n\n# ONLY THIS LINE CHANGES\nlisten_address: 112.522.6.62\n\n# ONLY THIS LINE CHANGES\nrpc_address: 112.522.6.62\n\n\nendpoint_snitch: GossipingPropertyFileSnitch\n\nauto_bootstrap: true\n</code></pre>"},{"location":"data_engineering/nosql/cassandra/setting_up_cluster/#server-3-example-for-step-2","title":"Server 3 Example for Step 2","text":"<pre><code>cluster_name: 'CassandraDBCluster'\n\n\nseed_provider:\n  - class_name: org.apache.cassandra.locator.SimpleSeedProvider\n    parameters:\n         - seeds: \"112.522.6.61, 112.522.6.62, 112.522.6.63\"\n\n\n# ONLY THIS LINE CHANGES\nlisten_address: 112.522.6.63\n\n# ONLY THIS LINE CHANGES\nrpc_address: 112.522.6.63\n\n\nendpoint_snitch: GossipingPropertyFileSnitch\n\nauto_bootstrap: true\n</code></pre>"},{"location":"data_engineering/nosql/cassandra/setting_up_cluster/#server-2-example-for-step-4","title":"Server 2 Example for Step 4","text":"<pre><code>sudo apt-get install iptables-persistent\n\n# ONLY THESE LINES CHANGE TO ALLOW COMMUNICATION WITH SERVER 1 AND 3\nsudo iptables -A INPUT -p tcp -s 112.522.6.61 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT\nsudo iptables -A INPUT -p tcp -s 112.522.6.63 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT\n\nsudo bash -c \"iptables-save &gt; /etc/iptables/rules.v4\"\n\nsudo netfilter-persistent reload\n\nsudo nodetool status\n</code></pre>"},{"location":"data_engineering/nosql/cassandra/setting_up_cluster/#server-3-example-for-step-4","title":"Server 3 Example for Step 4","text":"<pre><code>sudo apt-get install iptables-persistent\n\n# ONLY THESE LINES CHANGE TO ALLOW COMMUNICATION WITH SERVER 1 AND 2\nsudo iptables -A INPUT -p tcp -s 112.522.6.61 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT\nsudo iptables -A INPUT -p tcp -s 112.522.6.62 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT\n\nsudo bash -c \"iptables-save &gt; /etc/iptables/rules.v4\"\n\nsudo netfilter-persistent reload\n\nsudo nodetool status\n</code></pre>"},{"location":"data_engineering/nosql/cassandra/setting_up_cluster/#dead-node-fix","title":"Dead Node Fix","text":"<p>When your server/desktop restarts, you may face a dead node. You need replace the address with the same IP for it to work.</p> <pre><code>cd /etc/cassandra\nsubtl cassandra-env.sh\n</code></pre> <p>Add the following line in the last row assuming server 1 is dead where the IP is 112.522.6.61 <pre><code>JVM_OPTS=\"$JVM_OPTS -Dcassandra.replace_address=112.522.6.61\"\n</code></pre></p> <p>Then run the following bash commands</p> <pre><code>sudo rm -rf /var/lib/cassandra/data/system/*\nsudo service cassandra restart\nsudo nodetool status\n</code></pre> <p>You might have to wait awhile and re-run <code>sudo nodetool status</code> for the DB to get up and running.</p>"},{"location":"data_engineering/nosql/cassandra/setting_up_cluster/#summary","title":"Summary","text":"<p>We have successfully set up a 3-node Cassandra cluster DB after all these steps. We will move on to interacting with the cluster with CQLSH and the Python Driver in subsequent guides.</p>"},{"location":"data_engineering/nosql/cassandra/setting_up_cluster/#citation","title":"Citation","text":"<p>If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.</p> <p> </p>"},{"location":"deep_learning/course_progression/","title":"Course Progression","text":"<p>If you would like a smooth transition in learning deep learning concepts, you need to follow the materials in a sequential order. Some sections are still pending as I am working on them, and they will have the  icon beside them.</p>"},{"location":"deep_learning/course_progression/#1-practical-deep-learning-with-pytorch","title":"1. Practical Deep Learning with PyTorch","text":"<ul> <li> Matrices</li> <li> Gradients</li> <li> Linear Regression</li> <li> Logistic Regression</li> <li> Feedforward Neural Networks (FNN)</li> <li> Convolutional Neural Networks (CNN)</li> <li> Recurrent Neural Networks (RNN)</li> <li> Long Short Term Memory Neural Networks (LSTM)</li> <li> Autoencoders (AE)</li> <li> Fully-connected Overcomplete Autoencoder (AE)</li> <li>  Variational Autoencoders (VAE)</li> <li>  Adversarial Autoencoders (AAE)</li> <li>  Generative Adversarial Networks (GAN)</li> <li>  Transformers</li> </ul>"},{"location":"deep_learning/course_progression/#2-improving-deep-learning-models-with-pytorch","title":"2. Improving Deep Learning Models with PyTorch","text":"<ul> <li> Derivatives, Gradients and Jacobian</li> <li> Gradient Descent and Backpropagation (From Scratch FNN Regression)</li> <li> Learning Rate Scheduling</li> <li> Optimizers</li> <li> Advanced Learning Rate Optimization</li> <li> Weight Initializations and Activation Functions</li> <li>  Overfitting Prevention</li> <li>  Loss, Accuracy and Weight Visualizations</li> <li>  Data Preprocessing for Images and Videos</li> <li>  Data Preprocessing for Time Series</li> </ul>"},{"location":"deep_learning/course_progression/#3-deep-reinforcement-learning-with-pytorch","title":"3. Deep Reinforcement Learning with PyTorch","text":"<ul> <li> Supervised Learning to Reinforcement Learning</li> <li> Markov Decision Processes and Bellman Equations</li> <li> Dynamic Programming</li> <li>  Monte Carlo Approach</li> <li>  Temporal-Difference</li> <li>  Policy Gradient: REINFORCE</li> <li>  Policy Gradient: Actor-Critic</li> <li>  Policy Gradient: A2C/A3C</li> <li>  Policy Gradient: ACKTR</li> <li>  Policy Gradient: PPO</li> <li>  Policy Gradient: DPG</li> <li>  Policy Gradient: DDPG (DQN &amp; DPG)</li> </ul>"},{"location":"deep_learning/course_progression/#4-from-scratch-with-python-and-pytorch","title":"4. From Scratch with Python and PyTorch","text":"<ul> <li> From Scratch Logistic Regression Classification</li> <li>  From Scratch FNN Classification</li> <li>  From Scratch CNN Classification</li> <li>  From Scratch RNN Classification</li> <li>  From Scratch LSTM Classification</li> <li>  From Scratch AE</li> </ul>"},{"location":"deep_learning/course_progression/#5-large-language-models-with-pytorch","title":"5. Large Language Models with PyTorch","text":"<ul> <li>  Introduction</li> <li>  What is Temperature</li> </ul>"},{"location":"deep_learning/intro/","title":"Deep Learning and Deep Reinforcement Learning Theory and Programming Tutorials","text":"<p>We'll be covering both CPU and GPU implementations of deep learning and deep reinforcement learning algorithms</p> Packages and Languages you will Learn to Use <ul> <li>Python</li> <li>PyTorch</li> <li>NumPy</li> <li>Gym</li> </ul> <p>If you would like a more visual and guided experience, feel free to take our video course.</p>"},{"location":"deep_learning/intro/#paths","title":"Paths","text":"<p>Work in progress</p> <p>This open-source portion is still a work in progress, it may be sparse in explanation as traditionally all our explanation are done via video. Stay tuned while we gradually upload our tutorials and notes. Feel free to contact Ritchie Ng if you would like to contribute via our Facebook page.</p> <p>Also take note that these notes are best used as a referral. This is because we have yet to expand it comprehensively to be a stand-alone guide. Go head and take our video course that provides a much easier, proven-to-work, experience.</p> <p>All of our code allows you to run in a notebook for this deep learning section. Please use a <code>jupyter notebook</code> and run the examples from the start of the page to the end.</p> <p>Remember to <code>Right mouse click &gt; Open image in new tab</code> if you would like to zoom into the diagrams if you find them too small.</p> <p>Clarifications</p> <p>These are some clarifications we would like to highlight. </p> <ul> <li> When we state linear function, more specifically we meant affine function that comprises a linear function and a constant. We did this initially to make it easier as \"linear function\" was easier to digest.</li> <li> For all diagrams that says valid padding, they refer to no padding such that your output size will be smaller than your input size.</li> <li> For all diagrams that says same padding, they refer to zero padding (padding your input with zeroes) such that your output size will be equal to your input size.</li> </ul> <p>Errors to be corrected</p> <p>As we are rapidly prototyping there may be some errors. For these errors stated here, they will be corrected soon.</p> <p>Feel free to report bugs, corrections or improvements on our Github repository. </p> <p>If you did not go through all the materials, you would not be familiar with these, so go through them and come back to review these changes.</p> <ul> <li> For all diagrams that says dot product, they refer to matrix product. </li> </ul>"},{"location":"deep_learning/readings/","title":"Readings","text":"<p>Info</p> <p>This is a list of growing number of papers and implementations I think are interesting.</p>"},{"location":"deep_learning/readings/#long-tailed-recognition","title":"Long Tailed Recognition","text":"<ul> <li>Large-Scale Long-Tailed Recognition in an Open World<ul> <li>Frequently in real world scenario there're new unseen classes or samples within the tail classes</li> <li>This tackles the problem with dynamic embedding to bring associative memory to aid prediction of long-tailed classes</li> <li>The model essentially combines direct image features with embeddings from other classes</li> </ul> </li> </ul>"},{"location":"deep_learning/readings/#better-generalization-overfitting-prevention-or-regularization","title":"Better Generalization (Overfitting Prevention or Regularization)","text":"<ul> <li>Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World<ul> <li>They propose to use domain randomization to train deep learning algorithms on synthetic data and transferring to real-world data</li> <li>The idea is that with sufficient variability in the textures of synthetic data, real-world data becomes another variation of the synthetic data</li> <li>It works surprisingly well and it's a simple technique of varying image textures essentially enabling CNNs to be more robust to variations in image textures</li> </ul> </li> </ul>"},{"location":"deep_learning/readings/#optimization","title":"Optimization","text":"<ul> <li>Online Learning Rate Adaptation with Hypergradient Descent<ul> <li>Reduces the need for learning rate scheduling for SGD, SGD and nesterov momentum, and Adam</li> <li>Uses the concept of hypergradients (gradients w.r.t. learning rate) obtained via reverse-mode automatic differentiation to dynamically update learning rates in real-time alongside weight updates</li> <li>Little additional computation because just needs just one additional copy of original gradients store in memory</li> <li>Severely under-appreciated paper</li> </ul> </li> </ul>"},{"location":"deep_learning/readings/#network-compression","title":"Network Compression","text":"<ul> <li>Energy-constrained Compression for Deep Neural Networks via Weighted Sparse Projection and Layer Input Masking<ul> <li>More production applications of DNN require low-energy consumption environment like self-driving cars, VR goggles, and drones</li> <li>As such it's critical to optimize DNN not for its primary performance (accuracy etc.) but for its energy consumption performance too </li> <li>In the DNN training, this paper introduces an energy budget constraint on top of other optimization objectives</li> <li>This allows optimization of multiple objectives simultaneously (top-1 accuracy and energy consumption for example)</li> <li>It's done through weighted sparse projection and layer input masking</li> </ul> </li> </ul>"},{"location":"deep_learning/readings/#architecture-search","title":"Architecture Search","text":"<ul> <li>DARTS: Differentiable Architecture Search<ul> <li>Neural search algorithm based on gradient descent and continuous relaxation in the architecture space. </li> <li>A good move towards automatic architecture designs of neural networks.</li> </ul> </li> <li>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks<ul> <li>Scales all dimensions of a CNN, resolution/depth/width using compound coefficient</li> <li>Uses neural architecture search</li> </ul> </li> </ul>"},{"location":"deep_learning/readings/#network-pruning","title":"Network Pruning","text":"<ul> <li>EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis<ul> <li>Compared to existing Hessian-based methods, this works on the KFE</li> <li>Reported 10x reduction in model size and 8x reduction in FLOPs on Wide ResNet32 (WRN32)</li> </ul> </li> </ul>"},{"location":"deep_learning/readings/#bayesian-deep-learning","title":"Bayesian Deep Learning","text":"<ul> <li>Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam<ul> <li>Variational Adam (Vadam), an alternative to varianal inference via dropout.</li> <li>Vadam perturbs the network's weights when backpropagating, allowing low computation cost uncertainty estimates. </li> <li>Not as good as dropout in terms of performance, but a good direction for computationally cheaper options.</li> </ul> </li> </ul>"},{"location":"deep_learning/readings/#explainability","title":"Explainability","text":"<ul> <li>A Unified Approach to Intepreting Model Predictions<ul> <li>Introduces SHAP (SHapley Additive exPlanations)</li> <li>\"SHAP assigns each feature an importance value for a particular prediction\"<ul> <li>Higher positive SHAP values (red) = increase the probability of the class</li> <li>Higher negative SHAP values (blue) = decrease the probability of the class</li> </ul> </li> </ul> </li> <li>Hierarchical interpretations for neural network predictions<ul> <li>Given a prediction from the deep neural network, agglomerative contextual decomposition (ACD) produces a hierarchical clusters of input features alongside cluster-wise contribution to the final prediction.</li> <li>The hierarchical clustering is then optimized to identify learned clusters driving the DNN's predictions.</li> </ul> </li> </ul>"},{"location":"deep_learning/readings/#cautious","title":"Cautious","text":"<ul> <li>Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing<ul> <li>Shows through scatterplots that multiple toy datasets although visually very different can have similar summary statistics like mean, standard deviation and pearson correlation</li> <li>This paper emphasises the need to always visualize your data</li> </ul> </li> </ul>"},{"location":"deep_learning/readings/#visualization","title":"Visualization","text":"<ul> <li>Netron<ul> <li>Easily visualize your saved deep learning models (PyTorch .pth, TensorFlow .pb, MXNet .model, ONNX, and more)</li> <li>You can even check out each node's documentation quickly in the interface</li> </ul> </li> </ul>"},{"location":"deep_learning/readings/#missing-values","title":"Missing Values","text":"<ul> <li>BRITS<ul> <li>If you face problems in missing data in your time series and you use existing imputation methods, there is an alternative called BRITS where it learns missing values in time series via a bidirectional recurrency dynamical system</li> </ul> </li> </ul>"},{"location":"deep_learning/readings/#correlation","title":"Correlation","text":"<ul> <li>DCCA: Deep Canonical Correlation Analysis<ul> <li>Learn non-linear complex transformations such that resulting transformed data have high linear correlation</li> <li>Alternative to non-parametric methods like kernel canonical correlation analysis (KCCA) and non-linear extension of canonical correlation analysis (CCA)</li> <li>Shown to learn higher correlation representations than CCA/KCCA</li> </ul> </li> </ul>"},{"location":"deep_learning/readings/#deep-reinforcement-learning","title":"Deep Reinforcement Learning","text":"<ul> <li>An Empirical Analysis of Proximal Policy Optimization with Kronecker-factored Natural Gradients<ul> <li>Shows 2 SOTA for deep RL currently (2018 / early 2019): PPO and ACKTR</li> <li>Attempts to combined PPO objective with K-FAC natural gradient optimization: PPOKFAC</li> <li>Does not improve sample complexity, stick with either PPO/ACKTR for now</li> </ul> </li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/","title":"Derivative, Gradient and Jacobian","text":"<p>Run Jupyter Notebook</p> <p>You can run the code for this section in this jupyter notebook link.</p>"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#simplified-equation","title":"Simplified Equation","text":"<ul> <li>This is the simplified equation we have been using on how we update our parameters to reach good values (good local or global minima)</li> <li>\\(\\theta = \\theta - \\eta \\cdot \\nabla_\\theta\\)<ul> <li>\\(\\theta\\): parameters (our tensors with gradient accumulation abilities)</li> <li>\\(\\eta\\): learning rate (how fast we want to learn)</li> <li>\\(\\nabla_\\theta\\): gradients of loss with respect to the model's parameters</li> </ul> </li> <li>Even simplier equation in English: <code>parameters = parameters - learning_rate * parameters_gradients</code></li> <li>This process can be broken down into 2 sequential parts<ol> <li>Backpropagation</li> <li>Gradient descent</li> </ol> </li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#simplified-equation-breakdown","title":"Simplified Equation Breakdown","text":"<ul> <li>Our simplified equation can be broken down into 2 parts<ol> <li>Backpropagation: getting our gradients<ul> <li>Our partial derivatives of loss (scalar number) with respect to (w.r.t.) our model's parameters and w.r.t. our input</li> <li>Backpropagation gets us \\(\\nabla_\\theta\\) which is our gradient</li> </ul> </li> <li>Gradient descent: using our gradients to update our parameters<ul> <li>Somehow, the terms backpropagation and gradient descent are often mixed together. But they're totally different. </li> <li>Gradient descent relates to using our gradients obtained from backpropagation to update our weights.</li> <li>Gradient descent: \\(\\theta = \\theta - \\eta \\cdot \\nabla_\\theta\\)</li> </ul> </li> </ol> </li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#steps","title":"Steps","text":"<ul> <li>Derivatives</li> <li>Partial Derivatives</li> <li>Gradients</li> <li>Gradient, Jacobian and Generalized Jacobian Differences</li> <li>Backpropagation: computing gradients</li> <li>Gradient descent: using gradients to update parameters</li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#derivative","title":"Derivative","text":"<ul> <li>Given a simple cubic equation: \\(f(x) = 2x^3 + 5\\)</li> <li>Calculating the derivative \\(\\frac{df(x)}{dx}\\) is simply calculating the difference in values of \\(y\\) for an extremely small (infinitesimally) change in value of \\(x\\) which is frequently labelled as \\(h\\)<ul> <li>\\(\\frac{df(x)}{dx} = \\displaystyle{\\lim_{h \\to 0}} \\frac{f(x + h) - f(x)}{h}\\)<ul> <li>\\(\\frac{f(x + h) - f(x)}{h}\\) is the slope formula similar to what you may be familiar with:<ul> <li>Change in \\(y\\) over change in \\(x\\): \\(\\frac{\\Delta y}{\\Delta x}\\)</li> </ul> </li> <li>And the derivative is the slope when \\(h \\rightarrow 0\\), in essence a super teeny small \\(h\\)</li> <li></li> </ul> </li> </ul> </li> <li>Let's break down \\(\\frac{df}{dx} = \\displaystyle{\\lim_{h \\to 0}} \\frac{f(x + h) - f(x)}{h}\\)<ul> <li>\\(\\displaystyle{\\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}}\\)</li> <li>\\(\\displaystyle{\\lim_{h \\to 0} \\frac{(2(x+h)^3 + 5) - 2x^3 + 5}{h}}\\)</li> <li>\\(\\displaystyle{\\lim_{h \\to 0} \\frac{2(x^2 + 2xh + h^2)(x+h) - 2x^3}{h}}\\)</li> <li>\\(\\displaystyle{\\lim_{h \\to 0} \\frac{2(x^3 + 2x^2h + h^3 + x^2h + 2xh^2 + h^3) - 2x^3}{h}}\\)</li> <li>\\(\\displaystyle{\\lim_{h \\to 0}\\frac{2(x^3 + 3x^2h + h^3 + 2xh^2) - 2x^3}{h}}\\)</li> <li>\\(\\displaystyle{\\lim_{h \\to 0} \\frac{6x^2h + h^3 + 2xh^2}{h}}\\)</li> <li>\\(\\displaystyle{\\lim_{h \\to 0} 6x^2 + h^2 + 2xh} = 6x^2\\)</li> </ul> </li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#partial-derivative","title":"Partial Derivative","text":"<ul> <li>Ok, it's simple to calculate our derivative when we've only one variable in our function. </li> <li>If we've more than one (as with our parameters in our models), we need to calculate our partial derivatives of our function with respect to our variables</li> <li>Given a simple equation \\(f(x, z) = 4x^4z^3\\), let us get our partial derivatives</li> <li>2 parts: partial derivative of our function w.r.t. x and z<ol> <li>Partial derivative of our function w.r.t. x: \\(\\frac{\\delta f(x, z)}{\\delta x}\\)<ul> <li>Let \\(z\\) term be a constant, \\(a\\) </li> <li>\\(f(x, z) = 4x^4a\\)</li> <li>\\(\\frac{\\delta f(x, z)}{\\delta x} = 16x^3a\\)</li> <li>Now we substitute \\(a\\) with our z term, \\(a = z^3\\)</li> <li>\\(\\frac{\\delta f(x, z)}{\\delta x} = 16x^3z^3\\)</li> </ul> </li> <li>Partial derivative of our function w.r.t. z: \\(\\frac{\\delta f(x, z)}{\\delta z}\\)<ul> <li>Let \\(x\\) term be a constant, \\(a\\)</li> <li>\\(f(x, z) = 4az^3\\)</li> <li>\\(\\frac{\\delta f(x, z)}{\\delta z} = 12az^2\\)</li> <li>Now we substitute \\(a\\) with our \\(x\\) term, \\(a = x^4\\)</li> <li>\\(\\frac{\\delta f(x, z)}{\\delta z} = 12x^4z^2\\)</li> </ul> </li> </ol> </li> <li>Ta da! We made it, we calculated our partial derivatives of our function w.r.t. the different variables</li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#gradient","title":"Gradient","text":"<ul> <li>We can now put all our partial derivatives into a vector of partial derivatives<ul> <li>Also called \"gradient\"</li> <li>Represented by \\(\\nabla_{(x,z)}\\)</li> </ul> </li> <li>\\(\\nabla_{(x,z)} = \\begin{bmatrix} \\frac{df(x,z)}{dx} \\\\ \\frac{df(x,z)}{dz} \\end{bmatrix} = \\begin{bmatrix} 16x^3z^3 \\\\ 12x^4z^2 \\end{bmatrix}\\)</li> <li>It is critical to note that the term gradient applies for \\(f : \\mathbb{R}^N \\rightarrow \\mathbb{R}\\)<ul> <li>Where our function maps a vector input to a scalar output: in deep learning, our loss function that produces a scalar loss</li> </ul> </li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#gradient-jacobian-and-generalized-jacobian","title":"Gradient, Jacobian, and Generalized Jacobian","text":"<p>In the case where we have non-scalar outputs, these are the right terms of matrices or vectors containing our partial derivatives</p> <ol> <li>Gradient: vector input to scalar output<ul> <li>\\(f : \\mathbb{R}^N \\rightarrow \\mathbb{R}\\)</li> </ul> </li> <li>Jacobian: vector input to vector output<ul> <li>\\(f : \\mathbb{R}^N \\rightarrow \\mathbb{R}^M\\)</li> </ul> </li> <li>Generalized Jacobian: tensor input to tensor output<ul> <li>In this case, a tensor can be any number of dimensions.</li> </ul> </li> </ol>"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#summary","title":"Summary","text":"<p>We've learnt to...</p> <p>Success</p> <ul> <li> Calculate derivatives</li> <li> Calculate partial derivatives</li> <li> Get gradients</li> <li> Differentiate the concepts amongst gradients, Jacobian and Generalized Jacobian</li> </ul> <p>Now it is time to move on to backpropagation and gradient descent for a simple 1 hidden layer FNN with all these concepts in mind.</p>"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#citation","title":"Citation","text":"<p>If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.</p> <p> </p>"},{"location":"deep_learning/boosting_models_pytorch/forwardpropagation_backpropagation_gradientdescent/","title":"Forwardpropagation, Backpropagation and Gradient Descent with PyTorch","text":"<p>Run Jupyter Notebook</p> <p>You can run the code for this section in this jupyter notebook link.</p>"},{"location":"deep_learning/boosting_models_pytorch/forwardpropagation_backpropagation_gradientdescent/#transiting-to-backpropagation","title":"Transiting to Backpropagation","text":"<ul> <li>Let's go back to our simple FNN to put things in perspective<ul> <li>Let us ignore non-linearities for now to keep it simpler, but it's just a tiny change subsequently</li> <li>Given a linear transformation on our input (for simplicity instead of an affine transformation that includes a bias): \\(\\hat y = \\theta x\\)<ul> <li>\\(\\theta\\) is our parameters</li> <li>\\(x\\) is our input</li> <li>\\(\\hat y\\) is our prediction</li> </ul> </li> <li>Then we have our MSE loss function \\(L = \\frac{1}{2} (\\hat y - y)^2\\)</li> </ul> </li> <li>We need to calculate our partial derivatives of our loss w.r.t. our parameters to update our parameters: \\(\\nabla_{\\theta} = \\frac{\\delta L}{\\delta \\theta}\\)<ul> <li>With chain rule we have \\(\\frac{\\delta L}{\\delta \\theta} = \\frac{\\delta L}{\\delta \\hat y} \\frac{\\delta \\hat y}{\\delta \\theta}\\)<ul> <li>\\(\\frac{\\delta L}{\\delta \\hat y} = (\\hat y -  y)\\)</li> <li>\\(\\frac{\\delta \\hat y}{\\delta \\theta}\\) is our partial derivatives of \\(y\\) w.r.t. our parameters (our gradient) as we have covered previously</li> </ul> </li> </ul> </li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/forwardpropagation_backpropagation_gradientdescent/#forward-propagation-backward-propagation-and-gradient-descent","title":"Forward Propagation, Backward Propagation and Gradient Descent","text":"<ul> <li>All right, now let's put together what we have learnt on backpropagation and apply it on a simple feedforward neural network (FNN)</li> <li>Let us assume the following simple FNN architecture and take note that we do not have bias here to keep things simple<ul> <li>FNN architecture<ol> <li>Linear function: hidden size = 32</li> <li>Non-linear function: sigmoid</li> <li>Linear function: output size = 1</li> <li>Non-linear function: sigmoid</li> </ol> </li> <li>We will be going through a binary classification problem classifying 2 types of flowers<ul> <li>Output size: 1 (represented by 0 or 1 depending on the flower)</li> <li>Input size: 2 (features of the flower)</li> <li>Number of training samples: 100</li> </ul> </li> </ul> </li> </ul> <p>Load 3-class dataset</p> <p>We want to set a seed to encourage reproducibility so you can match our loss numbers.</p> <pre><code>import torch\nimport torch.nn as nn\n\n# Set manual seed\ntorch.manual_seed(2)\n</code></pre> <p>Here we want to load our flower classification dataset of 150 samples. There are 2 features, hence the input size would be 150x2. There is no one-hot encoding so the output would not be a size of 150x3 but a size of 150x1. <pre><code>from sklearn import datasets\nfrom sklearn import preprocessing\niris = datasets.load_iris()\nX = torch.tensor(preprocessing.normalize(iris.data[:, :2]), dtype=torch.float)\ny = torch.tensor(iris.target.reshape(-1, 1), dtype=torch.float)\n</code></pre></p> <pre><code>print(X.size())\nprint(y.size())\n</code></pre> <pre><code>torch.Size([150, 2])\ntorch.Size([150, 1])\n</code></pre> <p>From 3 class dataset to 2 class dataset</p> <p>We only want 2 classes because we want a binary classification problem. As mentioned, there is no one-hot encoding, so each class is represented by 0, 1, or 2. All we need to do is to filter out all samples with a label of 2 to have 2 classes. <pre><code># We only take 2 classes to make a binary classification problem\nX = X[:y[y &lt; 2].size()[0]]\ny = y[:y[y &lt; 2].size()[0]]\n````\n\n```python\nprint(X.size())\nprint(y.size())\n</code></pre></p> <pre><code>torch.Size([100, 2])\ntorch.Size([100, 1])\n</code></pre> <p>Building our FNN model class from scratch</p> <pre><code>class FNN(nn.Module):\n    def __init__(self, ):\n        super().__init__()\n\n        # Dimensions for input, hidden and output\n        self.input_dim = 2\n        self.hidden_dim = 32\n        self.output_dim = 1\n\n        # Learning rate definition\n        self.learning_rate = 0.001\n\n        # Our parameters (weights)\n        # w1: 2 x 32\n        self.w1 = torch.randn(self.input_dim, self.hidden_dim)\n\n        # w2: 32 x 1\n        self.w2 = torch.randn(self.hidden_dim, self.output_dim)\n\n    def sigmoid(self, s):\n        return 1 / (1 + torch.exp(-s))\n\n    def sigmoid_first_order_derivative(self, s):\n        return s * (1 - s)\n\n    # Forward propagation\n    def forward(self, X):\n        # First linear layer\n        self.y1 = torch.matmul(X, self.w1) # 3 X 3 \".dot\" does not broadcast in PyTorch\n\n        # First non-linearity\n        self.y2 = self.sigmoid(self.y1)\n\n        # Second linear layer\n        self.y3 = torch.matmul(self.y2, self.w2)\n\n        # Second non-linearity\n        y4 = self.sigmoid(self.y3)\n        return y4\n\n    # Backward propagation\n    def backward(self, X, l, y4):\n        # Derivative of binary cross entropy cost w.r.t. final output y4\n        self.dC_dy4 = y4 - l\n\n'''\n        Gradients for w2: partial derivative of cost w.r.t. w2\n        dC/dw2\n        '''\n        self.dy4_dy3 = self.sigmoid_first_order_derivative(y4)\n        self.dy3_dw2 = self.y2\n\n        # Y4 delta: dC_dy4 dy4_dy3\n        self.y4_delta = self.dC_dy4 * self.dy4_dy3\n\n        # This is our gradients for w1: dC_dy4 dy4_dy3 dy3_dw2\n        self.dC_dw2 = torch.matmul(torch.t(self.dy3_dw2), self.y4_delta)\n\n'''\n        Gradients for w1: partial derivative of cost w.r.t w1\n        dC/dw1\n        '''\n        self.dy3_dy2 = self.w2\n        self.dy2_dy1 = self.sigmoid_first_order_derivative(self.y2)\n\n        # Y2 delta: (dC_dy4 dy4_dy3) dy3_dy2 dy2_dy1\n        self.y2_delta = torch.matmul(self.y4_delta, torch.t(self.dy3_dy2)) * self.dy2_dy1\n\n        # Gradients for w1: (dC_dy4 dy4_dy3) dy3_dy2 dy2_dy1 dy1_dw1\n        self.dC_dw1 = torch.matmul(torch.t(X), self.y2_delta)\n\n        # Gradient descent on the weights from our 2 linear layers\n        self.w1 -= self.learning_rate * self.dC_dw1\n        self.w2 -= self.learning_rate * self.dC_dw2\n\n    def train(self, X, l):\n        # Forward propagation\n        y4 = self.forward(X)\n\n        # Backward propagation and gradient descent\n        self.backward(X, l, y4)\n</code></pre> <p>Training our FNN model</p> <pre><code># Instantiate our model class and assign it to our model object\nmodel = FNN()\n\n# Loss list for plotting of loss behaviour\nloss_lst = []\n\n# Number of times we want our FNN to look at all 100 samples we have, 100 implies looking through 100x\nnum_epochs = 101\n\n# Let's train our model with 100 epochs\nfor epoch in range(num_epochs):\n    # Get our predictions\n    y_hat = model(X)\n\n    # Cross entropy loss, remember this can never be negative by nature of the equation\n    # But it does not mean the loss can't be negative for other loss functions\n    cross_entropy_loss = -(y * torch.log(y_hat) + (1 - y) * torch.log(1 - y_hat))\n\n    # We have to take cross entropy loss over all our samples, 100 in this 2-class iris dataset\n    mean_cross_entropy_loss = torch.mean(cross_entropy_loss).detach().item()\n\n    # Print our mean cross entropy loss\n    if epoch % 20 == 0:\n        print('Epoch {} | Loss: {}'.format(epoch, mean_cross_entropy_loss))\n    loss_lst.append(mean_cross_entropy_loss)\n\n    # (1) Forward propagation: to get our predictions to pass to our cross entropy loss function\n    # (2) Back propagation: get our partial derivatives w.r.t. parameters (gradients)\n    # (3) Gradient Descent: update our weights with our gradients\n    model.train(X, y)\n</code></pre> <pre><code>Epoch 0 | Loss: 0.9228229522705078\nEpoch 20 | Loss: 0.6966760754585266\nEpoch 40 | Loss: 0.6714916229248047\nEpoch 60 | Loss: 0.6686137914657593\nEpoch 80 | Loss: 0.666690468788147\nEpoch 100 | Loss: 0.6648102402687073\n</code></pre> <p>Our loss is decreasing gradually, so it's learning. It has a possibility of reducing to almost 0 (overfitting) with sufficient model capacity (more layers or wider layers). We will explore overfitting and learning rate optimization subsequently.</p>"},{"location":"deep_learning/boosting_models_pytorch/forwardpropagation_backpropagation_gradientdescent/#summary","title":"Summary","text":"<p>We've learnt...</p> <p>Success</p> <ul> <li> The math behind forwardpropagation, backwardpropagation and gradient descent for FNN</li> <li> Implement a basic FNN from scratch with PyTorch</li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/forwardpropagation_backpropagation_gradientdescent/#citation","title":"Citation","text":"<p>If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.</p> <p> </p>"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/","title":"Learning Rate Scheduling","text":"<p>Run Jupyter Notebook</p> <p>You can run the code for this section in this jupyter notebook link.</p>"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#optimization-algorithm-mini-batch-stochastic-gradient-descent-sgd","title":"Optimization Algorithm: Mini-batch Stochastic Gradient Descent (SGD)","text":"<ul> <li>We will be using mini-batch gradient descent in all our examples here when scheduling our learning rate</li> <li>Combination of batch gradient descent &amp; stochastic gradient descent<ul> <li>\\(\\theta = \\theta - \\eta \\cdot  \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n})\\)</li> </ul> </li> <li>Characteristics<ul> <li>Compute the gradient of the lost function w.r.t. parameters for n sets of training sample (n input and n label), \\(\\nabla J(\\theta, x^{i: i+n}, y^{i:i+n})\\)</li> <li>Use this to update our parameters at every iteration</li> </ul> </li> <li>Typically in deep learning, some variation of mini-batch gradient is used where the batch size is a hyperparameter to be determined</li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#learning-intuition-recap","title":"Learning Intuition Recap","text":"<ul> <li>Learning process<ul> <li>Original parameters \\(\\rightarrow\\) given input, get output \\(\\rightarrow\\) compare with labels \\(\\rightarrow\\) get loss with comparison of input/output \\(\\rightarrow\\) get gradients of loss w.r.t parameters \\(\\rightarrow\\) update parameters so model can churn output closer to labels \\(\\rightarrow\\) repeat</li> </ul> </li> <li>For a detailed mathematical account of how this works and how to implement from scratch in Python and PyTorch, you can read our forward- and back-propagation and gradient descent post.</li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#learning-rate-pointers","title":"Learning Rate Pointers","text":"<ul> <li>Update parameters so model can churn output closer to labels, lower loss<ul> <li>\\(\\theta = \\theta - \\eta \\cdot  \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n})\\)</li> </ul> </li> <li>If we set \\(\\eta\\) to be a large value \\(\\rightarrow\\) learn too much (rapid learning)<ul> <li>Unable to converge to a good local minima (unable to effectively gradually decrease your loss, overshoot the local lowest value)</li> </ul> </li> <li>If we set \\(\\eta\\) to be a small value \\(\\rightarrow\\) learn too little (slow learning)<ul> <li>May take too long or unable to converge to a good local minima</li> </ul> </li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#need-for-learning-rate-schedules","title":"Need for Learning Rate Schedules","text":"<ul> <li>Benefits<ul> <li>Converge faster</li> <li>Higher accuracy </li> </ul> </li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#top-basic-learning-rate-schedules","title":"Top Basic Learning Rate Schedules","text":"<ol> <li>Step-wise Decay </li> <li>Reduce on Loss Plateau Decay</li> </ol>"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#step-wise-learning-rate-decay","title":"Step-wise Learning Rate Decay","text":""},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#step-wise-decay-every-epoch","title":"Step-wise Decay: Every Epoch","text":"<ul> <li>At every epoch,<ul> <li>\\(\\eta_t = \\eta_{t-1}\\gamma\\)</li> <li>\\(\\gamma = 0.1\\)</li> </ul> </li> <li>Optimization Algorithm 4: SGD Nesterov<ul> <li>Modification of SGD Momentum <ul> <li>\\(v_t = \\gamma v_{t-1} + \\eta \\cdot  \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n})\\)</li> <li>\\(\\theta = \\theta - v_t\\)</li> </ul> </li> </ul> </li> <li>Practical example<ul> <li>Given \\(\\eta_t = 0.1\\) and $ \\gamma = 0.01$</li> <li>Epoch 0: \\(\\eta_t = 0.1\\)</li> <li>Epoch 1: \\(\\eta_{t+1} = 0.1 (0.1) =  0.01\\)</li> <li>Epoch 2: \\(\\eta_{t+2} = 0.1 (0.1)^2 =  0.001\\)</li> <li>Epoch n: \\(\\eta_{t+n} = 0.1 (0.1)^n\\)</li> </ul> </li> </ul> <p>Code for step-wise learning rate decay at every epoch</p> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n# Set seed\ntorch.manual_seed(0)\n\n# Where to add a new import\nfrom torch.optim.lr_scheduler import StepLR\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass FeedforwardNeuralNetModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FeedforwardNeuralNetModel, self).__init__()\n        # Linear function\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity\n        self.relu = nn.ReLU()\n        # Linear function (readout)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n\n    def forward(self, x):\n        # Linear function\n        out = self.fc1(x)\n        # Non-linearity\n        out = self.relu(out)\n        # Linear function (readout)\n        out = self.fc2(out)\n        return out\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28*28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n\n'''\nSTEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS\n'''\n# step_size: at how many multiples of epoch you decay\n# step_size = 1, after every 1 epoch, new_lr = lr*gamma \n# step_size = 2, after every 2 epoch, new_lr = lr*gamma \n\n# gamma = decaying factor\nscheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    # Decay Learning Rate\n    scheduler.step()\n    # Print Learning Rate\n    print('Epoch:', epoch,'LR:', scheduler.get_lr())\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images\n        images = images.view(-1, 28*28).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images to a Torch Variable\n                images = images.view(-1, 28*28)\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Epoch: 0 LR: [0.1]\nIteration: 500. Loss: 0.15292978286743164. Accuracy: 96\nEpoch: 1 LR: [0.010000000000000002]\nIteration: 1000. Loss: 0.1207798570394516. Accuracy: 97\nEpoch: 2 LR: [0.0010000000000000002]\nIteration: 1500. Loss: 0.12287932634353638. Accuracy: 97\nEpoch: 3 LR: [0.00010000000000000003]\nIteration: 2000. Loss: 0.05614742264151573. Accuracy: 97\nEpoch: 4 LR: [1.0000000000000003e-05]\nIteration: 2500. Loss: 0.06775809079408646. Accuracy: 97\nIteration: 3000. Loss: 0.03737065941095352. Accuracy: 97\n</code></pre>"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#step-wise-decay-every-2-epochs","title":"Step-wise Decay: Every 2 Epochs","text":"<ul> <li>At every 2 epoch,<ul> <li>\\(\\eta_t = \\eta_{t-1}\\gamma\\)</li> <li>\\(\\gamma = 0.1\\)</li> </ul> </li> <li>Optimization Algorithm 4: SGD Nesterov<ul> <li>Modification of SGD Momentum <ul> <li>\\(v_t = \\gamma v_{t-1} + \\eta \\cdot  \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n})\\)</li> <li>\\(\\theta = \\theta - v_t\\)</li> </ul> </li> </ul> </li> <li>Practical example<ul> <li>Given \\(\\eta_t = 0.1\\) and \\(\\gamma = 0.01\\)</li> <li>Epoch 0: \\(\\eta_t = 0.1\\)</li> <li>Epoch 1: \\(\\eta_{t+1} = 0.1\\)</li> <li>Epoch 2: \\(\\eta_{t+2} = 0.1 (0.1) =  0.01\\)</li> </ul> </li> </ul> <p>Code for step-wise learning rate decay at every 2 epoch</p> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n# Set seed\ntorch.manual_seed(0)\n\n# Where to add a new import\nfrom torch.optim.lr_scheduler import StepLR\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass FeedforwardNeuralNetModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FeedforwardNeuralNetModel, self).__init__()\n        # Linear function\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity\n        self.relu = nn.ReLU()\n        # Linear function (readout)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n\n    def forward(self, x):\n        # Linear function\n        out = self.fc1(x)\n        # Non-linearity\n        out = self.relu(out)\n        # Linear function (readout)\n        out = self.fc2(out)\n        return out\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28*28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n\n'''\nSTEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS\n'''\n# step_size: at how many multiples of epoch you decay\n# step_size = 1, after every 2 epoch, new_lr = lr*gamma \n# step_size = 2, after every 2 epoch, new_lr = lr*gamma \n\n# gamma = decaying factor\nscheduler = StepLR(optimizer, step_size=2, gamma=0.1)\n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    # Decay Learning Rate\n    scheduler.step()\n    # Print Learning Rate\n    print('Epoch:', epoch,'LR:', scheduler.get_lr())\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as Variable\n        images = images.view(-1, 28*28).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images to a Torch Variable\n                images = images.view(-1, 28*28).requires_grad_()\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Epoch: 0 LR: [0.1]\nIteration: 500. Loss: 0.15292978286743164. Accuracy: 96\nEpoch: 1 LR: [0.1]\nIteration: 1000. Loss: 0.11253029108047485. Accuracy: 96\nEpoch: 2 LR: [0.010000000000000002]\nIteration: 1500. Loss: 0.14498558640480042. Accuracy: 97\nEpoch: 3 LR: [0.010000000000000002]\nIteration: 2000. Loss: 0.03691177815198898. Accuracy: 97\nEpoch: 4 LR: [0.0010000000000000002]\nIteration: 2500. Loss: 0.03511016443371773. Accuracy: 97\nIteration: 3000. Loss: 0.029424520209431648. Accuracy: 97\n</code></pre>"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#step-wise-decay-every-epoch-larger-gamma","title":"Step-wise Decay: Every Epoch, Larger Gamma","text":"<ul> <li>At every epoch,<ul> <li>\\(\\eta_t = \\eta_{t-1}\\gamma\\)</li> <li>\\(\\gamma = 0.96\\)</li> </ul> </li> <li>Optimization Algorithm 4: SGD Nesterov<ul> <li>Modification of SGD Momentum <ul> <li>\\(v_t = \\gamma v_{t-1} + \\eta \\cdot  \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n})\\)</li> <li>\\(\\theta = \\theta - v_t\\)</li> </ul> </li> </ul> </li> <li>Practical example<ul> <li>Given \\(\\eta_t = 0.1\\) and \\(\\gamma = 0.96\\)</li> <li>Epoch 1: \\(\\eta_t = 0.1\\)</li> <li>Epoch 2: \\(\\eta_{t+1} = 0.1 (0.96) =  0.096\\)</li> <li>Epoch 3: \\(\\eta_{t+2} = 0.1 (0.96)^2 =  0.092\\)</li> <li>Epoch n: \\(\\eta_{t+n} = 0.1 (0.96)^n\\)</li> </ul> </li> </ul> <p>Code for step-wise learning rate decay at every epoch with larger gamma</p> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n# Set seed\ntorch.manual_seed(0)\n\n# Where to add a new import\nfrom torch.optim.lr_scheduler import StepLR\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass FeedforwardNeuralNetModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FeedforwardNeuralNetModel, self).__init__()\n        # Linear function\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity\n        self.relu = nn.ReLU()\n        # Linear function (readout)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n\n    def forward(self, x):\n        # Linear function\n        out = self.fc1(x)\n        # Non-linearity\n        out = self.relu(out)\n        # Linear function (readout)\n        out = self.fc2(out)\n        return out\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28*28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n\n'''\nSTEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS\n'''\n# step_size: at how many multiples of epoch you decay\n# step_size = 1, after every 2 epoch, new_lr = lr*gamma \n# step_size = 2, after every 2 epoch, new_lr = lr*gamma \n\n# gamma = decaying factor\nscheduler = StepLR(optimizer, step_size=2, gamma=0.96)\n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    # Decay Learning Rate\n    scheduler.step()\n    # Print Learning Rate\n    print('Epoch:', epoch,'LR:', scheduler.get_lr())\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as Variable\n        images = images.view(-1, 28*28).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images to a Torch Variable\n                images = images.view(-1, 28*28)\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Epoch: 0 LR: [0.1]\nIteration: 500. Loss: 0.15292978286743164. Accuracy: 96\nEpoch: 1 LR: [0.1]\nIteration: 1000. Loss: 0.11253029108047485. Accuracy: 96\nEpoch: 2 LR: [0.096]\nIteration: 1500. Loss: 0.11864850670099258. Accuracy: 97\nEpoch: 3 LR: [0.096]\nIteration: 2000. Loss: 0.030942382290959358. Accuracy: 97\nEpoch: 4 LR: [0.09216]\nIteration: 2500. Loss: 0.04521659016609192. Accuracy: 97\nIteration: 3000. Loss: 0.027839098125696182. Accuracy: 97\n</code></pre>"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#pointers-on-step-wise-decay","title":"Pointers on Step-wise Decay","text":"<ul> <li>You would want to decay your LR gradually when you're training more epochs<ul> <li>Converge too fast, to a crappy loss/accuracy, if you decay rapidly</li> </ul> </li> <li>To decay slower<ul> <li>Larger \\(\\gamma\\)</li> <li>Larger interval of decay</li> </ul> </li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#reduce-on-loss-plateau-decay","title":"Reduce on Loss Plateau Decay","text":""},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#reduce-on-loss-plateau-decay-patience0-factor01","title":"Reduce on Loss Plateau Decay, Patience=0, Factor=0.1","text":"<ul> <li>Reduce learning rate whenever loss plateaus<ul> <li>Patience: number of epochs with no improvement after which learning rate will be reduced<ul> <li>Patience = 0</li> </ul> </li> <li>Factor: multiplier to decrease learning rate, \\(lr = lr*factor = \\gamma\\)<ul> <li>Factor = 0.1</li> </ul> </li> </ul> </li> <li>Optimization Algorithm: SGD Nesterov<ul> <li>Modification of SGD Momentum <ul> <li>\\(v_t = \\gamma v_{t-1} + \\eta \\cdot  \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n})\\)</li> <li>\\(\\theta = \\theta - v_t\\)</li> </ul> </li> </ul> </li> </ul> <p>Code for reduce on loss plateau learning rate decay of factor 0.1 and 0 patience</p> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n# Set seed\ntorch.manual_seed(0)\n\n# Where to add a new import\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 6000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass FeedforwardNeuralNetModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FeedforwardNeuralNetModel, self).__init__()\n        # Linear function\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity\n        self.relu = nn.ReLU()\n        # Linear function (readout)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n\n    def forward(self, x):\n        # Linear function\n        out = self.fc1(x)\n        # Non-linearity\n        out = self.relu(out)\n        # Linear function (readout)\n        out = self.fc2(out)\n        return out\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28*28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n\n'''\nSTEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS\n'''\n# lr = lr * factor \n# mode='max': look for the maximum validation accuracy to track\n# patience: number of epochs - 1 where loss plateaus before decreasing LR\n        # patience = 0, after 1 bad epoch, reduce LR\n# factor = decaying factor\nscheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=0, verbose=True)\n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as Variable\n        images = images.view(-1, 28*28).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images to a Torch Variable\n                images = images.view(-1, 28*28)\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                # Without .item(), it is a uint8 tensor which will not work when you pass this number to the scheduler\n                correct += (predicted == labels).sum().item()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            # print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy))\n\n    # Decay Learning Rate, pass validation accuracy for tracking at every epoch\n    print('Epoch {} completed'.format(epoch))\n    print('Loss: {}. Accuracy: {}'.format(loss.item(), accuracy))\n    print('-'*20)\n    scheduler.step(accuracy)\n</code></pre> <pre><code>Epoch 0 completed\nLoss: 0.17087846994400024. Accuracy: 96.26\n--------------------\nEpoch 1 completed\nLoss: 0.11688263714313507. Accuracy: 96.96\n--------------------\nEpoch 2 completed\nLoss: 0.035437121987342834. Accuracy: 96.78\n--------------------\nEpoch     2: reducing learning rate of group 0 to 1.0000e-02.\nEpoch 3 completed\nLoss: 0.0324370414018631. Accuracy: 97.7\n--------------------\nEpoch 4 completed\nLoss: 0.022194599732756615. Accuracy: 98.02\n--------------------\nEpoch 5 completed\nLoss: 0.007145566865801811. Accuracy: 98.03\n--------------------\nEpoch 6 completed\nLoss: 0.01673538237810135. Accuracy: 98.05\n--------------------\nEpoch 7 completed\nLoss: 0.025424446910619736. Accuracy: 98.01\n--------------------\nEpoch     7: reducing learning rate of group 0 to 1.0000e-03.\nEpoch 8 completed\nLoss: 0.014696130529046059. Accuracy: 98.05\n--------------------\nEpoch     8: reducing learning rate of group 0 to 1.0000e-04.\nEpoch 9 completed\nLoss: 0.00573748117312789. Accuracy: 98.04\n--------------------\nEpoch     9: reducing learning rate of group 0 to 1.0000e-05.\n</code></pre>"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#reduce-on-loss-plateau-decay-patience0-factor05","title":"Reduce on Loss Plateau Decay, Patience=0, Factor=0.5","text":"<ul> <li>Reduce learning rate whenever loss plateaus<ul> <li>Patience: number of epochs with no improvement after which learning rate will be reduced<ul> <li>Patience = 0</li> </ul> </li> <li>Factor: multiplier to decrease learning rate, \\(lr = lr*factor = \\gamma\\)<ul> <li>Factor = 0.5</li> </ul> </li> </ul> </li> <li>Optimization Algorithm 4: SGD Nesterov<ul> <li>Modification of SGD Momentum <ul> <li>\\(v_t = \\gamma v_{t-1} + \\eta \\cdot  \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n})\\)</li> <li>\\(\\theta = \\theta - v_t\\)</li> </ul> </li> </ul> </li> </ul> <p>Code for reduce on loss plateau learning rate decay with factor 0.5 and 0 patience</p> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n# Set seed\ntorch.manual_seed(0)\n\n# Where to add a new import\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 6000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass FeedforwardNeuralNetModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FeedforwardNeuralNetModel, self).__init__()\n        # Linear function\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity\n        self.relu = nn.ReLU()\n        # Linear function (readout)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n\n    def forward(self, x):\n        # Linear function\n        out = self.fc1(x)\n        # Non-linearity\n        out = self.relu(out)\n        # Linear function (readout)\n        out = self.fc2(out)\n        return out\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28*28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n\n'''\nSTEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS\n'''\n# lr = lr * factor \n# mode='max': look for the maximum validation accuracy to track\n# patience: number of epochs - 1 where loss plateaus before decreasing LR\n        # patience = 0, after 1 bad epoch, reduce LR\n# factor = decaying factor\nscheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=0, verbose=True)\n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as Variable\n        images = images.view(-1, 28*28).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images to a Torch Variable\n                images = images.view(-1, 28*28)\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                 # Without .item(), it is a uint8 tensor which will not work when you pass this number to the scheduler\n                correct += (predicted == labels).sum().item()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            # print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy))\n\n    # Decay Learning Rate, pass validation accuracy for tracking at every epoch\n    print('Epoch {} completed'.format(epoch))\n    print('Loss: {}. Accuracy: {}'.format(loss.item(), accuracy))\n    print('-'*20)\n    scheduler.step(accuracy)\n</code></pre> <pre><code>Epoch 0 completed\nLoss: 0.17087846994400024. Accuracy: 96.26\n--------------------\nEpoch 1 completed\nLoss: 0.11688263714313507. Accuracy: 96.96\n--------------------\nEpoch 2 completed\nLoss: 0.035437121987342834. Accuracy: 96.78\n--------------------\nEpoch     2: reducing learning rate of group 0 to 5.0000e-02.\nEpoch 3 completed\nLoss: 0.04893001914024353. Accuracy: 97.62\n--------------------\nEpoch 4 completed\nLoss: 0.020584167912602425. Accuracy: 97.86\n--------------------\nEpoch 5 completed\nLoss: 0.006022400688380003. Accuracy: 97.95\n--------------------\nEpoch 6 completed\nLoss: 0.028374142944812775. Accuracy: 97.87\n--------------------\nEpoch     6: reducing learning rate of group 0 to 2.5000e-02.\nEpoch 7 completed\nLoss: 0.013204765506088734. Accuracy: 98.0\n--------------------\nEpoch 8 completed\nLoss: 0.010137186385691166. Accuracy: 97.95\n--------------------\nEpoch     8: reducing learning rate of group 0 to 1.2500e-02.\nEpoch 9 completed\nLoss: 0.0035198689438402653. Accuracy: 98.01\n--------------------\n</code></pre>"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#pointers-on-reduce-on-loss-pleateau-decay","title":"Pointers on Reduce on Loss Pleateau Decay","text":"<ul> <li>In these examples, we used patience=1 because we are running few epochs<ul> <li>You should look at a larger patience such as 5 if for example you ran 500 epochs. </li> </ul> </li> <li>You should experiment with 2 properties <ul> <li>Patience</li> <li>Decay factor </li> </ul> </li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#summary","title":"Summary","text":"<p>We've learnt...</p> <p>Success</p> <ul> <li> Learning Rate Intuition<ul> <li> Update parameters so model can churn output closer to labels</li> <li> Gradual parameter updates</li> </ul> </li> <li> Learning Rate Pointers<ul> <li> If we set \\(\\eta\\) to be a large value \\(\\rightarrow\\) learn too much (rapid learning)</li> <li> If we set \\(\\eta\\) to be a small value \\(\\rightarrow\\) learn too little (slow learning)</li> </ul> </li> <li> Learning Rate Schedules<ul> <li> Step-wise Decay</li> <li> Reduce on Loss Plateau Decay</li> </ul> </li> <li> Step-wise Decay<ul> <li> Every 1 epoch</li> <li> Every 2 epoch</li> <li> Every 1 epoch, larger gamma</li> </ul> </li> <li> Step-wise Decay Pointers<ul> <li> Decay LR gradually<ul> <li> Larger \\(\\gamma\\)</li> <li> Larger interval of decay (increase epoch)</li> </ul> </li> </ul> </li> <li> Reduce on Loss Plateau Decay<ul> <li> Patience=0, Factor=1</li> <li> Patience=0, Factor=0.5</li> </ul> </li> <li> Pointers on Reduce on Loss Plateau Decay<ul> <li> Larger patience with more epochs</li> <li> 2 hyperparameters to experiment<ul> <li> Patience</li> <li> Decay factor</li> </ul> </li> </ul> </li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#citation","title":"Citation","text":"<p>If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.</p> <p> </p>"},{"location":"deep_learning/boosting_models_pytorch/optimizers/","title":"Optimization Algorithms","text":"<p>Run Jupyter Notebook</p> <p>You can run the code for this section in this jupyter notebook link.</p>"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#introduction-to-gradient-descent-optimizers","title":"Introduction to Gradient-descent Optimizers","text":""},{"location":"deep_learning/boosting_models_pytorch/optimizers/#model-recap-1-hidden-layer-feedforward-neural-network-relu-activation","title":"Model Recap: 1 Hidden Layer Feedforward Neural Network (ReLU Activation)","text":""},{"location":"deep_learning/boosting_models_pytorch/optimizers/#steps","title":"Steps","text":"<ul> <li>Step 1: Load Dataset</li> <li>Step 2: Make Dataset Iterable</li> <li>Step 3: Create Model Class</li> <li>Step 4: Instantiate Model Class</li> <li>Step 5: Instantiate Loss Class</li> <li>Step 6: Instantiate Optimizer Class</li> <li>Step 7: Train Model</li> </ul> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n# Set seed\ntorch.manual_seed(0)\n\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass FeedforwardNeuralNetModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FeedforwardNeuralNetModel, self).__init__()\n        # Linear function\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity\n        self.relu = nn.ReLU()\n        # Linear function (readout)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n\n    def forward(self, x):\n        # Linear function\n        out = self.fc1(x)\n        # Non-linearity\n        out = self.relu(out)\n        # Linear function (readout)\n        out = self.fc2(out)\n        return out\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28*28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as Variable\n        images = images.view(-1, 28*28).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images to a Torch Variable\n                images = images.view(-1, 28*28)\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Iteration: 500. Loss: 0.3440718352794647. Accuracy: 91\nIteration: 1000. Loss: 0.2057694047689438. Accuracy: 93\nIteration: 1500. Loss: 0.2646750807762146. Accuracy: 94\nIteration: 2000. Loss: 0.17563636600971222. Accuracy: 94\nIteration: 2500. Loss: 0.1361844837665558. Accuracy: 95\nIteration: 3000. Loss: 0.11089023947715759. Accuracy: 95\n</code></pre>"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#non-technical-process","title":"Non-Technical Process","text":"<ol> <li>Convert inputs/labels to variables</li> <li>Clear gradient buffers</li> <li>Get output given inputs </li> <li>Get loss by comparing with labels</li> <li>Get gradients w.r.t. parameters (backpropagation)</li> <li>Update parameters using gradients (gradient descent)<ul> <li><code>parameters = parameters - learning_rate * parameters_gradients</code></li> </ul> </li> <li>REPEAT</li> </ol>"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#why-is-it-called-gradient-descent","title":"Why is it called Gradient Descent?","text":"<ul> <li>Use gradients (calculated through backpropagation) \\(\\rightarrow\\) update parameters to minimize our loss (descent) \\(\\rightarrow\\) better predictive accuracy</li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#mathematical-interpretation-of-gradient-descent","title":"Mathematical Interpretation of Gradient Descent","text":"<ul> <li>Model's parameters: \\(\\theta \\in \u211d^d\\)</li> <li>Loss function: \\(J(\\theta)\\)</li> <li>Gradient w.r.t. parameters: \\(\\nabla J(\\theta)\\)</li> <li>Learning rate: \\(\\eta\\)</li> <li>Batch Gradient descent: \\(\\theta = \\theta - \\eta \\cdot  \\nabla J(\\theta)\\)</li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-1-batch-gradient-descent","title":"Optimization Algorithm 1: Batch Gradient Descent","text":"<ul> <li>What we've covered so far: batch gradient descent<ul> <li>\\(\\theta = \\theta - \\eta \\cdot  \\nabla J(\\theta)\\)</li> </ul> </li> <li>Characteristics<ul> <li>Compute the gradient of the lost function w.r.t. parameters for the entire training data, \\(\\nabla J(\\theta)\\) </li> <li>Use this to update our parameters at every iteration</li> </ul> </li> <li>Problems<ul> <li>Unable to fit whole datasets in memory </li> <li>Computationally slow as we attempt to compute a large gradient matrix \\(\\rightarrow\\) first order derivative, \\(\\nabla J(\\theta)\\)</li> </ul> </li> <li>Conceptually easy to understand \\(\\rightarrow\\) rarely used</li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-2-stochastic-gradient-descent","title":"Optimization Algorithm 2: Stochastic Gradient Descent","text":"<ul> <li>Modification of batch gradient descent<ul> <li>\\(\\theta = \\theta - \\eta \\cdot  \\nabla J(\\theta, x^{i}, y^{i})\\)</li> </ul> </li> <li>Characteristics<ul> <li>Compute the gradient of the lost function w.r.t. parameters for the one set of training sample (1 input and 1 label), \\(\\nabla J(\\theta, x^{i}, y^{i})\\)</li> <li>Use this to update our parameters at every iteration</li> </ul> </li> <li>Benefits<ul> <li>Able to fit large datasets</li> <li>Computationally faster \\(\\rightarrow\\) instead gradients w.r.t to the whole training data, we get the gradients w.r.t. training sample</li> </ul> </li> <li>Problems<ul> <li>Updating very frequently \\(\\rightarrow\\) huge variance in parameter updates \\(\\rightarrow\\) may overshoot local minima <ul> <li>Can be solved by carefully decaying your learning rate \\(\\rightarrow\\) take smaller steps in incorporating gradients to improve the parameters</li> </ul> </li> </ul> </li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-3-mini-batch-gradient-descent","title":"Optimization Algorithm 3: Mini-batch Gradient Descent","text":"<ul> <li>Combination of batch gradient descent &amp; stochastic gradient descent<ul> <li>\\(\\theta = \\theta - \\eta \\cdot  \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n})\\)</li> </ul> </li> <li>Characteristics<ul> <li>Compute the gradient of the lost function w.r.t. parameters for n sets of training sample (n input and n label), \\(\\nabla J(\\theta, x^{i: i+n}, y^{i:i+n})\\)</li> <li>Use this to update our parameters at every iteration</li> </ul> </li> <li>Benefits<ul> <li>Able to fit large datasets</li> <li>Computationally faster \\(\\rightarrow\\) instead gradients w.r.t to the whole training data, we get the gradients w.r.t. training sample</li> <li>Lower variance of parameter updates</li> </ul> </li> <li>This is often called SGD in deep learning frameworks .__. </li> </ul> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n# Set seed\ntorch.manual_seed(0)\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass FeedforwardNeuralNetModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FeedforwardNeuralNetModel, self).__init__()\n        # Linear function\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        # Non-linearity\n        self.relu = nn.ReLU()\n        # Linear function (readout)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n\n    def forward(self, x):\n        # Linear function\n        out = self.fc1(x)\n        # Non-linearity\n        out = self.relu(out)\n        # Linear function (readout)\n        out = self.fc2(out)\n        return out\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28*28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as Variable\n        images = images.view(-1, 28*28).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images to a Torch Variable\n                images = images.view(-1, 28*28).requires_grad_()\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Iteration: 500. Loss: 0.3440718352794647. Accuracy: 91\nIteration: 1000. Loss: 0.2057694047689438. Accuracy: 93\nIteration: 1500. Loss: 0.2646750807762146. Accuracy: 94\nIteration: 2000. Loss: 0.17563636600971222. Accuracy: 94\nIteration: 2500. Loss: 0.1361844837665558. Accuracy: 95\nIteration: 3000. Loss: 0.11089023947715759. Accuracy: 95\n</code></pre>"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-4-sgd-momentum","title":"Optimization Algorithm 4: SGD Momentum","text":"<ul> <li>Modification of SGD<ul> <li>\\(v_t = \\gamma v_{t-1} + \\eta \\cdot  \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n})\\)</li> <li>\\(\\theta = \\theta - v_t\\)</li> </ul> </li> <li>Characteristics<ul> <li>Compute the gradient of the lost function w.r.t. parameters for n sets of training sample (n input and n label), \\(\\nabla J(\\theta, x^{i: i+n}, y^{i:i+n})\\)</li> <li>Use this to add to the previous update vector \\(v_{t-1}\\)</li> <li>Momentum, usually set to \\(\\gamma = 0.9\\)</li> <li>Parameters updated with update vector, \\(v_t\\) that incorporates previous update vector<ul> <li>\\(\\gamma v_{t}\\) increases if gradient same sign/direction as \\(v_{t-1}\\) <ul> <li>Gives SGD the push when it is going in the right direction (minimizing loss)</li> <li>Accelerated convergence</li> </ul> </li> <li>\\(\\gamma v_{t}\\) decreases if gradient different sign/direction as \\(v_{t-1}\\)<ul> <li>Dampens SGD when it is going in a different direction</li> <li>Lower variation in loss minimization</li> </ul> </li> </ul> </li> </ul> </li> <li>Problems<ul> <li>It might go the wrong direction (higher loss) \\(\\rightarrow\\) continue to be accelerated to the wrong direction (higher loss) </li> </ul> </li> </ul> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n# Set seed\ntorch.manual_seed(0)\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass FeedforwardNeuralNetModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FeedforwardNeuralNetModel, self).__init__()\n        # Linear function\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity\n        self.relu = nn.ReLU()\n        # Linear function (readout)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n\n    def forward(self, x):\n        # Linear function\n        out = self.fc1(x)\n        # Non-linearity\n        out = self.relu(out)\n        # Linear function (readout)\n        out = self.fc2(out)\n        return out\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28*28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as Variable\n        images = images.view(-1, 28*28).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images to a Torch Variable\n                images = images.view(-1, 28*28)\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Iteration: 500. Loss: 0.16120098531246185. Accuracy: 96\nIteration: 1000. Loss: 0.15727552771568298. Accuracy: 96\nIteration: 1500. Loss: 0.1303034871816635. Accuracy: 96\nIteration: 2000. Loss: 0.022178759798407555. Accuracy: 97\nIteration: 2500. Loss: 0.07027597725391388. Accuracy: 97\nIteration: 3000. Loss: 0.02519878000020981. Accuracy: 97\n</code></pre>"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-4-sgd-nesterov","title":"Optimization Algorithm 4: SGD Nesterov","text":"<ul> <li>Modification of SGD Momentum <ul> <li>\\(v_t = \\gamma v_{t-1} + \\eta \\cdot  \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n})\\)</li> <li>\\(\\theta = \\theta - v_t\\)</li> </ul> </li> <li>Characteristics<ul> <li>Compute the gradient of the lost function w.r.t. future approximate parameters for n sets of training sample (n input and n label), \\(\\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n})\\)<ul> <li>Use this to add to the previous update vector \\(v_{t-1}\\)</li> <li>Momentum, usually set to \\(\\gamma = 0.9\\)</li> </ul> </li> <li>Gradients w.r.t. future approximate parameters \\(\\rightarrow\\) sense of where we will be \\(\\rightarrow\\) anticipate if we are going in the wrong direction in the next step \\(\\rightarrow\\) slow down accordingly</li> </ul> </li> </ul> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n# Set seed\ntorch.manual_seed(0)\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass FeedforwardNeuralNetModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FeedforwardNeuralNetModel, self).__init__()\n        # Linear function\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity\n        self.relu = nn.ReLU()\n        # Linear function (readout)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n\n    def forward(self, x):\n        # Linear function\n        out = self.fc1(x)\n        # Non-linearity\n        out = self.relu(out)\n        # Linear function (readout)\n        out = self.fc2(out)\n        return out\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28*28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as Variable\n        images = images.view(-1, 28*28).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images to a Torch Variable\n                images = images.view(-1, 28*28)\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Iteration: 500. Loss: 0.15292978286743164. Accuracy: 96\nIteration: 1000. Loss: 0.11253029108047485. Accuracy: 96\nIteration: 1500. Loss: 0.11986596137285233. Accuracy: 96\nIteration: 2000. Loss: 0.016192540526390076. Accuracy: 97\nIteration: 2500. Loss: 0.06744947284460068. Accuracy: 97\nIteration: 3000. Loss: 0.03692319989204407. Accuracy: 97\n</code></pre>"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-4-adam","title":"Optimization Algorithm 4: Adam","text":"<ul> <li>Adaptive Learning Rates<ul> <li>\\(m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t\\)<ul> <li>Keeping track of decaying gradient</li> <li>Estimate of the mean of gradients</li> </ul> </li> <li>\\(v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)g_t^2\\)<ul> <li>Keeping track of decaying squared gradient </li> <li>Estimate of the variance of gradients</li> </ul> </li> <li>When \\(m_t, v_t\\) initializes as 0, \\(m_t, v_t \\rightarrow 0\\) initially when decay rates small, \\(\\beta_1, \\beta_2 \\rightarrow 1\\) <ul> <li>Need to correct this with:</li> <li>\\(\\hat m_t = \\frac{m_t}{1- \\beta_1}\\)</li> <li>\\(\\hat v_t = \\frac{v_t}{1- \\beta_2}\\)</li> </ul> </li> <li>\\(\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat v_t} + \\epsilon}\\hat m_t\\)</li> <li>Default recommended values<ul> <li>\\(\\beta_1 = 0.9\\)</li> <li>\\(\\beta_2 = 0.999\\)</li> <li>\\(\\epsilon = 10^{-8}\\)</li> </ul> </li> </ul> </li> <li>Instead of learning rate \\(\\rightarrow\\) equations account for estimates of mean/variance of gradients to determine the next learning rate</li> </ul> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n# Set seed\ntorch.manual_seed(0)\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass FeedforwardNeuralNetModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FeedforwardNeuralNetModel, self).__init__()\n        # Linear function\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity\n        self.relu = nn.ReLU()\n        # Linear function (readout)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n\n    def forward(self, x):\n        # Linear function\n        out = self.fc1(x)\n        # Non-linearity\n        out = self.relu(out)\n        # Linear function (readout)\n        out = self.fc2(out)\n        return out\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28*28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\n# learning_rate = 0.001\n\noptimizer = torch.optim.Adam(model.parameters())\n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as Variable\n        images = images.view(-1, 28*28).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images to a Torch Variable\n                images = images.view(-1, 28*28)\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Iteration: 500. Loss: 0.2703690826892853. Accuracy: 93\nIteration: 1000. Loss: 0.15547044575214386. Accuracy: 95\nIteration: 1500. Loss: 0.17266806960105896. Accuracy: 95\nIteration: 2000. Loss: 0.0865858644247055. Accuracy: 96\nIteration: 2500. Loss: 0.07156120240688324. Accuracy: 96\nIteration: 3000. Loss: 0.04664849117398262. Accuracy: 97\n</code></pre>"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#other-adaptive-algorithms","title":"Other Adaptive Algorithms","text":"<ul> <li>Other adaptive algorithms (like Adam, adapting learning rates)<ul> <li>Adagrad</li> <li>Adadelta</li> <li>Adamax</li> <li>RMSProp</li> </ul> </li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-5-adagrad","title":"Optimization Algorithm 5: Adagrad","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n# Set seed\ntorch.manual_seed(0)\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass FeedforwardNeuralNetModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FeedforwardNeuralNetModel, self).__init__()\n        # Linear function\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity\n        self.relu = nn.ReLU()\n        # Linear function (readout)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n\n    def forward(self, x):\n        # Linear function\n        out = self.fc1(x)\n        # Non-linearity\n        out = self.relu(out)\n        # Linear function (readout)\n        out = self.fc2(out)\n        return out\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28*28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\n# learning_rate = 0.001\n\noptimizer = torch.optim.Adagrad(model.parameters())\n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as Variable\n        images = images.view(-1, 28*28).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images to a Torch Variable\n                images = images.view(-1, 28*28)\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Iteration: 500. Loss: 0.2757369875907898. Accuracy: 92\nIteration: 1000. Loss: 0.1992958039045334. Accuracy: 93\nIteration: 1500. Loss: 0.2227272093296051. Accuracy: 94\nIteration: 2000. Loss: 0.18628711998462677. Accuracy: 94\nIteration: 2500. Loss: 0.1470586657524109. Accuracy: 95\nIteration: 3000. Loss: 0.11748368293046951. Accuracy: 95\n</code></pre>"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-6-adadelta","title":"Optimization Algorithm 6: Adadelta","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n# Set seed\ntorch.manual_seed(0)\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass FeedforwardNeuralNetModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FeedforwardNeuralNetModel, self).__init__()\n        # Linear function\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity\n        self.relu = nn.ReLU()\n        # Linear function (readout)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n\n    def forward(self, x):\n        # Linear function\n        out = self.fc1(x)\n        # Non-linearity\n        out = self.relu(out)\n        # Linear function (readout)\n        out = self.fc2(out)\n        return out\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28*28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\n# learning_rate = 0.001\n\noptimizer = torch.optim.Adadelta(model.parameters())\n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as Variable\n        images = images.view(-1, 28*28).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images to a Torch Variable\n                images = Variable(images.view(-1, 28*28))\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy))\n</code></pre> <pre><code>Iteration: 500. Loss: 0.26303035020828247. Accuracy: 93.95\nIteration: 1000. Loss: 0.08731874823570251. Accuracy: 95.83\nIteration: 1500. Loss: 0.11502093076705933. Accuracy: 96.87\nIteration: 2000. Loss: 0.03550947830080986. Accuracy: 97.12\nIteration: 2500. Loss: 0.042649827897548676. Accuracy: 97.54\nIteration: 3000. Loss: 0.03061559610068798. Accuracy: 97.45\n</code></pre>"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-6-adamax","title":"Optimization Algorithm 6: Adamax","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n# Set seed\ntorch.manual_seed(0)\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass FeedforwardNeuralNetModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FeedforwardNeuralNetModel, self).__init__()\n        # Linear function\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity\n        self.relu = nn.ReLU()\n        # Linear function (readout)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n\n    def forward(self, x):\n        # Linear function\n        out = self.fc1(x)\n        # Non-linearity\n        out = self.relu(out)\n        # Linear function (readout)\n        out = self.fc2(out)\n        return out\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28*28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\n# learning_rate = 0.001\n\noptimizer = torch.optim.Adamax(model.parameters())\n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as Variable\n        images = images.view(-1, 28*28).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images to a Torch Variable\n                images = images.view(-1, 28*28)\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Iteration: 500. Loss: 0.29930350184440613. Accuracy: 92\nIteration: 1000. Loss: 0.18749120831489563. Accuracy: 93\nIteration: 1500. Loss: 0.21887679398059845. Accuracy: 95\nIteration: 2000. Loss: 0.14390651881694794. Accuracy: 95\nIteration: 2500. Loss: 0.10771607607603073. Accuracy: 96\nIteration: 3000. Loss: 0.0839928686618805. Accuracy: 96\n</code></pre>"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-7-rmsprop","title":"Optimization Algorithm 7: RMSProp","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n# Set seed\ntorch.manual_seed(0)\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass FeedforwardNeuralNetModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FeedforwardNeuralNetModel, self).__init__()\n        # Linear function\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity\n        self.relu = nn.ReLU()\n        # Linear function (readout)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n\n    def forward(self, x):\n        # Linear function\n        out = self.fc1(x)\n        # Non-linearity\n        out = self.relu(out)\n        # Linear function (readout)\n        out = self.fc2(out)\n        return out\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28*28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\n# learning_rate = 0.001\n\noptimizer = torch.optim.RMSprop(model.parameters())\n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as Variable\n        images = images.view(-1, 28*28).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images to a Torch Variable\n                images = images.view(-1, 28*28).requires_grad_()\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Iteration: 500. Loss: 0.25550296902656555. Accuracy: 95\nIteration: 1000. Loss: 0.17357593774795532. Accuracy: 93\nIteration: 1500. Loss: 0.10597744584083557. Accuracy: 96\nIteration: 2000. Loss: 0.03807783126831055. Accuracy: 96\nIteration: 2500. Loss: 0.10654022544622421. Accuracy: 96\nIteration: 3000. Loss: 0.05745543912053108. Accuracy: 96\n</code></pre>"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#summary-of-optimization-algorithms-performance","title":"Summary of Optimization Algorithms Performance","text":"<ul> <li>SGD: 95.78%</li> <li>SGD Momentum: 97.69%</li> <li>SGD Nesterov: 97.58%</li> <li>Adam: 97.20%</li> <li>Adagrad: 95.51%</li> <li>Adadelta: 97.45%</li> <li>Adamax: 96.58%</li> <li>RMSProp: 97.1%</li> </ul> <p>Performance is not definitive here</p> <p>I have used a seed to ensure you can reproduce results here. However, if you change the seed number you would realize that the performance of these optimization algorithms would change. A solution is to run each optimization on many seeds and get the average performance. Then you can compare the mean performance across all optimization algorithms. </p> <p>There are a lot of other factors like how Adam and SGD Momentum may have different ideal starting learning rates and require different learning rate scheduling. But off the hand, SGD and Adam are very robust optimization algorithms that you can rely on. </p> <p>Subsequently, we will look into more advanced optimization algorithms that are based mainly on SGD and Adam.</p>"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#simple-suggestions","title":"Simple Suggestions","text":"<ul> <li>Momentum/Nesterov<ul> <li>Powerful if we control the learning rate schedule</li> </ul> </li> <li>Adam<ul> <li>Lazy to control the learning rate schedule</li> </ul> </li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#summary","title":"Summary","text":"<p>We've learnt...</p> <p>Success</p> <ul> <li> Recap of 7 step process<ul> <li> Step 1: Load Dataset</li> <li> Step 2: Make Dataset Iterable</li> <li> Step 3: Create Model Class</li> <li> Step 4: Instantiate Model Class</li> <li> Step 5: Instantiate Loss Class</li> <li> Step 6: Instantiate Optimizer Class</li> <li> Step 7: Train Model</li> </ul> </li> <li> Step 6<ul> <li> Update parameters using gradients</li> <li> <code>parameters = parameters - learning_rate * parameters_gradients</code></li> </ul> </li> <li> Gradient descent<ul> <li> Using gradients (error signals from loss class) to update parameters</li> </ul> </li> <li> Mathematical interpretation: \\(\\theta = \\theta - \\eta \\cdot  \\nabla J(\\theta)\\)</li> <li> Optimisation Algorithms<ul> <li> Batch gradient descent</li> <li> Stochastic gradient descent</li> <li> Mini-batch gradient descent (SGD)</li> <li> SGD + Momentum</li> <li> SGD + Nesterov</li> <li> Adam</li> <li> Other adaptive algorithms: adagrad, adamax, adadelta, RMSProp</li> </ul> </li> <li> Recommendations<ul> <li> SGD+M</li> <li> SGD+N</li> <li> Adam</li> </ul> </li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#citation","title":"Citation","text":"<p>If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.</p> <p> </p>"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/","title":"Weight Initializations &amp; Activation Functions","text":"<p>Run Jupyter Notebook</p> <p>You can run the code for this section in this jupyter notebook link.</p>"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#recap-of-logistic-regression","title":"Recap of Logistic Regression","text":""},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#recap-of-feedforward-neural-network-activation-function","title":"Recap of Feedforward Neural Network Activation Function","text":""},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#sigmoid-logistic","title":"Sigmoid (Logistic)","text":"<ul> <li>\\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\)</li> <li>Input number \\(\\rightarrow\\) [0, 1]<ul> <li>Large negative number \\(\\rightarrow\\) 0</li> <li>Large positive number \\(\\rightarrow\\) 1</li> </ul> </li> <li>Cons: <ol> <li>Activation saturates at 0 or 1 with gradients \\(\\approx\\) 0<ul> <li>No signal to update weights \\(\\rightarrow\\) cannot learn</li> <li>Solution: Have to carefully initialize weights to prevent this</li> </ul> </li> </ol> </li> </ul> <pre><code>import matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\n\ndef sigmoid(x):\n    a = []\n    for item in x:\n        a.append(1/(1+np.exp(-item)))\n    return a\n\nx = np.arange(-10., 10., 0.2)\nsig = sigmoid(x)\n\nplt.style.use('ggplot')\nplt.plot(x,sig, linewidth=3.0)\n</code></pre>"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#tanh","title":"Tanh","text":"<ul> <li>\\(\\tanh(x) = 2 \\sigma(2x) -1\\)<ul> <li>A scaled sigmoid function</li> </ul> </li> <li>Input number \\(\\rightarrow\\) [-1, 1]</li> <li>Cons: <ol> <li>Activation saturates at 0 or 1 with gradients \\(\\approx\\) 0<ul> <li>No signal to update weights \\(\\rightarrow\\) cannot learn</li> <li>Solution: Have to carefully initialize weights to prevent this</li> </ul> </li> </ol> </li> </ul> <pre><code>x = np.arange(-10., 10., 0.2)\ntanh = np.dot(2, sigmoid(np.dot(2, x))) - 1\n\nplt.plot(x,tanh, linewidth=3.0)\n</code></pre>"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#relus","title":"ReLUs","text":"<ul> <li>\\(f(x) = \\max(0, x)\\)</li> <li>Pros:<ol> <li>Accelerates convergence \\(\\rightarrow\\) train faster</li> <li>Less computationally expensive operation compared to Sigmoid/Tanh exponentials</li> </ol> </li> <li>Cons:<ol> <li>Many ReLU units \"die\" \\(\\rightarrow\\) gradients = 0 forever<ul> <li>Solution: careful learning rate and weight initialization choice</li> </ul> </li> </ol> </li> </ul> <pre><code>x = np.arange(-10., 10., 0.2)\nrelu = np.maximum(x, 0)\n\nplt.plot(x,relu, linewidth=3.0)\n</code></pre>"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#why-do-we-need-weight-initializations-or-new-activation-functions","title":"Why do we need weight initializations or new activation functions?","text":"<ul> <li>To prevent vanishing/exploding gradients</li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#case-1-sigmoidtanh","title":"Case 1: Sigmoid/Tanh","text":"<ul> <li>Problem<ul> <li>If variance of input too large: gradients = 0 (vanishing gradients)</li> <li>If variance of input too small: linear \\(\\rightarrow\\) gradients = constant value</li> </ul> </li> <li>Solutions<ul> <li>Want a constant variance of input to achieve non-linearity \\(\\rightarrow\\) unique gradients for unique updates<ul> <li>Xavier Initialization (good constant variance for Sigmoid/Tanh)</li> <li>ReLU or Leaky ReLU</li> </ul> </li> </ul> </li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#case-2-relu","title":"Case 2: ReLU","text":"<ul> <li>Solution to Case 1<ul> <li>Regardless of variance of input: gradients = 0 or 1 </li> </ul> </li> <li>Problem<ul> <li>But those with 0: no updates (\"dead ReLU units\") </li> <li>Has unlimited output size with input &gt; 0 (explodes gradients subsequently)</li> </ul> </li> <li>Solutions<ul> <li>He Initialization (good constant variance)</li> <li>Leaky ReLU</li> </ul> </li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#case-3-leaky-relu","title":"Case 3: Leaky ReLU","text":"<ul> <li>Solution to Case 2<ul> <li>Solves the 0 signal issue when input &lt; 0  </li> </ul> </li> <li>Problem<ul> <li>Has unlimited output size with input &gt; 0 (explodes)</li> </ul> </li> <li>Solution<ul> <li>He Initialization (good constant variance)</li> </ul> </li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#summary-of-weight-initialization-solutions-to-activations","title":"Summary of weight initialization solutions to activations","text":"<ul> <li>Tanh/Sigmoid vanishing gradients can be solved with Xavier initialization<ul> <li>Good range of constant variance</li> </ul> </li> <li>ReLU/Leaky ReLU exploding gradients can be solved with He initialization<ul> <li>Good range of constant variance</li> </ul> </li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#types-of-weight-intializations","title":"Types of weight intializations","text":""},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#zero-initialization-set-all-weights-to-0","title":"Zero Initialization: set all weights to 0","text":"<ul> <li>Every neuron in the network computes the same output \\(\\rightarrow\\) computes the same gradient \\(\\rightarrow\\) same parameter updates </li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#normal-initialization-set-all-weights-to-random-small-numbers","title":"Normal Initialization: set all weights to random small numbers","text":"<ul> <li>Every neuron in the network computes different output \\(\\rightarrow\\) computes different gradient \\(\\rightarrow\\) different parameter updates </li> <li>\"Symmetry breaking\" </li> <li>Problem: variance that grows with the number of inputs</li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#lecun-initialization-normalize-variance","title":"Lecun Initialization: normalize variance","text":"<ul> <li>Solves growing variance with the number of inputs \\(\\rightarrow\\) constant variance </li> <li>Look at a simple feedforward neural network </li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#equations-for-lecun-initialization","title":"Equations for Lecun Initialization","text":"<ul> <li>\\(Y = AX + B\\)</li> <li>\\(y = a_1x_1 + a_2x_2 + \\cdot + a_n x_n + b\\)</li> <li>\\(Var(y) = Var(a_1x_1 + a_2x_2 + \\cdot + a_n x_n + b)\\)</li> <li>\\(Var(a_i x_i) = E(x_i)^2 Var(a_i) + E(a_i)^2Var(x_i) + Var(a_i)Var(x_i)\\)<ul> <li>General term, you might be more familiar with the following<ul> <li>\\(Var(XY) = E(X)^2 Var(Y) + E(Y)^2Var(X) + Var(X)Var(Y)\\)</li> </ul> </li> <li>\\(E(x_i)\\): expectation/mean of \\(x_i\\)</li> <li>\\(E(a_i)\\): expectation/mean of \\(a_i\\)</li> </ul> </li> <li>Assuming inputs/weights drawn i.i.d. with Gaussian distribution of mean=0<ul> <li>\\(E(x_i) = E(a_i) = 0\\)</li> <li>\\(Var(a_i x_i) = Var(a_i)Var(x_i)\\)</li> </ul> </li> <li>\\(Var(y) = Var(a_1)Var(x_1) + \\cdot + Var(a_n)Var(x_n)\\)<ul> <li>Since the bias, b, is a constant, \\(Var(b) = 0\\)</li> </ul> </li> <li>Since i.i.d.<ul> <li>\\(Var(y) = n \\times Var(a_i)Var(x_i)\\)</li> </ul> </li> <li>Since we want constant variance where \\(Var(y) = Var(x_i)\\)<ul> <li>\\(1 = nVar(a_i)\\)</li> <li>\\(Var(a_i) = \\frac{1}{n}\\)</li> </ul> </li> <li>This is essentially Lecun initialization, from his paper titled \"Efficient Backpropagation\"<ul> <li>We draw our weights i.i.d. with mean=0 and variance = \\(\\frac{1}{n}\\)</li> <li>Where \\(n\\) is the number of input units in the weight tensor</li> </ul> </li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#improvements-to-lecun-intialization","title":"Improvements to Lecun Intialization","text":"<ul> <li>They are essentially slight modifications to Lecun'98 initialization</li> <li>Xavier Intialization<ul> <li>Works better for layers with Sigmoid activations </li> <li>\\(var(a_i) = \\frac{1}{n_{in} + n_{out}}\\)<ul> <li>Where \\(n_{in}\\) and \\(n_{out}\\) are the number of input and output units in the weight tensor respectively</li> </ul> </li> </ul> </li> <li>Kaiming Initialization<ul> <li>Works better for layers with ReLU or LeakyReLU activations </li> <li>\\(var(a_i) = \\frac{2}{n_{in}}\\)</li> </ul> </li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#summary-of-weight-initializations","title":"Summary of  weight initializations","text":"<ul> <li>Normal Distribution</li> <li>Lecun Normal Distribution</li> <li>Xavier (Glorot) Normal Distribution </li> <li>Kaiming (He) Normal Distribution</li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#weight-initializations-with-pytorch","title":"Weight Initializations with PyTorch","text":""},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#normal-initialization-tanh-activation","title":"Normal Initialization: Tanh Activation","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nfrom torch.autograd import Variable\n\n# Set seed\ntorch.manual_seed(0)\n\n# Scheduler import\nfrom torch.optim.lr_scheduler import StepLR\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass FeedforwardNeuralNetModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FeedforwardNeuralNetModel, self).__init__()\n        # Linear function\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Linear weight, W,  Y = WX + B\n        nn.init.normal_(self.fc1.weight, mean=0, std=1)\n        # Non-linearity\n        self.tanh = nn.Tanh()\n        # Linear function (readout)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n        nn.init.normal_(self.fc2.weight, mean=0, std=1)\n\n    def forward(self, x):\n        # Linear function\n        out = self.fc1(x)\n        # Non-linearity\n        out = self.tanh(out)\n        # Linear function (readout)\n        out = self.fc2(out)\n        return out\n\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28*28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n\n'''\nSTEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS\n'''\n# step_size: at how many multiples of epoch you decay\n# step_size = 1, after every 2 epoch, new_lr = lr*gamma \n# step_size = 2, after every 2 epoch, new_lr = lr*gamma \n\n# gamma = decaying factor\nscheduler = StepLR(optimizer, step_size=1, gamma=0.96)\n\n'''\nSTEP 8: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    # Decay Learning Rate\n    scheduler.step()\n    # Print Learning Rate\n    print('Epoch:', epoch,'LR:', scheduler.get_lr())\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as tensors with gradient accumulation abilities\n        images = images.view(-1, 28*28).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images to a Torch Variable\n                images = images.view(-1, 28*28)\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n               # Total correct predictions\n                correct += (predicted.type(torch.FloatTensor).cpu() == labels.type(torch.FloatTensor)).sum()\n\n            accuracy = 100. * correct.item() / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Epoch: 0 LR: [0.1]\nIteration: 500. Loss: 0.5192779302597046. Accuracy: 87.9\nEpoch: 1 LR: [0.096]\nIteration: 1000. Loss: 0.4060308337211609. Accuracy: 90.15\nEpoch: 2 LR: [0.09216]\nIteration: 1500. Loss: 0.2880493104457855. Accuracy: 90.71\nEpoch: 3 LR: [0.08847359999999999]\nIteration: 2000. Loss: 0.23173095285892487. Accuracy: 91.99\nEpoch: 4 LR: [0.084934656]\nIteration: 2500. Loss: 0.23814399540424347. Accuracy: 92.32\nIteration: 3000. Loss: 0.19513173401355743. Accuracy: 92.55\n</code></pre>"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#lecun-initialization-tanh-activation","title":"Lecun Initialization: Tanh Activation","text":"<ul> <li>By default, PyTorch uses Lecun initialization, so nothing new has to be done here compared to using Normal, Xavier or Kaiming initialization.</li> </ul> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nfrom torch.autograd import Variable\n\n# Set seed\ntorch.manual_seed(0)\n\n# Scheduler import\nfrom torch.optim.lr_scheduler import StepLR\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass FeedforwardNeuralNetModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FeedforwardNeuralNetModel, self).__init__()\n        # Linear function\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity\n        self.tanh = nn.Tanh()\n        # Linear function (readout)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n\n    def forward(self, x):\n        # Linear function\n        out = self.fc1(x)\n        # Non-linearity\n        out = self.tanh(out)\n        # Linear function (readout)\n        out = self.fc2(out)\n        return out\n\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28*28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n\n'''\nSTEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS\n'''\n# step_size: at how many multiples of epoch you decay\n# step_size = 1, after every 2 epoch, new_lr = lr*gamma \n# step_size = 2, after every 2 epoch, new_lr = lr*gamma \n\n# gamma = decaying factor\nscheduler = StepLR(optimizer, step_size=1, gamma=0.96)\n\n'''\nSTEP 8: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    # Decay Learning Rate\n    scheduler.step()\n    # Print Learning Rate\n    print('Epoch:', epoch,'LR:', scheduler.get_lr())\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as tensors with gradient accumulation abilities\n        images = images.view(-1, 28*28).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images to a Torch Variable\n                images = images.view(-1, 28*28)\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted.type(torch.FloatTensor).cpu() == labels.type(torch.FloatTensor)).sum()\n\n            accuracy = 100. * correct.item() / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Epoch: 0 LR: [0.1]\nIteration: 500. Loss: 0.20123475790023804. Accuracy: 95.63\nEpoch: 1 LR: [0.096]\nIteration: 1000. Loss: 0.10885068774223328. Accuracy: 96.48\nEpoch: 2 LR: [0.09216]\nIteration: 1500. Loss: 0.1296212077140808. Accuracy: 97.22\nEpoch: 3 LR: [0.08847359999999999]\nIteration: 2000. Loss: 0.05178885534405708. Accuracy: 97.36\nEpoch: 4 LR: [0.084934656]\nIteration: 2500. Loss: 0.02619408629834652. Accuracy: 97.61\nIteration: 3000. Loss: 0.02096685953438282. Accuracy: 97.7\n</code></pre>"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#xavier-initialization-tanh-activation","title":"Xavier Initialization: Tanh Activation","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nfrom torch.autograd import Variable\n\n# Set seed\ntorch.manual_seed(0)\n\n# Scheduler import\nfrom torch.optim.lr_scheduler import StepLR\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass FeedforwardNeuralNetModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FeedforwardNeuralNetModel, self).__init__()\n        # Linear function\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Linear weight, W,  Y = WX + B\n        nn.init.xavier_normal_(self.fc1.weight)\n        # Non-linearity\n        self.tanh = nn.Tanh()\n        # Linear function (readout)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n        nn.init.xavier_normal_(self.fc2.weight)\n\n    def forward(self, x):\n        # Linear function\n        out = self.fc1(x)\n        # Non-linearity\n        out = self.tanh(out)\n        # Linear function (readout)\n        out = self.fc2(out)\n        return out\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28*28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n\n'''\nSTEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS\n'''\n# step_size: at how many multiples of epoch you decay\n# step_size = 1, after every 2 epoch, new_lr = lr*gamma \n# step_size = 2, after every 2 epoch, new_lr = lr*gamma \n\n# gamma = decaying factor\nscheduler = StepLR(optimizer, step_size=1, gamma=0.96)\n\n'''\nSTEP 8: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    # Decay Learning Rate\n    scheduler.step()\n    # Print Learning Rate\n    print('Epoch:', epoch,'LR:', scheduler.get_lr())\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as tensors with gradient accumulation abilities\n        images = images.view(-1, 28*28).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images to a Torch Variable\n                images = images.view(-1, 28*28)\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted.type(torch.FloatTensor).cpu() == labels.type(torch.FloatTensor)).sum()\n\n            accuracy = 100. * correct.item() / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Epoch: 0 LR: [0.1]\nIteration: 500. Loss: 0.14800140261650085. Accuracy: 95.43\nEpoch: 1 LR: [0.096]\nIteration: 1000. Loss: 0.17138008773326874. Accuracy: 96.58\nEpoch: 2 LR: [0.09216]\nIteration: 1500. Loss: 0.07987994700670242. Accuracy: 96.95\nEpoch: 3 LR: [0.08847359999999999]\nIteration: 2000. Loss: 0.07756654918193817. Accuracy: 97.23\nEpoch: 4 LR: [0.084934656]\nIteration: 2500. Loss: 0.05563584715127945. Accuracy: 97.6\nIteration: 3000. Loss: 0.07122127711772919. Accuracy: 97.49\n</code></pre>"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#xavier-initialization-relu-activation","title":"Xavier Initialization: ReLU Activation","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nfrom torch.autograd import Variable\n\n# Set seed\ntorch.manual_seed(0)\n\n# Scheduler import\nfrom torch.optim.lr_scheduler import StepLR\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass FeedforwardNeuralNetModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FeedforwardNeuralNetModel, self).__init__()\n        # Linear function\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Linear weight, W,  Y = WX + B\n        nn.init.xavier_normal_(self.fc1.weight)\n        # Non-linearity\n        self.relu = nn.ReLU()\n        # Linear function (readout)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n        nn.init.xavier_normal_(self.fc2.weight)\n\n    def forward(self, x):\n        # Linear function\n        out = self.fc1(x)\n        # Non-linearity\n        out = self.relu(out)\n        # Linear function (readout)\n        out = self.fc2(out)\n        return out\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28*28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n\n'''\nSTEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS\n'''\n# step_size: at how many multiples of epoch you decay\n# step_size = 1, after every 2 epoch, new_lr = lr*gamma \n# step_size = 2, after every 2 epoch, new_lr = lr*gamma \n\n# gamma = decaying factor\nscheduler = StepLR(optimizer, step_size=1, gamma=0.96)\n\n'''\nSTEP 8: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    # Decay Learning Rate\n    scheduler.step()\n    # Print Learning Rate\n    print('Epoch:', epoch,'LR:', scheduler.get_lr())\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as tensors with gradient accumulation abilities\n        images = images.view(-1, 28*28).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images to a Torch Variable\n                images = images.view(-1, 28*28)\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted.type(torch.FloatTensor).cpu() == labels.type(torch.FloatTensor)).sum()\n\n            accuracy = 100. * correct.item() / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Epoch: 0 LR: [0.1]\nIteration: 500. Loss: 0.1245984435081482. Accuracy: 95.82\nEpoch: 1 LR: [0.096]\nIteration: 1000. Loss: 0.14348150789737701. Accuracy: 96.72\nEpoch: 2 LR: [0.09216]\nIteration: 1500. Loss: 0.10421314090490341. Accuracy: 97.3\nEpoch: 3 LR: [0.08847359999999999]\nIteration: 2000. Loss: 0.04693891853094101. Accuracy: 97.29\nEpoch: 4 LR: [0.084934656]\nIteration: 2500. Loss: 0.06869587302207947. Accuracy: 97.61\nIteration: 3000. Loss: 0.056865859776735306. Accuracy: 97.48\n</code></pre>"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#he-initialization-relu-activation","title":"He Initialization: ReLU Activation","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nfrom torch.autograd import Variable\n\n# Set seed\ntorch.manual_seed(0)\n\n# Scheduler import\nfrom torch.optim.lr_scheduler import StepLR\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass FeedforwardNeuralNetModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FeedforwardNeuralNetModel, self).__init__()\n        # Linear function\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Linear weight, W,  Y = WX + B\n        nn.init.kaiming_normal_(self.fc1.weight)\n        # Non-linearity\n        self.relu = nn.ReLU()\n        # Linear function (readout)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n        nn.init.kaiming_normal_(self.fc2.weight)\n\n    def forward(self, x):\n        # Linear function\n        out = self.fc1(x)\n        # Non-linearity\n        out = self.relu(out)\n        # Linear function (readout)\n        out = self.fc2(out)\n        return out\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28*28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n\n'''\nSTEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS\n'''\n# step_size: at how many multiples of epoch you decay\n# step_size = 1, after every 2 epoch, new_lr = lr*gamma \n# step_size = 2, after every 2 epoch, new_lr = lr*gamma \n\n# gamma = decaying factor\nscheduler = StepLR(optimizer, step_size=1, gamma=0.96)\n\n'''\nSTEP 8: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    # Decay Learning Rate\n    scheduler.step()\n    # Print Learning Rate\n    print('Epoch:', epoch,'LR:', scheduler.get_lr())\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as tensors with gradient accumulation abilities\n        images = images.view(-1, 28*28).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images to a Torch Variable\n                images = images.view(-1, 28*28)\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted.type(torch.FloatTensor).cpu() == labels.type(torch.FloatTensor)).sum()\n\n            accuracy = 100. * correct.item() / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Epoch: 0 LR: [0.1]\nIteration: 500. Loss: 0.11658752709627151. Accuracy: 95.7\nEpoch: 1 LR: [0.096]\nIteration: 1000. Loss: 0.15525035560131073. Accuracy: 96.65\nEpoch: 2 LR: [0.09216]\nIteration: 1500. Loss: 0.09970294684171677. Accuracy: 97.07\nEpoch: 3 LR: [0.08847359999999999]\nIteration: 2000. Loss: 0.04063304886221886. Accuracy: 97.23\nEpoch: 4 LR: [0.084934656]\nIteration: 2500. Loss: 0.0719323456287384. Accuracy: 97.7\nIteration: 3000. Loss: 0.04470040276646614. Accuracy: 97.39\n</code></pre>"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#initialization-performance","title":"Initialization Performance","text":"Initialization: Activation Test Accuracy Normal: Tanh 92.55 Lecun: Tanh 97.7 Xavier: Tanh 97.49 Xavier: ReLU 97.48 He: ReLU 97.39 <p>Interpreting the Validation Accuracy Table</p> <p>Take note that these numbers would fluctuate slightly when you change seeds. </p> <p>However, the key point here is that all the other intializations are clearly much better than a basic normal distribution.</p> <p>Whether He, Xavier, or Lecun  intialization is better or any other initializations depends on the overall model's architecture (RNN/LSTM/CNN/FNN etc.), activation functions (ReLU, Sigmoid, Tanh etc.) and more.</p> <p>For example, more advanced initializations we will cover subsequently is orthogonal initialization that works better for RNN/LSTM. But due to the math involved in that, we will be covering such advanced initializations in a separate section.</p>"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#summary","title":"Summary","text":"<p>We've learnt...</p> <p>Success</p> <ul> <li> Recap of LG</li> <li> Recap of FNN</li> <li> Recap of Activation Functions<ul> <li> Sigmoid (Logistic)</li> <li> Tanh</li> <li> ReLU</li> </ul> </li> <li> Need for Weight Initializations<ul> <li> Sigmoid/Tanh: vanishing gradients<ul> <li> Constant Variance initialization with Lecun or Xavier </li> </ul> </li> <li> ReLU: exploding gradients with dead units<ul> <li> He Initialization</li> </ul> </li> <li> Leaky ReLU: exploding gradients only<ul> <li> He Initialization</li> </ul> </li> </ul> </li> <li> Types of weight initialisations<ul> <li> Zero</li> <li> Normal: growing weight variance</li> <li> Lecun: constant variance</li> <li> Xavier: constant variance for Sigmoid/Tanh</li> <li> Kaiming He: constant variance for ReLU activations</li> </ul> </li> <li> PyTorch implementation</li> </ul>"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#citation","title":"Citation","text":"<p>If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.</p> <p> </p>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/","title":"Markov Decision Processes (MDP) and Bellman Equations","text":""},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#markov-decision-processes-mdps","title":"Markov Decision Processes (MDPs)","text":"<ul> <li>Typically we can frame all RL tasks as MDPs<sup>1</sup><ul> <li>Intuitively, it's sort of a way to frame RL tasks such that we can solve them in a \"principled\" manner. We will go into the specifics throughout this tutorial</li> </ul> </li> <li>The key in MDPs is the Markov Property<ul> <li>Essentially the future depends on the present and not the past<ul> <li>More specifically, the future is independent of the past given the present</li> <li>There's an assumption the present state encapsulates past information. This is not always true, see the note below.</li> </ul> </li> </ul> </li> <li>Putting into the context of what we have covered so far: our agent can (1) control its action based on its current (2) completely known state<ul> <li>Back to the \"driving to avoid puppy\" example: given we know there is a dog in front of the car as the current state and the car is always moving forward (no reverse driving), the agent can decide to take a left/right turn to avoid colliding with the puppy in front</li> </ul> </li> </ul>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#two-main-characteristics-for-mdps","title":"Two main characteristics for MDPs","text":"<ol> <li>Control over state transitions</li> <li>States completely observable</li> </ol> <p>Other Markov Models</p> <p>Permutations of whether there is presence of the two main characteristics would lead to different Markov models. These are not important now, but it gives you an idea of what other frameworks we can use besides MDPs.</p>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#types-of-markov-models","title":"Types of Markov Models","text":"<ol> <li>{++Control++} over state transitions and {++completely observable++} states: MDPs</li> <li>{++Control++} over state transitions and partially observable states: Partially Observable MDPs (POMDPs)</li> <li>No control over state transitions and {++completely observable++} states: Markov Chain</li> <li>No control over state transitions and partially observable states: Hidden Markov Model</li> </ol>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#pomdps","title":"POMDPs","text":"<ul> <li>Imagine our driving example where we don't know if the car is going forward/backward in its state, but only know there is a puppy in the center lane in front, this is a partially observable state</li> <li>There are ways to counter this<ol> <li>Use the complete history to construct the current state</li> <li>Represent the current state as a probability distribution (bayesian approach) of what the agent perceives of the current state</li> <li>Using a RNN to form the current state that encapsulates the past<sup>2</sup></li> </ol> </li> </ul>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#5-components-of-mdps","title":"5 Components of MDPs","text":"<ol> <li>\\(\\mathcal{S}\\): set of states</li> <li>\\(\\mathcal{A}\\): set of actions</li> <li>\\(\\mathcal{R}\\): reward function</li> <li>\\(\\mathcal{P}\\): transition probability function</li> <li>\\(\\gamma\\): discount for future rewards</li> </ol> <p>Remembering 5 components with a mnemonic</p> <p>A mnemonic I use to remember the 5 components is the acronym \"SARPY\" (sar-py). I know \\(\\gamma\\) is not \\(\\mathcal{Y}\\) but it looks like a <code>y</code> so there's that.</p>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#moving-from-mdps-to-optimal-policy","title":"Moving From MDPs to Optimal Policy","text":"<ul> <li>We have an agent acting in an environment</li> <li>The way the environment reacts to the agent's actions (\\(a\\)) is dictated by a model</li> <li>The agent can take actions (\\(a\\)) to move from one state (\\(s\\)) to another new state (\\(s')\\)</li> <li>When the agent has transited to a new state (\\(s')\\), there will a reward (\\(r\\))</li> <li>We may or may not know our model<ol> <li>Model-based RL: this is where we can {++clearly define++} our (1) transition probabilities and/or (2) reward function<ul> <li>A global minima can be attained via Dynamic Programming (DP)</li> </ul> </li> <li>Model-free RL: this is where we cannot clearly define our (1) transition probabilities and/or (2) reward function<ul> <li>Most real-world problems are under this category so we will mostly place our attention on this category</li> </ul> </li> </ol> </li> <li>How the agent acts (\\(a\\)) in its current state (\\(s\\)) is specified by its policy (\\(\\pi(s)\\))<ul> <li>It can either be deterministic or stochastic<ol> <li>Deterministic policy: \\(a = \\pi(s)\\)</li> <li>Stochastic policy: \\(\\mathbb{P}_\\pi [A=a \\vert S=s] = \\pi(a | s)\\)<ul> <li>This is the proability of taking an action given the current state under the policy</li> <li>\\(\\mathcal{A}\\): set of all actions</li> <li>\\(\\mathcal{S}\\): set of all states</li> </ul> </li> </ol> </li> </ul> </li> <li>When the agent acts given its state under the policy (\\(\\pi(a | s)\\)), the transition probability function \\(\\mathcal{P}\\) determines the subsequent state (\\(s'\\))<ul> <li>\\(\\mathcal{P}_{ss'}^a = \\mathcal{P}(s' \\vert s, a)  = \\mathbb{P} [S_{t+1} = s' \\vert S_t = s, A_t = a]\\)</li> </ul> </li> <li>When the agent act based on its policy (\\(\\pi(a | s)\\)) and transited to the new state determined by the transition probability function \\(\\mathcal{P}_{ss'}^a\\) it gets a reward based on the reward function as a feedback<ul> <li>\\(\\mathcal{R}_s^a = \\mathbb{E} [\\mathcal{R}_{t+1} \\vert S_t = s, A_t = a]\\)</li> </ul> </li> <li>Rewards are short-term, given as feedback after the agent takes an action and transits to a new state. Summing all future rewards and discounting them would lead to our return \\(\\mathcal{G}\\)<ul> <li>\\(\\mathcal{G}_t = \\sum_{i=0}^{N} \\gamma^k \\mathcal{R}_{t+1+i}\\)<ul> <li>\\(\\gamma\\), our discount factor which ranges from 0 to 1 (inclusive) reduces the weightage of future rewards allowing us to balance between short-term and long-term goals</li> </ul> </li> </ul> </li> <li>With our return \\(\\mathcal{G}\\), we then have our state-value function \\(\\mathcal{V}_{\\pi}\\) (how good to stay in that state) and our action-value or q-value function \\(\\mathcal{Q}_{\\pi}\\) (how good to take the action)<ul> <li>\\(\\mathcal{V}_{\\pi}(s) = \\mathbb{E}_{\\pi}[\\mathcal{G}_t \\vert \\mathcal{S}_t = s]\\)</li> <li>\\(\\mathcal{Q}_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[\\mathcal{G}_t \\vert \\mathcal{S}_t = s, \\mathcal{A}_t = a]\\)</li> <li>The advantage function is simply the difference between the two functions \\(\\mathcal{A}_{\\pi}(s, a) = \\mathcal{Q}_{\\pi}(s, a) - \\mathcal{V}_{\\pi}(s)\\)<ul> <li>Seems useless at this stage, but this advantage function will be used in some key algorithms we are covering</li> </ul> </li> </ul> </li> <li>Since our policy determines how our agent acts given its state, achieving an optimal policy \\(\\pi_*\\) would mean achieving optimal actions that is exactly what we want!</li> </ul> <p>Basic Categories of Approaches</p> <p>We've covered state-value functions, action-value functions, model-free RL and model-based RL. They form general overarching categories of how we design our agent.</p>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#categories-of-design","title":"Categories of Design","text":"<ol> <li>State-value based: search for the optimal state-value function (goodness of action in the state)</li> <li>Action-value based: search for the optimal action-value function (goodness of policy)</li> <li>Actor-critic based: using both state-value and action-value function </li> <li>Model based: attempts to model the environment to find the best policy</li> <li>Model-free based: trial and error to optimize for the best policy to get the most rewards instead of modelling the environment explicitly</li> </ol>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#optimal-policy","title":"Optimal Policy","text":"<ul> <li>Optimal policy \\(\\pi_*\\) \u2192 optimal state-value and action-value functions \u2192 max return \u2192 argmax of value functions<ul> <li>\\(\\pi_{*} = \\arg\\max_{\\pi} \\mathcal{V}_{\\pi}(s) = \\arg\\max_{\\pi} \\mathcal{Q}_{\\pi}(s, a)\\)</li> </ul> </li> <li>To calculate argmax of value functions \u2192 we need max return \\(\\mathcal{G}_t\\) \u2192 need max sum of rewards \\(\\mathcal{R}_s^a\\)</li> <li>To get max sum of rewards \\(\\mathcal{R}_s^a\\) we will rely on the Bellman Equations.<sup>3</sup></li> </ul>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#bellman-equation","title":"Bellman Equation","text":"<ul> <li>Essentially, the Bellman Equation breaks down our value functions into two parts<ol> <li>Immediate reward</li> <li>Discounted future value function</li> </ol> </li> <li>State-value function can be broken into:<ul> <li>\\(\\begin{aligned}     \\mathcal{V}_{\\pi}(s) &amp;= \\mathbb{E}[\\mathcal{G}_t \\vert \\mathcal{S}_t = s] \\\\     &amp;= \\mathbb{E} [\\mathcal{R}_{t+1} + \\gamma \\mathcal{R}_{t+2} + \\gamma^2 \\mathcal{R}_{t+3} + \\dots \\vert \\mathcal{S}_t = s] \\\\     &amp;= \\mathbb{E} [\\mathcal{R}_{t+1} + \\gamma (\\mathcal{R}_{t+2} + \\gamma \\mathcal{R}_{t+3} + \\dots) \\vert \\mathcal{S}_t = s] \\\\     &amp;= \\mathbb{E} [\\mathcal{R}_{t+1} + \\gamma \\mathcal{G}_{t+1} \\vert \\mathcal{S}_t = s] \\\\     &amp;= \\mathbb{E} [\\mathcal{R}_{t+1} + \\gamma \\mathcal{V}_{\\pi}(\\mathcal{s}_{t+1}) \\vert \\mathcal{S}_t = s]     \\end{aligned}\\)</li> </ul> </li> <li>Action-value function can be broken into:<ul> <li>\\(\\mathcal{Q}_{\\pi}(s, a) = \\mathbb{E} [\\mathcal{R}_{t+1} + \\gamma \\mathcal{Q}_{\\pi}(\\mathcal{s}_{t+1}, \\mathcal{a}_{t+1}) \\vert \\mathcal{S}_t = s, \\mathcal{A} = a]\\)</li> </ul> </li> </ul>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#key-recap-on-value-functions","title":"Key Recap on Value Functions","text":"<ul> <li>\\(\\mathcal{V}_{\\pi}(s) = \\mathbb{E}_{\\pi}[\\mathcal{G}_t \\vert \\mathcal{S}_t = s]\\)<ul> <li>State-value function: tells us how good to be in that state</li> </ul> </li> <li>\\(\\mathcal{Q}_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[\\mathcal{G}_t \\vert \\mathcal{S}_t = s, \\mathcal{A}_t = a]\\)<ul> <li>Action-value function: tells us how good to take actions given state</li> </ul> </li> </ul>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#bellman-expectation-equations","title":"Bellman Expectation Equations","text":"<ul> <li>Now we can move from Bellman Equations into Bellman Expectation Equations</li> <li>Basic: State-value function \\(\\mathcal{V}_{\\pi}(s)\\)<ol> <li>Current state \\(\\mathcal{S}\\)</li> <li>Multiple possible actions determined by stochastic policy \\(\\pi(a | s)\\)</li> <li>Each possible action is associated with a action-value function \\(\\mathcal{Q}_{\\pi}(s, a)\\) returning a value of that particular action</li> <li>Multiplying the possible actions with the action-value function and summing them gives us an indication of how good it is to be in that state<ul> <li>Final equation: \\(\\mathcal{V}_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a | s) \\mathcal{Q}(s, a)\\)</li> <li>Loose intuitive interpretation: <ul> <li>state-value = sum(policy determining actions * respective action-values)</li> </ul> </li> </ul> </li> </ol> </li> <li>Basic: Action-value function \\(\\mathcal{Q}_{\\pi}(s, a)\\)<ol> <li>With a list of possible multiple actions, there is a list of possible subsequent states \\(s'\\) associated with:<ol> <li>state value function \\(\\mathcal{V}_{\\pi}(s')\\) </li> <li>transition probability function \\(\\mathcal{P}_{ss'}^a\\) determining where the agent could land in based on the action</li> <li>reward \\(\\mathcal{R}_s^a\\) for taking the action</li> </ol> </li> <li>Summing the reward and the transition probability function associated with the state-value function gives us an indication of how good it is to take the actions given our state<ul> <li>Final equation: \\(\\mathcal{Q}_{\\pi}(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a {V}_{\\pi}(s')\\)</li> <li>Loose intuitive interpretation: <ul> <li>action-value = reward + sum(transition outcomes determining states * respective state-values)</li> </ul> </li> </ul> </li> </ol> </li> <li>Expanded functions (substitution)<ul> <li>Substituting action-value function into the state-value function<ul> <li>\\(\\mathcal{V}_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a | s) (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a {V}_{\\pi}(s'))\\)</li> </ul> </li> <li>Substituting  state-value function into action-value function<ul> <li>\\(\\mathcal{Q}_{\\pi}(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\sum_{a' \\in \\mathcal{A}} \\pi(a' | s') \\mathcal{Q}(s', a')\\)</li> </ul> </li> </ul> </li> </ul>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#bellman-optimality-equations","title":"Bellman Optimality Equations","text":"<ul> <li>Remember optimal policy \\(\\pi_*\\) \u2192 optimal state-value and action-value functions \u2192 argmax of value functions<ul> <li>\\(\\pi_{*} = \\arg\\max_{\\pi} \\mathcal{V}_{\\pi}(s) = \\arg\\max_{\\pi} \\mathcal{Q}_{\\pi}(s, a)\\)</li> </ul> </li> <li>Finally with Bellman Expectation Equations derived from Bellman Equations, we can derive the equations for the argmax of our value functions</li> <li>Optimal state-value function<ul> <li>\\(\\mathcal{V}_*(s) = \\arg\\max_{\\pi} \\mathcal{V}_{\\pi}(s)\\)</li> <li>Given \\(\\mathcal{V}_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a | s) (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a {V}_{\\pi}(s'))\\)</li> <li>We have \\(\\mathcal{V}_*(s) = \\max_{a \\in \\mathcal{A}} (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a {V}_{*}(s')))\\)</li> </ul> </li> <li>Optimal action-value function<ul> <li>\\(\\mathcal{Q}_*(s) = \\arg\\max_{\\pi} \\mathcal{Q}_{\\pi}(s)\\)</li> <li>Given \\(\\mathcal{Q}_{\\pi}(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\sum_{a' \\in \\mathcal{A}} \\pi(a' | s') \\mathcal{Q}(s', a')\\)</li> <li>We have \\(\\mathcal{Q}_{*}(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a max_{a' \\in \\mathcal{A}} \\mathcal{Q}_{*}(s', a')\\)</li> </ul> </li> </ul>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#optimal-action-value-and-state-value-functions","title":"Optimal Action-value and State-value functions","text":"<ol> <li>If the entire environment is known, such that we know our reward function and transition probability function, then we can solve for the optimal action-value and state-value functions via Dynamic Programming like<ul> <li>Policy evaluation, policy improvement, and policy iteration</li> </ul> </li> <li>However, typically we don't know the environment entirely then there is not closed form solution in getting optimal action-value and state-value functions. Hence, we need other iterative approaches like<ol> <li>Monte-Carlo methods</li> <li>Temporal difference learning (model-free and learns with episodes)<ol> <li>On-policy TD: SARSA</li> <li>Off-policy TD: Q-Learning and Deep Q-Learning (DQN)</li> </ol> </li> <li>Policy gradient<ul> <li>REINFORCE</li> <li>Actor-Critic</li> <li>A2C/A3C</li> <li>ACKTR</li> <li>PPO</li> <li>DPG</li> <li>DDPG (DQN + DPG)</li> </ul> </li> </ol> </li> </ol> <p>Closed form solution</p> <p>If there is a closed form solution, then the variables' values can be obtained with a finite number of mathematical operations (for example add, subtract, divide, and multiply).</p> <p>For example, solving \\(2x = 8 - 6x\\) would yield \\(8x = 8\\) by adding \\(6x\\) on both sides of the equation and finally yielding the value of \\(x=1\\) by dividing both sides of the equation by \\(8\\). </p> <p>These finite 2 steps of mathematical operations allowed us to solve for the value of x as the equation has a closed-form solution.</p> <p>However, many cases in deep learning and reinforcement learning there are no closed-form solutions which requires all the iterative methods mentioned above.</p> <ol> <li> <p>Bellman, R. A Markovian Decision Process. Journal of Mathematics and Mechanics. 1957.\u00a0\u21a9</p> </li> <li> <p>Matthew J. Hausknecht and Peter Stone. Deep Recurrent Q-Learning for Partially Observable MDPs. 2015.\u00a0\u21a9</p> </li> <li> <p>R Bellman. On the Theory of Dynamic Programming. Proceedings of the National Academy of Sciences. 1952.\u00a0\u21a9</p> </li> </ol>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/","title":"Dynamic Programming","text":""},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#frozen-lake-introduction","title":"Frozen Lake Introduction","text":"<p>Run Jupyter Notebook</p> <p>You can run the code for this section in this jupyter notebook link.</p> <ul> <li>Fronze Lake is a simple game where you are on a frozen lake and you need to retrieve an item on the frozen lake where some parts are frozen and some parts are holes (if you walk into them you die)</li> <li>Actions: \\(\\mathcal{A} = \\{0, 1, 2, 3\\}\\)<ol> <li>LEFT: 0</li> <li>DOWN = 1</li> <li>RIGHT = 2</li> <li>UP = 3</li> </ol> </li> <li>Whole lake is a 4 x 4 grid world, \\(\\mathcal{S} = \\{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15\\}\\)<ul> <li></li> </ul> </li> <li>On each grid, there are 4 possibilities<ul> <li>S: starting point, safe (code = 'SFFF')</li> <li>F: frozen surface, safe (code = 'FHFH')</li> <li>H: hole, fall to your doom (code = 'FFFH')</li> <li>G: goal, where the frisbee is located ('HFFG')</li> <li></li> </ul> </li> </ul>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#goal-of-frozen-lake","title":"Goal of Frozen Lake","text":"<p>The key here is we want to get to G without falling into the hole H in the shortest amount of time</p>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#why-dynamic-programming","title":"Why Dynamic Programming?","text":"<p>In this game, we know our transition probability function and reward function, essentially the whole environment, allowing us to turn this game into a simple planning problem via dynamic programming through 4 simple functions: (1) policy evaluation (2) policy improvement (3) policy iteration or (4) value iteration</p> <p>Before we explore how to solve this game, let's first understand how the game works in detail.</p>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#deterministic-policy-environment","title":"Deterministic Policy Environment","text":"<p>Make OpenAI Gym Environment for Frozen Lake</p> <pre><code># Import gym, installable via `pip install gym`\nimport gym\n\n# Environment environment Slippery (stochastic policy, move left probability = 1/3) comes by default!\n# If we want deterministic policy, we need to create new environment\n# Make environment No Slippery (deterministic policy, move left = left)\n\ngym.envs.register(\n    id='FrozenLakeNotSlippery-v0',\n    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n    kwargs={'map_name' : '4x4', 'is_slippery': False},\n    max_episode_steps=100,\n    reward_threshold=0.78, # optimum = .8196\n)\n\n# You can only register once\n# To delete any new environment\n# del gym.envs.registry.env_specs['FrozenLakeNotSlippery-v0']\n\n# Make the environment based on deterministic policy\nenv = gym.make('FrozenLakeNotSlippery-v0')\n</code></pre> <p>Observation space</p> <pre><code># State space\nprint(env.observation_space)\n</code></pre> <pre><code>Discrete(16)\n</code></pre> <p>State space</p> <pre><code>S_n = env.observation_space.n\nprint(S_n)\n</code></pre> <pre><code>16\n</code></pre> <p>Sampling state space</p> <pre><code># We should expect to see 15 possible grids from 0 to 15 when\n# we uniformly randomly sample from our observation space\nfor i in range(10):\n    print(env.observation_space.sample())\n</code></pre> <pre><code>11\n12\n7\n13\n7\n11\n14\n14\n4\n12\n</code></pre> <p>Action space</p> <pre><code># Action space\nprint(env.action_space)\n</code></pre> <pre><code>A_n = env.action_space.n\nprint(A_n)\n</code></pre> <pre><code>Discrete(4)\n\n4\n</code></pre> <p>Random sampling of actions</p> <pre><code># We should expect to see 4 actions when\n# we uniformly randomly sample:\n#     1. LEFT: 0\n#     2. DOWN = 1\n#     3. RIGHT = 2\n#     4. UP = 3\nfor i in range(10):\n    print(env.action_space.sample())\n</code></pre> <pre><code>0\n3\n1\n3\n3\n2\n3\n2\n2\n2\n</code></pre>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#making-steps","title":"Making Steps","text":"<p>Initial state</p> <pre><code># This sets the initial state at S, our starting point\n# We can render the environment to see where we are on the 4x4 frozenlake gridworld\nenv.reset()\nenv.render()\n</code></pre> <pre><code>[S]FFF\nFHFH\nFFFH\nHFFG\n</code></pre> <p>Go left</p> <pre><code># Go left (action=0), nothing should happen, and we should stay at the starting point, because there's no grid on the left\nenv.reset()\naction = 0\n(observation, reward, done, prob) = env.step(action)\nenv.render()\n\n# Observation = 1: move to grid number 1 (unchanged)\n# Prob = 1: deterministic policy, if we choose to go left, we'll go left\nprint(observation, reward, done, prob)\n</code></pre> <pre><code>(Left)\n[S]FFF\nFHFH\nFFFH\nHFFG\n\n0 0.0 False {'prob': 1.0}\n</code></pre> <p>Go down</p> <pre><code># Go down (action = 1), we should be safe as we step on frozen grid\nenv.reset()\naction = 1\n(observation, reward, done, prob) = env.step(action)\nenv.render()\n\n# Observation = 4: move to grid number 4\n# Prob = 1: deterministic policy, if we choose to go down we'll go down\nprint(observation, reward, done, prob)\n</code></pre> <pre><code>(Down)\nSFFF\n[F]HFH\nFFFH\nHFFG\n\n4 0.0 False {'prob': 1.0}\n</code></pre> <p>Go right</p> <pre><code># Go right (action = 2), we should be safe as we step on frozen grid\nenv.reset()\naction = 2\n(observation, reward, done, prob) = env.step(action)\nenv.render()\n\n# Observation = 1: move to grid number 1\n# Prob = 1: deterministic policy, if we choose to go right we'll go right\nprint(observation, reward, done, prob)\n</code></pre> <pre><code>(Right)\nS[F]FF\nFHFH\nFFFH\nHFFG\n\n1 0.0 False {'prob': 1.0}\n</code></pre> <p>Go right twice</p> <pre><code># Go right twice (action = 2), we should be safe as we step on 2 frozen grids\nenv.reset()\naction = 2\n(observation, reward, done, prob) = env.step(action)\nenv.render()\n(observation, reward, done, prob) = env.step(action)\nenv.render()\n\n# Observation = 2: move to the right twice from grid 0 to grid 2\n# Prob = 1: deterministic policy, if we choose to go right twice we'll go right twice\nprint(observation, reward, done, prob)\n</code></pre> <pre><code>(Right)\nS[F]FF\nFHFH\nFFFH\nHFFG\n\n(Right)\nSF[F]F\nFHFH\nFFFH\nHFFG\n\n2 0.0 False {'prob': 1.0}\n</code></pre>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#dying-drop-in-hole-grid-12-h","title":"Dying: drop in hole grid 12,  H","text":"<p>Go down thrice</p> <pre><code># Go down thrice (action = 1), we will die as we step onto the grid with a hole\nenv.reset()\naction = 1\n(observation, reward, done, prob) = env.step(action)\nenv.render()\n(observation, reward, done, prob) = env.step(action)\nenv.render()\n(observation, reward, done, prob) = env.step(action)\nenv.render()\n\n# Observation = 2: move to the right twice from grid 0 to grid 2\n# Prob = 1: deterministic policy, if we choose to go right twice we'll go right twice\n# Done = True because the game ends when we die (go onto hole grid (H) or finish the game (G))\nprint(observation, reward, done, prob)\n</code></pre> <pre><code>(Down)\nSFFF\n[F]HFH\nFFFH\nHFFG\n\n(Down)\nSFFF\nFHFH\n[F]FFH\nHFFG\n\n(Down)\nSFFF\nFHFH\nFFFH\n[H]FFG\n\n12 0.0 True {'prob': 1.0}\n</code></pre>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#winning-get-to-grid-15-g","title":"Winning: get to grid 15, G","text":"<p>Go right twice, go down thrice, go right once</p> <pre><code># Go right twice (action = 2), go down thrice (action = 1), go right once (action = 2)\nenv.reset()\n\n# Right Twice\naction = 2\n(observation, reward, done, prob) = env.step(action)\nenv.render()\n(observation, reward, done, prob) = env.step(action)\nenv.render()\n\n# Down Thrice\naction = 1\n(observation, reward, done, prob) = env.step(action)\nenv.render()\n(observation, reward, done, prob) = env.step(action)\nenv.render()\n(observation, reward, done, prob) = env.step(action)\nenv.render()\n\n# Right Once\naction = 2\n(observation, reward, done, prob) = env.step(action)\nenv.render()\n\n# Observation = 2: move to the right twice from grid 0 to grid 2\n# Prob = 1: deterministic policy, if we choose to go right twice we'll go right twice\n# Done = True because the game ends when we die (go onto hole grid (H) or finish the game (G))\nprint(observation, reward, done, prob)\n</code></pre> <pre><code>(Right)\nS[F]FF\nFHFH\nFFFH\nHFFG\n\n(Right)\nSF[F]F\nFHFH\nFFFH\nHFFG\n\n(Down)\nSFFF\nFH[F]H\nFFFH\nHFFG\n\n(Down)\nSFFF\nFHFH\nFF[F]H\nHFFG\n\n(Down)\nSFFF\nFHFH\nFFFH\nHF[F]mG\n\n(Right)\nSFFF\nFHFH\nFFFH\nHFF[G]\n\n15 1.0 True {'prob': 1.0}\n</code></pre>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#non-deterministic-policy-environment","title":"Non-deterministic Policy Environment","text":"<p>Go right?</p> <pre><code># Make the environment based on non-deterministic policy\nenv = gym.make('FrozenLake-v0')\n\n# Go right once (action = 2), we should go to the right but we did not!\nenv.seed(8)\nenv.reset()\naction = 2\n(observation, reward, done, prob) = env.step(action)\nenv.render()\n\n# Observation = 0: move to the right once from grid 0 to grid 1\n# Prob = 1/3: non-deterministic policy, if we choose to go right, there's only a 1/3 probability we would go to the right and with this environment seed we did not\nprint(observation, reward, done, prob)\n</code></pre> <pre><code>(Right)\n[S]FFF\nFHFH\nFFFH\nHFFG\n\n0 0.0 False {'prob': 0.3333333333333333}\n</code></pre> <p>Go right 10 times?</p> <pre><code># Try to go to the right 10 times, let's see how many times it goes to the right, by right we won't die because we would end up at the extreme right of grid 3\n# See how it can go down/left/up/nothing instead of just right? \n# Intuitively when we are moving on a frozen lake, some times when we want to walk one direction we may end up in another direction as it's slippery\n# Setting seed here of the environment so you can reproduce my results, otherwise stochastic policy will yield different results for each run\nenv.seed(8)\nenv.reset()\nfor i in range(10):\n    action = 2\n    (observation, reward, done, prob) = env.step(action)\n    env.render()\n</code></pre> <pre><code>(Right)\n[S]FFF\nFHFH\nFFFH\nHFFG\n\n(Right)\nS[F]FF\nFHFH\nFFFH\nHFFG\n\n(Right)\nSF[F]F\nFHFH\nFFFH\nHFFG\n\n(Right)\nSFFF\nFH[F]H\nFFFH\nHFFG\n\n(Right)\nSFFF\nFHF[H]\nFFFH\nHFFG\n\n(Right)\nSFFF\nFHF[4H]\nFFFH\nHFFG\n\n(Right)\nSFFF\nFHF[H]\nFFFH\nHFFG\n\n(Right)\nSFFF\nFHF[H]\nFFFH\nHFFG\n\n(Right)\nSFFF\nFHF[H]\nFFFH\nHFFG\n\n(Right)\nSFFF\nFHF[H]\nFFFH\nHFFG\n</code></pre>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#custom-frozen-lake-non-deterministic-policy-environment","title":"Custom Frozen Lake Non-deterministic Policy Environment","text":"<ul> <li>Because original code from OpenAI only allows us to run <code>env.step(action)</code>, this is challenging if we want to do some visualization of our state-value and action-value (q-value) functions for learning</li> <li>Hence, we'll be copying the whole code from OpenAI Frozen Lake implementation and adding just one line to make sure we can get P via <code>self.P = P</code></li> <li>This code is not important, you can just copy it</li> </ul> <pre><code>import sys\nfrom contextlib import closing\n\nimport numpy as np\nfrom six import StringIO, b\n\nfrom gym import utils\nfrom gym.envs.toy_text import discrete\n\nLEFT = 0\nDOWN = 1\nRIGHT = 2\nUP = 3\n\nMAPS = {\n    \"4x4\": [\n        \"SFFF\",\n        \"FHFH\",\n        \"FFFH\",\n        \"HFFG\"\n    ],\n    \"8x8\": [\n        \"SFFFFFFF\",\n        \"FFFFFFFF\",\n        \"FFFHFFFF\",\n        \"FFFFFHFF\",\n        \"FFFHFFFF\",\n        \"FHHFFFHF\",\n        \"FHFFHFHF\",\n        \"FFFHFFFG\"\n    ],\n}\n\n# Generates a random valid map (one that has a path from start to goal)\n# @params size, size of each side of the grid\n# @prams p, probability that a tile is frozen\ndef generate_random_map(size=8, p=0.8):\n    valid = False\n\n    #BFS to check that it's a valid path\n    def is_valid(arr, r=0, c=0):\n        if arr[r][c] == 'G':\n            return True\n\n        tmp = arr[r][c]\n        arr[r][c] = \"#\"\n\n        if r+1 &lt; size and arr[r+1][c] not in '#H':\n            if is_valid(arr, r+1, c) == True:\n                arr[r][c] = tmp\n                return True\n\n        if c+1 &lt; size and arr[r][c+1] not in '#H':\n            if is_valid(arr, r, c+1) == True:\n                arr[r][c] = tmp\n                return True\n\n        if r-1 &gt;= 0 and arr[r-1][c] not in '#H':\n            if is_valid(arr, r-1, c) == True:\n                arr[r][c] = tmp\n                return True\n\n        if c-1 &gt;= 0 and arr[r][c-1] not in '#H':\n            if is_valid(arr,r, c-1) == True:\n                arr[r][c] = tmp\n                return True\n        arr[r][c] = tmp\n        return False\n\n    while not valid:\n        p = min(1, p)\n        res = np.random.choice(['F','H'], (size, size), p=[p, 1-p])\n        res[0][0] = 'S'\n        res[-1][-1] = 'G'\n        valid = is_valid(res)\n    return [\"\".join(x) for x in res]\n\n\nclass FrozenLakeEnv(discrete.DiscreteEnv):\n\"\"\"\n    Winter is here. You and your friends were tossing around a frisbee at the park\n    when you made a wild throw that left the frisbee out in the middle of the lake.\n    The water is mostly frozen, but there are a few holes where the ice has melted.\n    If you step into one of those holes, you'll fall into the freezing water.\n    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n    you navigate across the lake and retrieve the disc.\n    However, the ice is slippery, so you won't always move in the direction you intend.\n    The surface is described using a grid like the following\n        SFFF\n        FHFH\n        FFFH\n        HFFG\n    S : starting point, safe\n    F : frozen surface, safe\n    H : hole, fall to your doom\n    G : goal, where the frisbee is located\n    The episode ends when you reach the goal or fall in a hole.\n    You receive a reward of 1 if you reach the goal, and zero otherwise.\n    \"\"\"\n\n    metadata = {'render.modes': ['human', 'ansi']}\n\n    def __init__(self, desc=None, map_name=\"4x4\",is_slippery=True):\n        if desc is None and map_name is None:\n            desc = generate_random_map()\n        elif desc is None:\n            desc = MAPS[map_name]\n        self.desc = desc = np.asarray(desc,dtype='c')\n        self.nrow, self.ncol = nrow, ncol = desc.shape\n        self.reward_range = (0, 1)\n\n        nA = 4\n        nS = nrow * ncol\n\n        isd = np.array(desc == b'S').astype('float64').ravel()\n        isd /= isd.sum()\n\n        P = {s : {a : [] for a in range(nA)} for s in range(nS)}\n\n        def to_s(row, col):\n            return row*ncol + col\n\n        def inc(row, col, a):\n            if a==0: # left\n                col = max(col-1,0)\n            elif a==1: # down\n                row = min(row+1,nrow-1)\n            elif a==2: # right\n                col = min(col+1,ncol-1)\n            elif a==3: # up\n                row = max(row-1,0)\n            return (row, col)\n\n        for row in range(nrow):\n            for col in range(ncol):\n                s = to_s(row, col)\n                for a in range(4):\n                    li = P[s][a]\n                    letter = desc[row, col]\n                    if letter in b'GH':\n                        li.append((1.0, s, 0, True))\n                    else:\n                        if is_slippery:\n                            for b in [(a-1)%4, a, (a+1)%4]:\n                                newrow, newcol = inc(row, col, b)\n                                newstate = to_s(newrow, newcol)\n                                newletter = desc[newrow, newcol]\n                                done = bytes(newletter) in b'GH'\n                                rew = float(newletter == b'G')\n                                li.append((1.0/3.0, newstate, rew, done))\n                        else:\n                            newrow, newcol = inc(row, col, a)\n                            newstate = to_s(newrow, newcol)\n                            newletter = desc[newrow, newcol]\n                            done = bytes(newletter) in b'GH'\n                            rew = float(newletter == b'G')\n                            li.append((1.0, newstate, rew, done))\n\n        # New change because environment only allows step without\n        # specific state for learning environment!\n        self.P = P\n\n        super(FrozenLakeEnv, self).__init__(nS, nA, P, isd)\n\n    def render(self, mode='human'):\n        outfile = StringIO() if mode == 'ansi' else sys.stdout\n\n        row, col = self.s // self.ncol, self.s % self.ncol\n        desc = self.desc.tolist()\n        desc = [[c.decode('utf-8') for c in line] for line in desc]\n        desc[row][col] = utils.colorize(desc[row][col], \"red\", highlight=True)\n        if self.lastaction is not None:\n            outfile.write(\"  ({})\\n\".format([\"Left\",\"Down\",\"Right\",\"Up\"][self.lastaction]))\n        else:\n            outfile.write(\"\\n\")\n        outfile.write(\"\\n\".join(''.join(line) for line in desc)+\"\\n\")\n\n        if mode != 'human':\n            with closing(outfile):\n                return outfile.getvalue()\n</code></pre>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#policy-evaluation","title":"Policy Evaluation","text":""},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#transition-probability-function","title":"Transition Probability Function","text":"<ul> <li>\\(\\mathcal{P}_{ss'}^a = \\mathcal{P}(s' \\vert s, a)  = \\mathbb{P} [S_{t+1} = s' \\vert S_t = s, A_t = a]\\)</li> </ul>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#deterministic-environment","title":"Deterministic Environment","text":"<ul> <li>There's no probability distribution, if you decide to go left you'll go left</li> <li>Hence in this example, given <code>current_state = 8</code> and <code>action = 0</code> which is left, we will end up with <code>probability = 1</code> in <code>new_state = 9</code></li> </ul> <pre><code># Deterministic\nenv = FrozenLakeEnv(is_slippery=False)\n\ncurrent_state = 10  # State from S_n=16 State space\naction = 0  # Left action from A_n=4 Action space\n[(probability, new_state, reward, done)] = env.P[current_state][action]\n\nprint('Probability {}, New State {}'.format(probability, new_state))\n</code></pre> <pre><code>Probability 1.0, New State 9\n</code></pre>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#stochastic-environment","title":"Stochastic Environment","text":"<ul> <li>Given \\(S_t = 10, A_t = 0\\) in a stochastic environment, the transition probability functions indicate you can end up in grid 6, 9, 14 each with \u2153 probability:<ul> <li>\\(\\mathbb{P} [S_{t+1} = 6 \\vert S_t = 10, A_t = 0] = \\frac{1}{3}\\)</li> <li>\\(\\mathbb{P} [S_{t+1} = 9 \\vert S_t = 10, A_t = 0] = \\frac{1}{3}\\)</li> <li>\\(\\mathbb{P} [S_{t+1} = 14 \\vert S_t = 10, A_t = 0] = \\frac{1}{3}\\)</li> </ul> </li> </ul> <pre><code># Stochastic\nenv = FrozenLakeEnv(is_slippery=True)\n\ncurrent_state = 10  # State from S_n=16 State space\naction = 0  # Left action from A_n=4 Action space\nenv.P[current_state][action]\n</code></pre> <pre><code>[(0.3333333333333333, 6, 0.0, False),\n (0.3333333333333333, 9, 0.0, False),\n (0.3333333333333333, 14, 0.0, False)]\n</code></pre>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#random-policy-function","title":"Random Policy Function","text":"<p>Random Policy function</p> <pre><code># Random policy generation\ndef generate_random_policy(S_n, A_n):\n    # return np.random.randint(A_n, size=(S_n, A_n))\n    return np.ones([S_n, A_n]) / A_n\n\n# Given the total number of states S_n = 16\n# For each state out of 16 states, we can take 4 actions\n# Since this is a stochastic environment, we'll initialize a policy to have equal probabilities 0.25 of doing each action each state\npolicy = generate_random_policy(S_n, A_n)\nprint(policy.shape)\n</code></pre> <pre><code>(16, 4)\n</code></pre> <p>Policy plot</p> <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.figure(figsize=(5, 16))\nsns.heatmap(policy,  cmap=\"YlGnBu\", annot=True, cbar=False);\n</code></pre> <p></p>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#policy-evaluation-function-comprising-state-value-function","title":"Policy Evaluation Function comprising State-value Function","text":"<ul> <li>How: \\(\\mathcal{V}_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a | s) \\sum_{s' \\in \\mathcal{S}}  \\mathcal{P}_{ss'}^a \\big[\\mathcal{R}_s^a + \\gamma  {V}_{\\pi}(s')\\big]\\)<ul> <li>Simple code equation:<ul> <li>Values of state given policy = sum ( action probability * transition probability * [reward + discount * value of new state] )</li> </ul> </li> </ul> </li> <li>Aim: getting state-values</li> </ul> <pre><code>import numpy as np\n\ndef policy_evaluation(env, policy, gamma=1., theta=1e-8):\nr\"\"\"Policy evaluation function. Loop until state values stable, delta &lt; theta.\n\n    Returns V comprising values of states under given policy.\n\n    Args:\n        env (gym.env): OpenAI environment class instantiated and assigned to an object.\n        policy (np.array): policy array to evaluate\n        gamma (float): discount rate for rewards\n        theta (float): tiny positive number, anything below it indicates value function convergence\n    \"\"\"\n    # 1. Create state-value array (16,)\n    V = np.zeros(S_n)\n    while True:\n        delta = 0\n\n        # 2. Loop through states\n        for s in range(S_n):\n            Vs = 0\n\n            # 2.1 Loop through actions for the unique state\n            # Given each state, we've 4 actions associated with different probabilities\n            # 0.25 x 4 in this case, so we'll be looping 4 times (4 action probabilities) at each state\n            for a, action_prob in enumerate(policy[s]):\n                # 2.1.1 Loop through to get transition probabilities, next state, rewards and whether the game ended\n                for prob, next_state, reward, done in env.P[s][a]:\n                    # State-value function to get our values of states given policy\n                    Vs += action_prob * prob * (reward + gamma * V[next_state])\n\n            # This simple equation allows us to stop this loop when we've converged\n            # How do we know? The new value of the state is smaller than a tiny positive value we set\n            # State value change is tiny compared to what we have so we just stop!\n            delta = max(delta, np.abs(V[s]-Vs))\n\n            # 2.2 Update our state value for that state\n            V[s] = Vs\n\n        # 3. Stop policy evaluation if our state values changes are smaller than our tiny positive number\n        if delta &lt; theta:\n            break\n\n    return V\n\n# Generate random policy with equal probabilities of each action given any state\nrand_policy = generate_random_policy(S_n, A_n)\n\n# Evaluate the policy to get state values\nV = policy_evaluation(env, rand_policy)\n\n# Plot heatmap\nplt.figure(figsize=(8, 8))\nsns.heatmap(V.reshape(4, 4),  cmap=\"YlGnBu\", annot=True, cbar=False);\n</code></pre> <pre><code># This is our environment\n# Notice how the state values near the goal have higher values?\n# Those with \"H\" = hole, where you die if you step, have 0 values indicating those are bad areas to be in\nenv.render()\n</code></pre> <pre><code>[S]FFF\nFHFH\nFFFH\nHFFG\n</code></pre>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#policy-improvement","title":"Policy Improvement","text":""},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#action-value-q-value-function-from-state-value-function","title":"Action-value (Q-value) function from State-value function","text":"<ul> <li>How: \\(\\mathcal{Q}_{\\pi}(s, a) = \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\big[ \\mathcal{R}_s^a + \\gamma  \\mathcal{V}_{\\pi}(s') \\big]\\)<ul> <li>Code equation<ul> <li>Values of action = sum ( transition probability * [reward + discount * value of next state] )</li> </ul> </li> </ul> </li> <li>Aim: getting q-values (action-values)</li> </ul> <pre><code>def q_value(env, V, s, gamma=1):\nr\"\"\"Q-value (action-value) function from state-value function\n\n    Returns Q values, values of actions.\n\n    Args:\n        env (gym.env): OpenAI environment class instantiated and assigned to an object.\n        V (np.array): array of state-values obtained from policy evaluation function.\n        s (integer): integer representing current state in the gridworld\n        gamma (float): discount rate for rewards.\n    \"\"\"\n    # 1. Create q-value array for one state\n    # We have 4 actions, so let's create an array with the size of 4\n    q = np.zeros(A_n)\n\n    # 2. Loop through each action\n    for a in range(A_n):\n        # 2.1 For each action, we've our transition probabilities, next state, rewards and whether the game ended\n        for prob, next_state, reward, done in env.P[s][a]:\n            # 2.1.1 Get our action-values from state-values\n            q[a] += prob * (reward + gamma * V[next_state])\n\n    # Return action values\n    return q\n\n# For every state, we've 4 actions, hence we've 16 x 4 q values\nQ = np.zeros([S_n, A_n])\n\n# Loop through each state out of 16\n# For each state, we will get the 4 q-values associated with the 4 actions\nfor s in range(env.nS):\n    Q[s] = q_value(env, V, s)\n\nplt.figure(figsize=(5, 16))\nsns.heatmap(Q,  cmap=\"YlGnBu\", annot=True, cbar=False);\n</code></pre> <pre><code># Notice how 13/14, those in the last row of the gridworld just before reaching the goal of finishing the game, their action values are large?\nenv.render()\n</code></pre> <pre><code>[S]FFF\nFHFH\nFFFH\nHFFG\n</code></pre>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#policy-improvement-function","title":"Policy Improvement Function","text":"<ul> <li>How: maximizing q-values per state by choosing actions with highest q-values</li> <li>Aim: get improved policy</li> </ul> <pre><code>def policy_improvement(env, V, gamma=1.):\nr\"\"\"Function to improve the policy by utilizing state values and action (q) values.\n\n    Args:\n        env (gym.env): OpenAI environment class instantiated and assigned to an objects\n        V (np.array): array of state-values obtained from policy evaluation function\n        gamma (float): discount of rewards\n    \"\"\"\n    # 1. Blank policy\n    policy = np.zeros([env.nS, env.nA]) / env.nA\n\n    # 2. For each state in 16 states\n    for s in range(env.nS):\n\n        # 2.1 Get q values: q.shape returns (4,)\n        q = q_value(env, V, s, gamma)\n\n        # 2.2 Find best action based on max q-value\n        # np.argwhere(q==np.max(q)) gives the position of largest q value\n        # given array([0.00852356, 0.01163091, 0.0108613 , 0.01550788]), this would return array([[3]]) of shape (1, 1)\n        # .flatten() reduces the shape to (1,) where we've array([3])\n        best_a = np.argwhere(q==np.max(q)).flatten()\n\n        # 2.3 One-hot encode best action and store into policy array's row for that state\n        # In our case where the best action is array([3]), this would return\n        # array([0., 0., 0., 1.]) where position 3 is the best action\n        # Now we can store the best action into our policy\n        policy[s] = np.sum([np.eye(env.nA)[i] for i in best_a], axis=0)/len(best_a)\n\n    return policy\n\n\nnew_policy = policy_improvement(env, V)\n\nplt.figure(figsize=(5, 16))\nsns.heatmap(new_policy,  cmap=\"YlGnBu\", annot=True, cbar=False);\n</code></pre> <pre><code># Compared to this equiprobable policy, the one above is making some improvements by maximizing q-values per state\nplt.figure(figsize=(5, 16))\nsns.heatmap(rand_policy,  cmap=\"YlGnBu\", annot=True, cbar=False);\n</code></pre>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#policy-iteration-function","title":"Policy Iteration Function","text":"<ul> <li>How: loop through policy evaluation (get state-values) and policy improvement functions (use state-values to calculate q-values to improve policy) until optimal policy obtained</li> <li>Aim: improve policy until convergence<ul> <li>Convergence: difference of state values between old and new policies is very small (less than theta, a very small positive number)</li> </ul> </li> </ul> <pre><code>import copy\ndef policy_iteration(env, gamma=1, theta=1e-8):\n    # 1. Create equiprobable policy where every state has 4 actions with equal probabilities as a starting policy\n    policy = np.ones([env.nS, env.nA]) / env.nA\n\n    # 2. Loop through policy_evaluation and policy_improvement functions\n    while True:\n        # 2.1 Get state-values\n        V = policy_evaluation(env, policy, gamma, theta)\n\n        # 2.2 Get new policy by getting q-values and maximizing q-values per state to get best action per state\n        new_policy = policy_improvement(env, V)\n\n        # 2.3 Stop if the value function estimates for successive policies has converged\n        if np.max(abs(policy_evaluation(env, policy) - policy_evaluation(env, new_policy))) &lt; theta * 1e2:\n            break;\n\n        # 2.4 Replace policy with new policy\n        policy = copy.copy(new_policy)\n    return policy, V\n\n# obtain the optimal policy and optimal state-value function\npolicy_pi, V_pi = policy_iteration(env)\n\n# Optimal policy (pi) \n# LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3\nplt.figure(figsize=(5, 16))\nsns.heatmap(policy_pi,  cmap=\"YlGnBu\", annot=True, cbar=False, square=True);\n</code></pre> <pre><code># State values\nplt.figure(figsize=(8, 8))\nsns.heatmap(V_pi.reshape(4, 4),  cmap=\"YlGnBu\", annot=True, cbar=False, square=True);\n</code></pre> <pre><code># State values without policy improvement, just evaluation\nplt.figure(figsize=(8, 8))\nsns.heatmap(V.reshape(4, 4),  cmap=\"YlGnBu\", annot=True, cbar=False);\n</code></pre>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#value-iteration","title":"Value iteration","text":"<ul> <li>Alternative to policy iteration</li> <li>How: loop through to find optimal value function then get one-off policy</li> <li>Aim: improve value function until convergence<ul> <li>Convergence: until difference in new and old state values are small (smaller than theta, small positive number)</li> </ul> </li> </ul> <pre><code>def value_iteration(env, gamma=1, theta=1e-8):\n    # 1. Create state values of shape (16,)\n    V = np.zeros(env.nS)\n\n    # 2. Loop through q-value function until convergence\n    while True:\n        delta = 0\n\n        # 2.1 Loop through each state\n        for s in range(env.nS):\n            # 2.2 Archive old state value\n            v = V[s]\n\n            # 2.3 New state value = max of q-value\n            V[s] = max(q_value(env, V, s, gamma))\n\n            delta = max(delta, abs(V[s] - v))\n\n        # 2.2 If state value changes small, converged\n        if delta &lt; theta:\n            break\n\n    # 3. Extract one-off policy with optimal state values\n    policy = policy_improvement(env, V, gamma)\n\n    return policy, V\n\npolicy_vi, V_vi = value_iteration(env)\n\n# Optimal policy\n# LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3\nplt.figure(figsize=(5, 16))\nsns.heatmap(policy_vi,  cmap=\"YlGnBu\", annot=True, cbar=False, square=True);\n</code></pre> <pre><code># State values\nplt.figure(figsize=(8, 8))\nsns.heatmap(V_vi.reshape(4, 4),  cmap=\"YlGnBu\", annot=True, cbar=False, square=True);\n</code></pre>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/supervised_to_rl/","title":"Supervised Learning to Reinforcement Learning","text":""},{"location":"deep_learning/deep_reinforcement_learning_pytorch/supervised_to_rl/#supervised-learning","title":"Supervised Learning","text":"<ul> <li>The tasks we've covered so far fall under the category of supervised learning<ul> <li>Before, we have gone through 2 major tasks: classification and regression with labels</li> <li>Classification: we've a number of MNIST images, we take them as input and we use a neural network for a classification task where we use the ground truth (whether the digits are 0-9) to construct our cross entropy loss<ul> <li>Classification loss function (cross entropy loss): \\(- \\sum^K_1 L_i log(S_i)\\)<ul> <li>\\(K\\): number of classes</li> <li>\\(L_i\\): ground truth (label/target) of i-th class</li> <li>\\(S_i\\): output of softmax for i-th class</li> </ul> </li> </ul> </li> <li>Regression: alternatively we go through a regression task of say, predicting a time-series, but we still have the ground truth we use to construct our loss function<ul> <li>Regression loss function (mean squared error): \\(\\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i)^2\\)<ul> <li>\\(\\hat{y}\\): prediction</li> <li>\\(y\\): ground truth (label/target)</li> </ul> </li> </ul> </li> </ul> </li> <li>The key emphasis here is that we have mainly gone through supervised learning tasks that requires labels. Without them, we would not be able to properly construct our loss functions for us to do 2 critical steps (1) backpropagate to get our gradients and (2) gradient descent to update our weights with our gradients</li> </ul> <p>Loss functions</p> <p>We have covered 2 basic loss functions such as cross entropy loss (classification task) and mean squared error (regression task).</p> <p>However there are many more loss functions for both classification and regression task that will be covered in a separate section. </p> <p>For example, there are alternatives to mean squared error (MSE) like mean absolute error (MAE or L1 Loss), Smooth L1 Loss (less sensitive to outliers), quantile regression loss function (allowing confidence intervals) and many more.</p>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/supervised_to_rl/#reinforcement-learning-is-not-supervised-learning","title":"Reinforcement learning is not supervised learning","text":"<ul> <li>One difference is that there is no ground truth (label/target)<ul> <li>There is typically no label as to what is the definitively right prediction, we have to explore to find out what's \"right\" (essentially, the best possible prediction)</li> </ul> </li> <li>Instead of minimizing a loss function comprising a target and a prediction (as with supervised learning), in reinforcement learning we are typically concerned with maximizing our reward function by trying different actions and exploring what those actions yield in an environment<ul> <li>Let's use a simple game example of driving and not colliding with puppies crossing the road<ul> <li>Agent<ul> <li>Driver</li> </ul> </li> <li>Environment<ul> <li>3 lane road, puppies crossing and the agent</li> </ul> </li> <li>States<ul> <li>Left, center or right lane</li> </ul> </li> <li>Actions<ul> <li>To move from one state to another</li> <li>Turn left, center or right</li> </ul> </li> <li>Reward<ul> <li>Feedback on whether action is good/bad, essentially the goal of the problem</li> <li>Colliding with the puppy: -10 points</li> <li>Too close to the puppy (scares the puppy): -2 points</li> <li>Safe distance from the puppy: 10 points</li> </ul> </li> <li>Value function<ul> <li>Defines what is good in the long-run as compared to rewards which is immediate after an action takes the agent to another state</li> <li>It's somewhat the discounted sum of the rewards the agent is expected to get</li> </ul> </li> <li>Policy<ul> <li>This defines how the agent acts in its states</li> </ul> </li> <li>In this case, the agent might first collide with the puppy and learn it's bad (-10 points), then try not collide as the second action and still learn it's bad to be too close (-2 points) and finally as the third action learn to steer clear of puppies (+10 points) as it yields the largest reward<ul> <li>Gradually it'll learn to drive at a safe distance from puppies to collect points (+10 points for safe distance)</li> <li>To do this, the agent needs to go try different actions and learn from its mistakes (trial and error), attempting to maximize its long-term</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/supervised_to_rl/#2-distinguishing-properties-of-reinforcement-learning","title":"2 Distinguishing Properties of reinforcement learning","text":"<ul> <li>Essentially, 2 distinguishing properties of reinforcement learning are: <sup>1</sup><ul> <li>(1) \"Trial-and-error search\"</li> <li>(2) \"Delayed reward\"</li> </ul> </li> </ul> <p>In the next section, we'll be covering the terms we'll dive into these key terms through the lens of Markov Decision Processes (MDPs) and Bellman Equations.</p> <ol> <li> <p>Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction; 2<sup>nd</sup> Edition. 2017.\u00a0\u21a9</p> </li> </ol>"},{"location":"deep_learning/fromscratch/fromscratch_cnn/","title":"CNN from Scratch","text":"<p>This is an implementation of a simple CNN (one convolutional function, one non-linear function, one max pooling function, one affine function and one softargmax function) for a 10-class MNIST classification task.</p>"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#data-pull","title":"Data Pull","text":""},{"location":"deep_learning/fromscratch/fromscratch_cnn/#imports","title":"Imports","text":"<pre><code>from sklearn.datasets import fetch_openml\nfrom sklearn.utils import check_random_state\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport torch\n%matplotlib inline\n</code></pre>"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#load-data","title":"Load Data","text":"<pre><code># Load data from https://www.openml.org/d/554\nX, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n</code></pre> <pre><code>### Plot Single Sample\n</code></pre> <pre><code># Print dataset\nprint(f'X shape: {X.shape}')\nprint(f'y shape: {y.shape}')\n\n# Print size of resized single sample\nsingle_sample = X[0, :].reshape(28, 28)\nprint(f'Single sample shape: {single_sample.shape}')\n\n# Plot single sample (M x N matrix)\nplt.gray()\nplt.matshow(single_sample)\nplt.show()\n</code></pre> <pre><code>X shape: (70000, 784)\ny shape: (70000,)\nSingle sample shape: (28, 28)\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n</code></pre>"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#train-test-data-split","title":"Train-Test Data Split","text":"<pre><code># Train-test split\ntrain_samples = 60000\nrandom_state = check_random_state(0)\npermutation = random_state.permutation(X.shape[0])\nX = X[permutation]\ny = y[permutation]\nX = X.reshape((X.shape[0], -1))\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_samples, test_size=10000)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Print shapes\nprint(f'Training shape {X_train.shape}')\nprint(f'Testing shape {X_test.shape}')\n</code></pre> <pre><code>Training shape (60000, 784)\nTesting shape (10000, 784)\n</code></pre>"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#forwardpropagation","title":"Forwardpropagation","text":""},{"location":"deep_learning/fromscratch/fromscratch_cnn/#convolutional-layer","title":"Convolutional Layer","text":"<pre><code>class ConvolutionalLayer:\n    def __init__(self, num_kernels, kernel_shape):\n        # Number of kernels: 1D\n        self.num_kernels = num_kernels\n        # Shape of kernels: 2D\n        self.kernel_shape = kernel_shape\n        self.k = self.kernel_shape[0]\n        # Kernel weights: 3D\n        self.kernels_theta = torch.randn(self.num_kernels, self.kernel_shape[0], self.kernel_shape[1])\n\n    def slider(self, inp):\n'''\n        Sliding generator that yields square areas of shape\n        (kernel_shape, kernel_shape) sliding across our input. \n        This assumes valid padding (no padding) and step size 1.\n        '''\n        h, w = inp.shape\n        for h_idx in range(h - (self.k - 1)):\n            for w_idx in range(w - (self.k - 1)):\n                single_slide_area = inp[h_idx:(h_idx + self.k), w_idx:(w_idx + self.k)]\n                yield single_slide_area, h_idx,w_idx\n\n    def forward(self, inp):\n'''\n        Slides kernel across image doing an element-wise MM then summing.\n        Results in forward pass of convolutional layer of shape\n        (output shape, output shape, number of kernels).\n        '''\n        # Input: 2D of (height, width)\n        assert single_sample.dim() == 2, f'Input not 2D, given {single_sample.dim()}D'\n\n        # Output via Valid Padding (No Padding): 3D of (height, width, number of kernels)\n        _, w  = inp.shape\n        # P = 0\n        p = 0\n        # O = ((W - K + 2P) / S) + 1 = (28 - 3 + 0) + 1 = 25\n        o = (w - self.k) + 1\n        # Print shapes\n        print('Padding shape: \\t', p)\n        print('Output shape: \\t', o)\n        # Initialize blank tensor\n        output = torch.zeros((o, o, self.num_kernels))\n\n        # Iterate through region\n        for single_slide_area, h_idx, w_idx in self.slider(inp):\n            if h_idx == 0 and w_idx == 0:\n                print('Region shape: \\t', list(single_slide_area.shape))\n                print('Kernel shape: \\t', list(self.kernels_theta.shape))\n                print('Single Slide: \\t', list(output[h_idx, w_idx].shape))\n\n            # Sum values with each element-wise matrix multiplication across each kernel\n            # Instead of doing another loop of each kernel, you simply just do a element-wise MM\n            # of the single slide area with all the kernels yield, then summing the patch\n            output[h_idx, w_idx] = torch.sum(single_slide_area * self.kernels_theta, axis=(1, 2))\n\n        # Pass through non-linearity (sigmoid): 1 / 1 + exp(-output)\n        output = 1. / (1. + torch.exp(-output))\n\n        return output\n\n# Convert numpy array to torch tensor\nsingle_sample = X[0, :].reshape(28, 28)\nsingle_sample = torch.tensor(single_sample)\n\nprint('='*50)\nprint(f'Input shape: \\t {list(single_sample.shape)}')\nprint('='*50)\n\n# Forward: conv\nconv = ConvolutionalLayer(num_kernels=8, kernel_shape=[5, 5])\noutput = conv.forward(single_sample)\n\nprint('='*50)\nprint(f'Conv (f) shape: \\t {list(output.shape)}')\nprint('='*50)\n</code></pre> <pre><code>==================================================\nInput shape:     [28, 28]\n==================================================\nPadding shape:   0\nOutput shape:    24\nRegion shape:    [5, 5]\nKernel shape:    [8, 5, 5]\nSingle Slide:    [8]\n==================================================\nConv (f) shape:      [24, 24, 8]\n==================================================\n</code></pre>"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#max-pooling-layer","title":"Max Pooling Layer","text":"<pre><code>class MaxPoolLayer:\n    # O = ((W - K) / S) + 1\n    def __init__(self, pooling_kernel_shape):\n        # Assume simplicity of K = S then O = W / S\n        self.k = pooling_kernel_shape\n\n    def slider(self, inp):\n'''\n        Sliding generator that yields areas for max pooling.\n        '''\n        h, w, _ = inp.shape\n        output_size = int(w / self.k)  # Assume S = K\n\n        for h_idx in range(output_size):\n            for w_idx in range(output_size):\n                single_slide_area = inp[h_idx * self.k:h_idx * self.k + self.k,\n                                        w_idx * self.k:w_idx * self.k + self.k]\n                yield single_slide_area, h_idx, w_idx\n\n    def forward(self, inp):\n'''\n        Performs a forward pass of the maxpool layer using the given input.\n        Returns a 3d numpy array with dimensions (h / 2, w / 2, num_filters).\n        - input is a 3d numpy array with dimensions (h, w, num_filters)\n        '''\n        self.last_input = inp\n\n        h, w, num_kernels = inp.shape\n        output_size = int(w / self.k)  # Assume S = K\n        output = torch.zeros(output_size, output_size, num_kernels)\n\n        for single_slide_area, h_idx, w_idx in self.slider(inp):\n            single_slide_area = torch.flatten(single_slide_area, start_dim=0, end_dim=1)\n            output[h_idx, w_idx] = torch.max(single_slide_area, dim=0).values\n\n        return output\n\nprint('='*50)\nprint(f'Input shape: \\t {list(output.shape)}')\nprint('='*50)\n\n# Forward: pool\npool = MaxPoolLayer(pooling_kernel_shape=2)\noutput = pool.forward(output)\n\nprint('='*50)\nprint(f'Pool (f) shape: \\t {list(output.shape)}')\nprint('='*50)\n</code></pre> <pre><code>==================================================\nInput shape:     [24, 24, 8]\n==================================================\n==================================================\nPool (f) shape:      [12, 12, 8]\n==================================================\n</code></pre>"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#affine-and-softargmax-layer","title":"Affine and Soft(arg)max Layer","text":"<pre><code>class AffineAndSoftmaxLayer:\n    def __init__(self, affine_weight_shape):\n        self.affine_weight_shape = affine_weight_shape\n        # Weight shape: flattened input x output shape\n        self.w = torch.zeros(self.affine_weight_shape[0] * self.affine_weight_shape[1] * self.affine_weight_shape[2], self.affine_weight_shape[3])\n        self.b = torch.zeros(self.affine_weight_shape[3])\n\n        # Initialize weight/bias via Lecun initialization of 1 / N standard deviation\n        # Refer to DLW guide on weight initialization mathematical derivation:\n        # https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/\n        print(f'Lecun initialization SD: {1/self.affine_weight_shape[3]}')\n        self.w = torch.nn.init.normal_(self.w, mean=0, std=1/self.affine_weight_shape[3])\n        self.b = torch.nn.init.normal_(self.b, mean=0, std=1/self.affine_weight_shape[3])\n\n    def forward(self, inp):\n'''\n        Performs Linear (Affine) Function &amp; Soft(arg)max Function\n        that returns our vector (1D) of probabilities.\n        '''\n        inp = inp.reshape(1, -1)\n        print(f'input shape: \\t {inp.shape}')\n        print(f'weight shape: \\t {self.w.shape}')\n        print(f'bias shape: \\t {self.b.shape}')\n        logits = torch.mm(inp, self.w) + self.b\n        probas = torch.exp(logits) / torch.sum(torch.exp(logits))\n        return probas\n\n\nprint('='*50)\nprint(f'Input shape: \\t {list(output.shape)}')\nprint('='*50)\n\n# Forward: Affine and Softmax\naffinesoftmax = AffineAndSoftmaxLayer(affine_weight_shape=(output.shape[0], output.shape[1], output.shape[2], len(np.unique(y_train))))\noutput = affinesoftmax.forward(output)\n\nprint('='*50)\nprint(f'Affine &amp; Soft(arg)max (f) shape: \\t {list(output.shape)}')\nprint('='*50)\n\nprint(f'Probas: {pd.DataFrame(output.numpy()).to_string(index=False, header=False)}')\n</code></pre> <pre><code>==================================================\nInput shape:     [12, 12, 8]\n==================================================\nLecun initialization SD: 0.1\ninput shape:     torch.Size([1, 1152])\nweight shape:    torch.Size([1152, 10])\nbias shape:      torch.Size([10])\n==================================================\nAffine &amp; Soft(arg)max (f) shape:     [1, 10]\n==================================================\nProbas:  0.72574  0.001391  0.000083  0.202983  0.000503  0.037023  0.000428  0.000144  0.031293  0.000412\n</code></pre> <p>Dot Product, Matrix Multiplication, and Hadamard Product</p> <p>Hadamard product: element-wise multiplicaton of 2 matrices. Matrix Multiplication: take the first row of the first matrix and perform dot product with each of the N columns in the second matrix to form N columns in the first row of the new matrix. Repeat for remaining rows for the first matrix.    </p> <pre><code># Backward &amp; GD: linear + softmax\n\n# Backward &amp; GD: pool\n\n# Backward &amp; GD: conv\n</code></pre>"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#note-on-contiguous-vs-non-contiguous","title":"Note on Contiguous vs Non-Contiguous","text":"<ul> <li>Often in the space of passing tensors, and doing all our dot products, hadamard products, matrix multiplications, transpose, reshape operations and more, you would inevitably one day encounter into an error that says your tensor is <code>not contiguous</code>. </li> <li>This is a memory allocation problem. </li> <li>Certain tensor operations like <code>transpose, view, expand, narrow etc</code> do not change the original tensor, instead they modify the properties of the tensor. <ul> <li>For example, <code>transpose</code>, demonstrated here, would change the shape (index), but both the old and modified property tensor share the same memory block with different indexes/addresses.</li> <li>This is why it's non-contiguous and we need to make it contiguous for some operations and typically for efficiency purpose.</li> </ul> </li> <li>This is not a blanket statement, but you typically want your tensors to be contiguous as it prevents additional overhead incurred from translating addresses.<ul> <li>Whenever there's a warning that prompts you the tensor is not contiguous, just call <code>.contiguous()</code> and you typically should be good to go.</li> </ul> </li> </ul>"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#contiguous-10-by-5-tensor","title":"Contiguous 10 by 5 tensor","text":"<pre><code>contiguous_tensor = torch.arange(50).view(10, 5)\nprint(contiguous_tensor.shape)\n# Pretty print quick hack via Pandas DataFrame\nprint(pd.DataFrame(contiguous_tensor.numpy()).to_string(index=False, header=False))\n</code></pre> <pre><code>torch.Size([10, 5])\n  0   1   2   3   4\n  5   6   7   8   9\n 10  11  12  13  14\n 15  16  17  18  19\n 20  21  22  23  24\n 25  26  27  28  29\n 30  31  32  33  34\n 35  36  37  38  39\n 40  41  42  43  44\n 45  46  47  48  49\n</code></pre>"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#stride-5-by-1","title":"Stride 5 by 1","text":"<ul> <li>Stride here shows how we need:<ul> <li>5 steps to move one row to the next &amp;</li> <li>1 step to move from one column to the next (contiguous)<ul> <li>If this becomes anything other than 1 step, it becomes not contiguous</li> </ul> </li> </ul> </li> </ul> <pre><code>print(contiguous_tensor.stride())\nprint(contiguous_tensor.is_contiguous())\n</code></pre> <pre><code>(5, 1)\nTrue\n</code></pre>"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#non-contiguous-tensor-via-transpose-operation","title":"Non-contiguous Tensor via Transpose Operation","text":"<ul> <li>In order to access the next \"column\" value of 5, we have to take 5 steps despite the transpose as if we would do in our original tensor</li> <li>Because the original tensor and this transposed tensor share the same memory block!</li> </ul> <pre><code>non_contiguous_tensor = contiguous_tensor.t()\nprint(non_contiguous_tensor.shape)\nprint(pd.DataFrame(non_contiguous_tensor.numpy()).to_string(index=False, header=False))\n</code></pre> <pre><code>torch.Size([5, 10])\n 0  5  10  15  20  25  30  35  40  45\n 1  6  11  16  21  26  31  36  41  46\n 2  7  12  17  22  27  32  37  42  47\n 3  8  13  18  23  28  33  38  43  48\n 4  9  14  19  24  29  34  39  44  49\n</code></pre>"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#stride-1-by-5","title":"Stride 1 by 5","text":"<pre><code>print(non_contiguous_tensor.stride())\nprint(non_contiguous_tensor.is_contiguous())\n</code></pre> <pre><code>(1, 5)\nFalse\n</code></pre>"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#convert-to-contiguous","title":"Convert to Contiguous","text":"<pre><code># Convert to contiguous\nconvert_contiguous = non_contiguous_tensor.contiguous()\nprint(convert_contiguous.is_contiguous())\n</code></pre> <pre><code>True\n</code></pre>"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/","title":"Logistic Regression from Scratch","text":"<p>This is an implementation of a simple logistic regression for binary class labels. We will be attempting to classify 2 flowers based on their petal width and height: setosa and versicolor.</p>"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#imports","title":"Imports","text":"<pre><code>%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport torch\nimport torch.nn.functional as F\n\nfrom sklearn import datasets\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\n</code></pre>"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#preparing-a-custom-2-class-iris-dataset","title":"Preparing a custom 2-class IRIS dataset","text":""},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#load-data","title":"Load Data","text":"<pre><code># Instantiate dataset class and assign to object\niris = datasets.load_iris()\n\n# Load features and target\n# Take only 2 classes, and 2 features (sepal length/width)\nX = iris.data[:-50, :2]\n# For teaching the math rather than preprocessing techniques,\n# we'll be using this simple scaling method. However, you must\n# be cautious to scale your training/testing sets subsequently.\nX = preprocessing.scale(X)\ny = iris.target[:-50] \n</code></pre>"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#print-data-details","title":"Print Data Details","text":"<pre><code># 50 of each iris flower\nprint(Counter(y))\n\n# Type of flower\nprint(list(iris.target_names[:-1]))\n\n# Shape of features\nprint(X.shape)\n</code></pre> <pre><code>Counter({0: 50, 1: 50})\n['setosa', 'versicolor']\n(100, 2)\n</code></pre>"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#scatterplot-2-classes","title":"Scatterplot 2 Classes","text":"<pre><code>plt.scatter(X[:, 0], X[:, 1], c=y);\n</code></pre>"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#traintest-split","title":"Train/Test Split","text":"<pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nprint(f'X train size: {X_train.shape}')\nprint(f'X test size: {X_test.shape}')\nprint(f'y train size: {y_train.shape}')\nprint(f'y test size: {y_test.shape}')\n\n# Distribution of both classes are roughly equal using train_test_split function\nprint(Counter(y_train))\n</code></pre> <pre><code>X train size: (80, 2)\nX test size: (20, 2)\ny train size: (80,)\ny test size: (20,)\nCounter({0: 41, 1: 39})\n</code></pre>"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#math","title":"Math","text":""},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#1-forwardpropagation","title":"1. Forwardpropagation","text":"<ul> <li>Get our logits and probabilities</li> <li>Affine function/transformation: \\(z = \\theta x + b\\)</li> <li>Sigmoid/logistic function: \\(\\hat y = \\frac{1}{1 + e^{-z}}\\)</li> </ul>"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#2-backwardpropagation","title":"2. Backwardpropagation","text":"<ul> <li>Calculate gradients / partial derivatives w.r.t. weights and bias</li> <li>Loss: \\(L = ylog(\\hat y) + (1-y) log (1 - \\hat y)\\)</li> <li>Partial derivative of loss w.r.t weights: \\(\\frac{\\delta L}{\\delta w} =\\frac{\\delta L}{\\delta z} \\frac{\\delta z}{\\delta w} = (\\hat y - y)(x^T)\\)</li> <li>Partial derivative of loss w.r.t. bias: \\(\\frac{\\delta L}{\\delta b} = \\frac{\\delta L}{\\delta z} \\frac{\\delta z}{\\delta b} = (\\hat y - y)(1)\\)<ul> <li>\\(\\frac{\\delta L}{\\delta z} = \\hat y - y\\)</li> <li>\\(\\frac{\\delta z}{\\delta w} = x\\)</li> <li>\\(\\frac{\\delta z}{\\delta b} = 1\\)</li> </ul> </li> </ul>"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#2a-loss-function-clarification","title":"2a. Loss function clarification","text":"<ul> <li>Actually, why is our loss equation \\(L = ylog(\\hat y) + (1-y) log (1 - \\hat y)\\)?<ul> <li>We have given the intuition in the Logistic Regression tutorial on why it works.</li> <li>Here we will cover the derivation which essentially is merely maximizing the log likelihood of the parameters (maximizing the probability of our predicted output given our input and parameters</li> <li>Given:<ul> <li>\\(\\hat y = \\frac{1}{1 + e^{-z}}\\).</li> </ul> </li> <li>Then:<ul> <li>\\(P(y=1 \\mid x;\\theta) = \\hat y\\)</li> <li>\\(P(y=0 \\mid x;\\theta) = 1 - \\hat y\\)</li> </ul> </li> <li>Simplified further:<ul> <li>\\(p(y \\mid x; \\theta) = (\\hat y)^y(1 - \\hat y)^{1-y}\\)</li> </ul> </li> <li>Given m training samples, the likelihood of the parameters is simply the product of probabilities:<ul> <li>\\(L(\\theta) = \\displaystyle \\prod_{i=1}^{m} p(y^i \\mid x^i; \\theta)\\)</li> <li>\\(L(\\theta) = \\displaystyle \\prod_{i=1}^{m} (\\hat y^{i})^{y^i}(1 - \\hat y^{i})^{1-y^{i}}\\)</li> <li>Essentially, we want to maximize the probability of our ouput given our input and parameters</li> </ul> </li> <li>But it's easier to maximize the log likelihood, so we take the natural logarithm. <ul> <li>\\(L(\\theta) = \\displaystyle \\sum_{i=1}^{m} y^{i}log (\\hat y^{i}) + (1 - y^{i})log(1 - \\hat y^{i})\\)</li> </ul> </li> <li>Why is is easier to maximize the log likelihood?<ul> <li>The natural logarithm is a function that monotonically increases.</li> <li>This allows us to find the \"max\" of the log likelihood easier compared to a non-monotonically increasing function (like a wave up and down).</li> </ul> </li> </ul> </li> </ul>"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#3-gradient-descent-updating-weights","title":"3. Gradient descent: updating weights","text":"<ul> <li>\\(w = w - \\alpha (\\hat y - y)(x^T)\\)</li> <li>\\(b = b - \\alpha (\\hat y - y).1\\)</li> </ul>"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#training-from-scratch","title":"Training from Scratch","text":"<pre><code>learning_rate = 0.1\nnum_features = X.shape[1]\nweights = torch.zeros(num_features, 1, dtype=torch.float32)\nbias = torch.zeros(1, dtype=torch.float32)\n\nX_train = torch.from_numpy(X_train).type(torch.float32)\ny_train = torch.from_numpy(y_train).type(torch.float32)\n\nfor epoch in range(num_epochs):        \n    # 1. Forwardpropagation:\n    # 1a. Affine Transformation: z = \\theta x + b\n    z = torch.add(torch.mm(X_train, weights), bias)\n    # 2a. Sigmoid/Logistic Function: y_hat = 1 / (1 + e^{-z})\n    y_hat = 1. / (1. + torch.exp(-z))\n\n    # Backpropagation:\n    # 1. Calculate binary cross entropy \n    l = torch.mm(-y_train.view(1, -1), torch.log(y_hat)) - torch.mm((1 - y_train).view(1, -1), torch.log(1 - y_hat))\n\n    # 2. Calculate dl/dz\n    dl_dz = y_train - y_hat.view(-1)\n\n    # 2. Calculate partial derivative of cost w.r.t weights (gradients)\n    # dl_dw = dl_dz dz_dw = (y_hat - y)(x^T)\n    grad = torch.mm(X_train.transpose(0, 1), dl_dz.view(-1, 1))\n\n    # Gradient descent:\n    # update our weights and bias with our gradients\n    weights += learning_rate * grad\n    bias += learning_rate * torch.sum(dl_dz)\n\n    # Accuracy\n    total = y_hat.shape[0]\n    predicted = (y_hat &gt; 0.5).float().squeeze()\n    correct = (predicted == y_train).sum()\n    acc = 100 * correct / total \n\n    # Print accuracy and cost\n    print(f'Epoch: {epoch} | Accuracy: {acc.item() :.4f} | Cost: {l.item() :.4f}')\n\nprint(f'Weights \\n {weights.data}')\nprint(f'Bias \\n {bias.data}')\n</code></pre> <pre><code>X train size: (80, 2)\nX test size: (20, 2)\ny train size: (80,)\ny test size: (20,)\nCounter({1: 41, 0: 39})\nEpoch: 0 | Accuracy: 48.0000 | Cost: 55.4518\nEpoch: 1 | Accuracy: 100.0000 | Cost: 5.6060\nEpoch: 2 | Accuracy: 100.0000 | Cost: 5.0319\nEpoch: 3 | Accuracy: 100.0000 | Cost: 4.6001\nEpoch: 4 | Accuracy: 100.0000 | Cost: 4.2595\nEpoch: 5 | Accuracy: 100.0000 | Cost: 3.9819\nEpoch: 6 | Accuracy: 100.0000 | Cost: 3.7498\nEpoch: 7 | Accuracy: 100.0000 | Cost: 3.5521\nEpoch: 8 | Accuracy: 100.0000 | Cost: 3.3810\nEpoch: 9 | Accuracy: 100.0000 | Cost: 3.2310\nEpoch: 10 | Accuracy: 100.0000 | Cost: 3.0981\nEpoch: 11 | Accuracy: 100.0000 | Cost: 2.9794\nEpoch: 12 | Accuracy: 100.0000 | Cost: 2.8724\nEpoch: 13 | Accuracy: 100.0000 | Cost: 2.7754\nEpoch: 14 | Accuracy: 100.0000 | Cost: 2.6869\nEpoch: 15 | Accuracy: 100.0000 | Cost: 2.6057\nEpoch: 16 | Accuracy: 100.0000 | Cost: 2.5308\nEpoch: 17 | Accuracy: 100.0000 | Cost: 2.4616\nEpoch: 18 | Accuracy: 100.0000 | Cost: 2.3973\nEpoch: 19 | Accuracy: 100.0000 | Cost: 2.3374\nWeights \n tensor([[ 4.9453],\n        [-3.6849]])\nBias \n tensor([0.5570])\n</code></pre>"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#inference","title":"Inference","text":"<pre><code># Port to tensors\nX_test = torch.from_numpy(X_test).type(torch.float32)\ny_test = torch.from_numpy(y_test).type(torch.float32)\n\n# 1. Forwardpropagation:\n# 1a. Affine Transformation: z = ax + b\nz = torch.add(torch.mm(X_test, weights), bias)\n# 2a. Sigmoid/Logistic Function: y_hat = 1 / (1 + e^{-z})\ny_hat = 1. / (1. + torch.exp(-z))\n\ntotal = y_test.shape[0]\npredicted = (y_hat &gt; 0.5).float().squeeze()\ncorrect = (predicted == y_test).sum()\nacc = 100 * correct / total \n\n# Print accuracy\nprint(f'Validation Accuracy: {acc.item() :.4f}')\n</code></pre> <pre><code>Validation Accuracy: 100.0000\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_autoencoder/","title":"Autoencoders with PyTorch","text":"<p>Run Jupyter Notebook</p> <p>You can run the code for this section in this jupyter notebook link.</p>"},{"location":"deep_learning/practical_pytorch/pytorch_autoencoder/#about-autoencoders","title":"About Autoencoders","text":""},{"location":"deep_learning/practical_pytorch/pytorch_autoencoder/#feedforward-neural-network-fnn-to-autoencoders-aes","title":"Feedforward Neural Network (FNN) to Autoencoders (AEs)","text":"<ul> <li>Autoencoder is a form of unsupervised learning.<ul> <li>This is a big deviation from what we have been doing: classification and regression which are under supervised learning.</li> <li>There are no labels required, inputs are used as labels.<ul> <li>This is a bit mind-boggling for some, but there're many conrete use cases as you'll soon realize.</li> <li>Just a quick preview of use cases we will be covering:<ul> <li>Denoising overcomplete AEs: recreate images without the random noises originally injected.</li> <li>Undercomplete AEs for anomaly detection: use AEs for credit card fraud detection via anomaly detection.</li> <li>Variational AEs for creating synthetic faces: with a convolutional VAEs, we can make fake faces.</li> </ul> </li> </ul> </li> </ul> </li> <li>An autoencoder's purpose is to learn an approximation of the identity function (mapping \\(x\\) to \\(\\hat x\\)).<ul> <li>Essentially we are trying to learn a function that can take our input \\(x\\) and recreate it \\(\\hat x\\).<ul> <li>Technically we can do an exact recreation of our in-sample input if we use a very wide and deep neural network.</li> </ul> </li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_autoencoder/#undercomplete-and-overcomplete-autoencoders","title":"Undercomplete and Overcomplete Autoencoders","text":"<ul> <li>When we highlighted some use cases, did you notice how we mentioned undercomplete and autocomplete AEs?</li> <li>The only difference between the two is in the encoding output's size.<ul> <li>In the diagram above, this refers to the encoding output's size after our first affine function (yellow box) and non-linear function (pink box).</li> </ul> </li> <li>Undercomplete AEs: smaller<ul> <li>This is when our encoding output's dimension is smaller than our input's dimension.<ul> <li>Essentially we reduced the dimension of our data (dimensionality reduction) with an undercomplete AE</li> </ul> </li> </ul> </li> <li>Overcomplete AEs: larger<ul> <li>This is when our encoding output's dimension is larger than our input's dimension<ul> <li>Essentially we increased the dimension of our data with an overcomplete AE</li> </ul> </li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_autoencoder/#fully-connected-and-convolutional-autoencoders","title":"Fully-connected and Convolutional Autoencoders","text":"<ul> <li>Another important point is that, in our diagram we've used the example of our Feedforward Neural Networks (FNN) where we use fully-connected layers. <ul> <li>This is called Fully-connected AE.</li> </ul> </li> <li>However, we can easily swap those fully-connected layers with convolutional layers.<ul> <li>This is called Convolutional AE.</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_autoencoder/#autoencoders-series","title":"Autoencoders Series","text":"<ul> <li>We'll be covering a series of autoencoders in this order<ul> <li>Fully-connected Overcomplete Autoencoder (AEs): Denoising Images</li> <li>Fully-connected Undercomplete Autoencoder (AEs): Credit Card Fraud Detection</li> <li>Convolutional Overcomplete Variational Autoencoder (VAEs): Generate Fake Human Faces</li> <li>Convolutional Overcomplete Adversarial Autoencoder (AAEs): Generate Fake Human Faces</li> <li>Generative Adversarial Networks (GANs): Generate Better Fake Human Faces</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/","title":"Convolutional Neural Network with PyTorch","text":"<p>Run Jupyter Notebook</p> <p>You can run the code for this section in this jupyter notebook link.</p>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#about-convolutional-neural-network","title":"About Convolutional Neural Network","text":""},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#transition-from-feedforward-neural-network","title":"Transition From Feedforward Neural Network","text":""},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#hidden-layer-feedforward-neural-network","title":"Hidden Layer Feedforward Neural Network","text":"<p>Recap of FNN</p> <p>So let's do a recap of what we covered in the Feedforward Neural Network (FNN) section using a simple FNN with 1 hidden layer (a pair of affine function and non-linear function)</p> <ol> <li>[Yellow box] Pass input into an affine function \\(\\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b}\\)</li> <li>[Pink box] Pass logits to non-linear function, for example sigmoid, tanh (hyperbolic tangent), ReLU, or LeakyReLU</li> <li>[Blue box] Pass output of non-linear function to another affine function</li> <li>[Red box] Pass output of final affine function to softmax function to get our probability distribution over K classes</li> <li>[Purple box] Finally we can get our loss by using our cross entropy function </li> </ol> <p></p>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#basic-convolutional-neural-network-cnn","title":"Basic Convolutional Neural Network (CNN)","text":"<ul> <li>A basic CNN just requires 2 additional layers!<ul> <li>Convolution and pooling layers before our feedforward neural network</li> </ul> </li> </ul> <p>Fully Connected (FC) Layer</p> <p>A layer with an affine function &amp; non-linear function is called a Fully Connected (FC) layer</p> <p></p>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#one-convolutional-layer-high-level-view","title":"One Convolutional Layer: High Level View","text":""},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#one-convolutional-layer-high-level-view-summary","title":"One Convolutional Layer: High Level View Summary","text":"<ul> <li>As the kernel is sliding/convolving across the image \\(\\rightarrow\\) 2 operations done per patch<ol> <li>Element-wise multiplication</li> <li>Summation</li> </ol> </li> <li>More kernels \\(=\\) more feature map channels<ul> <li>Can capture more information about the input</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#multiple-convolutional-layers-high-level-view","title":"Multiple Convolutional Layers: High Level View","text":""},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#pooling-layer-high-level-view","title":"Pooling Layer: High Level View","text":"<ul> <li>2 Common Types<ul> <li>Max Pooling</li> <li>Average Pooling</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#multiple-pooling-layers-high-level-view","title":"Multiple Pooling Layers: High Level View","text":""},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#padding","title":"Padding","text":""},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#padding-summary","title":"Padding Summary","text":"<ul> <li>Valid Padding (No Padding)<ul> <li>Output size &lt; Input Size</li> </ul> </li> <li>Same Padding (Zero Padding)<ul> <li>Output size = Input Size</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#dimension-calculations","title":"Dimension Calculations","text":"<ul> <li>\\(O = \\frac {W - K + 2P}{S} + 1\\)<ul> <li>\\(O\\): output height/length</li> <li>\\(W\\): input height/length</li> <li>\\(K\\): filter size (kernel size)</li> <li>\\(P\\): padding<ul> <li>\\(P = \\frac{K - 1}{2}\\)</li> </ul> </li> <li>\\(S\\): stride</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#example-1-output-dimension-calculation-for-valid-padding","title":"Example 1: Output Dimension Calculation for Valid Padding","text":"<ul> <li>\\(W = 4\\)</li> <li>\\(K = 3\\)</li> <li>\\(P = 0\\)</li> <li>\\(S = 1\\)</li> <li>\\(O = \\frac {4 - 3 + 2*0}{1} + 1 = \\frac {1}{1} + 1 = 1 + 1 = 2\\)</li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#example-2-output-dimension-calculation-for-same-padding","title":"Example 2: Output Dimension Calculation for Same Padding","text":"<ul> <li>\\(W = 5\\)</li> <li>\\(K = 3\\)</li> <li>\\(P = \\frac{3 - 1}{2} = \\frac{2}{2} = 1\\)</li> <li>\\(S = 1\\)</li> <li>\\(O = \\frac {5 - 3 + 2*1}{1} + 1 = \\frac {4}{1} + 1 = 5\\)</li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#building-a-convolutional-neural-network-with-pytorch","title":"Building a Convolutional Neural Network with PyTorch","text":""},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#model-a","title":"Model A:","text":"<ul> <li>2 Convolutional Layers<ul> <li>Same Padding (same output size)</li> </ul> </li> <li>2 Max Pooling Layers</li> <li>1 Fully Connected Layer</li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#steps","title":"Steps","text":"<ul> <li>Step 1: Load Dataset</li> <li>Step 2: Make Dataset Iterable</li> <li>Step 3: Create Model Class</li> <li>Step 4: Instantiate Model Class</li> <li>Step 5: Instantiate Loss Class</li> <li>Step 6: Instantiate Optimizer Class</li> <li>Step 7: Train Model</li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#step-1-loading-mnist-train-dataset","title":"Step 1: Loading MNIST Train Dataset","text":"<p>Images from 1 to 9</p> <p>MNIST Dataset and Size of Training Dataset (Excluding Labels)</p> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n</code></pre> <pre><code>train_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n</code></pre> <pre><code>print(train_dataset.train_data.size())\n</code></pre> <pre><code>torch.Size([60000, 28, 28])\n</code></pre> <p>Size of our training dataset labels</p> <pre><code>print(train_dataset.train_labels.size())\n</code></pre> <pre><code>torch.Size([60000])\n</code></pre> <p>Size of our testing dataset (excluding labels)</p> <pre><code>print(test_dataset.test_data.size())\n</code></pre> <pre><code>torch.Size([10000, 28, 28])\n</code></pre> <p>Size of our testing dataset labels</p> <pre><code>print(test_dataset.test_labels.size())\n</code></pre> <pre><code>torch.Size([10000])\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#step-2-make-dataset-iterable","title":"Step 2: Make Dataset Iterable","text":"<p>Load Dataset into Dataloader</p> <pre><code>batch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#step-3-create-model-class","title":"Step 3: Create Model Class","text":""},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#output-formula-for-convolution","title":"Output Formula for Convolution","text":"<ul> <li>\\(O = \\frac {W - K + 2P}{S} + 1\\)<ul> <li>\\(O\\): output height/length</li> <li>\\(W\\): input height/length</li> <li>\\(K\\): filter size (kernel size) = 5</li> <li>\\(P\\): same padding (non-zero)<ul> <li>\\(P = \\frac{K - 1}{2}  = \\frac{5 - 1}{2} = 2\\)</li> </ul> </li> <li>\\(S\\): stride = 1</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#output-formula-for-pooling","title":"Output Formula for Pooling","text":"<ul> <li>\\(O = \\frac {W - K}{S} + 1\\)<ul> <li>W: input height/width</li> <li>K: filter size = 2</li> <li>S: stride size = filter size, PyTorch defaults the stride to kernel filter size<ul> <li>If using PyTorch default stride, this will result in the formula \\(O = \\frac {W}{K}\\)</li> <li>By default, in our tutorials, we do this for simplicity.</li> </ul> </li> </ul> </li> </ul> <p>Define our simple 2 convolutional layer CNN</p> <pre><code>class CNNModel(nn.Module):\n    def __init__(self):\n        super(CNNModel, self).__init__()\n\n        # Convolution 1\n        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n        self.relu1 = nn.ReLU()\n\n        # Max pool 1\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n\n        # Convolution 2\n        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n        self.relu2 = nn.ReLU()\n\n        # Max pool 2\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n\n        # Fully connected 1 (readout)\n        self.fc1 = nn.Linear(32 * 7 * 7, 10) \n\n    def forward(self, x):\n        # Convolution 1\n        out = self.cnn1(x)\n        out = self.relu1(out)\n\n        # Max pool 1\n        out = self.maxpool1(out)\n\n        # Convolution 2 \n        out = self.cnn2(out)\n        out = self.relu2(out)\n\n        # Max pool 2 \n        out = self.maxpool2(out)\n\n        # Resize\n        # Original size: (100, 32, 7, 7)\n        # out.size(0): 100\n        # New out size: (100, 32*7*7)\n        out = out.view(out.size(0), -1)\n\n        # Linear function (readout)\n        out = self.fc1(out)\n\n        return out\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#step-4-instantiate-model-class","title":"Step 4: Instantiate Model Class","text":"<p>Our model</p> <pre><code>model = CNNModel()\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#step-5-instantiate-loss-class","title":"Step 5: Instantiate Loss Class","text":"<ul> <li>Convolutional Neural Network: Cross Entropy Loss<ul> <li>Feedforward Neural Network: Cross Entropy Loss</li> <li>Logistic Regression: Cross Entropy Loss</li> <li>Linear Regression: MSE</li> </ul> </li> </ul> <p>Our cross entropy loss</p> <pre><code>criterion = nn.CrossEntropyLoss()\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#step-6-instantiate-optimizer-class","title":"Step 6: Instantiate Optimizer Class","text":"<ul> <li>Simplified equation<ul> <li>\\(\\theta = \\theta - \\eta \\cdot \\nabla_\\theta\\)<ul> <li>\\(\\theta\\): parameters</li> <li>\\(\\eta\\): learning rate (how fast we want to learn)</li> <li>\\(\\nabla_\\theta\\): parameters' gradients</li> </ul> </li> </ul> </li> <li>Even simplier equation<ul> <li><code>parameters = parameters - learning_rate * parameters_gradients</code></li> <li>At every iteration, we update our model's parameters</li> </ul> </li> </ul> <p>Optimizer</p> <pre><code>learning_rate = 0.01\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#parameters-in-depth","title":"Parameters In-Depth","text":"<p>Print model's parameter</p> <pre><code>print(model.parameters())\n\nprint(len(list(model.parameters())))\n\n# Convolution 1: 16 Kernels\nprint(list(model.parameters())[0].size())\n\n# Convolution 1 Bias: 16 Kernels\nprint(list(model.parameters())[1].size())\n\n# Convolution 2: 32 Kernels with depth = 16\nprint(list(model.parameters())[2].size())\n\n# Convolution 2 Bias: 32 Kernels with depth = 16\nprint(list(model.parameters())[3].size())\n\n# Fully Connected Layer 1\nprint(list(model.parameters())[4].size())\n\n# Fully Connected Layer Bias\nprint(list(model.parameters())[5].size())\n</code></pre> <pre><code>&lt;generator object Module.parameters at 0x7f9864363c50&gt;\n6\ntorch.Size([16, 1, 5, 5])\ntorch.Size([16])\ntorch.Size([32, 16, 5, 5])\ntorch.Size([32])\ntorch.Size([10, 1568])\ntorch.Size([10])\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#step-7-train-model","title":"Step 7: Train Model","text":"<ul> <li>Process <ol> <li>Convert inputs to tensors with gradient accumulation abilities<ul> <li>CNN Input: (1, 28, 28) </li> <li>Feedforward NN Input: (1, 28*28)</li> </ul> </li> <li>Clear gradient buffets</li> <li>Get output given inputs </li> <li>Get loss</li> <li>Get gradients w.r.t. parameters</li> <li>Update parameters using gradients<ul> <li><code>parameters = parameters - learning_rate * parameters_gradients</code></li> </ul> </li> <li>REPEAT</li> </ol> </li> </ul> <p>Model training</p> <pre><code>iter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images\n        images = images.requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images\n                images = images.requires_grad_()\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Iteration: 500. Loss: 0.43324267864227295. Accuracy: 90\nIteration: 1000. Loss: 0.2511480152606964. Accuracy: 92\nIteration: 1500. Loss: 0.13431282341480255. Accuracy: 94\nIteration: 2000. Loss: 0.11173319816589355. Accuracy: 95\nIteration: 2500. Loss: 0.06409914791584015. Accuracy: 96\nIteration: 3000. Loss: 0.14377528429031372. Accuracy: 96\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#model-b","title":"Model B:","text":"<ul> <li>2 Convolutional Layers<ul> <li>Same Padding (same output size)</li> </ul> </li> <li>2 Average Pooling Layers</li> <li>1 Fully Connected Layer</li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#steps_1","title":"Steps","text":"<ul> <li>Step 1: Load Dataset</li> <li>Step 2: Make Dataset Iterable</li> <li>Step 3: Create Model Class</li> <li>Step 4: Instantiate Model Class</li> <li>Step 5: Instantiate Loss Class</li> <li>Step 6: Instantiate Optimizer Class</li> <li>Step 7: Train Model</li> </ul> <p>2 Conv + 2 Average Pool + 1 FC (Zero Padding, Same Padding)</p> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass CNNModel(nn.Module):\n    def __init__(self):\n        super(CNNModel, self).__init__()\n\n        # Convolution 1\n        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n        self.relu1 = nn.ReLU()\n\n        # Average pool 1\n        self.avgpool1 = nn.AvgPool2d(kernel_size=2)\n\n        # Convolution 2\n        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n        self.relu2 = nn.ReLU()\n\n        # Average pool 2\n        self.avgpool2 = nn.AvgPool2d(kernel_size=2)\n\n        # Fully connected 1 (readout)\n        self.fc1 = nn.Linear(32 * 7 * 7, 10) \n\n    def forward(self, x):\n        # Convolution 1\n        out = self.cnn1(x)\n        out = self.relu1(out)\n\n        # Average pool 1\n        out = self.avgpool1(out)\n\n        # Convolution 2 \n        out = self.cnn2(out)\n        out = self.relu2(out)\n\n        # Max pool 2 \n        out = self.avgpool2(out)\n\n        # Resize\n        # Original size: (100, 32, 7, 7)\n        # out.size(0): 100\n        # New out size: (100, 32*7*7)\n        out = out.view(out.size(0), -1)\n\n        # Linear function (readout)\n        out = self.fc1(out)\n\n        return out\n\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\n\nmodel = CNNModel()\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.01\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as tensors with gradient accumulation abilities\n        images = images.requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images to tensors with gradient accumulation abilities\n                images = images.requires_grad_()\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Iteration: 500. Loss: 0.6850348711013794. Accuracy: 85\nIteration: 1000. Loss: 0.36549052596092224. Accuracy: 88\nIteration: 1500. Loss: 0.31540098786354065. Accuracy: 89\nIteration: 2000. Loss: 0.3522164225578308. Accuracy: 90\nIteration: 2500. Loss: 0.2680729925632477. Accuracy: 91\nIteration: 3000. Loss: 0.26440390944480896. Accuracy: 92\n</code></pre> <p>Comparison of accuracies</p> <p>It seems like average pooling test accuracy is less than the max pooling accuracy! Does this mean average pooling is better? This is not definitive and depends on a lot of factors including the model's architecture, seed (that affects random weight initialization) and more.</p>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#model-c","title":"Model C:","text":"<ul> <li>2 Convolutional Layers<ul> <li>Valid Padding (smaller output size)</li> </ul> </li> <li>2 Max Pooling Layers</li> <li>1 Fully Connected Layer</li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#steps_2","title":"Steps","text":"<ul> <li>Step 1: Load Dataset</li> <li>Step 2: Make Dataset Iterable</li> <li>Step 3: Create Model Class</li> <li>Step 4: Instantiate Model Class</li> <li>Step 5: Instantiate Loss Class</li> <li>Step 6: Instantiate Optimizer Class</li> <li>Step 7: Train Model</li> </ul> <p>2 Conv + 2 Max Pool + 1 FC (Valid Padding, No Padding)</p> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass CNNModel(nn.Module):\n    def __init__(self):\n        super(CNNModel, self).__init__()\n\n        # Convolution 1\n        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)\n        self.relu1 = nn.ReLU()\n\n        # Max pool 1\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n\n        # Convolution 2\n        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)\n        self.relu2 = nn.ReLU()\n\n        # Max pool 2\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n\n        # Fully connected 1 (readout)\n        self.fc1 = nn.Linear(32 * 4 * 4, 10) \n\n    def forward(self, x):\n        # Convolution 1\n        out = self.cnn1(x)\n        out = self.relu1(out)\n\n        # Max pool 1\n        out = self.maxpool1(out)\n\n        # Convolution 2 \n        out = self.cnn2(out)\n        out = self.relu2(out)\n\n        # Max pool 2 \n        out = self.maxpool2(out)\n\n        # Resize\n        # Original size: (100, 32, 7, 7)\n        # out.size(0): 100\n        # New out size: (100, 32*7*7)\n        out = out.view(out.size(0), -1)\n\n        # Linear function (readout)\n        out = self.fc1(out)\n\n        return out\n\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\n\nmodel = CNNModel()\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.01\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as tensors with gradient accumulation abilities\n        images = images.requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images to tensors with gradient accumulation abilities\n                images = images.requires_grad_()\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Iteration: 500. Loss: 0.5153220295906067. Accuracy: 88\nIteration: 1000. Loss: 0.28784745931625366. Accuracy: 92\nIteration: 1500. Loss: 0.4086027443408966. Accuracy: 94\nIteration: 2000. Loss: 0.09390712529420853. Accuracy: 95\nIteration: 2500. Loss: 0.07138358801603317. Accuracy: 95\nIteration: 3000. Loss: 0.05396252125501633. Accuracy: 96\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#summary-of-results","title":"Summary of Results","text":"Model A Model B Model C Max Pooling Average Pooling Max Pooling Same Padding Same Padding Valid Padding 97.04% 93.59% 96.5% All Models INPUT \\(\\rightarrow\\) CONV \\(\\rightarrow\\) POOL \\(\\rightarrow\\) CONV \\(\\rightarrow\\) POOL \\(\\rightarrow\\) FC Convolution Kernel Size = 5 x 5 Convolution Kernel Stride = 1 Pooling Kernel Size = 2 x 2"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#general-deep-learning-notes-on-cnn-and-fnn","title":"General Deep Learning Notes on CNN and FNN","text":"<ul> <li>3 ways to expand a convolutional neural network<ul> <li>More convolutional layers </li> <li>Less aggressive downsampling<ul> <li>Smaller kernel size for pooling (gradually downsampling)</li> </ul> </li> <li>More fully connected layers </li> </ul> </li> <li>Cons<ul> <li>Need a larger dataset<ul> <li>Curse of dimensionality</li> </ul> </li> <li>Does not necessarily mean higher accuracy</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#3-building-a-convolutional-neural-network-with-pytorch-gpu","title":"3. Building a Convolutional Neural Network with PyTorch (GPU)","text":""},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#model-a_1","title":"Model A","text":"<p>GPU: 2 things must be on GPU - <code>model</code> - <code>tensors</code></p>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#steps_3","title":"Steps","text":"<ul> <li>Step 1: Load Dataset</li> <li>Step 2: Make Dataset Iterable</li> <li>Step 3: Create Model Class</li> <li>Step 4: Instantiate Model Class</li> <li>Step 5: Instantiate Loss Class</li> <li>Step 6: Instantiate Optimizer Class</li> <li>Step 7: Train Model</li> </ul> <p>2 Conv + 2 Max Pooling + 1 FC (Same Padding, Zero Padding)</p> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets \n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass CNNModel(nn.Module):\n    def __init__(self):\n        super(CNNModel, self).__init__()\n\n        # Convolution 1\n        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)\n        self.relu1 = nn.ReLU()\n\n        # Max pool 1\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n\n        # Convolution 2\n        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)\n        self.relu2 = nn.ReLU()\n\n        # Max pool 2\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n\n        # Fully connected 1 (readout)\n        self.fc1 = nn.Linear(32 * 4 * 4, 10) \n\n    def forward(self, x):\n        # Convolution 1\n        out = self.cnn1(x)\n        out = self.relu1(out)\n\n        # Max pool 1\n        out = self.maxpool1(out)\n\n        # Convolution 2 \n        out = self.cnn2(out)\n        out = self.relu2(out)\n\n        # Max pool 2 \n        out = self.maxpool2(out)\n\n        # Resize\n        # Original size: (100, 32, 7, 7)\n        # out.size(0): 100\n        # New out size: (100, 32*7*7)\n        out = out.view(out.size(0), -1)\n\n        # Linear function (readout)\n        out = self.fc1(out)\n\n        return out\n\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\n\nmodel = CNNModel()\n\n#######################\n#  USE GPU FOR MODEL  #\n#######################\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.01\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n\n        #######################\n        #  USE GPU FOR MODEL  #\n        #######################\n        images = images.requires_grad_().to(device)\n        labels = labels.to(device)\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                #######################\n                #  USE GPU FOR MODEL  #\n                #######################\n                images = images.requires_grad_().to(device)\n                labels = labels.to(device)\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                #######################\n                #  USE GPU FOR MODEL  #\n                #######################\n                # Total correct predictions\n                if torch.cuda.is_available():\n                    correct += (predicted.cpu() == labels.cpu()).sum()\n                else:\n                    correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Iteration: 500. Loss: 0.36831170320510864. Accuracy: 88\nIteration: 1000. Loss: 0.31790846586227417. Accuracy: 92\nIteration: 1500. Loss: 0.1510857343673706. Accuracy: 94\nIteration: 2000. Loss: 0.08368007838726044. Accuracy: 95\nIteration: 2500. Loss: 0.13419771194458008. Accuracy: 96\nIteration: 3000. Loss: 0.16750787198543549. Accuracy: 96\n</code></pre> <p>More Efficient Convolutions via Toeplitz Matrices</p> <p>This is beyond the scope of this particular lesson. But now that we understand how convolutions work, it is critical to know that it is quite an inefficient operation if we use for-loops to perform our 2D convolutions (5 x 5 convolution kernel size for example) on our 2D images (28 x 28 MNIST image for example).</p> <p>A more efficient implementation is in converting our convolution kernel into a doubly block circulant/Toeplitz matrix (special case Toeplitz matrix) and our image (input) into a vector. Then, we will do just one matrix operation using our doulby block Toeplitz matrix and our input vector.</p> <p>There will be a whole lesson dedicated to this operation released down the road. </p>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#summary","title":"Summary","text":"<p>We've learnt to...</p> <p>Success</p> <ul> <li> Transition from Feedforward Neural Network<ul> <li> Addition of Convolutional &amp; Pooling Layers before Linear Layers</li> </ul> </li> <li> One Convolutional Layer Basics</li> <li> One Pooling Layer Basics<ul> <li> Max pooling</li> <li> Average pooling</li> </ul> </li> <li> Padding</li> <li> Output Dimension Calculations and Examples<ul> <li> \\(O = \\frac {W - K + 2P}{S} + 1\\)</li> </ul> </li> <li> Convolutional Neural Networks<ul> <li> Model A: 2 Conv + 2 Max pool + 1 FC<ul> <li> Same Padding</li> </ul> </li> <li> Model B: 2 Conv + 2 Average pool + 1 FC<ul> <li> Same Padding</li> </ul> </li> <li> Model C: 2 Conv + 2 Max pool + 1 FC<ul> <li> Valid Padding</li> </ul> </li> </ul> </li> <li> Model Variation in Code<ul> <li> Modifying only step 3</li> </ul> </li> <li> Ways to Expand Model\u2019s Capacity<ul> <li> More convolutions</li> <li> Gradual pooling</li> <li> More fully connected layers</li> </ul> </li> <li> GPU Code<ul> <li> 2 things on GPU<ul> <li> model</li> <li> tensors with gradient accumulation abilities</li> </ul> </li> <li> Modifying only Step 4 &amp; Step 7</li> </ul> </li> <li> 7 Step Model Building Recap<ul> <li> Step 1: Load Dataset</li> <li> Step 2: Make Dataset Iterable</li> <li> Step 3: Create Model Class</li> <li> Step 4: Instantiate Model Class</li> <li> Step 5: Instantiate Loss Class</li> <li> Step 6: Instantiate Optimizer Class</li> <li> Step 7: Train Model</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#citation","title":"Citation","text":"<p>If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.</p> <p></p>"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/","title":"Overcomplete Autoencoders with PyTorch","text":"<p>Run Jupyter Notebook</p> <p>You can run the code for this section in this jupyter notebook link.</p>"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#introduction","title":"Introduction","text":"<ul> <li>Recap!<ul> <li>An autoencoder's purpose is to learn an approximation of the identity function (mapping \\(x\\) to \\(\\hat x\\)).<ul> <li>Essentially we are trying to learn a function that can take our input \\(x\\) and recreate it \\(\\hat x\\).<ul> <li>Technically we can do an exact recreation of our in-sample input if we use a very wide and deep neural network.</li> </ul> </li> </ul> </li> </ul> </li> <li>In this particular tutorial, we will be covering denoising autoencoder through overcomplete encoders. <ul> <li>Essentially given noisy images, you can denoise and make them less noisy with this tutorial through overcomplete encoders.</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#fashion-mnist-dataset-exploration","title":"Fashion MNIST Dataset Exploration","text":""},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#imports","title":"Imports","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\nfigsize=(15, 6)\nplt.style.use('fivethirtyeight')\n</code></pre> Name Content Examples Size Link MD5 Checksum <code>train-images-idx3-ubyte.gz</code> training set images 60,000 26 MBytes Download <code>8d4fb7e6c68d591d4c3dfef9ec88bf0d</code> <code>train-labels-idx1-ubyte.gz</code> training set labels 60,000 29 KBytes Download <code>25c81989df183df01b3e8a0aad5dffbe</code> <code>t10k-images-idx3-ubyte.gz</code> test set images 10,000 4.3 MBytes Download <code>bef4ecab320f06d8554ea6380940ec79</code> <code>t10k-labels-idx1-ubyte.gz</code> test set labels 10,000 5.1 KBytes Download <code>bb300cfdad3c16e7a12a480ee83cd310</code>"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#load-dataset-step-1","title":"Load Dataset (Step 1)","text":"<ul> <li>Typically we've been leveraging on <code>dsets.MNIST()</code>, now we simply change to <code>dsets.FashionMNIST</code>!</li> </ul> <pre><code># Fashion-MNIST data loader\ntrain_dataset = dsets.FashionMNIST(root='./data',\n                                   train=True,\n                                   transform=transforms.ToTensor(),\n                                   download=True)\n\ntest_dataset = dsets.FashionMNIST(root='./data',\n                                  train=False,\n                                  transform=transforms.ToTensor())\n</code></pre> <pre><code>Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nProcessing...\nDone!\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#load-data-loader-step-2","title":"Load Data Loader (Step 2)","text":"<pre><code># Batch size, iterations and epochs\nbatch_size = 100\nn_iters = 5000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#labels","title":"Labels","text":"<p>Each training and test example is assigned to one of the following labels:</p> Label Description 0 T-shirt/top 1 Trouser 2 Pullover 3 Dress 4 Coat 5 Sandal 6 Shirt 7 Sneaker 8 Bag 9 Ankle boot"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#sample-boot","title":"Sample: Boot","text":"<pre><code># Sample 0: boot\nsample_num = 0\nshow_img = train_dataset[sample_num][0].numpy().reshape(28, 28)\nlabel = train_dataset[sample_num][1]\nprint(f'Label {label}')\nplt.imshow(show_img, cmap='gray');\n</code></pre> <pre><code>Label 9\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#sample-shirt","title":"Sample: shirt","text":"<pre><code># Sample 1: shirt\nsample_num = 1\nshow_img = train_dataset[sample_num][0].numpy().reshape(28, 28)\nlabel = train_dataset[sample_num][1]\nprint(f'Label {label}')\nplt.imshow(show_img, cmap='gray');\n</code></pre> <pre><code>Label 0\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#sample-dress","title":"Sample: dress","text":"<pre><code># Sample 3: dress\nsample_num = 3\nshow_img = train_dataset[sample_num][0].numpy().reshape(28, 28)\nlabel = train_dataset[sample_num][1]\nprint(f'Label {label}')\nplt.imshow(show_img, cmap='gray');\n</code></pre> <pre><code>Label 3\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#maximumminimum-pixel-values","title":"Maximum/minimum pixel values","text":"<ul> <li>Pixel values range from 0 to 1</li> </ul> <pre><code>min_pixel_value = train_dataset[sample_num][0].min()\nmax_pixel_value = train_dataset[sample_num][0].max()\n\nprint(f'Minimum pixel value: {min_pixel_value}')\nprint(f'Maximum pixel value: {max_pixel_value}')\n</code></pre> <pre><code>Minimum pixel value: 0.0\nMaximum pixel value: 1.0\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#overcomplete-autoencoder","title":"Overcomplete Autoencoder","text":""},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#sigmoid-function","title":"Sigmoid Function","text":"<ul> <li>Sigmoid function was introduced earlier, where the function allows to bound our output from 0 to 1 inclusive given our input.</li> <li>This is introduced and clarified here as we would want this in our final layer of our overcomplete autoencoder as we want to bound out final output to the pixels' range of 0 and 1.</li> </ul> <pre><code># Sigmoid function has function bounded by min=0 and max=1\n# So this will be what we will be using for the final layer's function\nx = torch.arange(-10., 10., 0.1)\nplt.figure(figsize=figsize);\nplt.plot(x.numpy(), torch.sigmoid(x).numpy())\nplt.title('Sigmoid Function')\n</code></pre> <pre><code>Text(0.5, 1.0, 'Sigmoid Function')\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#steps","title":"Steps","text":"<ul> <li>These steps should be familiar by now! Our famous 7 steps. But I will be adding one more step here, Step 8 where we run our inference.<ul> <li>Step 1: Load Dataset</li> <li>Step 2: Make Dataset Iterable</li> <li>Step 3: Create Model Class</li> <li>Step 4: Instantiate Model Class</li> <li>Step 5: Instantiate Loss Class</li> <li>Step 6: Instantiate Optimizer Class</li> <li>Step 7: Train Model</li> <li>Step 8: Model Inference</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#step-3-create-model-class","title":"Step 3: Create Model Class","text":"<pre><code># Model definition\nclass FullyConnectedAutoencoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        # Encoder: affine function\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        # Decoder: affine function\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        # Encoder: affine function\n        out = self.fc1(x)\n        # Encoder: non-linear function\n        out = F.leaky_relu(out)\n\n        # Decoder: affine function\n        out = self.fc2(out)\n        # Decoder: non-linear function\n        out = torch.sigmoid(out)\n\n        return out\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#step-4-instantiate-model-class","title":"Step 4: Instantiate Model Class","text":"<pre><code># Dimensions for overcomplete (larger latent representation)\ninput_dim = 28*28\nhidden_dim = int(input_dim * 1.5)\noutput_dim = input_dim\n\n# Instantiate Fully-connected Autoencoder (FC-AE)\n# And assign to model object\nmodel = FullyConnectedAutoencoder(input_dim, hidden_dim, output_dim)\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#step-5-instantiate-loss-class","title":"Step 5: Instantiate Loss Class","text":"<pre><code># We want to minimize the per pixel reconstruction loss\n# So we've to use the mean squared error (MSE) loss\n# This is similar to our regression tasks' loss\ncriterion = nn.MSELoss()\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#step-6-instantiate-optimizer-class","title":"Step 6: Instantiate Optimizer Class","text":"<pre><code># Using basic Adam optimizer\nlearning_rate = 1e-3\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#inspect-parameter-groups","title":"Inspect Parameter Groups","text":"<pre><code># Parameter inspection\nnum_params_group = len(list(model.parameters()))\nfor group_idx in range(num_params_group):\n    print(list(model.parameters())[group_idx].size())\n</code></pre> <pre><code>torch.Size([1176, 784])\ntorch.Size([1176])\ntorch.Size([784, 1176])\ntorch.Size([784])\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#step-7-train-model","title":"Step 7: Train Model","text":"<ul> <li>Take note there's a critical line of <code>dropout = nn.Dropout(0.5)</code> here which basically allows us to make noisy images from our original Fashion MNIST images.</li> <li>It basically drops out 50% of all pixels randomly.</li> </ul> <pre><code>idx = 0\n\n# Dropout for creating noisy images\n# by dropping out pixel with a 50% probability\ndropout = nn.Dropout(0.5)\n\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images with gradient accumulation capabilities\n        images = images.view(-1, 28*28).requires_grad_()\n\n        # Noisy images\n        noisy_images = dropout(torch.ones(images.shape)) * images\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output\n        outputs = model(noisy_images)\n\n        # Calculate Loss: MSE Loss based on pixel-to-pixel comparison\n        loss = criterion(outputs, images)\n\n        # Getting gradients w.r.t. parameters via backpropagation\n        loss.backward()\n\n        # Updating parameters via gradient descent\n        optimizer.step()\n\n        idx += 1\n\n        if idx % 500 == 0:\n            # Calculate MSE Test Loss\n            total_test_loss = 0\n            total_samples = 0\n\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Noisy images\n                noisy_images = dropout(torch.ones(images.shape)) * images\n\n                # Forward pass only to get logits/output\n                outputs = model(noisy_images.view(-1, 28*28))\n\n                # Test loss\n                test_loss = criterion(outputs, images.view(-1, 28*28))\n\n                # Total number of labels\n                total_samples += labels.size(0)\n\n                # Total test loss\n                total_test_loss += test_loss\n\n            mean_test_loss = total_test_loss / total_samples\n\n            # Print Loss\n            print(f'Iteration: {idx}. Average Test Loss: {mean_test_loss.item()}.')\n</code></pre> <pre><code>Iteration: 500. Average Test Loss: 0.0001664187147980556.\nIteration: 1000. Average Test Loss: 0.00014121478307060897.\nIteration: 1500. Average Test Loss: 0.0001341506722383201.\nIteration: 2000. Average Test Loss: 0.0001252180663868785.\nIteration: 2500. Average Test Loss: 0.00012206179235363379.\nIteration: 3000. Average Test Loss: 0.00011766648094635457.\nIteration: 3500. Average Test Loss: 0.00011584569438127801.\nIteration: 4000. Average Test Loss: 0.00011396891932236031.\nIteration: 4500. Average Test Loss: 0.00011224475019844249.\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#model-inference","title":"Model Inference","text":""},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#raw-sample-1","title":"Raw Sample 1","text":"<pre><code># Test sample: Raw\nsample_num = 10\nraw_img = test_dataset[sample_num][0]\nshow_img = raw_img.numpy().reshape(28, 28)\nlabel = test_dataset[sample_num][1]\nprint(f'Label {label}')\nplt.imshow(show_img, cmap='gray');\n</code></pre> <pre><code>Label 4\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#test-sample-1-noisy-to-denoised","title":"Test Sample 1: Noisy to Denoised","text":"<pre><code># Test sample: Noisy\nsample_num = 10\nraw_img = test_dataset[sample_num][0]\nnoisy_image = dropout(torch.ones(raw_img.shape)) * raw_img\nshow_img = noisy_image.numpy().reshape(28, 28)\nlabel = test_dataset[sample_num][1]\nprint(f'Label {label}')\nplt.imshow(show_img, cmap='gray');\n</code></pre> <pre><code>Label 4\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#summary","title":"Summary","text":"<ul> <li>Introduction<ul> <li>Recap of Autoencoders</li> <li>Introduction of denoising autoencoders</li> </ul> </li> <li>Dataset Exploration<ul> <li>Fashion MNIST<ul> <li>10 classes</li> <li>Similar to MNIST but fashion images instead of digits</li> </ul> </li> </ul> </li> <li>8 Step Model Building Recap<ul> <li>Step 1: Load Dataset</li> <li>Step 2: Make Dataset Iterable</li> <li>Step 3: Create Model Class</li> <li>Step 4: Instantiate Model Class</li> <li>Step 5: Instantiate Loss Class</li> <li>Step 6: Instantiate Optimizer Class</li> <li>Step 7: Train Model</li> <li>Step 8: Model Inference<ul> <li>Raw Sample 1</li> <li>Noisy to Denoised Sample 1</li> </ul> </li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/","title":"Feedforward Neural Network with PyTorch","text":"<p>Run Jupyter Notebook</p> <p>You can run the code for this section in this jupyter notebook link.</p>"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#about-feedforward-neural-network","title":"About Feedforward Neural Network","text":""},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#logistic-regression-transition-to-neural-networks","title":"Logistic Regression Transition to Neural Networks","text":""},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#logistic-regression-review","title":"Logistic Regression Review","text":"<p>Define logistic regression model</p> <p>Import our relevant torch modules. <pre><code>import torch\nimport torch.nn as nn\n</code></pre></p> <p>Define our model class. <pre><code>class LogisticRegressionModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LogisticRegressionModel, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        out = self.linear(x)\n        return out\n</code></pre></p> <p>Instantiate the logistic regression model. <pre><code>input_dim = 28*28\noutput_dim = 10\n\nmodel = LogisticRegressionModel(input_dim, output_dim)\n</code></pre></p> <p>When we inspect the model, we would have an input size of 784 (derived from 28 x 28) and output size of 10 (which is the number of classes we are classifying from 0 to 9). <pre><code>print(model)\n</code></pre></p> <pre><code>LogisticRegressionModel(\n  (linear): Linear(in_features=784, out_features=10, bias=True)\n)\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#logistic-regression-problems","title":"Logistic Regression Problems","text":"<ul> <li>Can represent linear functions well<ul> <li>\\(y = 2x + 3\\)</li> <li>\\(y = x_1 + x_2\\)</li> <li>\\(y = x_1 + 3x_2 + 4x_3\\)</li> </ul> </li> <li>Cannot represent non-linear functions<ul> <li>\\(y = 4x_1 + 2x_2^2 +3x_3^3\\)</li> <li>\\(y = x_1x_2\\)</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#introducing-a-non-linear-function","title":"Introducing a Non-linear Function","text":""},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#non-linear-function-in-depth","title":"Non-linear Function In-Depth","text":"<ul> <li>Function: takes a number &amp; perform mathematical operation</li> <li>Common Types of Non-linearity<ul> <li>ReLUs (Rectified Linear Units)      </li> <li>Sigmoid</li> <li>Tanh</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#sigmoid-logistic","title":"Sigmoid (Logistic)","text":"<ul> <li>\\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\)</li> <li>Input number \\(\\rightarrow\\) [0, 1]<ul> <li>Large negative number \\(\\rightarrow\\) 0</li> <li>Large positive number \\(\\rightarrow\\) 1</li> </ul> </li> <li>Cons: <ol> <li>Activation saturates at 0 or 1 with gradients \\(\\approx\\) 0<ul> <li>No signal to update weights \\(\\rightarrow\\) cannot learn</li> <li>Solution: Have to carefully initialize weights to prevent this</li> </ul> </li> <li>Outputs not centered around 0 <ul> <li>If output always positive \\(\\rightarrow\\) gradients always positive or negative \\(\\rightarrow\\) bad for gradient updates </li> </ul> </li> </ol> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#tanh","title":"Tanh","text":"<ul> <li>\\(\\tanh(x) = 2 \\sigma(2x) -1\\)<ul> <li>A scaled sigmoid function</li> </ul> </li> <li>Input number \\(\\rightarrow\\) [-1, 1]</li> <li>Cons: <ol> <li>Activation saturates at 0 or 1 with gradients \\(\\approx\\) 0<ul> <li>No signal to update weights \\(\\rightarrow\\) cannot learn</li> <li>Solution: Have to carefully initialize weights to prevent this</li> </ul> </li> </ol> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#relus","title":"ReLUs","text":"<ul> <li>\\(f(x) = \\max(0, x)\\)</li> <li>Pros:<ol> <li>Accelerates convergence \\(\\rightarrow\\) train faster</li> <li>Less computationally expensive operation compared to Sigmoid/Tanh exponentials</li> </ol> </li> <li>Cons:<ol> <li>Many ReLU units \"die\" \\(\\rightarrow\\) gradients = 0 forever<ul> <li>Solution: careful learning rate choice</li> </ul> </li> </ol> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#building-a-feedforward-neural-network-with-pytorch","title":"Building a Feedforward Neural Network with PyTorch","text":""},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#model-a-1-hidden-layer-feedforward-neural-network-sigmoid-activation","title":"Model A: 1 Hidden Layer Feedforward Neural Network (Sigmoid Activation)","text":""},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#steps","title":"Steps","text":"<ul> <li>Step 1: Load Dataset</li> <li>Step 2: Make Dataset Iterable</li> <li>Step 3: Create Model Class</li> <li>Step 4: Instantiate Model Class</li> <li>Step 5: Instantiate Loss Class</li> <li>Step 6: Instantiate Optimizer Class</li> <li>Step 7: Train Model</li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-1-loading-mnist-train-dataset","title":"Step 1: Loading MNIST Train Dataset","text":"<p>Images from 1 to 9</p> <p>Similar to what we did in logistic regression, we will be using the same MNIST dataset where we load our training and testing datasets.</p> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n</code></pre> <pre><code>train_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-2-make-dataset-iterable","title":"Step 2: Make Dataset Iterable","text":"<p>Batch sizes and iterations</p> <p>Because we have 60000 training samples (images), we need to split them up to small groups (batches) and pass these batches of samples to our feedforward neural network subsesquently.</p> <p>There are a few reasons why we split them into batches. Passing your whole dataset as a single batch would:</p> <p>(1) require a lot of RAM/VRAM on your CPU/GPU and this might result in Out-of-Memory (OOM) errors. </p> <p>(2) cause unstable training if you just use all the errors accumulated in 60,000 images to update the model rather than gradually update the model. In layman terms, imagine you accumulated errors for a student taking an exam with 60,000 questions and punish the student all at the same time. It is much harder for the student to learn compared to letting the student learn it made mistakes and did well in smaller batches of questions like mini-tests!</p> <p>If we have 60,000 images and we want a batch size of 100, then we would have 600 iterations where each iteration involves passing 600 images to the model and getting their respective predictions. </p> <pre><code>60000 / 100\n</code></pre> <pre><code>600.0\n</code></pre> <p>Epochs</p> <p>An epoch means that you have successfully passed the whole training set, 60,000 images, to the model. Continuing our example above, an epoch consists of 600 iterations.</p> <p>If we want to go through the whole dataset 5 times (5 epochs) for the model to learn, then we need 3000 iterations (600 x 5). </p> <pre><code>600 * 5\n</code></pre> <pre><code>3000.0\n</code></pre> <p>Bringing batch size, iterations and epochs together</p> <p>As we have gone through above, we want to have 5 epochs, where each epoch would have 600 iterations and each iteration has a batch size of 100.</p> <p>Because we want 5 epochs, we need a total of 3000 iterations.</p> <pre><code>batch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-3-create-model-class","title":"Step 3: Create Model Class","text":"<p>Creating our feedforward neural network</p> <p>Compared to logistic regression with only a single linear layer, we know for an FNN we need an additional linear layer and non-linear layer. </p> <p>This translates to just 4 more lines of code! </p> <pre><code>class FeedforwardNeuralNetModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FeedforwardNeuralNetModel, self).__init__()\n        # Linear function\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n\n        # Non-linearity\n        self.sigmoid = nn.Sigmoid()\n\n        # Linear function (readout)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n\n    def forward(self, x):\n        # Linear function  # LINEAR\n        out = self.fc1(x)\n\n        # Non-linearity  # NON-LINEAR\n        out = self.sigmoid(out)\n\n        # Linear function (readout)  # LINEAR\n        out = self.fc2(out)\n        return out\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-4-instantiate-model-class","title":"Step 4: Instantiate Model Class","text":"<ul> <li>Input dimension: 784 <ul> <li>Size of image</li> <li>\\(28 \\times 28 = 784\\)</li> </ul> </li> <li>Output dimension: 10<ul> <li>0, 1, 2, 3, 4, 5, 6, 7, 8, 9</li> </ul> </li> <li>Hidden dimension: 100<ul> <li>Can be any number</li> <li>Similar term<ul> <li>Number of neurons</li> <li>Number of non-linear activation functions</li> </ul> </li> </ul> </li> </ul> <p>Instantiating our model class</p> <p>Our input size is determined by the size of the image (numbers ranging from 0 to 9) which has a width of 28 pixels and a height of 28 pixels. Hence the size of our input is 784 (28 x 28).</p> <p>Our output size is what we are trying to predict. When we pass an image to our model, it will try to predict if it's 0, 1, 2, 3, 4, 5, 6, 7, 8, or 9. That is a total of 10 classes, hence we have an output size of 10.</p> <p>Now the tricky part is in determining our hidden layer size, that is the size of our first linear layer prior to the non-linear layer. This can be any number, a larger number implies a bigger model with more parameters. Intuitively we think a bigger model equates to a better model, but a bigger model requires more training samples to learn and converge to a good model (also called curse of dimensionality). Hence, it is wise to pick the model size for the problem at hand. Because it is a simple problem of recognizing digits, we typically would not need a big model to achieve state-of-the-art results.</p> <p>On the flipside, too small of a hidden size would mean there would be insufficient model capacity to predict competently. In layman terms, too small of a capacity implies a smaller brain capacity so no matter how many training samples you give it, it has a maximum capacity in terms of its predictive power. </p> <pre><code>input_dim = 28*28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-5-instantiate-loss-class","title":"Step 5: Instantiate Loss Class","text":"<ul> <li>Feedforward Neural Network: Cross Entropy Loss<ul> <li>Logistic Regression: Cross Entropy Loss</li> <li>Linear Regression: MSE</li> </ul> </li> </ul> <p>Loss class</p> <p>This is exactly the same as what we did in logistic regression. Because we are going through a classification problem, cross entropy function is required to compute the loss between our softmax outputs and our binary labels.</p> <pre><code>criterion = nn.CrossEntropyLoss()\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-6-instantiate-optimizer-class","title":"Step 6: Instantiate Optimizer Class","text":"<ul> <li>Simplified equation<ul> <li>\\(\\theta = \\theta - \\eta \\cdot \\nabla_\\theta\\)<ul> <li>\\(\\theta\\): parameters (our tensors with gradient accumulation capabilities)</li> <li>\\(\\eta\\): learning rate (how fast we want to learn)</li> <li>\\(\\nabla_\\theta\\): parameters' gradients</li> </ul> </li> </ul> </li> <li>Even simplier equation<ul> <li><code>parameters = parameters - learning_rate * parameters_gradients</code></li> <li>At every iteration, we update our model's parameters</li> </ul> </li> </ul> <p>Optimizer class</p> <p>Learning rate determines how fast the algorithm learns. Too small and the algorithm learns too slowly, too large and the algorithm learns too fast resulting in instabilities.</p> <p>Intuitively, we would think a larger learning rate would be better because we learn faster. But that's not true. Imagine we pass 10 images to a human to learn how to recognize whether the image is a hot dog or not, and it got half right and half wrong. </p> <p>A well defined learning rate (neither too small or large) is equivalent to rewarding the human with a sweet for getting the first half right, and punishing the other half the human got wrong with a smack on the palm. </p> <p>A large learning rate would be equivalent to feeding a thousand sweets to the human and smacking a thousand times on the human's palm. This would lead in a very unstable learning environment. Similarly, we will observe that the algorithm's convergence path will be extremely unstable if you use a large learning rate without reducing it subsequently. </p> <p>We are using an optimization algorithm called Stochastic Gradient Descent (SGD) which is essentially what we covered above on calculating the parameters' gradients multiplied by the learning rate then using it to update our parameters gradually. There's an in-depth analysis of various optimization algorithms on top of SGD in another section.</p> <pre><code>learning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#parameters-in-depth","title":"Parameters In-Depth","text":"<p>Linear layers' parameters</p> <p>In a simple linear layer it's \\(Y = AX + B\\), and our parameters are \\(A\\) and bias \\(B\\). </p> <p>Hence, each linear layer would have 2 groups of parameters  \\(A\\) and \\(B\\). It is critical to take note that our non-linear layers have no parameters to update. They are merely mathematical functions performed on \\(Y\\), the output of our linear layers.</p> <p>This would return a Python generator object, so you need to call list on the generator object to access anything meaningful. <pre><code>print(model.parameters())\n</code></pre></p> <p>Here we call list on the generator object and getting the length of the list. This would return 4 because we've 2 linear layers, and each layer has 2 groups of parameters \\(A\\) and \\(b\\).</p> <pre><code>print(len(list(model.parameters())))\n</code></pre> <p>Our first linear layer parameters, \\(A_1\\), would be of size 100 x 784. This is because we've an input size of 784 (28 x 28) and a hidden size of 100. <pre><code># FC 1 Parameters \nprint(list(model.parameters())[0].size())\n</code></pre></p> <p>Our first linear layer bias parameters, \\(B_1\\), would be of size 100 which is our hidden size. <pre><code># FC 1 Bias Parameters\nprint(list(model.parameters())[1].size())\n</code></pre></p> <p>Our second linear layer is our readout layer, where the parameters \\(A_2\\) would be of size 10 x 100. This is because our output size is 10 and hidden size is 100.</p> <pre><code># FC 2 Parameters\nprint(list(model.parameters())[2].size())\n</code></pre> <p>Likewise our readout layer's bias \\(B_1\\) would just be 10, the size of our output.</p> <pre><code># FC 2 Bias Parameters\nprint(list(model.parameters())[3].size())\n</code></pre> <p>The diagram below shows the interaction amongst our input \\(X\\) and our linear layers' parameters \\(A_1\\), \\(B_1\\), \\(A_2\\), and \\(B_2\\) to reach to the final size of 10 x 1.</p> <p>If you're still unfamiliar with matrix product, go ahead and review the previous quick lesson where we covered it in logistic regression.</p> <pre><code>&lt;generator object Module.parameters at 0x7f1d530fa678&gt;\n4\ntorch.Size([100, 784])\ntorch.Size([100])\ntorch.Size([10, 100])\ntorch.Size([10])\n</code></pre> <p></p>"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-7-train-model","title":"Step 7: Train Model","text":"<ul> <li>Process <ol> <li>Convert inputs to tensors with gradient accumulation capabilities</li> <li>Clear gradient buffers</li> <li>Get output given inputs </li> <li>Get loss</li> <li>Get gradients w.r.t. parameters</li> <li>Update parameters using gradients<ul> <li><code>parameters = parameters - learning_rate * parameters_gradients</code></li> </ul> </li> <li>REPEAT</li> </ol> </li> </ul> <p>7-step training process</p> <pre><code>iter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images with gradient accumulation capabilities\n        images = images.view(-1, 28*28).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images with gradient accumulation capabilities\n                images = images.view(-1, 28*28).requires_grad_()\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Iteration: 500. Loss: 0.6457265615463257. Accuracy: 85\nIteration: 1000. Loss: 0.39627206325531006. Accuracy: 89\nIteration: 1500. Loss: 0.2831554412841797. Accuracy: 90\nIteration: 2000. Loss: 0.4409525394439697. Accuracy: 91\nIteration: 2500. Loss: 0.2397005707025528. Accuracy: 91\nIteration: 3000. Loss: 0.3160165846347809. Accuracy: 91\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#model-b-1-hidden-layer-feedforward-neural-network-tanh-activation","title":"Model B: 1 Hidden Layer Feedforward Neural Network (Tanh Activation)","text":""},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#steps_1","title":"Steps","text":"<ul> <li>Step 1: Load Dataset</li> <li>Step 2: Make Dataset Iterable</li> <li>Step 3: Create Model Class</li> <li>Step 4: Instantiate Model Class</li> <li>Step 5: Instantiate Loss Class</li> <li>Step 6: Instantiate Optimizer Class</li> <li>Step 7: Train Model</li> </ul> <p>1-layer FNN with Tanh Activation</p> <p>The only difference here compared to previously is that we are using Tanh activation instead of Sigmoid activation. This affects step 3. <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass FeedforwardNeuralNetModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FeedforwardNeuralNetModel, self).__init__()\n        # Linear function\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity\n        self.tanh = nn.Tanh()\n        # Linear function (readout)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n\n    def forward(self, x):\n        # Linear function\n        out = self.fc1(x)\n        # Non-linearity\n        out = self.tanh(out)\n        # Linear function (readout)\n        out = self.fc2(out)\n        return out\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28*28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images with gradient accumulation capabilities\n        images = images.view(-1, 28*28).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images with gradient accumulation capabilities\n                images = images.view(-1, 28*28).requires_grad_()\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre></p> <pre><code>Iteration: 500. Loss: 0.4128190577030182. Accuracy: 91\nIteration: 1000. Loss: 0.14497484266757965. Accuracy: 92\nIteration: 1500. Loss: 0.272532194852829. Accuracy: 93\nIteration: 2000. Loss: 0.2758277952671051. Accuracy: 94\nIteration: 2500. Loss: 0.1603182554244995. Accuracy: 94\nIteration: 3000. Loss: 0.08848697692155838. Accuracy: 95\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#model-c-1-hidden-layer-feedforward-neural-network-relu-activation","title":"Model C: 1 Hidden Layer Feedforward Neural Network (ReLU Activation)","text":""},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#steps_2","title":"Steps","text":"<ul> <li>Step 1: Load Dataset</li> <li>Step 2: Make Dataset Iterable</li> <li>Step 3: Create Model Class</li> <li>Step 4: Instantiate Model Class</li> <li>Step 5: Instantiate Loss Class</li> <li>Step 6: Instantiate Optimizer Class</li> <li>Step 7: Train Model</li> </ul> <p>1-layer FNN with ReLU Activation</p> <p>The only difference again is in using ReLU activation and it affects step 3. <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass FeedforwardNeuralNetModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FeedforwardNeuralNetModel, self).__init__()\n        # Linear function\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity\n        self.relu = nn.ReLU()\n        # Linear function (readout)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n\n    def forward(self, x):\n        # Linear function\n        out = self.fc1(x)\n        # Non-linearity\n        out = self.relu(out)\n        # Linear function (readout)\n        out = self.fc2(out)\n        return out\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28*28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images with gradient accumulation capabilities\n        images = images.view(-1, 28*28).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images with gradient accumulation capabilities\n                images = images.view(-1, 28*28).requires_grad_()\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre></p> <pre><code>Iteration: 500. Loss: 0.3179700970649719. Accuracy: 91\nIteration: 1000. Loss: 0.17288273572921753. Accuracy: 93\nIteration: 1500. Loss: 0.16829034686088562. Accuracy: 94\nIteration: 2000. Loss: 0.25494423508644104. Accuracy: 94\nIteration: 2500. Loss: 0.16818439960479736. Accuracy: 95\nIteration: 3000. Loss: 0.11110792309045792. Accuracy: 95\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#model-d-2-hidden-layer-feedforward-neural-network-relu-activation","title":"Model D: 2 Hidden Layer Feedforward Neural Network (ReLU Activation)","text":""},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#steps_3","title":"Steps","text":"<ul> <li>Step 1: Load Dataset</li> <li>Step 2: Make Dataset Iterable</li> <li>Step 3: Create Model Class</li> <li>Step 4: Instantiate Model Class</li> <li>Step 5: Instantiate Loss Class</li> <li>Step 6: Instantiate Optimizer Class</li> <li>Step 7: Train Model</li> </ul> <p>2-layer FNN with ReLU Activation</p> <p>This is a bigger difference that increases your model's capacity by adding another linear layer and non-linear layer which affects step 3. <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass FeedforwardNeuralNetModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FeedforwardNeuralNetModel, self).__init__()\n        # Linear function 1: 784 --&gt; 100\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity 1\n        self.relu1 = nn.ReLU()\n\n        # Linear function 2: 100 --&gt; 100\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 2\n        self.relu2 = nn.ReLU()\n\n        # Linear function 3 (readout): 100 --&gt; 10\n        self.fc3 = nn.Linear(hidden_dim, output_dim)  \n\n    def forward(self, x):\n        # Linear function 1\n        out = self.fc1(x)\n        # Non-linearity 1\n        out = self.relu1(out)\n\n        # Linear function 2\n        out = self.fc2(out)\n        # Non-linearity 2\n        out = self.relu2(out)\n\n        # Linear function 3 (readout)\n        out = self.fc3(out)\n        return out\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28*28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images with gradient accumulation capabilities\n        images = images.view(-1, 28*28).requires_grad_()\n        labels = labels\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images with gradient accumulation capabilities\n                images = images.view(-1, 28*28).requires_grad_()\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre></p> <pre><code>Iteration: 500. Loss: 0.2995373010635376. Accuracy: 91\nIteration: 1000. Loss: 0.3924565613269806. Accuracy: 93\nIteration: 1500. Loss: 0.1283276081085205. Accuracy: 94\nIteration: 2000. Loss: 0.10905527323484421. Accuracy: 95\nIteration: 2500. Loss: 0.11943754553794861. Accuracy: 96\nIteration: 3000. Loss: 0.15632082521915436. Accuracy: 96\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#model-e-3-hidden-layer-feedforward-neural-network-relu-activation","title":"Model E: 3 Hidden Layer Feedforward Neural Network (ReLU Activation)","text":""},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#steps_4","title":"Steps","text":"<ul> <li>Step 1: Load Dataset</li> <li>Step 2: Make Dataset Iterable</li> <li>Step 3: Create Model Class</li> <li>Step 4: Instantiate Model Class</li> <li>Step 5: Instantiate Loss Class</li> <li>Step 6: Instantiate Optimizer Class</li> <li>Step 7: Train Model</li> </ul> <p>3-layer FNN with ReLU Activation</p> <p>Let's add one more layer! Bigger model capacity. But will it be better? Remember what we talked about on curse of dimensionality?</p> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass FeedforwardNeuralNetModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FeedforwardNeuralNetModel, self).__init__()\n        # Linear function 1: 784 --&gt; 100\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity 1\n        self.relu1 = nn.ReLU()\n\n        # Linear function 2: 100 --&gt; 100\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 2\n        self.relu2 = nn.ReLU()\n\n        # Linear function 3: 100 --&gt; 100\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 3\n        self.relu3 = nn.ReLU()\n\n        # Linear function 4 (readout): 100 --&gt; 10\n        self.fc4 = nn.Linear(hidden_dim, output_dim)  \n\n    def forward(self, x):\n        # Linear function 1\n        out = self.fc1(x)\n        # Non-linearity 1\n        out = self.relu1(out)\n\n        # Linear function 2\n        out = self.fc2(out)\n        # Non-linearity 2\n        out = self.relu2(out)\n\n        # Linear function 2\n        out = self.fc3(out)\n        # Non-linearity 2\n        out = self.relu3(out)\n\n        # Linear function 4 (readout)\n        out = self.fc4(out)\n        return out\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28*28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images with gradient accumulation capabilities\n        images = images.view(-1, 28*28).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images with gradient accumulation capabilities\n                images = images.view(-1, 28*28).requires_grad_()\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Iteration: 500. Loss: 0.33234935998916626. Accuracy: 89\nIteration: 1000. Loss: 0.3098006248474121. Accuracy: 94\nIteration: 1500. Loss: 0.12461677193641663. Accuracy: 95\nIteration: 2000. Loss: 0.14346086978912354. Accuracy: 96\nIteration: 2500. Loss: 0.03763459622859955. Accuracy: 96\nIteration: 3000. Loss: 0.1397182047367096. Accuracy: 97\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#general-comments-on-fnns","title":"General Comments on FNNs","text":"<ul> <li>2 ways to expand a neural network<ul> <li>More non-linear activation units (neurons)</li> <li>More hidden layers </li> </ul> </li> <li>Cons<ul> <li>Need a larger dataset<ul> <li>Curse of dimensionality</li> </ul> </li> <li>Does not necessarily mean higher accuracy</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#3-building-a-feedforward-neural-network-with-pytorch-gpu","title":"3. Building a Feedforward Neural Network with PyTorch (GPU)","text":"<p>GPU: 2 things must be on GPU - <code>model</code> - <code>tensors</code></p>"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#steps_5","title":"Steps","text":"<ul> <li>Step 1: Load Dataset</li> <li>Step 2: Make Dataset Iterable</li> <li>Step 3: Create Model Class</li> <li>Step 4: Instantiate Model Class</li> <li>Step 5: Instantiate Loss Class</li> <li>Step 6: Instantiate Optimizer Class</li> <li>Step 7: Train Model</li> </ul> <p>3-layer FNN with ReLU Activation on GPU</p> <p>Only step 4 and 7 of the CPU code will be affected and it's a simple change.</p> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass FeedforwardNeuralNetModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FeedforwardNeuralNetModel, self).__init__()\n        # Linear function 1: 784 --&gt; 100\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity 1\n        self.relu1 = nn.ReLU()\n\n        # Linear function 2: 100 --&gt; 100\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 2\n        self.relu2 = nn.ReLU()\n\n        # Linear function 3: 100 --&gt; 100\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 3\n        self.relu3 = nn.ReLU()\n\n        # Linear function 4 (readout): 100 --&gt; 10\n        self.fc4 = nn.Linear(hidden_dim, output_dim)  \n\n    def forward(self, x):\n        # Linear function 1\n        out = self.fc1(x)\n        # Non-linearity 1\n        out = self.relu1(out)\n\n        # Linear function 2\n        out = self.fc2(out)\n        # Non-linearity 2\n        out = self.relu2(out)\n\n        # Linear function 2\n        out = self.fc3(out)\n        # Non-linearity 2\n        out = self.relu3(out)\n\n        # Linear function 4 (readout)\n        out = self.fc4(out)\n        return out\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28*28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n\n#######################\n#  USE GPU FOR MODEL  #\n#######################\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n\n        #######################\n        #  USE GPU FOR MODEL  #\n        #######################\n        images = images.view(-1, 28*28).requires_grad_().to(device)\n        labels = labels.to(device)\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                #######################\n                #  USE GPU FOR MODEL  #\n                #######################\n                images = images.view(-1, 28*28).requires_grad_().to(device)\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                #######################\n                #  USE GPU FOR MODEL  #\n                #######################\n                # Total correct predictions\n                if torch.cuda.is_available():\n                    correct += (predicted.cpu() == labels.cpu()).sum()\n                else:\n                    correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Iteration: 500. Loss: 0.3877025246620178. Accuracy: 90\nIteration: 1000. Loss: 0.1337055265903473. Accuracy: 93\nIteration: 1500. Loss: 0.2038637101650238. Accuracy: 95\nIteration: 2000. Loss: 0.17892278730869293. Accuracy: 95\nIteration: 2500. Loss: 0.14455552399158478. Accuracy: 96\nIteration: 3000. Loss: 0.024540524929761887. Accuracy: 96\n</code></pre> <p>Alternative Term of Neural Network</p> <p>The alternative term is Universal Function Approximator. This is because ultimately we are trying to find a function that maps our input, \\(X\\), to our output, \\(y\\). </p>"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#summary","title":"Summary","text":"<p>We've learnt to...</p> <p>Success</p> <ul> <li> Logistic Regression Problems for Non-Linear Functions Representation<ul> <li> Cannot represent non-linear functions<ul> <li> $ y = 4x_1 + 2x_2^2 +3x_3^3 $</li> <li> $ y = x_1x_2$</li> </ul> </li> </ul> </li> <li> Introduced Non-Linearity to Logistic Regression to form a Neural Network</li> <li> Types of Non-Linearity<ul> <li> Sigmoid</li> <li> Tanh</li> <li> ReLU</li> </ul> </li> <li> Feedforward Neural Network Models<ul> <li> Model A: 1 hidden layer (sigmoid activation)</li> <li> Model B: 1 hidden layer (tanh activation)</li> <li> Model C: 1 hidden layer (ReLU activation)</li> <li> Model D: 2 hidden layers (ReLU activation)</li> <li> Model E: 3 hidden layers (ReLU activation)</li> </ul> </li> <li> Models Variation in Code<ul> <li> Modifying only step 3</li> </ul> </li> <li> Ways to Expand Model\u2019s Capacity<ul> <li> More non-linear activation units (neurons)</li> <li> More hidden layers</li> </ul> </li> <li> Cons of Expanding Capacity<ul> <li> Need more data</li> <li> Does not necessarily mean higher accuracy</li> </ul> </li> <li> GPU Code<ul> <li> 2 things on GPU<ul> <li> model</li> <li> tensors with gradient accumulation capabilities</li> </ul> </li> <li> Modifying only Step 4 &amp; Step 7</li> </ul> </li> <li> 7 Step Model Building Recap<ul> <li> Step 1: Load Dataset</li> <li> Step 2: Make Dataset Iterable</li> <li> Step 3: Create Model Class</li> <li> Step 4: Instantiate Model Class</li> <li> Step 5: Instantiate Loss Class</li> <li> Step 6: Instantiate Optimizer Class</li> <li> Step 7: Train Model</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#citation","title":"Citation","text":"<p>If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.</p> <p></p>"},{"location":"deep_learning/practical_pytorch/pytorch_gradients/","title":"Gradients with PyTorch","text":"<p>Run Jupyter Notebook</p> <p>You can run the code for this section in this jupyter notebook link.</p>"},{"location":"deep_learning/practical_pytorch/pytorch_gradients/#tensors-with-gradients","title":"Tensors with Gradients","text":""},{"location":"deep_learning/practical_pytorch/pytorch_gradients/#creating-tensors-with-gradients","title":"Creating Tensors with Gradients","text":"<ul> <li>Allows accumulation of gradients</li> </ul> <p>Method 1: Create tensor with gradients</p> <p>It is very similar to creating a tensor, all you need to do is to add an additional argument.</p> <pre><code>import torch\n</code></pre> <pre><code>a = torch.ones((2, 2), requires_grad=True)\na\n</code></pre> <pre><code>tensor([[ 1.,  1.],\n        [ 1.,  1.]])\n</code></pre> <p>Check if tensor requires gradients</p> <p>This should return True otherwise you've not done it right. <pre><code>a.requires_grad\n</code></pre></p> <pre><code>True\n</code></pre> <p>Method 2: Create tensor with gradients</p> <p>This allows you to create a tensor as usual then an additional line to allow it to accumulate gradients.</p> <pre><code># Normal way of creating gradients\na = torch.ones((2, 2))\n\n# Requires gradient\na.requires_grad_()\n\n# Check if requires gradient\na.requires_grad\n</code></pre> <pre><code>True\n</code></pre> <p>A tensor without gradients just for comparison</p> <p>If you do not do either of the methods above, you'll realize you will get False for checking for gradients. <pre><code># Not a variable\nno_gradient = torch.ones(2, 2)\n</code></pre></p> <pre><code>no_gradient.requires_grad\n</code></pre> <pre><code>False\n</code></pre> <p>Tensor with gradients addition operation</p> <pre><code># Behaves similarly to tensors\nb = torch.ones((2, 2), requires_grad=True)\nprint(a + b)\nprint(torch.add(a, b))\n</code></pre> <pre><code>tensor([[ 2.,  2.],\n        [ 2.,  2.]])\n\ntensor([[ 2.,  2.],\n        [ 2.,  2.]])\n</code></pre> <p>Tensor with gradients multiplication operation</p> <p>As usual, the operations we learnt previously for tensors apply for tensors with gradients. Feel free to try divisions, mean or standard deviation! <pre><code>print(a * b)\nprint(torch.mul(a, b))\n</code></pre></p> <pre><code>tensor([[ 1.,  1.],\n        [ 1.,  1.]])\ntensor([[ 1.,  1.],\n        [ 1.,  1.]])\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_gradients/#manually-and-automatically-calculating-gradients","title":"Manually and Automatically Calculating Gradients","text":"<p>What exactly is <code>requires_grad</code>? - Allows calculation of gradients w.r.t. the tensor that all allows gradients accumulation</p> \\[y_i = 5(x_i+1)^2\\] <p>Create tensor of size 2x1 filled with 1's that requires gradient</p> <pre><code>x = torch.ones(2, requires_grad=True)\nx\n</code></pre> <pre><code>tensor([ 1.,  1.])\n</code></pre> <p>Simple linear equation with x tensor created</p> \\[y_i\\bigr\\rvert_{x_i=1} = 5(1 + 1)^2 = 5(2)^2 = 5(4) = 20\\] <p>We should get a value of 20 by replicating this simple equation </p> <pre><code>y = 5 * (x + 1) ** 2\ny\n</code></pre> <pre><code>tensor([ 20.,  20.])\n</code></pre> <p>Simple equation with y tensor</p> <p>Backward should be called only on a scalar (i.e. 1-element tensor) or with gradient w.r.t. the variable</p> <p>Let's reduce y to a scalar then...</p> \\[o = \\frac{1}{2}\\sum_i y_i\\] <p>As you can see above, we've a tensor filled with 20's, so average them would return 20</p> <pre><code>o = (1/2) * torch.sum(y)\no\n</code></pre> <pre><code>tensor(20.)\n</code></pre> <p>Calculating first derivative</p> <p> Recap <code>y</code> equation: \\(y_i = 5(x_i+1)^2\\) </p> <p> Recap <code>o</code> equation: \\(o = \\frac{1}{2}\\sum_i y_i\\) </p> <p> Substitute <code>y</code> into <code>o</code> equation: \\(o = \\frac{1}{2} \\sum_i 5(x_i+1)^2\\) </p> \\[\\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i+1)]\\] \\[\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10}{2}(2) = 10\\] <p>We should expect to get 10, and it's so simple to do this with PyTorch with the following line...</p> <p>Get first derivative: <pre><code>o.backward()\n</code></pre></p> <p>Print out first derivative:</p> <pre><code>x.grad\n</code></pre> <pre><code>tensor([ 10.,  10.])\n</code></pre> <p>If x requires gradient and you create new objects with it, you get all gradients</p> <pre><code>print(x.requires_grad)\nprint(y.requires_grad)\nprint(o.requires_grad)\n</code></pre> <pre><code>True\nTrue\nTrue\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_gradients/#summary","title":"Summary","text":"<p>We've learnt to...</p> <p>Success</p> <ul> <li> Tensor with Gradients<ul> <li> Wraps a tensor for gradient accumulation</li> </ul> </li> <li> Gradients<ul> <li> Define original equation</li> <li> Substitute equation with <code>x</code> values</li> <li> Reduce to scalar output, <code>o</code> through <code>mean</code></li> <li> Calculate gradients with <code>o.backward()</code></li> <li> Then access gradients of the <code>x</code> tensor with <code>requires_grad</code> through <code>x.grad</code></li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_gradients/#citation","title":"Citation","text":"<p>If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.</p> <p></p>"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/","title":"Linear Regression with PyTorch","text":"<p>Run Jupyter Notebook</p> <p>You can run the code for this section in this jupyter notebook link.</p>"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#about-linear-regression","title":"About Linear Regression","text":""},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#simple-linear-regression-basics","title":"Simple Linear Regression Basics","text":"<ul> <li>Allows us to understand relationship between two continuous variables</li> <li>Example<ul> <li>x: independent variable<ul> <li>weight</li> </ul> </li> <li>y: dependent variable<ul> <li>height</li> </ul> </li> </ul> </li> <li>\\(y = \\alpha x + \\beta\\)</li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#example-of-simple-linear-regression","title":"Example of simple linear regression","text":"<p>Create plot for simple linear regression</p> <p>Take note that this code is not important at all. It simply creates random data points and does a simple best-fit line to best approximate the underlying function if one even exists.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Creates 50 random x and y numbers\nnp.random.seed(1)\nn = 50\nx = np.random.randn(n)\ny = x * np.random.randn(n)\n\n# Makes the dots colorful\ncolors = np.random.rand(n)\n\n# Plots best-fit line via polyfit\nplt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n\n# Plots the random x and y data points we created\n# Interestingly, alpha makes it more aesthetically pleasing\nplt.scatter(x, y, c=colors, alpha=0.5)\nplt.show()\n</code></pre> <p></p>"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#aim-of-linear-regression","title":"Aim of Linear Regression","text":"<ul> <li>Minimize the distance between the points and the line (\\(y = \\alpha x + \\beta\\))</li> <li>Adjusting<ul> <li>Coefficient: \\(\\alpha\\)</li> <li>Bias/intercept: \\(\\beta\\)</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#building-a-linear-regression-model-with-pytorch","title":"Building a Linear Regression Model with PyTorch","text":""},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#example","title":"Example","text":"<ul> <li>Coefficient: \\(\\alpha = 2\\)</li> <li>Bias/intercept: \\(\\beta = 1\\)</li> <li>Equation: \\(y = 2x + 1\\)</li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#building-a-toy-dataset","title":"Building a Toy Dataset","text":"<p>Create a list of values from 0 to 11</p> <pre><code>x_values = [i for i in range(11)]\n</code></pre> <pre><code>x_values\n</code></pre> <pre><code>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n</code></pre> <p>Convert list of numbers to numpy array</p> <pre><code># Convert to numpy\nx_train = np.array(x_values, dtype=np.float32)\nx_train.shape\n</code></pre> <pre><code>(11,)\n</code></pre> <p>Convert to 2-dimensional array</p> <p>If you don't this you will get an error stating you need 2D. Simply just reshape accordingly if you ever face such errors down the road. <pre><code># IMPORTANT: 2D required\nx_train = x_train.reshape(-1, 1)\nx_train.shape\n</code></pre></p> <pre><code>(11, 1)\n</code></pre> <p>Create list of y values</p> <p>We want y values for every x value we have above. </p> <p>\\(y = 2x + 1\\)</p> <pre><code>y_values = [2*i + 1 for i in x_values]\n</code></pre> <pre><code>y_values\n</code></pre> <pre><code>[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]\n</code></pre> <p>Alternative to create list of y values</p> <p>If you're weak in list iterators, this might be an easier alternative. <pre><code># In case you're weak in list iterators...\ny_values = []\nfor i in x_values:\n    result = 2*i + 1\n    y_values.append(result) \n</code></pre></p> <pre><code>y_values\n</code></pre> <pre><code>[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]\n</code></pre> <p>Convert to numpy array</p> <p>You will slowly get a hang on how when you deal with PyTorch tensors, you just keep on making sure your raw data is in numpy form to make sure everything's good.</p> <pre><code>y_train = np.array(y_values, dtype=np.float32)\ny_train.shape\n</code></pre> <pre><code>(11,)\n</code></pre> <p>Reshape y numpy array to 2-dimension</p> <pre><code># IMPORTANT: 2D required\ny_train = y_train.reshape(-1, 1)\ny_train.shape\n</code></pre> <pre><code>(11, 1)\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#building-model","title":"Building Model","text":"<p>Critical Imports</p> <pre><code>import torch\nimport torch.nn as nn\n</code></pre> <p>Create Model</p> <ol> <li>Linear model<ul> <li>True Equation: \\(y = 2x + 1\\)</li> </ul> </li> <li>Forward<ul> <li>Example<ul> <li>Input \\(x = 1\\)</li> <li>Output \\(\\hat y = ?\\)</li> </ul> </li> </ul> </li> </ol> <pre><code># Create class\nclass LinearRegressionModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LinearRegressionModel, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)  \n\n    def forward(self, x):\n        out = self.linear(x)\n        return out\n</code></pre> <p>Instantiate Model Class</p> <ul> <li>input: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</li> <li>desired output: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]</li> </ul> <pre><code>input_dim = 1\noutput_dim = 1\n\nmodel = LinearRegressionModel(input_dim, output_dim)\n</code></pre> <p>Instantiate Loss Class</p> <ul> <li>MSE Loss: Mean Squared Error</li> <li>\\(MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i)^2\\)<ul> <li>\\(\\hat y\\): prediction</li> <li>\\(y\\): true value</li> </ul> </li> </ul> <pre><code>criterion = nn.MSELoss()\n</code></pre> <p>Instantiate Optimizer Class</p> <ul> <li>Simplified equation<ul> <li>\\(\\theta = \\theta - \\eta \\cdot \\nabla_\\theta\\)<ul> <li>\\(\\theta\\): parameters (our variables)</li> <li>\\(\\eta\\): learning rate (how fast we want to learn)</li> <li>\\(\\nabla_\\theta\\): parameters' gradients</li> </ul> </li> </ul> </li> <li>Even simplier equation<ul> <li><code>parameters = parameters - learning_rate * parameters_gradients</code><ul> <li>parameters: \\(\\alpha\\) and \\(\\beta\\) in \\(y = \\alpha x + \\beta\\)</li> <li>desired parameters: \\(\\alpha = 2\\) and \\(\\beta = 1\\) in \\(y = 2x + 1\\) </li> </ul> </li> </ul> </li> </ul> <pre><code>learning_rate = 0.01\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n</code></pre> <p>Train Model</p> <ul> <li> <p>1 epoch: going through the whole x_train data once</p> <ul> <li>100 epochs: <ul> <li>100x mapping <code>x_train = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</code></li> </ul> </li> </ul> </li> <li> <p>Process </p> <ol> <li>Convert inputs/labels to tensors with gradients</li> <li>Clear gradient buffets</li> <li>Get output given inputs </li> <li>Get loss</li> <li>Get gradients w.r.t. parameters</li> <li>Update parameters using gradients<ul> <li><code>parameters = parameters - learning_rate * parameters_gradients</code></li> </ul> </li> <li>REPEAT</li> </ol> </li> </ul> <pre><code>epochs = 100\n</code></pre> <pre><code>for epoch in range(epochs):\n    epoch += 1\n    # Convert numpy array to torch Variable\n    inputs = torch.from_numpy(x_train).requires_grad_()\n    labels = torch.from_numpy(y_train)\n\n    # Clear gradients w.r.t. parameters\n    optimizer.zero_grad() \n\n    # Forward to get output\n    outputs = model(inputs)\n\n    # Calculate Loss\n    loss = criterion(outputs, labels)\n\n    # Getting gradients w.r.t. parameters\n    loss.backward()\n\n    # Updating parameters\n    optimizer.step()\n\n    print('epoch {}, loss {}'.format(epoch, loss.item()))\n</code></pre> <pre><code>epoch 1, loss 140.58143615722656\nepoch 2, loss 11.467253684997559\nepoch 3, loss 0.9358152747154236\nepoch 4, loss 0.07679400593042374\nepoch 5, loss 0.0067212567664682865\nepoch 6, loss 0.0010006226366385818\nepoch 7, loss 0.0005289533291943371\nepoch 8, loss 0.0004854927829001099\nepoch 9, loss 0.00047700389404781163\nepoch 10, loss 0.0004714332753792405\nepoch 11, loss 0.00046614606981165707\nepoch 12, loss 0.0004609318566508591\nepoch 13, loss 0.0004557870561257005\nepoch 14, loss 0.00045069155748933554\nepoch 15, loss 0.00044567222357727587\nepoch 16, loss 0.00044068993884138763\nepoch 17, loss 0.00043576463940553367\nepoch 18, loss 0.00043090470717288554\nepoch 19, loss 0.00042609183583408594\nepoch 20, loss 0.0004213254142086953\nepoch 21, loss 0.0004166301223449409\nepoch 22, loss 0.0004119801160413772\nepoch 23, loss 0.00040738462121225893\nepoch 24, loss 0.0004028224211651832\nepoch 25, loss 0.0003983367350883782\nepoch 26, loss 0.0003938761365134269\nepoch 27, loss 0.000389480876037851\nepoch 28, loss 0.00038514015614055097\nepoch 29, loss 0.000380824290914461\nepoch 30, loss 0.00037657516077160835\nepoch 31, loss 0.000372376263840124\nepoch 32, loss 0.0003682126116473228\nepoch 33, loss 0.0003640959912445396\nepoch 34, loss 0.00036003670538775623\nepoch 35, loss 0.00035601368290372193\nepoch 36, loss 0.00035203873994760215\nepoch 37, loss 0.00034810820943675935\nepoch 38, loss 0.000344215368386358\nepoch 39, loss 0.0003403784066904336\nepoch 40, loss 0.00033658024040050805\nepoch 41, loss 0.0003328165039420128\nepoch 42, loss 0.0003291067841928452\nepoch 43, loss 0.0003254293987993151\nepoch 44, loss 0.0003217888588551432\nepoch 45, loss 0.0003182037326041609\nepoch 46, loss 0.0003146533854305744\nepoch 47, loss 0.00031113551813177764\nepoch 48, loss 0.0003076607536058873\nepoch 49, loss 0.00030422292184084654\nepoch 50, loss 0.00030083119054324925\nepoch 51, loss 0.00029746422660537064\nepoch 52, loss 0.0002941471466328949\nepoch 53, loss 0.00029085995629429817\nepoch 54, loss 0.0002876132493838668\nepoch 55, loss 0.00028440452297218144\nepoch 56, loss 0.00028122696676291525\nepoch 57, loss 0.00027808290906250477\nepoch 58, loss 0.00027497278642840683\nepoch 59, loss 0.00027190230321139097\nepoch 60, loss 0.00026887087733484805\nepoch 61, loss 0.0002658693410921842\nepoch 62, loss 0.0002629039518069476\nepoch 63, loss 0.00025996880140155554\nepoch 64, loss 0.0002570618235040456\nepoch 65, loss 0.00025419273879379034\nepoch 66, loss 0.00025135406758636236\nepoch 67, loss 0.0002485490695107728\nepoch 68, loss 0.0002457649679854512\nepoch 69, loss 0.0002430236927466467\nepoch 70, loss 0.00024031475186347961\nepoch 71, loss 0.00023762597993481904\nepoch 72, loss 0.00023497406800743192\nepoch 73, loss 0.0002323519001947716\nepoch 74, loss 0.00022976362379267812\nepoch 75, loss 0.0002271933335578069\nepoch 76, loss 0.00022465786605607718\nepoch 77, loss 0.00022214400814846158\nepoch 78, loss 0.00021966728672850877\nepoch 79, loss 0.0002172116219298914\nepoch 80, loss 0.00021478648704942316\nepoch 81, loss 0.00021239375928416848\nepoch 82, loss 0.0002100227284245193\nepoch 83, loss 0.00020767028036061674\nepoch 84, loss 0.00020534756185952574\nepoch 85, loss 0.00020305956422816962\nepoch 86, loss 0.0002007894654525444\nepoch 87, loss 0.00019854879064951092\nepoch 88, loss 0.00019633043848443776\nepoch 89, loss 0.00019413618429098278\nepoch 90, loss 0.00019197272195015103\nepoch 91, loss 0.0001898303598864004\nepoch 92, loss 0.00018771187751553953\nepoch 93, loss 0.00018561164324637502\nepoch 94, loss 0.00018354636267758906\nepoch 95, loss 0.00018149390234611928\nepoch 96, loss 0.0001794644631445408\nepoch 97, loss 0.00017746571393217891\nepoch 98, loss 0.00017548113828524947\nepoch 99, loss 0.00017352371651213616\nepoch 100, loss 0.00017157981346827\n</code></pre> <p>Looking at predicted values</p> <pre><code># Purely inference\npredicted = model(torch.from_numpy(x_train).requires_grad_()).data.numpy()\npredicted\n</code></pre> <pre><code>array([[ 0.9756333],\n       [ 2.9791424],\n       [ 4.982651 ],\n       [ 6.9861603],\n       [ 8.98967  ],\n       [10.993179 ],\n       [12.996688 ],\n       [15.000196 ],\n       [17.003706 ],\n       [19.007215 ],\n       [21.010725 ]], dtype=float32)\n</code></pre> <p>Looking at training values</p> <p>These are the true values, you can see how it's able to predict similar values.</p> <pre><code># y = 2x + 1 \ny_train\n</code></pre> <pre><code>array([[ 1.],\n       [ 3.],\n       [ 5.],\n       [ 7.],\n       [ 9.],\n       [11.],\n       [13.],\n       [15.],\n       [17.],\n       [19.],\n       [21.]], dtype=float32)\n</code></pre> <p>Plot of predicted and actual values</p> <pre><code># Clear figure\nplt.clf()\n\n# Get predictions\npredicted = model(torch.from_numpy(x_train).requires_grad_()).data.numpy()\n\n# Plot true data\nplt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)\n\n# Plot predictions\nplt.plot(x_train, predicted, '--', label='Predictions', alpha=0.5)\n\n# Legend and plot\nplt.legend(loc='best')\nplt.show()\n</code></pre> <p></p> <p>Save Model</p> <pre><code>save_model = False\nif save_model is True:\n    # Saves only parameters\n    # alpha &amp; beta\n    torch.save(model.state_dict(), 'awesome_model.pkl')\n</code></pre> <p>Load Model</p> <pre><code>load_model = False\nif load_model is True:\n    model.load_state_dict(torch.load('awesome_model.pkl'))\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#building-a-linear-regression-model-with-pytorch-gpu","title":"Building a Linear Regression Model with PyTorch (GPU)","text":"<p>CPU Summary</p> <pre><code>import torch\nimport torch.nn as nn\n\n'''\nSTEP 1: CREATE MODEL CLASS\n'''\nclass LinearRegressionModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LinearRegressionModel, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)  \n\n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\n'''\nSTEP 2: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 1\noutput_dim = 1\n\nmodel = LinearRegressionModel(input_dim, output_dim)\n\n'''\nSTEP 3: INSTANTIATE LOSS CLASS\n'''\n\ncriterion = nn.MSELoss()\n\n'''\nSTEP 4: INSTANTIATE OPTIMIZER CLASS\n'''\n\nlearning_rate = 0.01\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n'''\nSTEP 5: TRAIN THE MODEL\n'''\nepochs = 100\nfor epoch in range(epochs):\n    epoch += 1\n    # Convert numpy array to torch Variable\n    inputs = torch.from_numpy(x_train).requires_grad_()\n    labels = torch.from_numpy(y_train)\n\n    # Clear gradients w.r.t. parameters\n    optimizer.zero_grad() \n\n    # Forward to get output\n    outputs = model(inputs)\n\n    # Calculate Loss\n    loss = criterion(outputs, labels)\n\n    # Getting gradients w.r.t. parameters\n    loss.backward()\n\n    # Updating parameters\n    optimizer.step()\n</code></pre> <p>GPU Summary</p> <ul> <li>Just remember always 2 things must be on GPU<ul> <li><code>model</code></li> <li><code>tensors</code></li> </ul> </li> </ul> <pre><code>import torch\nimport torch.nn as nn\nimport numpy as np\n\n'''\nSTEP 1: CREATE MODEL CLASS\n'''\nclass LinearRegressionModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LinearRegressionModel, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)  \n\n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\n'''\nSTEP 2: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 1\noutput_dim = 1\n\nmodel = LinearRegressionModel(input_dim, output_dim)\n\n\n#######################\n#  USE GPU FOR MODEL  #\n#######################\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n'''\nSTEP 3: INSTANTIATE LOSS CLASS\n'''\n\ncriterion = nn.MSELoss()\n\n'''\nSTEP 4: INSTANTIATE OPTIMIZER CLASS\n'''\n\nlearning_rate = 0.01\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n'''\nSTEP 5: TRAIN THE MODEL\n'''\nepochs = 100\nfor epoch in range(epochs):\n    epoch += 1\n    # Convert numpy array to torch Variable\n\n    #######################\n    #  USE GPU FOR MODEL  #\n    #######################\n    inputs = torch.from_numpy(x_train).to(device)\n    labels = torch.from_numpy(y_train).to(device)\n\n    # Clear gradients w.r.t. parameters\n    optimizer.zero_grad() \n\n    # Forward to get output\n    outputs = model(inputs)\n\n    # Calculate Loss\n    loss = criterion(outputs, labels)\n\n    # Getting gradients w.r.t. parameters\n    loss.backward()\n\n    # Updating parameters\n    optimizer.step()\n\n    # Logging\n    print('epoch {}, loss {}'.format(epoch, loss.item()))\n</code></pre> <pre><code>epoch 1, loss 336.0314025878906\nepoch 2, loss 27.67657470703125\nepoch 3, loss 2.5220539569854736\nepoch 4, loss 0.46732547879219055\nepoch 5, loss 0.2968060076236725\nepoch 6, loss 0.2800087630748749\nepoch 7, loss 0.27578213810920715\nepoch 8, loss 0.2726128399372101\nepoch 9, loss 0.269561231136322\nepoch 10, loss 0.2665504515171051\nepoch 11, loss 0.2635740041732788\nepoch 12, loss 0.26063060760498047\nepoch 13, loss 0.2577202618122101\nepoch 14, loss 0.2548423111438751\nepoch 15, loss 0.25199657678604126\nepoch 16, loss 0.24918246269226074\nepoch 17, loss 0.24639996886253357\nepoch 18, loss 0.24364829063415527\nepoch 19, loss 0.24092751741409302\nepoch 20, loss 0.2382371574640274\nepoch 21, loss 0.23557686805725098\nepoch 22, loss 0.2329462170600891\nepoch 23, loss 0.2303449958562851\nepoch 24, loss 0.22777271270751953\nepoch 25, loss 0.2252292037010193\nepoch 26, loss 0.22271405160427094\nepoch 27, loss 0.22022713720798492\nepoch 28, loss 0.21776780486106873\nepoch 29, loss 0.21533599495887756\nepoch 30, loss 0.21293145418167114\nepoch 31, loss 0.21055366098880768\nepoch 32, loss 0.20820240676403046\nepoch 33, loss 0.2058774083852768\nepoch 34, loss 0.20357847213745117\nepoch 35, loss 0.20130516588687897\nepoch 36, loss 0.1990572065114975\nepoch 37, loss 0.19683438539505005\nepoch 38, loss 0.19463638961315155\nepoch 39, loss 0.19246290624141693\nepoch 40, loss 0.1903136670589447\nepoch 41, loss 0.1881885528564453\nepoch 42, loss 0.18608702719211578\nepoch 43, loss 0.18400898575782776\nepoch 44, loss 0.18195408582687378\nepoch 45, loss 0.17992223799228668\nepoch 46, loss 0.17791320383548737\nepoch 47, loss 0.17592646181583405\nepoch 48, loss 0.17396186292171478\nepoch 49, loss 0.17201924324035645\nepoch 50, loss 0.17009828984737396\nepoch 51, loss 0.16819894313812256\nepoch 52, loss 0.16632060706615448\nepoch 53, loss 0.16446338593959808\nepoch 54, loss 0.16262666881084442\nepoch 55, loss 0.16081078350543976\nepoch 56, loss 0.15901507437229156\nepoch 57, loss 0.15723931789398193\nepoch 58, loss 0.15548335015773773\nepoch 59, loss 0.15374726057052612\nepoch 60, loss 0.1520303338766098\nepoch 61, loss 0.15033268928527832\nepoch 62, loss 0.14865389466285706\nepoch 63, loss 0.14699392020702362\nepoch 64, loss 0.14535246789455414\nepoch 65, loss 0.14372935891151428\nepoch 66, loss 0.14212435483932495\nepoch 67, loss 0.14053721725940704\nepoch 68, loss 0.13896773755550385\nepoch 69, loss 0.1374160647392273\nepoch 70, loss 0.1358814686536789\nepoch 71, loss 0.13436420261859894\nepoch 72, loss 0.13286370038986206\nepoch 73, loss 0.1313801407814026\nepoch 74, loss 0.12991292774677277\nepoch 75, loss 0.12846232950687408\nepoch 76, loss 0.1270277351140976\nepoch 77, loss 0.12560924887657166\nepoch 78, loss 0.12420656532049179\nepoch 79, loss 0.12281957268714905\nepoch 80, loss 0.1214480847120285\nepoch 81, loss 0.12009195983409882\nepoch 82, loss 0.1187509223818779\nepoch 83, loss 0.11742479354143143\nepoch 84, loss 0.11611353605985641\nepoch 85, loss 0.11481687426567078\nepoch 86, loss 0.11353478580713272\nepoch 87, loss 0.11226697266101837\nepoch 88, loss 0.11101329326629639\nepoch 89, loss 0.10977360606193542\nepoch 90, loss 0.10854770988225937\nepoch 91, loss 0.10733554512262344\nepoch 92, loss 0.10613703727722168\nepoch 93, loss 0.10495180636644363\nepoch 94, loss 0.10377981513738632\nepoch 95, loss 0.10262089222669601\nepoch 96, loss 0.10147502273321152\nepoch 97, loss 0.1003417894244194\nepoch 98, loss 0.09922132641077042\nepoch 99, loss 0.0981132984161377\nepoch 100, loss 0.09701769798994064\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#summary","title":"Summary","text":"<p>We've learnt to...</p> <p>Success</p> <ul> <li> Simple linear regression basics<ul> <li> \\(y = Ax + B\\)</li> <li> \\(y = 2x + 1\\)</li> </ul> </li> <li> Example of simple linear regression</li> <li> Aim of linear regression<ul> <li> Minimizing distance between the points and the line<ul> <li> Calculate \"distance\" through <code>MSE</code></li> <li> Calculate <code>gradients</code></li> <li> Update parameters with <code>parameters = parameters - learning_rate * gradients</code></li> <li> Slowly update parameters \\(A\\) and \\(B\\) model the linear relationship between \\(y\\) and \\(x\\) of the form \\(y = 2x + 1\\)</li> </ul> </li> </ul> </li> <li> Built a linear regression model in CPU and GPU<ul> <li> Step 1: Create Model Class</li> <li> Step 2: Instantiate Model Class</li> <li> Step 3: Instantiate Loss Class</li> <li> Step 4: Instantiate Optimizer Class</li> <li> Step 5: Train Model</li> </ul> </li> <li> Important things to be on GPU<ul> <li> <code>model</code></li> <li> <code>tensors with gradients</code></li> </ul> </li> <li> How to bring to GPU?<ul> <li><code>model_name.to(device)</code></li> <li><code>variable_name.to(device)</code></li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#citation","title":"Citation","text":"<p>If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.</p> <p></p>"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/","title":"Logistic Regression with PyTorch","text":"<p>Run Jupyter Notebook</p> <p>You can run the code for this section in this jupyter notebook link.</p>"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#about-logistic-regression","title":"About Logistic Regression","text":""},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#logistic-regression-basics","title":"Logistic Regression Basics","text":""},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#classification-algorithm","title":"Classification algorithm","text":"<ul> <li>Example: Spam vs No Spam<ul> <li>Input: Bunch of words</li> <li>Output: Probability spam or not</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#basic-comparison","title":"Basic Comparison","text":"<ul> <li>Linear regression<ul> <li>Output: numeric value given inputs</li> </ul> </li> <li>Logistic regression:<ul> <li>Output: probability [0, 1] given input belonging to a class</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#inputoutput-comparison","title":"Input/Output Comparison","text":"<ul> <li>Linear regression: Multiplication<ul> <li>Input: [1]<ul> <li>Output: 2</li> </ul> </li> <li>Input: [2]<ul> <li>Output: 4</li> </ul> </li> <li>Trying to model the relationship <code>y = 2x</code></li> </ul> </li> <li>Logistic regression: Spam<ul> <li>Input: \"Sign up to get 1 million dollars by tonight\"<ul> <li>Output: p = 0.8</li> </ul> </li> <li>Input: \"This is a receipt for your recent purchase with Amazon\"<ul> <li>Output: p = 0.3</li> </ul> </li> <li>p: probability it is spam</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#problems-of-linear-regression","title":"Problems of Linear Regression","text":"<ul> <li>Example<ul> <li>Fever</li> <li>Input: temperature</li> <li>Output: fever or no fever</li> </ul> </li> <li>Remember<ul> <li>Linear regression: minimize error between points and line</li> </ul> </li> </ul> <p>Linear Regression Problem 1: Fever value can go negative (below 0) and positive (above 1)</p> <p>If you simply tried to do a simple linear regression on this fever problem, you would realize an apparent error. Fever can go beyond 1 and below 0 which does not make sense in this context. <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nx = [1, 5, 10, 10, 25, 50, 70, 75, 100,]\ny = [0, 0, 0, 0, 0, 1, 1, 1, 1]\n\ncolors = np.random.rand(len(x))\nplt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\nplt.ylabel(\"Fever\")\nplt.xlabel(\"Temperature\")\n\nplt.scatter(x, y, c=colors, alpha=0.5)\nplt.show()\n</code></pre></p> <p></p> <p>Linear Regression Problem 2: Fever points are not predicted with the presence of outliers</p> <p>Previously at least some points could be properly predicted. However, with the presence of outliers, everything goes wonky for simple linear regression, having no predictive capacity at all. <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nx = [1, 5, 10, 10, 25, 50, 70, 75, 300]\ny = [0, 0, 0, 0, 0, 1, 1, 1, 1]\n\ncolors = np.random.rand(len(x))\nplt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\nplt.ylabel(\"Fever\")\nplt.xlabel(\"Temperature\")\n\nplt.scatter(x, y, c=colors, alpha=0.5)\nplt.show()\n</code></pre></p> <p></p>"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#logistic-regression-in-depth","title":"Logistic Regression In-Depth","text":""},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#predicting-probability","title":"Predicting Probability","text":"<ul> <li>Linear regression doesn't work</li> <li>Instead of predicting direct values: predict probability</li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#logistic-function-g","title":"Logistic Function g()","text":"<ul> <li>\"Two-class logistic regression\"</li> <li>\\(\\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b}\\)<ul> <li>Where \\(\\boldsymbol{y}\\) is a vector comprising the 2-class prediction \\(y_0\\) and \\(y_1\\)</li> <li>Where the labels are \\(y_0 = 0\\)  and \\(y_1 = 1\\)</li> <li>Also, it's bolded because it's a vector, not a matrix.</li> </ul> </li> <li>\\(g(y_1) = \\frac {1} {1 + e^{-y_1}}\\)<ul> <li>\\(g(y_1)\\) = Estimated probability that \\(y = 1\\)</li> </ul> </li> <li>\\(g(y_0) = 1 - g(y_1)\\)<ul> <li>\\(g(y_0)\\) = Estimated probability that \\(y = 0\\)</li> </ul> </li> <li>For our illustration above, we have 4 classes, so we have to use softmax function explained below</li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#softmax-function-g","title":"Softmax Function g()","text":"<ul> <li>\"Multi-class logistic regression\"<ul> <li>Generalization of logistic function, where you can derive back to the logistic function if you've a 2 class classification problem</li> <li>Here, we will use a 4 class example (K = 4) as shown above to be very clear in how it relates back to that simple examaple.</li> </ul> </li> <li>\\(\\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b}\\)<ul> <li>Where \\(\\boldsymbol{y}\\) is a vector comprising the 4-class prediction \\(y_0, y_1, y_2, y_3\\)</li> <li>Where the 4 labels (K = 4) are \\(y_0 = 0, y_1 = 1, y_2 = 2, y_3 = 3\\)</li> </ul> </li> <li>\\(g(y_i) = \\frac {e^{y_i} } {\\sum^K_i e^{y_i}}\\) where K = 4 because we have 4 classes<ul> <li>To put numbers to this equation in relation to the illustration above where we've \\(y_0 = 1.3, y_1 = 1.2, y = 4.5, y = 4.8\\)<ul> <li>\\(g(y_0) = \\frac {e^{1.3}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.017\\)</li> <li>\\(g(y_1) = \\frac {e^{1.2}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.015\\)</li> <li>\\(g(y_2) = \\frac {e^{4.5}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.412\\)</li> <li>\\(g(y_3) = \\frac {e^{4.8}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.556\\)</li> <li>\\(g(y_0) + g(y_1) + g(y_2) + g(y_3) = 1.0\\)</li> <li>All softmax outputs have to sum to one as they represent a probability distribution over K classes. </li> </ul> </li> </ul> </li> <li>Take note how these numbers are not exactly as in the illustration in the softmax box but the concept is important (intentionally made so).<ul> <li>\\(y_0\\) and \\(y_1\\) are approximately similar in values and they return similar probabilities.</li> <li>Similarly, \\(y_2\\) and \\(y_3\\) are approximately similar in values and they return similar probabilities.</li> </ul> </li> </ul> <p>Softmax versus Soft(arg)max</p> <p>Do you know many researchers and anyone in deep learning in general use the term softmax when it should be soft(arg)max.</p> <p>This is because soft(arg)max returns the probability distribution over K classes, a vector. </p> <p>However, softmax only returns the max! This means you will be getting a scalar value versus a probability distribution.</p> <p>According to my friend, Alfredo Canziani (postdoc in NYU under Yann Lecun), it was actually a mistake made in the original paper previously but it was too late because the term softmax was adopted. Full credits to him for this tip.</p>"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#cross-entropy-function-d-for-2-class","title":"Cross Entropy Function D() for 2 Class","text":"<ul> <li>Take note that here, \\(S\\) is our softmax outputs and \\(L\\) are our labels</li> <li>\\(D(S, L) = -(L log S + (1-L)log(1-S))\\)<ul> <li>If L = 0 (label)<ul> <li>\\(D(S, 0) = - log(1-S)\\)<ul> <li>\\(- log(1-S)\\): less positive if \\(S \\longrightarrow 0\\)</li> <li>\\(- log(1-S)\\): more positive if \\(S \\longrightarrow 1\\) (BIGGER LOSS)</li> </ul> </li> </ul> </li> <li>If L = 1 (label)<ul> <li>\\(D(S, 1) = - log S\\)<ul> <li>\\(-log(S)\\): less positive if \\(S \\longrightarrow 1\\)</li> <li>\\(-log(S)\\): more positive if \\(S \\longrightarrow 0\\) (BIGGER LOSS)</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>Numerical example of bigger or small loss</p> <p>You get a small error of 1e-5 if your label = 0 and your S is closer to 0 (very correct prediction). <pre><code>import math\nprint(-math.log(1 - 0.00001))\n</code></pre></p> <p>You get a large error of 11.51 if your label is 0 and S is near to 1 (very wrong prediction). <pre><code>print(-math.log(1 - 0.99999)) \n</code></pre></p> <p>You get a small error of -1e-5 if your label is 1 and S is near 1 (very correct prediction). <pre><code>print(-math.log(0.99999))\n</code></pre></p> <p>You get a big error of -11.51 if your label is 1 and S is near 0 (very wrong prediction). <pre><code>print(-math.log(0.00001))\n</code></pre></p> <pre><code>1.0000050000287824e-05\n11.51292546497478\n1.0000050000287824e-05\n11.512925464970229\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#cross-entropy-function-d-for-more-than-2-class","title":"Cross Entropy Function D() for More Than 2 Class","text":"<ul> <li>For the case where we have more than 2 class, we need a more generalized function</li> <li>\\(D(S, L) = - \\sum^K_1 L_i log(S_i)\\)<ul> <li>\\(K\\): number of classes</li> <li>\\(L_i\\): label of i-th class, 1 if that's the class else 0</li> <li>\\(S_i\\): output of softmax for i-th class</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#cross-entropy-loss-over-n-samples","title":"Cross Entropy Loss over N samples","text":"<ul> <li>Goal: Minimizing Cross Entropy Loss, L</li> <li>\\(Loss = \\frac {1}{N} \\sum_j^N D_j\\)<ul> <li>\\(D_j\\): j-th sample of cross entropy function \\(D(S, L)\\)</li> <li>\\(N\\): number of samples</li> <li>\\(Loss\\): average cross entropy loss over N samples</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#building-a-logistic-regression-model-with-pytorch","title":"Building a Logistic Regression Model with PyTorch","text":""},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#steps","title":"Steps","text":"<ul> <li>Step 1: Load Dataset</li> <li>Step 2: Make Dataset Iterable</li> <li>Step 3: Create Model Class</li> <li>Step 4: Instantiate Model Class</li> <li>Step 5: Instantiate Loss Class</li> <li>Step 6: Instantiate Optimizer Class</li> <li>Step 7: Train Model</li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-1a-loading-mnist-train-dataset","title":"Step 1a: Loading MNIST Train Dataset","text":"<p>Images from 1 to 9</p> <p>Inspect length of training dataset</p> <p>You can easily load MNIST dataset with PyTorch. Here we inspect the training set, where our algorithms will learn from, and you will discover it is made up of 60,000 images. <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n</code></pre></p> <pre><code>train_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n</code></pre> <pre><code>len(train_dataset)\n</code></pre> <pre><code>60000\n</code></pre> <p>Inspecting a single image</p> <p>So this is how a single image is represented in numbers. It's actually a 28 pixel x 28 pixel image which is why you would end up with this 28x28 matrix of numbers. <pre><code>train_dataset[0]\n</code></pre></p> <pre><code>(tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0118,  0.0706,\n            0.0706,  0.0706,  0.4941,  0.5333,  0.6863,  0.1020,  0.6510,\n            1.0000,  0.9686,  0.4980,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.1176,  0.1412,  0.3686,  0.6039,  0.6667,  0.9922,\n            0.9922,  0.9922,  0.9922,  0.9922,  0.8824,  0.6745,  0.9922,\n            0.9490,  0.7647,  0.2510,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.1922,  0.9333,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922,\n            0.9922,  0.9922,  0.9922,  0.9843,  0.3647,  0.3216,  0.3216,\n            0.2196,  0.1529,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0706,  0.8588,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922,\n            0.7765,  0.7137,  0.9686,  0.9451,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.3137,  0.6118,  0.4196,  0.9922,  0.9922,  0.8039,\n            0.0431,  0.0000,  0.1686,  0.6039,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0549,  0.0039,  0.6039,  0.9922,  0.3529,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.5451,  0.9922,  0.7451,\n            0.0078,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0431,  0.7451,  0.9922,\n            0.2745,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1373,  0.9451,\n            0.8824,  0.6275,  0.4235,  0.0039,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.3176,\n            0.9412,  0.9922,  0.9922,  0.4667,  0.0980,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.1765,  0.7294,  0.9922,  0.9922,  0.5882,  0.1059,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0627,  0.3647,  0.9882,  0.9922,  0.7333,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.9765,  0.9922,  0.9765,  0.2510,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.1804,  0.5098,  0.7176,  0.9922,  0.9922,  0.8118,  0.0078,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1529,  0.5804,\n            0.8980,  0.9922,  0.9922,  0.9922,  0.9804,  0.7137,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0941,  0.4471,  0.8667,  0.9922,\n            0.9922,  0.9922,  0.9922,  0.7882,  0.3059,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0902,  0.2588,  0.8353,  0.9922,  0.9922,  0.9922,\n            0.9922,  0.7765,  0.3176,  0.0078,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0706,\n            0.6706,  0.8588,  0.9922,  0.9922,  0.9922,  0.9922,  0.7647,\n            0.3137,  0.0353,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2157,  0.6745,  0.8863,\n            0.9922,  0.9922,  0.9922,  0.9922,  0.9569,  0.5216,  0.0431,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.5333,  0.9922,  0.9922,\n            0.9922,  0.8314,  0.5294,  0.5176,  0.0627,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]),\n tensor(5))\n</code></pre> <p>Inspecting a single data point in the training dataset</p> <p>When you load MNIST dataset, each data point is actually a tuple containing the image matrix and the label.</p> <pre><code>type(train_dataset[0])\n</code></pre> <pre><code>tuple\n</code></pre> <p>Inspecting training dataset first element of tuple</p> <p>This means to access the image, you need to access the first element in the tuple.</p> <pre><code># Input Matrix\ntrain_dataset[0][0].size()\n</code></pre> <pre><code># A 28x28 sized image of a digit\ntorch.Size([1, 28, 28])\n</code></pre> <p>Inspecting training dataset second element of tuple</p> <p>The second element actually represents the image's label. Meaning if the second element says 5, it means the 28x28 matrix of numbers represent a digit 5.</p> <pre><code># Label\ntrain_dataset[0][1]\n</code></pre> <pre><code>tensor(5)\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#displaying-mnist","title":"Displaying MNIST","text":"<p>Verifying shape of MNIST image</p> <p>As mentioned, a single MNIST image is of the shape 28 pixel x 28 pixel.</p> <pre><code>import matplotlib.pyplot as plt\n%matplotlib inline  \nimport numpy as np\n</code></pre> <pre><code>train_dataset[0][0].numpy().shape\n</code></pre> <pre><code>(1, 28, 28)\n</code></pre> <p>Plot image of MNIST image</p> <pre><code>show_img = train_dataset[0][0].numpy().reshape(28, 28)\n</code></pre> <pre><code>plt.imshow(show_img, cmap='gray')\n</code></pre> <p></p> <p>Second element of tuple shows label</p> <p>As you would expect, the label is 5. <pre><code># Label\ntrain_dataset[0][1]\n</code></pre></p> <pre><code>tensor(5)\n</code></pre> <p>Plot second image of MNIST image</p> <pre><code>show_img = train_dataset[1][0].numpy().reshape(28, 28)\n</code></pre> <pre><code>plt.imshow(show_img, cmap='gray')\n</code></pre> <p></p> <p>Second element of tuple shows label</p> <p>We should see 0 here as the label. <pre><code># Label\ntrain_dataset[1][1]\n</code></pre></p> <pre><code>tensor(0)\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-1b-loading-mnist-test-dataset","title":"Step 1b: Loading MNIST Test Dataset","text":"<ul> <li>Show our algorithm works beyond the data we have trained on.</li> <li>Out-of-sample</li> </ul> <p>Load test dataset</p> <p>Compared to the 60k images in the training set, the testing set where the model will not be trained on has 10k images to check for its out-of-sample performance. <pre><code>test_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n</code></pre></p> <pre><code>len(test_dataset)\n</code></pre> <pre><code>10000\n</code></pre> <p>Test dataset elements</p> <p>Exactly like the training set, the testing set has 10k tuples containing the 28x28 matrices and their respective labels. <pre><code>type(test_dataset[0])\n</code></pre></p> <pre><code>tuple\n</code></pre> <p>Test dataset first element in tuple</p> <p>This contains the image matrix, similar to the training set. <pre><code># Image matrix\ntest_dataset[0][0].size()\n</code></pre></p> <pre><code>torch.Size([1, 28, 28])\n</code></pre> <p>Plot image sample from test dataset</p> <pre><code>show_img = test_dataset[0][0].numpy().reshape(28, 28)\nplt.imshow(show_img, cmap='gray')\n</code></pre> <p></p> <p>Test dataset second element in tuple</p> <pre><code># Label\ntest_dataset[0][1]\n</code></pre> <pre><code>tensor(7)\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-2-make-dataset-iterable","title":"Step 2: Make Dataset Iterable","text":"<ul> <li>Aim: make the dataset iterable</li> <li>totaldata: 60000</li> <li>minibatch: 100<ul> <li>Number of examples in 1 iteration</li> </ul> </li> <li>iterations: 3000<ul> <li>1 iteration: one mini-batch forward &amp; backward pass</li> </ul> </li> <li>epochs<ul> <li>1 epoch: running through the whole dataset once</li> <li>\\(epochs = iterations \\div \\frac{totaldata}{minibatch} = 3000 \\div \\frac{60000}{100} = 5\\)</li> </ul> </li> </ul> <p>Recap training dataset</p> <p>Remember training dataset has 60k images and testing dataset has 10k images. <pre><code>len(train_dataset)\n</code></pre></p> <pre><code>60000\n</code></pre> <p>Defining epochs</p> <p>When the model goes through the whole 60k images once, learning how to classify 0-9, it's consider 1 epoch. </p> <p>However, there's a concept of batch size where it means the model would look at 100 images before updating the model's weights, thereby learning. When the model updates its weights (parameters) after looking at all the images, this is considered 1 iteration.</p> <pre><code>batch_size = 100\n</code></pre> <p>We arbitrarily set 3000 iterations here which means the model would update 3000 times. <pre><code>n_iters = 3000\n</code></pre></p> <p>One epoch consists of 60,000 / 100 = 600 iterations. Because we would like to go through 3000 iterations, this implies we would have 3000 / 600 = 5 epochs as each epoch has 600 iterations. </p> <pre><code>num_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\nnum_epochs\n</code></pre> <pre><code>5\n</code></pre> <p>Create Iterable Object: Training Dataset</p> <pre><code>train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n</code></pre> <p>Check Iterability</p> <pre><code>import collections\nisinstance(train_loader, collections.Iterable)\n</code></pre> <pre><code>True\n</code></pre> <p>Create Iterable Object: Testing Dataset</p> <pre><code># Iterable object\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n</code></pre> <p>Check iterability of testing dataset</p> <pre><code>isinstance(test_loader, collections.Iterable)\n</code></pre> <pre><code>True\n</code></pre> <p>Iterate through dataset</p> <p>This is just a simplified example of what we're doing above where we're creating an iterable object <code>lst</code> to loop through so we can access all the images <code>img_1</code> and <code>img_2</code>.</p> <p>Above, the equivalent of <code>lst</code> is <code>train_loader</code> and <code>test_loader</code>.</p> <pre><code>img_1 = np.ones((28, 28))\nimg_2 = np.ones((28, 28))\nlst = [img_1, img_2]\n</code></pre> <pre><code># Need to iterate\n# Think of numbers as the images\nfor i in lst:\n    print(i.shape)\n</code></pre> <pre><code>(28, 28)\n(28, 28)\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-3-building-model","title":"Step 3: Building Model","text":"<p>Create model class</p> <pre><code># Same as linear regression! \nclass LogisticRegressionModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LogisticRegressionModel, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        out = self.linear(x)\n        return out\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-4-instantiate-model-class","title":"Step 4: Instantiate Model Class","text":"<ul> <li>Input dimension: <ul> <li>Size of image</li> <li>\\(28 \\times 28 = 784\\)</li> </ul> </li> <li>Output dimension: 10<ul> <li>0, 1, 2, 3, 4, 5, 6, 7, 8, 9</li> </ul> </li> </ul> <p>Check size of dataset</p> <p>This should be 28x28. <pre><code># Size of images\ntrain_dataset[0][0].size()\n</code></pre></p> <pre><code>torch.Size([1, 28, 28])\n</code></pre> <p>Instantiate model class based on input and out dimensions</p> <p>As we're trying to classify digits 0-9 a total of 10 classes, our output dimension is 10. </p> <p>And we're feeding the model with 28x28 images, hence our input dimension is 28x28. <pre><code>input_dim = 28*28\noutput_dim = 10\n\nmodel = LogisticRegressionModel(input_dim, output_dim)\n</code></pre></p>"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-5-instantiate-loss-class","title":"Step 5: Instantiate Loss Class","text":"<ul> <li>Logistic Regression: Cross Entropy Loss<ul> <li>Linear Regression: MSE</li> </ul> </li> </ul> <p>Create Cross Entry Loss Class</p> <p>Unlike linear regression, we do not use MSE here, we need Cross Entry Loss to calculate our loss before we backpropagate and update our parameters.</p> <pre><code>criterion = nn.CrossEntropyLoss()  \n</code></pre> <p>What happens in nn.CrossEntropyLoss()?</p> <p>It does 2 things at the same time. </p> <p> 1. Computes softmax (logistic/softmax function)  2. Computes cross entropy</p> <p></p>"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-6-instantiate-optimizer-class","title":"Step 6: Instantiate Optimizer Class","text":"<ul> <li>Simplified equation<ul> <li>\\(\\theta = \\theta - \\eta \\cdot \\nabla_\\theta\\)<ul> <li>\\(\\theta\\): parameters (our variables)</li> <li>\\(\\eta\\): learning rate (how fast we want to learn)</li> <li>\\(\\nabla_\\theta\\): parameters' gradients</li> </ul> </li> </ul> </li> <li>Even simplier equation<ul> <li><code>parameters = parameters - learning_rate * parameters_gradients</code></li> <li>At every iteration, we update our model's parameters</li> </ul> </li> </ul> <p>Create optimizer</p> <p>Similar to what we've covered above, this calculates the parameters' gradients and update them subsequently. <pre><code>learning_rate = 0.001\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n</code></pre></p> <p>Parameters In-Depth</p> <p>You'll realize we have 2 sets of parameters, 10x784 which is A and 10x1 which is b in the \\(y = AX + b\\) equation where X is our input of size 784.</p> <p>We'll go into details subsequently how these parameters interact with our input to produce our 10x1 output. </p> <pre><code># Type of parameter object\nprint(model.parameters())\n\n# Length of parameters\nprint(len(list(model.parameters())))\n\n# FC 1 Parameters \nprint(list(model.parameters())[0].size())\n\n# FC 1 Bias Parameters\nprint(list(model.parameters())[1].size())\n</code></pre> <pre><code>&lt;generator object Module.parameters at 0x7ff7c884f830&gt;\n2\ntorch.Size([10, 784])\ntorch.Size([10])\n</code></pre> <p>Quick Matrix Product Review</p> <ul> <li>Example 1: matrix product<ul> <li>\\(A: (100, 10)\\)</li> <li>\\(B: (10, 1)\\)</li> <li>\\(A \\cdot B = (100, 10) \\cdot (10, 1) = (100, 1)\\)</li> </ul> </li> <li>Example 2: matrix product<ul> <li>\\(A: (50, 5)\\)</li> <li>\\(B: (5, 2)\\)</li> <li>\\(A \\cdot B = (50, 5) \\cdot (5, 2) = (50, 2)\\)</li> </ul> </li> <li>Example 3: element-wise addition<ul> <li>\\(A: (10, 1)\\)</li> <li>\\(B: (10, 1)\\)</li> <li>\\(A + B = (10, 1)\\)</li> </ul> </li> </ul> <p></p>"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-7-train-model","title":"Step 7: Train Model","text":"<p>7 step process for training models</p> <ul> <li>Process <ol> <li>Convert inputs/labels to tensors with gradients</li> <li>Clear gradient buffets</li> <li>Get output given inputs</li> <li>Get loss</li> <li>Get gradients w.r.t. parameters</li> <li>Update parameters using gradients<ul> <li><code>parameters = parameters - learning_rate * parameters_gradients</code></li> </ul> </li> <li>REPEAT</li> </ol> </li> </ul> <pre><code>iter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as Variable\n        images = images.view(-1, 28*28).requires_grad_()\n        labels = labels\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images to a Torch Variable\n                images = images.view(-1, 28*28).requires_grad_()\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Iteration: 500. Loss: 1.8513233661651611. Accuracy: 70\nIteration: 1000. Loss: 1.5732524394989014. Accuracy: 77\nIteration: 1500. Loss: 1.3840199708938599. Accuracy: 79\nIteration: 2000. Loss: 1.1711134910583496. Accuracy: 81\nIteration: 2500. Loss: 1.1094708442687988. Accuracy: 82\nIteration: 3000. Loss: 1.002761721611023. Accuracy: 82\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#break-down-accuracy-calculation","title":"Break Down Accuracy Calculation","text":"<p>Printing outputs of our model</p> <p>As we've trained our model, we can extract the accuracy calculation portion to understand what's happening without re-training the model.</p> <p>This would print out the output of the model's predictions on your notebook.</p> <pre><code>iter_test = 0\nfor images, labels in test_loader:\n    iter_test += 1\n    images = images.view(-1, 28*28).requires_grad_()\n    outputs = model(images)\n    if iter_test == 1:\n        print('OUTPUTS')\n        print(outputs)\n    _, predicted = torch.max(outputs.data, 1)\n</code></pre> <pre><code>OUTPUTS\ntensor([[-0.4181, -1.0784, -0.4840, -0.0985, -0.2394, -0.1801, -1.1639,\n          2.9352, -0.1552,  0.8852],\n        [ 0.5117, -0.1099,  1.5295,  0.8863, -1.8813,  0.5967,  1.3632,\n         -1.8977,  0.4183, -1.4990],\n        [-1.0126,  2.4112,  0.2373,  0.0857, -0.7007, -0.2015, -0.3428,\n         -0.2548,  0.1659, -0.4703],\n        [ 2.8072, -2.2973, -0.0984, -0.4313, -0.9619,  0.8670,  1.2201,\n          0.3752, -0.2873, -0.3272],\n        [-0.0343, -2.0043,  0.5081, -0.6452,  1.8647, -0.6924,  0.1435,\n          0.4330,  0.2958,  1.0339],\n        [-1.5392,  2.9070,  0.2297,  0.3139, -0.6863, -0.2734, -0.8377,\n         -0.1238,  0.3285, -0.3004],\n        [-1.2037, -1.3739, -0.5947,  0.3530,  1.4205,  0.0593, -0.7307,\n          0.6642,  0.3937,  0.8004],\n        [-1.4439, -0.3284, -0.7652, -0.0952,  0.9323,  0.3006,  0.0238,\n         -0.0810,  0.0612,  1.3295],\n        [ 0.5409, -0.5266,  0.9914, -1.2369,  0.6583,  0.0992,  0.8525,\n         -1.0562,  0.2013,  0.0462],\n        [-0.6548, -0.7253, -0.9825, -1.1663,  0.9076, -0.0694, -0.3708,\n          1.8270,  0.2457,  1.5921],\n        [ 3.2147, -1.7689,  0.8531,  1.2320, -0.8126,  1.1251, -0.2776,\n         -1.4244,  0.5930, -1.6183],\n        [ 0.7470, -0.5545,  1.0251,  0.0529,  0.4384, -0.5934,  0.7666,\n         -1.0084,  0.5313, -0.3465],\n        [-0.7916, -1.7064, -0.7805, -1.1588,  1.3284, -0.1708, -0.2092,\n          0.9495,  0.1033,  2.0208],\n        [ 3.0602, -2.3578, -0.2576, -0.2198, -0.2372,  0.9765, -0.1514,\n         -0.5380,  0.7970,  0.1374],\n        [-1.2613,  2.8594, -0.0874,  0.1974, -1.2018, -0.0064, -0.0923,\n         -0.2142,  0.2575, -0.3218],\n        [ 0.4348, -0.7216,  0.0021,  1.2864, -0.5062,  0.7761, -0.3236,\n         -0.5667,  0.5431, -0.7781],\n        [-0.2157, -2.0200,  0.1829, -0.6882,  1.3815, -0.7609, -0.0902,\n          0.8647,  0.3679,  1.8843],\n        [ 0.0950, -1.5009, -0.6347,  0.3662, -0.4679, -0.0359, -0.7671,\n          2.7155, -0.3991,  0.5737],\n        [-0.7005, -0.5366, -0.0434,  1.1289, -0.5873,  0.2555,  0.8187,\n         -0.6557,  0.1241, -0.4297],\n        [-1.0635, -1.5991, -0.4677, -0.1231,  2.0445,  0.1128, -0.1825,\n          0.1075,  0.0348,  1.4317],\n        [-1.0319, -0.1595, -1.3415,  0.1095,  0.5339,  0.1973, -1.3272,\n          1.5765,  0.4784,  1.4176],\n        [-0.4928, -1.5653, -0.0672,  0.3325,  0.5359,  0.5368,  2.1542,\n         -1.4276,  0.3605,  0.0587],\n        [-0.4761,  0.2958,  0.6597, -0.2658,  1.1279, -1.0676,  1.2506,\n         -0.2059, -0.1489,  0.1051],\n        [-0.0764, -0.9274, -0.6838,  0.3464, -0.2656,  1.4099,  0.4486,\n         -0.9527,  0.5682,  0.0156],\n        [-0.6900, -0.9611,  0.1395, -0.0079,  1.5424, -0.3208, -0.2682,\n          0.3586, -0.2771,  1.0389],\n        [ 4.3606, -2.8621,  0.6310, -0.9657, -0.2486,  1.2009,  1.1873,\n         -0.8255, -0.2103, -1.2172],\n        [-0.1000, -1.4268, -0.4627, -0.1041,  0.2959, -0.1392, -0.6855,\n          1.8622, -0.2580,  1.1347],\n        [-0.3625, -2.1323, -0.2224, -0.8754,  2.4684,  0.0295,  0.1161,\n         -0.2660,  0.3037,  1.4570],\n        [ 2.8688, -2.4517,  0.1782,  1.1149, -1.0898,  1.1062, -0.0681,\n         -0.5697,  0.8888, -0.6965],\n        [-1.0429,  1.4446, -0.3349,  0.1254, -0.5017,  0.2286,  0.2328,\n         -0.3290,  0.3949, -0.2586],\n        [-0.8476, -0.0004, -1.1003,  2.2806, -1.2226,  0.9251, -0.3165,\n          0.4957,  0.0690,  0.0232],\n        [-0.9108,  1.1355, -0.2715,  0.2233, -0.3681,  0.1442, -0.0001,\n         -0.0174,  0.1454,  0.2286],\n        [-1.0663, -0.8466, -0.7147,  2.5685, -0.2090,  1.2993, -0.3057,\n         -0.8314,  0.7046, -0.0176],\n        [ 1.7013, -1.8051,  0.7541, -1.5248,  0.8972,  0.1518,  1.4876,\n         -0.8454, -0.2022, -0.2829],\n        [-0.8179, -0.1239,  0.8630, -0.2137, -0.2275, -0.5411, -1.3448,\n          1.7354,  0.7751,  0.6234],\n        [ 0.6515, -1.0431,  2.7165,  0.1873, -1.0623,  0.1286,  0.3597,\n         -0.2739,  0.3871, -1.6699],\n        [-0.2828, -1.4663,  0.1182, -0.0896, -0.3640, -0.5129, -0.4905,\n          2.2914, -0.2227,  0.9463],\n        [-1.2596,  2.0468, -0.4405, -0.0411, -0.8073,  0.0490, -0.0604,\n         -0.1206,  0.3504, -0.1059],\n        [ 0.6089,  0.5885,  0.7898,  1.1318, -1.9008,  0.5875,  0.4227,\n         -1.1815,  0.5652, -1.3590],\n        [-1.4551,  2.9537, -0.2805,  0.2372, -1.4180,  0.0297, -0.1515,\n         -0.6111,  0.6140, -0.3354],\n        [-0.7182,  1.6778,  0.0553,  0.0461, -0.5446, -0.0338, -0.0215,\n         -0.0881,  0.1506, -0.2107],\n        [-0.8027, -0.7854, -0.1275, -0.3177, -0.1600, -0.1964, -0.6084,\n          2.1285, -0.1815,  1.1911],\n        [-2.0656, -0.4959, -0.1154, -0.1363,  2.2426, -0.7441, -0.8413,\n          0.4675,  0.3269,  1.7279],\n        [-0.3004,  1.0166,  1.1175, -0.0618, -0.0937, -0.4221,  0.1943,\n         -1.1020,  0.3670, -0.4683],\n        [-1.0720,  0.2252,  0.0175,  1.3644, -0.7409,  0.4655,  0.5439,\n          0.0380,  0.1279, -0.2302],\n        [ 0.2409, -1.2622, -0.6336,  1.8240, -0.5951,  1.3408,  0.2130,\n         -1.3789,  0.8363, -0.2101],\n        [-1.3849,  0.3773, -0.0585,  0.6896, -0.0998,  0.2804,  0.0696,\n         -0.2529,  0.3143,  0.3409],\n        [-0.9103, -0.1578,  1.6673, -0.4817,  0.4088, -0.5484,  0.6103,\n         -0.2287, -0.0665,  0.0055],\n        [-1.1692, -2.8531, -1.2499, -0.0257,  2.8580,  0.2616, -0.7122,\n         -0.0551,  0.8112,  2.3233],\n        [-0.2790, -1.9494,  0.6096, -0.5653,  2.2792, -1.0687,  0.1634,\n          0.3122,  0.1053,  1.0884],\n        [ 0.1267, -1.2297, -0.1315,  0.2428, -0.5436,  0.4123,  2.3060,\n         -0.9278, -0.1528, -0.4224],\n        [-0.0235, -0.9137, -0.1457,  1.6858, -0.7552,  0.7293,  0.2510,\n         -0.3955, -0.2187, -0.1505],\n        [ 0.5643, -1.2783, -1.4149,  0.0304,  0.8375,  1.5018,  0.0338,\n         -0.3875, -0.0117,  0.5751],\n        [ 0.2926, -0.7486, -0.3238,  1.0384,  0.0308,  0.6792, -0.0170,\n         -0.5797,  0.2819, -0.3510],\n        [ 0.1219, -0.5862,  1.5817, -0.1297,  0.4730, -0.9171,  0.7886,\n         -0.7022, -0.0501, -0.2812],\n        [ 1.7587, -2.4511, -0.7369,  0.4082, -0.6426,  1.1784,  0.6052,\n         -0.7178,  1.6161, -0.2220],\n        [-0.1267, -2.6719,  0.0505, -0.4972,  2.9027, -0.1461,  0.2807,\n         -0.2921,  0.2231,  1.1327],\n        [-0.9892,  2.4401,  0.1274,  0.2838, -0.7535, -0.1684, -0.6493,\n         -0.1908,  0.2290, -0.2150],\n        [-0.2071, -2.1351, -0.9191, -0.9309,  1.7747, -0.3046,  0.0183,\n          1.0136, -0.1016,  2.1288],\n        [-0.0103,  0.3280, -0.6974, -0.2504,  0.3187,  0.4390, -0.1879,\n          0.3954,  0.2332, -0.1971],\n        [-0.2280, -1.6754, -0.7438,  0.5078,  0.2544, -0.1020, -0.2503,\n          2.0799, -0.5033,  0.5890],\n        [ 0.3972, -0.9369,  1.2696, -1.6713, -0.4159, -0.0221,  0.6489,\n         -0.4777,  1.2497,  0.3931],\n        [-0.7566, -0.8230, -0.0785, -0.3083,  0.7821,  0.1880,  0.1037,\n         -0.0956,  0.4219,  1.0798],\n        [-1.0328, -0.1700,  1.3806,  0.5445, -0.2624, -0.0780, -0.3595,\n         -0.6253,  0.4309,  0.1813],\n        [-1.0360, -0.4704,  0.1948, -0.7066,  0.6600, -0.4633, -0.3602,\n          1.7494,  0.1522,  0.6086],\n        [-1.2032, -0.7903, -0.5754,  0.4722,  0.6068,  0.5752,  0.2151,\n         -0.2495,  0.3420,  0.9278],\n        [ 0.2247, -0.1361,  0.9374, -0.1543,  0.4921, -0.6553,  0.5885,\n          0.2617, -0.2216, -0.3736],\n        [-0.2867, -1.4486,  0.6658, -0.8755,  2.3195, -0.7627, -0.2132,\n          0.2488,  0.3484,  1.0860],\n        [-1.4031, -0.4518, -0.3181,  2.8268, -0.5371,  1.0154, -0.9247,\n         -0.7385,  1.1031,  0.0422],\n        [ 2.8604, -1.5413,  0.6241, -0.8017, -1.4104,  0.6314,  0.4614,\n         -0.0218, -0.3411, -0.2609],\n        [ 0.2113, -1.2348, -0.8535, -0.1041, -0.2703, -0.1294, -0.7057,\n          2.7552, -0.4429,  0.4517],\n        [ 4.5191, -2.7407,  1.1091,  0.3975, -0.9456,  1.2277,  0.3616,\n         -1.6564,  0.5063, -1.4274],\n        [ 1.4615, -1.0765,  1.8388,  1.5006, -1.2351,  0.2781,  0.2830,\n         -0.8491,  0.2222, -1.7779],\n        [-1.2160,  0.8502,  0.2413, -0.0798, -0.7880, -0.4286, -0.8060,\n          0.7194,  1.2663,  0.6412],\n        [-1.3318,  2.3388, -0.4003, -0.1094, -1.0285,  0.1021, -0.0388,\n         -0.0497,  0.5137, -0.2507],\n        [-1.7853,  0.5884, -0.6108, -0.5557,  0.8696, -0.6226, -0.7983,\n          1.7169, -0.0145,  0.8231],\n        [-0.1739,  0.1562, -0.2933,  2.3195, -0.9480,  1.2019, -0.4834,\n         -1.0567,  0.5685, -0.6841],\n        [-0.7920, -0.3339,  0.7452, -0.6529, -0.3307, -0.6092, -0.0950,\n          1.7311, -0.3481,  0.3801],\n        [-1.7810,  1.0676, -0.7611,  0.3658, -0.0431, -0.1012, -0.6048,\n          0.3089,  0.9998,  0.7164],\n        [-0.5856, -0.5261, -0.4859, -1.0551, -0.1838, -0.2144, -1.2599,\n          3.3891,  0.4691,  0.7566],\n        [-0.4984, -1.7770, -1.1998, -0.1075,  1.0882,  0.4539, -0.5651,\n          1.4381, -0.5678,  1.7479],\n        [ 0.2938, -1.8536,  0.4259, -0.5429,  0.0066,  0.4120,  2.3793,\n         -0.3666, -0.2604,  0.0382],\n        [-0.4080, -0.9851,  4.0264,  0.1099, -0.1766, -1.1557,  0.6419,\n         -0.8147,  0.7535, -1.1452],\n        [-0.4636, -1.7323, -0.6433, -0.0274,  0.7227, -0.1799, -0.9336,\n          2.1881, -0.2073,  1.6522],\n        [-0.9617, -0.0348, -0.3980, -0.4738,  0.7790,  0.4671, -0.6115,\n         -0.7067,  1.3036,  0.4923],\n        [-1.0151, -2.5385, -0.6072,  0.2902,  3.1570,  0.1062, -0.2169,\n         -0.4491,  0.6326,  1.6829],\n        [-1.8852,  0.6066, -0.2840, -0.4475, -0.1147, -0.7858, -1.1805,\n          3.0723,  0.3960,  0.9720],\n        [ 0.0344, -1.4878, -0.9675,  1.9649, -0.3146,  1.2183,  0.6730,\n         -0.3650,  0.0646, -0.0898],\n        [-0.2118, -2.0350,  0.9917, -0.8993,  1.2334, -0.6723,  2.5847,\n         -0.0454, -0.4149,  0.3927],\n        [-1.7365,  3.0447,  0.5115,  0.0786, -0.7544, -0.2158, -0.4876,\n         -0.2891,  0.5089, -0.6719],\n        [ 0.3652, -0.5457, -0.1167,  2.9056, -1.1622,  0.8192, -1.3245,\n         -0.6414,  0.8097, -0.4958],\n        [-0.8755, -0.6983,  0.2208, -0.6463,  0.5276,  0.1145,  2.7229,\n         -1.0316,  0.1905,  0.2090],\n        [-0.9702,  0.1265, -0.0007, -0.5106,  0.4970, -0.0804,  0.0017,\n          0.0607,  0.6164,  0.4490],\n        [-0.8271, -0.6822, -0.7434,  2.6457, -1.6143,  1.1486, -1.0705,\n          0.5611,  0.6422,  0.1250],\n        [-1.9979,  1.8175, -0.1658, -0.0343, -0.6292,  0.1774,  0.3150,\n         -0.4633,  0.9266,  0.0252],\n        [-0.9039, -0.6030, -0.2173, -1.1768,  2.3198, -0.5072,  0.3418,\n         -0.1551,  0.1282,  1.4250],\n        [-0.9891,  0.5212, -0.4518,  0.3267, -0.0759,  0.3826, -0.0341,\n          0.0382,  0.2451,  0.3658],\n        [-2.1217,  1.5102, -0.7828,  0.3554, -0.4192, -0.0772,  0.0578,\n          0.8070,  0.1701,  0.5880],\n        [ 1.0665, -1.3826,  0.6243, -0.8096, -0.4227,  0.5925,  1.8112,\n         -0.9946,  0.2010, -0.7731],\n        [-1.1263, -1.7484,  0.0041, -0.5439,  1.7242, -0.9475, -0.3835,\n          0.8452,  0.3077,  2.2689]])\n</code></pre> <p>Printing output size</p> <p>This produces a 100x10 matrix because each iteration has a batch size of 100 and each prediction across the 10 classes, with the largest number indicating the likely number it is predicting. <pre><code>iter_test = 0\nfor images, labels in test_loader:\n    iter_test += 1\n    images = images.view(-1, 28*28).requires_grad_()\n    outputs = model(images)\n    if iter_test == 1:\n        print('OUTPUTS')\n        print(outputs.size())\n    _, predicted = torch.max(outputs.data, 1)\n</code></pre></p> <pre><code>OUTPUTS\ntorch.Size([100, 10])\n</code></pre> <p>Printing one output</p> <p>This would be a 1x10 matrix where the largest number is what the model thinks the image is. Here we can see that in the tensor, position 7 has the largest number, indicating the model thinks the image is 7.</p> <p>number 0: -0.4181  number 1: -1.0784 ...  number 7: 2.9352 <pre><code>iter_test = 0\nfor images, labels in test_loader:\n    iter_test += 1\n    images = images.view(-1, 28*28).requires_grad_()\n    outputs = model(images)\n    if iter_test == 1:\n        print('OUTPUTS')\n        print(outputs[0, :])\n    _, predicted = torch.max(outputs.data, 1)\n</code></pre></p> <pre><code>OUTPUTS\ntensor([-0.4181, -1.0784, -0.4840, -0.0985, -0.2394, -0.1801, -1.1639,\n         2.9352, -0.1552,  0.8852])\n</code></pre> <p>Printing prediction output</p> <p>Because our output is of size 100 (our batch size), our prediction size would also of the size 100.</p> <pre><code>iter_test = 0\nfor images, labels in test_loader:\n    iter_test += 1\n    images = images.view(-1, 28*28).requires_grad_()\n    outputs = model(images)\n    _, predicted = torch.max(outputs.data, 1)\n    if iter_test == 1:\n        print('PREDICTION')\n        print(predicted.size())\n</code></pre> <pre><code>PREDICTION\ntorch.Size([100])\n</code></pre> <p>Print prediction value</p> <p>We are printing our prediction which as verified above, should be digit 7.</p> <pre><code>iter_test = 0\nfor images, labels in test_loader:\n    iter_test += 1\n    images = images.view(-1, 28*28).requires_grad_()\n    outputs = model(images)\n    _, predicted = torch.max(outputs.data, 1)\n    if iter_test == 1:\n        print('PREDICTION')\n        print(predicted[0])\n</code></pre> <pre><code>PREDICTION\ntensor(7)\n</code></pre> <p>Print prediction, label and label size</p> <p>We are trying to show what we are predicting and the actual values. In this case, we're predicting the right value 7!</p> <pre><code>iter_test = 0\nfor images, labels in test_loader:\n    iter_test += 1\n    images = images.view(-1, 28*28).requires_grad_()\n    outputs = model(images)\n    _, predicted = torch.max(outputs.data, 1)\n    if iter_test == 1:\n        print('PREDICTION')\n        print(predicted[0])\n\n        print('LABEL SIZE')\n        print(labels.size())\n\n        print('LABEL FOR IMAGE 0')\n        print(labels[0])\n</code></pre> <pre><code>PREDICTION\ntensor(7)\n\nLABEL SIZE\ntorch.Size([100])\n\nLABEL FOR IMAGE 0\ntensor(7)\n</code></pre> <p>Print second prediction and ground truth</p> <p>Again, the prediction is correct. Naturally, as our model is quite competent in this simple task.</p> <pre><code>iter_test = 0\nfor images, labels in test_loader:\n    iter_test += 1\n    images = images.view(-1, 28*28).requires_grad_()\n    outputs = model(images)\n    _, predicted = torch.max(outputs.data, 1)\n\n    if iter_test == 1:\n        print('PREDICTION')\n        print(predicted[1])\n\n        print('LABEL SIZE')\n        print(labels.size())\n\n        print('LABEL FOR IMAGE 1')\n        print(labels[1])\n</code></pre> <pre><code>PREDICTION\ntensor(2)\n\nLABEL SIZE\ntorch.Size([100])\n\nLABEL FOR IMAGE 1\ntensor(2)\n</code></pre> <p>Print accuracy</p> <p>Now we know what each object represents, we can understand how we arrived at our accuracy numbers.</p> <p>One last thing to note is that <code>correct.item()</code> has this syntax is because <code>correct</code> is a PyTorch tensor and to get the value to compute with <code>total</code> which is an integer, we need to do this. <pre><code>correct = 0\ntotal = 0\niter_test = 0\nfor images, labels in test_loader:\n    iter_test += 1\n    images = images.view(-1, 28*28).requires_grad_()\n    outputs = model(images)\n    _, predicted = torch.max(outputs.data, 1)\n\n    # Total number of labels\n    total += labels.size(0)\n\n    # Total correct predictions\n    correct += (predicted == labels).sum()\n\naccuracy = 100 * (correct.item() / total)\n\nprint(accuracy)\n</code></pre></p> <pre><code>82.94\n</code></pre> <p>Explanation of Python's .sum() function</p> <p>Python's .sum() function allows you to do a comparison between two matrices and sum the ones that return <code>True</code> or in our case, those predictions that match actual labels (correct predictions).</p> <pre><code># Explaining .sum() python built-in function\n# correct += (predicted == labels).sum()\nimport numpy as np\na = np.ones((10))\nprint(a)\nb = np.ones((10))\nprint(b)\n\nprint(a == b)\n\nprint((a == b).sum())\n</code></pre> <pre><code># matrix a\n[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n\n# matrix b\n[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n\n# boolean array\n[ True  True  True  True  True  True  True  True  True  True]\n\n# number of elementswhere a matches b\n10\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#saving-model","title":"Saving Model","text":"<p>Saving PyTorch model</p> <p>This is how you save your model. Feel free to just change <code>save_model = True</code> to save your model <pre><code>save_model = False\nif save_model is True:\n    # Saves only parameters\n    torch.save(model.state_dict(), 'awesome_model.pkl')\n</code></pre></p>"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#building-a-logistic-regression-model-with-pytorch-gpu","title":"Building a Logistic Regression Model with PyTorch (GPU)","text":"<p>CPU version</p> <p>The usual 7-step process, getting repetitive by now which we like. </p> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass LogisticRegressionModel(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(LogisticRegressionModel, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28*28\noutput_dim = 10\n\nmodel = LogisticRegressionModel(input_dim, output_dim)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.001\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as Variable\n        images = images.view(-1, 28*28).requires_grad_()\n        labels = labels\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        # 100 x 10\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images to a Torch Variable\n                images = images.view(-1, 28*28).requires_grad_()\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                # 100 x 1\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct.item() / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Iteration: 500. Loss: 1.876196026802063. Accuracy: 64.44\nIteration: 1000. Loss: 1.5153584480285645. Accuracy: 75.68\nIteration: 1500. Loss: 1.3521136045455933. Accuracy: 78.98\nIteration: 2000. Loss: 1.2136967182159424. Accuracy: 80.95\nIteration: 2500. Loss: 1.0934826135635376. Accuracy: 81.97\nIteration: 3000. Loss: 1.024120569229126. Accuracy: 82.49\n</code></pre> <p>GPU version</p> <p>2 things must be on GPU - <code>model</code> - <code>tensors</code></p> <p>Remember step 4 and 7 will be affected and this will be the same for all model building moving forward.</p> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n'''\nSTEP 1: LOADING DATASET\n'''\n\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\nclass LogisticRegressionModel(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(LogisticRegressionModel, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28*28\noutput_dim = 10\n\nmodel = LogisticRegressionModel(input_dim, output_dim)\n\n#######################\n#  USE GPU FOR MODEL  #\n#######################\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.001\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n\n        #######################\n        #  USE GPU FOR MODEL  #\n        #######################\n        images = images.view(-1, 28*28).requires_grad_().to(device)\n        labels = labels.to(device)\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                #######################\n                #  USE GPU FOR MODEL  #\n                #######################\n                images = images.view(-1, 28*28).to(device)\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                #######################\n                #  USE GPU FOR MODEL  #\n                #######################\n                # Total correct predictions\n                if torch.cuda.is_available():\n                    correct += (predicted.cpu() == labels.cpu()).sum()\n                else:\n                    correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct.item() / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Iteration: 500. Loss: 1.8571407794952393. Accuracy: 68.99\nIteration: 1000. Loss: 1.5415704250335693. Accuracy: 75.86\nIteration: 1500. Loss: 1.2755383253097534. Accuracy: 78.92\nIteration: 2000. Loss: 1.2468739748001099. Accuracy: 80.72\nIteration: 2500. Loss: 1.0708973407745361. Accuracy: 81.73\nIteration: 3000. Loss: 1.0359245538711548. Accuracy: 82.74\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#summary","title":"Summary","text":"<p>We've learnt to...</p> <p>Success</p> <ul> <li> Logistic regression basics</li> <li> Problems of linear regression</li> <li> In-depth Logistic Regression<ul> <li> Get logits</li> <li> Get softmax</li> <li> Get cross-entropy loss</li> </ul> </li> <li> Aim: reduce cross-entropy loss</li> <li> Built a logistic regression model in CPU and GPU<ul> <li> Step 1: Load Dataset</li> <li> Step 2: Make Dataset Iterable</li> <li> Step 3: Create Model Class</li> <li> Step 4: Instantiate Model Class</li> <li> Step 5: Instantiate Loss Class</li> <li> Step 6: Instantiate Optimizer Class</li> <li> Step 7: Train Model</li> </ul> </li> <li> Important things to be on GPU<ul> <li> <code>model</code></li> <li> <code>tensors with gradients</code></li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#citation","title":"Citation","text":"<p>If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.</p> <p></p>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/","title":"Long Short-Term Memory (LSTM) network with PyTorch","text":"<p>Run Jupyter Notebook</p> <p>You can run the code for this section in this jupyter notebook link.</p>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#about-lstms-special-rnn","title":"About LSTMs: Special RNN","text":"<ul> <li>Capable of learning long-term dependencies</li> <li>LSTM = RNN on super juice</li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#rnn-transition-to-lstm","title":"RNN Transition to LSTM","text":""},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#building-an-lstm-with-pytorch","title":"Building an LSTM with PyTorch","text":""},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#model-a-1-hidden-layer","title":"Model A: 1 Hidden Layer","text":"<ul> <li>Unroll 28 time steps<ul> <li>Each step input size: 28 x 1</li> <li>Total per unroll: 28 x 28<ul> <li>Feedforward Neural Network input size: 28 x 28 </li> </ul> </li> </ul> </li> <li>1 Hidden layer</li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#steps","title":"Steps","text":"<ul> <li>Step 1: Load Dataset</li> <li>Step 2: Make Dataset Iterable</li> <li>Step 3: Create Model Class</li> <li>Step 4: Instantiate Model Class</li> <li>Step 5: Instantiate Loss Class</li> <li>Step 6: Instantiate Optimizer Class</li> <li>Step 7: Train Model</li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#step-1-loading-mnist-train-dataset","title":"Step 1: Loading MNIST Train Dataset","text":"<p>Images from 1 to 9</p> <p>The usual loading of our MNIST dataset</p> <p>As usual, we've 60k training images and 10k testing images. </p> <p>Subsequently, we'll have 3 groups: training, validation and testing for a more robust evaluation of algorithms.</p> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n</code></pre> <pre><code>train_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n</code></pre> <pre><code>print(train_dataset.train_data.size())\n</code></pre> <pre><code>print(train_dataset.train_labels.size())\n</code></pre> <pre><code>print(test_dataset.test_data.size())\n\n\n```python\nprint(test_dataset.test_labels.size())\n</code></pre> <pre><code>torch.Size([60000, 28, 28])\ntorch.Size([60000])\n\ntorch.Size([10000, 28, 28])\ntorch.Size([10000])\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#step-2-make-dataset-iterable","title":"Step 2: Make Dataset Iterable","text":"<p>Creating an iterable object for our dataset</p> <pre><code>batch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#step-3-create-model-class","title":"Step 3: Create Model Class","text":"<p>Creating an LSTM model class</p> <p>It is very similar to RNN in terms of the shape of our input of <code>batch_dim x seq_dim x feature_dim</code>.</p> <p>The only change is that we have our cell state on top of our hidden state. PyTorch's LSTM module handles all the other weights for our other gates.</p> <pre><code>class LSTMModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n        super(LSTMModel, self).__init__()\n        # Hidden dimensions\n        self.hidden_dim = hidden_dim\n\n        # Number of hidden layers\n        self.layer_dim = layer_dim\n\n        # Building your LSTM\n        # batch_first=True causes input/output tensors to be of shape\n        # (batch_dim, seq_dim, feature_dim)\n        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n\n        # Readout layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        # Initialize hidden state with zeros\n        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n\n        # Initialize cell state\n        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n\n        # 28 time steps\n        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n        # If we don't, we'll backprop all the way to the start even after going through another batch\n        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n\n        # Index hidden state of last time step\n        # out.size() --&gt; 100, 28, 100\n        # out[:, -1, :] --&gt; 100, 100 --&gt; just want last time step hidden states! \n        out = self.fc(out[:, -1, :]) \n        # out.size() --&gt; 100, 10\n        return out\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#step-4-instantiate-model-class","title":"Step 4: Instantiate Model Class","text":"<ul> <li>28 time steps<ul> <li>Each time step: input dimension = 28</li> </ul> </li> <li>1 hidden layer</li> <li>MNIST 1-9 digits \\(\\rightarrow\\) output dimension = 10</li> </ul> <p>Instantiate our LSTM model</p> <pre><code>input_dim = 28\nhidden_dim = 100\nlayer_dim = 1\noutput_dim = 10\n</code></pre> <pre><code>model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#step-5-instantiate-loss-class","title":"Step 5: Instantiate Loss Class","text":"<ul> <li>Long Short-Term Memory Neural Network: Cross Entropy Loss<ul> <li>Recurrent Neural Network: Cross Entropy Loss</li> <li>Convolutional Neural Network: Cross Entropy Loss</li> <li>Feedforward Neural Network: Cross Entropy Loss</li> <li>Logistic Regression: Cross Entropy Loss</li> <li>Linear Regression: MSE</li> </ul> </li> </ul> <p>Cross Entry Loss Function</p> <p>Because we are doing a classification problem we'll be using a Cross Entropy function. If we were to do a regression problem, then we would typically use a MSE function.</p> <pre><code>criterion = nn.CrossEntropyLoss()\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#step-6-instantiate-optimizer-class","title":"Step 6: Instantiate Optimizer Class","text":"<ul> <li>Simplified equation<ul> <li>\\(\\theta = \\theta - \\eta \\cdot \\nabla_\\theta\\)<ul> <li>\\(\\theta\\): parameters (our variables)</li> <li>\\(\\eta\\): learning rate (how fast we want to learn)</li> <li>\\(\\nabla_\\theta\\): parameters' gradients</li> </ul> </li> </ul> </li> <li>Even simplier equation<ul> <li><code>parameters = parameters - learning_rate * parameters_gradients</code></li> <li>At every iteration, we update our model's parameters</li> </ul> </li> </ul> <p>Mini-batch Stochastic Gradient Descent</p> <pre><code>learning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-in-depth","title":"Parameters In-Depth","text":"<p>1 Layer LSTM Groups of Parameters</p> <p>We will have 6 groups of parameters here comprising weights and biases from:     - Input to Hidden Layer Affine Function     - Hidden Layer to Output Affine Function     - Hidden Layer to Hidden Layer Affine Function</p> <p>Notice how this is exactly the same number of groups of parameters as our RNN? But the sizes of these groups will be larger for an LSTM due to its gates.</p> <pre><code>len(list(model.parameters()))\n</code></pre> <pre><code>6\n</code></pre> <p>In-depth Parameters Analysis</p> <p>Comparing to RNN's parameters, we've the same number of groups but for LSTM we've 4x the number of parameters!</p> <pre><code>for i in range(len(list(model.parameters()))):\n    print(list(model.parameters())[i].size())\n</code></pre> <pre><code>torch.Size([400, 28])\ntorch.Size([400, 100])\n\ntorch.Size([400])\ntorch.Size([400])\n\ntorch.Size([10, 100])\ntorch.Size([10])\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-breakdown","title":"Parameters Breakdown","text":"<ul> <li>This is the breakdown of the parameters associated with the respective affine functions </li> <li>Input \\(\\rightarrow\\) Gates<ul> <li>\\([400, 28] \\rightarrow w_1, w_3, w_5, w_7\\)</li> <li>\\([400] \\rightarrow b_1, b_3, b_5, b_7\\)</li> </ul> </li> <li>Hidden State \\(\\rightarrow\\) Gates<ul> <li>\\([400,100] \\rightarrow w_2, w_4, w_6, w_8\\)</li> <li>\\([400] \\rightarrow b_2, b_4, b_6, b_8\\)</li> </ul> </li> <li>Hidden State \\(\\rightarrow\\) Output<ul> <li>\\([10, 100] \\rightarrow w_9\\)</li> <li>\\([10] \\rightarrow b_9\\)</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#step-7-train-model","title":"Step 7: Train Model","text":"<ul> <li>Process <ol> <li>Convert inputs/labels to variables<ul> <li>LSTM Input: (1, 28)</li> <li>RNN Input: (1, 28)</li> <li>CNN Input: (1, 28, 28) </li> <li>FNN Input: (1, 28*28)</li> </ul> </li> <li>Clear gradient buffets</li> <li>Get output given inputs </li> <li>Get loss</li> <li>Get gradients w.r.t. parameters</li> <li>Update parameters using gradients<ul> <li><code>parameters = parameters - learning_rate * parameters_gradients</code></li> </ul> </li> <li>REPEAT</li> </ol> </li> </ul> <p>Training 1 Hidden Layer LSTM</p> <pre><code># Number of steps to unroll\nseq_dim = 28  \n\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as a torch tensor with gradient accumulation abilities\n        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        # outputs.size() --&gt; 100, 10\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Resize images\n                images = images.view(-1, seq_dim, input_dim)\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Iteration: 500. Loss: 0.8390830755233765. Accuracy: 72\nIteration: 1000. Loss: 0.46470555663108826. Accuracy: 85\nIteration: 1500. Loss: 0.31465113162994385. Accuracy: 91\nIteration: 2000. Loss: 0.19143860042095184. Accuracy: 94\nIteration: 2500. Loss: 0.16134005784988403. Accuracy: 95\nIteration: 3000. Loss: 0.255976140499115. Accuracy: 95\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#model-b-2-hidden-layer","title":"Model B: 2 Hidden Layer","text":"<ul> <li>Unroll 28 time steps<ul> <li>Each step input size: 28 x 1</li> <li>Total per unroll: 28 x 28<ul> <li>Feedforward Neural Network inpt size: 28 x 28 </li> </ul> </li> </ul> </li> <li>2 Hidden layer</li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#steps_1","title":"Steps","text":"<ul> <li>Step 1: Load Dataset</li> <li>Step 2: Make Dataset Iterable</li> <li>Step 3: Create Model Class</li> <li>Step 4: Instantiate Model Class</li> <li>Step 5: Instantiate Loss Class</li> <li>Step 6: Instantiate Optimizer Class</li> <li>Step 7: Train Model</li> </ul> <p>Train 2 Hidden Layer LSTM</p> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n'''\nSTEP 1: LOADING DATASET\n'''\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\n\nclass LSTMModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n        super(LSTMModel, self).__init__()\n        # Hidden dimensions\n        self.hidden_dim = hidden_dim\n\n        # Number of hidden layers\n        self.layer_dim = layer_dim\n\n        # Building your LSTM\n        # batch_first=True causes input/output tensors to be of shape\n        # (batch_dim, seq_dim, feature_dim)\n        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n\n        # Readout layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        # Initialize hidden state with zeros\n        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n\n        # Initialize cell state\n        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n\n        # One time step\n        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n        # If we don't, we'll backprop all the way to the start even after going through another batch\n        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n\n        # Index hidden state of last time step\n        # out.size() --&gt; 100, 28, 100\n        # out[:, -1, :] --&gt; 100, 100 --&gt; just want last time step hidden states! \n        out = self.fc(out[:, -1, :]) \n        # out.size() --&gt; 100, 10\n        return out\n\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28\nhidden_dim = 100\nlayer_dim = 2  # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER\noutput_dim = 10\n\nmodel = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n\n# JUST PRINTING MODEL &amp; PARAMETERS \nprint(model)\nprint(len(list(model.parameters())))\nfor i in range(len(list(model.parameters()))):\n    print(list(model.parameters())[i].size())\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\n\n# Number of steps to unroll\nseq_dim = 28  \n\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as torch tensor with gradient accumulation abilities\n        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        # outputs.size() --&gt; 100, 10\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Resize image\n                images = images.view(-1, seq_dim, input_dim)\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>LSTMModel(\n  (lstm): LSTM(28, 100, num_layers=2, batch_first=True)\n  (fc): Linear(in_features=100, out_features=10, bias=True)\n)\n\n10\n\ntorch.Size([400, 28])\ntorch.Size([400, 100])\ntorch.Size([400])\ntorch.Size([400])\ntorch.Size([400, 100])\ntorch.Size([400, 100])\ntorch.Size([400])\ntorch.Size([400])\ntorch.Size([10, 100])\ntorch.Size([10])\n\nIteration: 500. Loss: 2.3074915409088135. Accuracy: 11\nIteration: 1000. Loss: 1.8854578733444214. Accuracy: 35\nIteration: 1500. Loss: 0.5317062139511108. Accuracy: 80\nIteration: 2000. Loss: 0.15290376543998718. Accuracy: 92\nIteration: 2500. Loss: 0.19500978291034698. Accuracy: 93\nIteration: 3000. Loss: 0.10683634132146835. Accuracy: 95\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-breakdown-layer-1","title":"Parameters Breakdown (Layer 1)","text":"<ul> <li>Input \\(\\rightarrow\\) Gates<ul> <li>\\([400, 28]\\)</li> <li>\\([400]\\)</li> </ul> </li> <li>Hidden State \\(\\rightarrow\\) Gates<ul> <li>\\([400,100]\\)</li> <li>\\([400]\\)</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-breakdown-layer-2","title":"Parameters Breakdown (Layer 2)","text":"<ul> <li>Input \\(\\rightarrow\\) Gates <ul> <li>\\([400, 100]\\)</li> <li>\\([400]\\)</li> </ul> </li> <li>Hidden State \\(\\rightarrow\\) Gates <ul> <li>\\([400,100]\\)</li> <li>\\([400]\\)</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-breakdown-readout-layer","title":"Parameters Breakdown (Readout Layer)","text":"<ul> <li>Hidden State \\(\\rightarrow\\) Output<ul> <li>\\([10, 100]\\)</li> <li>\\([10]\\)</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#model-c-3-hidden-layer","title":"Model C: 3 Hidden Layer","text":"<ul> <li>Unroll 28 time steps<ul> <li>Each step input size: 28 x 1</li> <li>Total per unroll: 28 x 28<ul> <li>Feedforward Neural Network inpt size: 28 x 28 </li> </ul> </li> </ul> </li> <li>3 Hidden layer</li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#steps_2","title":"Steps","text":"<ul> <li>Step 1: Load Dataset</li> <li>Step 2: Make Dataset Iterable</li> <li>Step 3: Create Model Class</li> <li>Step 4: Instantiate Model Class</li> <li>Step 5: Instantiate Loss Class</li> <li>Step 6: Instantiate Optimizer Class</li> <li>Step 7: Train Model</li> </ul> <p>3 Hidden Layer LSTM</p> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n'''\nSTEP 1: LOADING DATASET\n'''\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\n\nclass LSTMModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n        super(LSTMModel, self).__init__()\n        # Hidden dimensions\n        self.hidden_dim = hidden_dim\n\n        # Number of hidden layers\n        self.layer_dim = layer_dim\n\n        # Building your LSTM\n        # batch_first=True causes input/output tensors to be of shape\n        # (batch_dim, seq_dim, feature_dim)\n        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n\n        # Readout layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        # Initialize hidden state with zeros\n        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n\n        # Initialize cell state\n        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n\n        # One time step\n        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n        # If we don't, we'll backprop all the way to the start even after going through another batch\n        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n\n        # Index hidden state of last time step\n        # out.size() --&gt; 100, 28, 100\n        # out[:, -1, :] --&gt; 100, 100 --&gt; just want last time step hidden states! \n        out = self.fc(out[:, -1, :]) \n        # out.size() --&gt; 100, 10\n        return out\n\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28\nhidden_dim = 100\nlayer_dim = 3  # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER\noutput_dim = 10\n\nmodel = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n\n# JUST PRINTING MODEL &amp; PARAMETERS \nprint(model)\nprint(len(list(model.parameters())))\nfor i in range(len(list(model.parameters()))):\n    print(list(model.parameters())[i].size())\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\n\n# Number of steps to unroll\nseq_dim = 28  \n\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as Variable\n        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        # outputs.size() --&gt; 100, 10\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images to a Torch Variable\n                images = images.view(-1, seq_dim, input_dim).requires_grad_()\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>LSTMModel(\n  (lstm): LSTM(28, 100, num_layers=3, batch_first=True)\n  (fc): Linear(in_features=100, out_features=10, bias=True)\n)\n\n14\n\ntorch.Size([400, 28])\ntorch.Size([400, 100])\ntorch.Size([400])\ntorch.Size([400])\ntorch.Size([400, 100])\ntorch.Size([400, 100])\ntorch.Size([400])\ntorch.Size([400])\ntorch.Size([400, 100])\ntorch.Size([400, 100])\ntorch.Size([400])\ntorch.Size([400])\ntorch.Size([10, 100])\ntorch.Size([10])\n\nIteration: 500. Loss: 2.2927396297454834. Accuracy: 11\nIteration: 1000. Loss: 2.29740309715271. Accuracy: 11\nIteration: 1500. Loss: 2.1950502395629883. Accuracy: 20\nIteration: 2000. Loss: 1.0738657712936401. Accuracy: 59\nIteration: 2500. Loss: 0.5988132357597351. Accuracy: 79\nIteration: 3000. Loss: 0.4107239246368408. Accuracy: 88\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-breakdown-layer-1_1","title":"Parameters Breakdown (Layer 1)","text":"<ul> <li>Input \\(\\rightarrow\\) Gates<ul> <li>[400, 28]</li> <li>[400]</li> </ul> </li> <li>Hidden State \\(\\rightarrow\\) Gates<ul> <li>[400,100]</li> <li>[400]</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-breakdown-layer-2_1","title":"Parameters Breakdown (Layer 2)","text":"<ul> <li>Input \\(\\rightarrow\\) Gates <ul> <li>[400, 100]</li> <li>[400]</li> </ul> </li> <li>Hidden State \\(\\rightarrow\\) Gates <ul> <li>[400,100]</li> <li>[400]</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-breakdown-layer-3","title":"Parameters Breakdown (Layer 3)","text":"<ul> <li>Input \\(\\rightarrow\\) Gates <ul> <li>[400, 100]</li> <li>[400]</li> </ul> </li> <li>Hidden State \\(\\rightarrow\\) Gates <ul> <li>[400,100]</li> <li>[400]</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-breakdown-readout-layer_1","title":"Parameters Breakdown (Readout Layer)","text":"<ul> <li>Hidden State \\(\\rightarrow\\) Output<ul> <li>[10, 100]</li> <li>[10]</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#comparison-with-rnn","title":"Comparison with RNN","text":"Model A RNN Model B RNN Model C RNN ReLU ReLU Tanh 1 Hidden Layer 2 Hidden Layers 3 Hidden Layers 100 Hidden Units 100 Hidden Units 100 Hidden Units 92.48% 95.09% 95.54% Model A LSTM Model B LSTM Model C LSTM 1 Hidden Layer 2 Hidden Layers 3 Hidden Layers 100 Hidden Units 100 Hidden Units 100 Hidden Units 96.05% 95.24% 91.22%"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#deep-learning-notes","title":"Deep Learning Notes","text":"<ul> <li>2 ways to expand a recurrent neural network<ul> <li>More hidden units<ul> <li><code>(o, i, f, g) gates</code></li> </ul> </li> <li>More hidden layers</li> </ul> </li> <li>Cons<ul> <li>Need a larger dataset<ul> <li>Curse of dimensionality</li> </ul> </li> <li>Does not necessarily mean higher accuracy</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#3-building-a-recurrent-neural-network-with-pytorch-gpu","title":"3. Building a Recurrent Neural Network with PyTorch (GPU)","text":""},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#model-a-3-hidden-layers","title":"Model A: 3 Hidden Layers","text":"<p>GPU: 2 things must be on GPU - <code>model</code> - <code>tensors</code></p>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#steps_3","title":"Steps","text":"<ul> <li>Step 1: Load Dataset</li> <li>Step 2: Make Dataset Iterable</li> <li>Step 3: Create Model Class</li> <li>Step 4: Instantiate Model Class</li> <li>Step 5: Instantiate Loss Class</li> <li>Step 6: Instantiate Optimizer Class</li> <li>Step 7: Train Model</li> </ul> <p>3 Hidden Layer LSTM on GPU</p> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n'''\nSTEP 1: LOADING DATASET\n'''\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\n\nclass LSTMModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n        super(LSTMModel, self).__init__()\n        # Hidden dimensions\n        self.hidden_dim = hidden_dim\n\n        # Number of hidden layers\n        self.layer_dim = layer_dim\n\n        # Building your LSTM\n        # batch_first=True causes input/output tensors to be of shape\n        # (batch_dim, seq_dim, feature_dim)\n        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n\n        # Readout layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        # Initialize hidden state with zeros\n        #######################\n        #  USE GPU FOR MODEL  #\n        #######################\n        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n\n        # Initialize cell state\n        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n\n        # One time step\n        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n\n        # Index hidden state of last time step\n        # out.size() --&gt; 100, 28, 100\n        # out[:, -1, :] --&gt; 100, 100 --&gt; just want last time step hidden states! \n        out = self.fc(out[:, -1, :]) \n        # out.size() --&gt; 100, 10\n        return out\n\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28\nhidden_dim = 100\nlayer_dim = 3  # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER\noutput_dim = 10\n\nmodel = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n\n#######################\n#  USE GPU FOR MODEL  #\n#######################\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\n\n# Number of steps to unroll\nseq_dim = 28  \n\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as Variable\n        #######################\n        #  USE GPU FOR MODEL  #\n        #######################\n        images = images.view(-1, seq_dim, input_dim).requires_grad_().to(device)\n        labels = labels.to(device)\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        # outputs.size() --&gt; 100, 10\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                #######################\n                #  USE GPU FOR MODEL  #\n                #######################\n                images = images.view(-1, seq_dim, input_dim).to(device)\n                labels = labels.to(device)\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                #######################\n                #  USE GPU FOR MODEL  #\n                #######################\n                if torch.cuda.is_available():\n                    correct += (predicted.cpu() == labels.cpu()).sum()\n                else:\n                    correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Iteration: 500. Loss: 2.3068575859069824. Accuracy: 11\nIteration: 1000. Loss: 2.291989803314209. Accuracy: 14\nIteration: 1500. Loss: 1.909593105316162. Accuracy: 28\nIteration: 2000. Loss: 0.7345633506774902. Accuracy: 71\nIteration: 2500. Loss: 0.45030108094215393. Accuracy: 86\nIteration: 3000. Loss: 0.2627193331718445. Accuracy: 89\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#summary","title":"Summary","text":"<p>We've learnt to...</p> <p>Success</p> <ul> <li> RNN transition to LSTM</li> <li> LSTM Models in PyTorch<ul> <li> Model A: 1 Hidden Layer LSTM</li> <li> Model B: 2 Hidden Layer LSTM</li> <li> Model C: 3 Hidden Layer LSTM</li> </ul> </li> <li> Models Variation in Code<ul> <li> Modifying only step 4</li> </ul> </li> <li> Ways to Expand Model\u2019s Capacity<ul> <li> More hidden units</li> <li> More hidden layers</li> </ul> </li> <li> Cons of Expanding Capacity<ul> <li> Need more data</li> <li> Does not necessarily mean higher accuracy</li> </ul> </li> <li> GPU Code<ul> <li> 2 things on GPU<ul> <li> model</li> <li> tensors</li> </ul> </li> <li> Modifying only Step 3, 4 and 7</li> </ul> </li> <li> 7 Step Model Building Recap<ul> <li> Step 1: Load Dataset</li> <li> Step 2: Make Dataset Iterable</li> <li> Step 3: Create Model Class</li> <li> Step 4: Instantiate Model Class</li> <li> Step 5: Instantiate Loss Class</li> <li> Step 6: Instantiate Optimizer Class</li> <li> Step 7: Train Model</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#citation","title":"Citation","text":"<p>If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.</p> <p></p>"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/","title":"Matrices with PyTorch","text":"<p>Run Jupyter Notebook</p> <p>You can run the code for this section in this jupyter notebook link.</p>"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#matrices","title":"Matrices","text":""},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#matrices-brief-introduction","title":"Matrices Brief Introduction","text":"<ul> <li> Basic definition: rectangular array of numbers.</li> <li> Tensors (PyTorch)</li> <li> Ndarrays (NumPy)</li> </ul> <p>2 x 2 Matrix (R x C)</p> 1 1 1 1 <p>2 x 3 Matrix</p> 1 1 1 1 1 1"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#creating-matrices","title":"Creating Matrices","text":"<p>Create list</p> <pre><code># Creating a 2x2 array\narr = [[1, 2], [3, 4]]\nprint(arr)\n</code></pre> <pre><code>[[1, 2], [3, 4]]\n</code></pre> <p>Create numpy array via list</p> <p><pre><code>import numpy as np\n</code></pre> <pre><code># Convert to NumPy\nnp.array(arr)\n</code></pre></p> <pre><code>array([[1, 2],\n       [3, 4]])\n</code></pre> <p>Convert numpy array to PyTorch tensor</p> <pre><code>import torch\n</code></pre> <pre><code># Convert to PyTorch Tensor\ntorch.Tensor(arr)\n</code></pre> <pre><code>1  2\n3  4\n[torch.FloatTensor of size 2x2]\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#create-matrices-with-default-values","title":"Create Matrices with Default Values","text":"<p>Create 2x2 numpy array of 1's</p> <pre><code>np.ones((2, 2))\n</code></pre> <pre><code>array([[ 1.,  1.],\n       [ 1.,  1.]])\n</code></pre> <p>Create 2x2 torch tensor of 1's</p> <pre><code>torch.ones((2, 2))\n</code></pre> <pre><code> 1  1\n 1  1\n[torch.FloatTensor of size 2x2]\n</code></pre> <p>Create 2x2 numpy array of random numbers</p> <pre><code>np.random.rand(2, 2)\n</code></pre> <pre><code>array([[ 0.68270631,  0.87721678],\n       [ 0.07420986,  0.79669375]])\n</code></pre> <p>Create 2x2 PyTorch tensor of random numbers</p> <pre><code>torch.rand(2, 2)\n</code></pre> <pre><code>0.3900  0.8268\n0.3888  0.5914\n[torch.FloatTensor of size 2x2]\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#seeds-for-reproducibility","title":"Seeds for Reproducibility","text":"<p>Why do we need seeds?</p> <p>We need seeds to enable reproduction of experimental results. This becomes critical later on where you can easily let people reproduce your code's output exactly as you've produced.</p> <p>Create seed to enable fixed numbers for random number generation </p> <pre><code># Seed\nnp.random.seed(0)\nnp.random.rand(2, 2)\n</code></pre> <pre><code>array([[ 0.5488135 ,  0.71518937],\n       [ 0.60276338,  0.54488318]])\n</code></pre> <p>Repeat random array generation to check</p> <p>If you do not set the seed, you would not get the same set of numbers like here. <pre><code># Seed\nnp.random.seed(0)\nnp.random.rand(2, 2)\n</code></pre></p> <pre><code>array([[ 0.5488135 ,  0.71518937],\n       [ 0.60276338,  0.54488318]])\n</code></pre> <p>Create a numpy array without seed</p> <p>Notice how you get different numbers compared to the first 2 tries? <pre><code># No seed\nnp.random.rand(2, 2)\n</code></pre></p> <pre><code>array([[ 0.56804456,  0.92559664],\n       [ 0.07103606,  0.0871293 ]])\n</code></pre> <p>Repeat numpy array generation without seed</p> <p>You get the point now, you get a totally different set of numbers. <pre><code># No seed\nnp.random.rand(2, 2)\n</code></pre></p> <pre><code>array([[ 0.0202184 ,  0.83261985],\n       [ 0.77815675,  0.87001215]])\n</code></pre> <p>Create a PyTorch tensor with a fixed seed</p> <pre><code># Torch Seed\ntorch.manual_seed(0)\ntorch.rand(2, 2)\n</code></pre> <pre><code>0.5488  0.5928\n0.7152  0.8443\n[torch.FloatTensor of size 2x2]\n</code></pre> <p>Repeat creating a PyTorch fixed seed tensor</p> <pre><code># Torch Seed\ntorch.manual_seed(0)\ntorch.rand(2, 2)\n</code></pre> <pre><code>0.5488  0.5928\n0.7152  0.8443\n[torch.FloatTensor of size 2x2]\n</code></pre> <p>Creating a PyTorch tensor without seed</p> <p>Like with a numpy array of random numbers without seed, you will not get the same results as above. <pre><code># Torch No Seed\ntorch.rand(2, 2)\n</code></pre></p> <pre><code>0.6028  0.8579\n0.5449  0.8473\n[torch.FloatTensor of size 2x2]\n</code></pre> <p>Repeat creating a PyTorch tensor without seed</p> <p>Notice how these are different numbers again? <pre><code># Torch No Seed\ntorch.rand(2, 2)\n</code></pre></p> <pre><code>0.4237  0.6236\n0.6459  0.3844\n[torch.FloatTensor of size 2x2]\n</code></pre> <p>Seed for GPU is different for now...</p> <p>Fix a seed for GPU tensors</p> <p>When you conduct deep learning experiments, typically you want to use GPUs to accelerate your computations and fixing seed for tensors on GPUs is different from CPUs as we have done above.  <pre><code>if torch.cuda.is_available():\n    torch.cuda.manual_seed_all(0)\n</code></pre></p>"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#numpy-and-torch-bridge","title":"NumPy and Torch Bridge","text":""},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#numpy-to-torch","title":"NumPy to Torch","text":"<p>Create a numpy array of 1's</p> <pre><code># Numpy array\nnp_array = np.ones((2, 2))\n</code></pre> <pre><code>print(np_array)\n</code></pre> <pre><code>[[ 1.  1.]\n[ 1.  1.]]\n</code></pre> <p>Get the type of class for the numpy array</p> <pre><code>print(type(np_array))\n</code></pre> <pre><code>&lt;class 'numpy.ndarray'&gt;\n</code></pre> <p>Convert numpy array to PyTorch tensor</p> <pre><code># Convert to Torch Tensor\ntorch_tensor = torch.from_numpy(np_array)\n</code></pre> <pre><code>print(torch_tensor)\n</code></pre> <pre><code> 1  1\n 1  1\n[torch.DoubleTensor of size 2x2]\n</code></pre> <p>Get type of class for PyTorch tensor</p> <p>Notice how it shows it's a torch DoubleTensor? There're actually tensor types and it depends on the numpy data type. <pre><code>print(type(torch_tensor))\n</code></pre></p> <pre><code>&lt;class 'torch.DoubleTensor'&gt;\n</code></pre> <p>Create PyTorch tensor from a different numpy datatype</p> <p>You will get an error running this code because PyTorch tensor don't support all datatype.  <pre><code># Data types matter: intentional error\nnp_array_new = np.ones((2, 2), dtype=np.int8)\ntorch.from_numpy(np_array_new)\n</code></pre></p> <pre><code>---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n\n&lt;ipython-input-57-b8b085f9b39d&gt; in &lt;module&gt;()\n      1 # Data types matter\n      2 np_array_new = np.ones((2, 2), dtype=np.int8)\n----&gt; 3 torch.from_numpy(np_array_new)\n\n\nRuntimeError: can't convert a given np.ndarray to a tensor - it has an invalid type. The only supported types are: double, float, int64, int32, and uint8.\n</code></pre> <p>What conversion support does Numpy to PyTorch tensor bridge gives?</p> <ul> <li><code>double</code></li> <li><code>float</code> </li> <li><code>int64</code>, <code>int32</code>, <code>uint8</code> </li> </ul> <p>Create PyTorch long tensor</p> <p>See how a int64 numpy array gives you a PyTorch long tensor? <pre><code># Data types matter\nnp_array_new = np.ones((2, 2), dtype=np.int64)\ntorch.from_numpy(np_array_new)\n</code></pre></p> <pre><code>1  1\n1  1\n[torch.LongTensor of size 2x2]\n</code></pre> <p>Create PyTorch int tensor</p> <pre><code># Data types matter\nnp_array_new = np.ones((2, 2), dtype=np.int32)\ntorch.from_numpy(np_array_new)\n</code></pre> <pre><code>1  1\n1  1\n[torch.IntTensor of size 2x2]\n</code></pre> <p>Create PyTorch byte tensor</p> <pre><code># Data types matter\nnp_array_new = np.ones((2, 2), dtype=np.uint8)\ntorch.from_numpy(np_array_new)\n</code></pre> <pre><code>1  1\n1  1\n[torch.ByteTensor of size 2x2]\n</code></pre> <p>Create PyTorch Double Tensor</p> <pre><code># Data types matter\nnp_array_new = np.ones((2, 2), dtype=np.float64)\ntorch.from_numpy(np_array_new)\n</code></pre> <p>Alternatively you can do this too via <code>np.double</code></p> <pre><code># Data types matter\nnp_array_new = np.ones((2, 2), dtype=np.double)\ntorch.from_numpy(np_array_new)\n</code></pre> <pre><code>1  1\n1  1\n[torch.DoubleTensor of size 2x2]\n</code></pre> <p>Create PyTorch Float Tensor</p> <pre><code># Data types matter\nnp_array_new = np.ones((2, 2), dtype=np.float32)\ntorch.from_numpy(np_array_new)\n</code></pre> <pre><code>1  1\n1  1\n[torch.FloatTensor of size 2x2]\n</code></pre> <p>Summary</p> <p>Tensor Type Bug Guide</p> <p>These things don't matter much now. But later when you see error messages that require these particular tensor types, refer to this guide!</p> NumPy Array Type Torch Tensor Type int64 LongTensor int32 IntegerTensor uint8 ByteTensor float64 DoubleTensor float32 FloatTensor double DoubleTensor"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#torch-to-numpy","title":"Torch to NumPy","text":"<p>Create PyTorch tensor of 1's</p> <p>You would realize this defaults to a float tensor by default if you do this.</p> <pre><code>torch_tensor = torch.ones(2, 2)\n</code></pre> <pre><code>type(torch_tensor)\n</code></pre> <pre><code>torch.FloatTensor\n</code></pre> <p>Convert tensor to numpy</p> <p>It's as simple as this.</p> <pre><code>torch_to_numpy = torch_tensor.numpy()\n</code></pre> <pre><code>type(torch_to_numpy)\n</code></pre> <pre><code># Wowza, we did it.\nnumpy.ndarray\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#tensors-on-cpu-vs-gpu","title":"Tensors on CPU vs GPU","text":"<p>Move tensor to CPU and back</p> <p>This by default creates a tensor on CPU. You do not need to do anything. <pre><code># CPU\ntensor_cpu = torch.ones(2, 2)\n</code></pre></p> <p>If you would like to send a tensor to your GPU, you just need to do a simple <code>.cuda()</code></p> <pre><code># CPU to GPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntensor_cpu.to(device)\n</code></pre> <p>And if you want to move that tensor on the GPU back to the CPU, just do the following.</p> <pre><code># GPU to CPU\ntensor_cpu.cpu()\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#tensor-operations","title":"Tensor Operations","text":""},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#resizing-tensor","title":"Resizing Tensor","text":"<p>Creating a 2x2 tensor</p> <pre><code>a = torch.ones(2, 2)\nprint(a)\n</code></pre> <pre><code>1  1\n1  1\n[torch.FloatTensor of size 2x2]\n</code></pre> <p>Getting size of tensor</p> <pre><code>print(a.size())\n</code></pre> <pre><code>torch.Size([2, 2])\n</code></pre> <p>Resize tensor to 4x1</p> <pre><code>a.view(4)\n</code></pre> <pre><code>1\n1\n1\n1\n[torch.FloatTensor of size 4]\n</code></pre> <p>Get size of resized tensor</p> <pre><code>a.view(4).size()\n</code></pre> <pre><code>torch.Size([4])\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#element-wise-addition","title":"Element-wise Addition","text":"<p>Creating first 2x2 tensor</p> <pre><code>a = torch.ones(2, 2)\nprint(a)\n</code></pre> <pre><code>1  1\n1  1\n[torch.FloatTensor of size 2x2]\n</code></pre> <p>Creating second 2x2 tensor</p> <pre><code>b = torch.ones(2, 2)\nprint(b)\n</code></pre> <pre><code>1  1\n1  1\n[torch.FloatTensor of size 2x2]\n</code></pre> <p>Element-wise addition of 2 tensors</p> <pre><code># Element-wise addition\nc = a + b\nprint(c)\n</code></pre> <pre><code> 2  2\n 2  2\n[torch.FloatTensor of size 2x2]\n</code></pre> <p>Alternative element-wise addition of 2 tensors</p> <pre><code># Element-wise addition\nc = torch.add(a, b)\nprint(c)\n</code></pre> <pre><code> 2  2\n 2  2\n[torch.FloatTensor of size 2x2]\n</code></pre> <p>In-place element-wise addition</p> <p>This would replace the c tensor values with the new addition. </p> <pre><code># In-place addition\nprint('Old c tensor')\nprint(c)\n\nc.add_(a)\n\nprint('-'*60)\nprint('New c tensor')\nprint(c)\n</code></pre> <pre><code>Old c tensor\n\n 2  2\n 2  2\n[torch.FloatTensor of size 2x2]\n\n------------------------------------------------------------\nNew c tensor\n\n 3  3\n 3  3\n[torch.FloatTensor of size 2x2]\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#element-wise-subtraction","title":"Element-wise Subtraction","text":"<p>Check values of tensor a and b'</p> <p>Take note that you've created tensor a and b of sizes 2x2 filled with 1's each above.  <pre><code>print(a)\nprint(b)\n</code></pre></p> <pre><code> 1  1\n 1  1\n[torch.FloatTensor of size 2x2]\n\n\n 1  1\n 1  1\n[torch.FloatTensor of size 2x2]\n</code></pre> <p>Element-wise subtraction: method 1</p> <pre><code>a - b\n</code></pre> <pre><code>0  0\n0  0\n[torch.FloatTensor of size 2x2]\n</code></pre> <p>Element-wise subtraction: method 2</p> <pre><code># Not in-place\nprint(a.sub(b))\nprint(a)\n</code></pre> <pre><code>0  0\n0  0\n[torch.FloatTensor of size 2x2]\n\n\n1  1\n1  1\n[torch.FloatTensor of size 2x2]\n</code></pre> <p>Element-wise subtraction: method 3</p> <p>This will replace a with the final result filled with 2's <pre><code># Inplace\nprint(a.sub_(b))\nprint(a)\n</code></pre></p> <pre><code>0  0\n0  0\n[torch.FloatTensor of size 2x2]\n\n\n0  0\n0  0\n[torch.FloatTensor of size 2x2]\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#element-wise-multiplication","title":"Element-Wise Multiplication","text":"<p>Create tensor a and b of sizes 2x2 filled with 1's and 0's</p> <pre><code>a = torch.ones(2, 2)\nprint(a)\nb = torch.zeros(2, 2)\nprint(b)\n</code></pre> <pre><code>1  1\n1  1\n[torch.FloatTensor of size 2x2]\n\n\n0  0\n0  0\n[torch.FloatTensor of size 2x2]\n</code></pre> <p>Element-wise multiplication: method 1</p> <pre><code>a * b\n</code></pre> <pre><code>0  0\n0  0\n[torch.FloatTensor of size 2x2]\n</code></pre> <p>Element-wise multiplication: method 2</p> <pre><code># Not in-place\nprint(torch.mul(a, b))\nprint(a)\n</code></pre> <pre><code>0  0\n0  0\n[torch.FloatTensor of size 2x2]\n\n1  1\n1  1\n[torch.FloatTensor of size 2x2]\n</code></pre> <p>Element-wise multiplication: method 3</p> <pre><code># In-place\nprint(a.mul_(b))\nprint(a)\n</code></pre> <pre><code>0  0\n0  0\n[torch.FloatTensor of size 2x2]\n\n0  0\n0  0\n[torch.FloatTensor of size 2x2]\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#element-wise-division","title":"Element-Wise Division","text":"<p>Create tensor a and b of sizes 2x2 filled with 1's and 0's</p> <pre><code>a = torch.ones(2, 2)\nprint(a)\nb = torch.zeros(2, 2)\nprint(b)\n</code></pre> <pre><code>1  1\n1  1\n[torch.FloatTensor of size 2x2]\n\n\n0  0\n0  0\n[torch.FloatTensor of size 2x2]\n</code></pre> <p>Element-wise division: method 1</p> <pre><code>b / a\n</code></pre> <pre><code>0  0\n0  0\n[torch.FloatTensor of size 2x2]\n</code></pre> <p>Element-wise division: method 2</p> <pre><code>torch.div(b, a)\n</code></pre> <pre><code>0  0\n0  0\n[torch.FloatTensor of size 2x2]\n</code></pre> <p>Element-wise division: method 3</p> <pre><code># Inplace\nb.div_(a)\n</code></pre> <pre><code>0  0\n0  0\n[torch.FloatTensor of size 2x2]\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#tensor-mean","title":"Tensor Mean","text":"\\[1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55\\] \\[ mean = 55 /10 = 5.5 \\] <p>Create tensor of size 10 filled from 1 to 10</p> <pre><code>a = torch.Tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\na.size()\n</code></pre> <pre><code>torch.Size([10])\n</code></pre> <p>Get tensor mean</p> <p>Here we get 5.5 as we've calculated manually above.</p> <pre><code>a.mean(dim=0)\n</code></pre> <pre><code>5.5000\n[torch.FloatTensor of size 1]\n</code></pre> <p>Get tensor mean on second dimension</p> <p>Here we get an error because the tensor is of size 10 and not 10x1 so there's no second dimension to calculate.</p> <pre><code>a.mean(dim=1)\n</code></pre> <pre><code>RuntimeError                              Traceback (most recent call last)\n\n&lt;ipython-input-7-81aec0cf1c00&gt; in &lt;module&gt;()\n----&gt; 1 a.mean(dim=1)\n\n\nRuntimeError: dimension out of range (expected to be in range of [-1, 0], but got 1)\n</code></pre> <p>Create a 2x10 Tensor, of 1-10 digits each</p> <p><pre><code>a = torch.Tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n</code></pre> <pre><code>a.size()\n</code></pre></p> <pre><code>torch.Size([2, 10])\n</code></pre> <p>Get tensor mean on second dimension</p> <p>Here we won't get an error like previously because we've a tensor of size 2x10</p> <pre><code>a.mean(dim=1)\n</code></pre> <pre><code> 5.5000\n 5.5000\n[torch.FloatTensor of size 2x1]\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#tensor-standard-deviation","title":"Tensor Standard Deviation","text":"<p>Get standard deviation of tensor</p> <pre><code>a = torch.Tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\na.std(dim=0)\n</code></pre> <pre><code> 3.0277\n[torch.FloatTensor of size 1]\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#summary","title":"Summary","text":"<p>We've learnt to...</p> <p>Success</p> <ul> <li> Create Matrices</li> <li> Create Matrices with Default Initialization Values<ul> <li> Zeros </li> <li> Ones</li> </ul> </li> <li> Initialize Seeds for Reproducibility on GPU and CPU</li> <li> Convert Matrices: NumPy to Torch and Torch to NumPy</li> <li> Move Tensors: CPU to GPU and GPU to CPU</li> <li> Run Important Tensor Operations<ul> <li> Element-wise addition, subtraction, multiplication and division</li> <li> Resize</li> <li> Calculate mean </li> <li> Calculate standard deviation</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#citation","title":"Citation","text":"<p>If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.</p> <p></p>"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/","title":"Recurrent Neural Network with PyTorch","text":"<p>Run Jupyter Notebook</p> <p>You can run the code for this section in this jupyter notebook link.</p>"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#about-recurrent-neural-network","title":"About Recurrent Neural Network","text":""},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#feedforward-neural-networks-transition-to-1-layer-recurrent-neural-networks-rnn","title":"Feedforward Neural Networks Transition to 1 Layer Recurrent Neural Networks (RNN)","text":"<ul> <li>RNN is essentially an FNN but with a hidden layer (non-linear output) that passes on information to the next FNN</li> <li>Compared to an FNN, we've one additional set of weight and bias that allows information to flow from one FNN to another FNN sequentially that allows time-dependency.</li> <li>The diagram below shows the only difference between an FNN and a RNN. </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#2-layer-rnn-breakdown","title":"2 Layer RNN Breakdown","text":""},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#building-a-recurrent-neural-network-with-pytorch","title":"Building a Recurrent Neural Network with PyTorch","text":""},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#model-a-1-hidden-layer-relu","title":"Model A: 1 Hidden Layer (ReLU)","text":"<ul> <li>Unroll 28 time steps<ul> <li>Each step input size: 28 x 1</li> <li>Total per unroll: 28 x 28<ul> <li>Feedforward Neural Network input size: 28 x 28 </li> </ul> </li> </ul> </li> <li>1 Hidden layer</li> <li>ReLU Activation Function</li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#steps","title":"Steps","text":"<ul> <li>Step 1: Load Dataset</li> <li>Step 2: Make Dataset Iterable</li> <li>Step 3: Create Model Class</li> <li>Step 4: Instantiate Model Class</li> <li>Step 5: Instantiate Loss Class</li> <li>Step 6: Instantiate Optimizer Class</li> <li>Step 7: Train Model</li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#step-1-loading-mnist-train-dataset","title":"Step 1: Loading MNIST Train Dataset","text":"<p>Images from 1 to 9</p> <p>Looking into the MNIST Dataset</p> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n</code></pre> <pre><code>train_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n</code></pre> <p>We would have 60k training images of size 28 x 28 pixels.</p> <pre><code>print(train_dataset.train_data.size())\n</code></pre> <pre><code>print(train_dataset.train_labels.size())\n</code></pre> <p>Here we would have 10k testing images of the same size, 28 x 28 pixels.</p> <pre><code>print(test_dataset.test_data.size())\n</code></pre> <pre><code>print(test_dataset.test_labels.size())\n</code></pre> <pre><code>torch.Size([60000, 28, 28])\n\ntorch.Size([60000])\n\ntorch.Size([10000, 28, 28])\n\ntorch.Size([10000])\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#step-2-make-dataset-iterable","title":"Step 2: Make Dataset Iterable","text":"<p>Creating iterable objects to loop through subsequently</p> <pre><code>batch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#step-3-create-model-class","title":"Step 3: Create Model Class","text":"<p>1 Layer RNN</p> <p></p> <pre><code>class RNNModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n        super(RNNModel, self).__init__()\n        # Hidden dimensions\n        self.hidden_dim = hidden_dim\n\n        # Number of hidden layers\n        self.layer_dim = layer_dim\n\n        # Building your RNN\n        # batch_first=True causes input/output tensors to be of shape\n        # (batch_dim, seq_dim, input_dim)\n        # batch_dim = number of samples per batch\n        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n\n        # Readout layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        # Initialize hidden state with zeros\n        # (layer_dim, batch_size, hidden_dim)\n        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n\n        # We need to detach the hidden state to prevent exploding/vanishing gradients\n        # This is part of truncated backpropagation through time (BPTT)\n        out, hn = self.rnn(x, h0.detach())\n\n        # Index hidden state of last time step\n        # out.size() --&gt; 100, 28, 10\n        # out[:, -1, :] --&gt; 100, 10 --&gt; just want last time step hidden states! \n        out = self.fc(out[:, -1, :]) \n        # out.size() --&gt; 100, 10\n        return out\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#step-4-instantiate-model-class","title":"Step 4: Instantiate Model Class","text":"<ul> <li>28 time steps<ul> <li>Each time step: input dimension = 28</li> </ul> </li> <li>1 hidden layer</li> <li>MNIST 1-9 digits \\(\\rightarrow\\) output dimension = 10</li> </ul> <p>Instantiate model class and assign to an object</p> <pre><code>input_dim = 28\nhidden_dim = 100\nlayer_dim = 1\noutput_dim = 10\n</code></pre> <pre><code>model = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#step-5-instantiate-loss-class","title":"Step 5: Instantiate Loss Class","text":"<ul> <li>Recurrent Neural Network: Cross Entropy Loss<ul> <li>Convolutional Neural Network: Cross Entropy Loss</li> <li>Feedforward Neural Network: Cross Entropy Loss</li> <li>Logistic Regression: Cross Entropy Loss</li> <li>Linear Regression: MSE</li> </ul> </li> </ul> <p>Cross Entropy Loss for Classification Task</p> <pre><code>criterion = nn.CrossEntropyLoss()\n</code></pre> <p>Cross Entropy vs MSE</p> <p>Take note that there are cases where RNN, CNN and FNN use MSE as a loss function.</p> <p>We use cross entropy for classification tasks (predicting 0-9 digits in MNIST for example).</p> <p>And we use MSE for regression tasks (predicting temperatures in every December in San Francisco for example).</p>"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#step-6-instantiate-optimizer-class","title":"Step 6: Instantiate Optimizer Class","text":"<ul> <li>Simplified equation<ul> <li>\\(\\theta = \\theta - \\eta \\cdot \\nabla_\\theta\\)<ul> <li>\\(\\theta\\): parameters (our tensors with gradient accumulation abilities)</li> <li>\\(\\eta\\): learning rate (how fast we want to learn)</li> <li>\\(\\nabla_\\theta\\): gradients of loss with respect to the model's parameters</li> </ul> </li> </ul> </li> <li>Even simplier equation<ul> <li><code>parameters = parameters - learning_rate * parameters_gradients</code></li> <li>At every iteration, we update our model's parameters</li> </ul> </li> </ul> <pre><code>learning_rate = 0.01\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#parameters-in-depth","title":"Parameters In-Depth","text":"<ul> <li>Input to Hidden Layer Affine Function<ul> <li>A1, B1</li> </ul> </li> <li>Hidden Layer to Output Affine Function<ul> <li>A2, B2</li> </ul> </li> <li>Hidden Layer to Hidden Layer Affine Function<ul> <li>A3, B3</li> </ul> </li> </ul> <p>Total groups of parameters</p> <p>We should have 6 groups as shown above.</p> <pre><code>len(list(model.parameters()))\n</code></pre> <pre><code>6\n</code></pre> <p>Input to Hidden Weight</p> <p>Remember we defined our hidden layer to have a size of 100. Because our input is a size of 28 at each time step, this gives rise to a weight matrix of 100 x 28.</p> <pre><code># Input --&gt; Hidden (A1)\nlist(model.parameters())[0].size()\n</code></pre> <pre><code>torch.Size([100, 28])\n</code></pre> <p>Input to Hidden Bias</p> <pre><code># Input --&gt; Hidden BIAS (B1)\nlist(model.parameters())[2].size()\n</code></pre> <pre><code>torch.Size([100])\n</code></pre> <p>Hidden to Hidden</p> <pre><code># Hidden --&gt; Hidden (A3)\nlist(model.parameters())[1].size()\n</code></pre> <pre><code>torch.Size([100, 100])\n</code></pre> <p>Hidden to Hidden Bias</p> <pre><code># Hidden --&gt; Hidden BIAS(B3)\nlist(model.parameters())[3].size()\n</code></pre> <pre><code>torch.Size([100])\n</code></pre> <p>Hidden to Output</p> <pre><code># Hidden --&gt; Output (A2)\nlist(model.parameters())[4].size()\n</code></pre> <pre><code>torch.Size([10, 100])\n</code></pre> <p>Hidden to Output Bias</p> <pre><code># Hidden --&gt; Output BIAS (B2)\nlist(model.parameters())[5].size()\n</code></pre> <pre><code>torch.Size([10])\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#step-7-train-model","title":"Step 7: Train Model","text":"<ul> <li>Process <ol> <li>Convert inputs/labels to tensors with gradient accumulation abilities<ul> <li>RNN Input: (1, 28)</li> <li>CNN Input: (1, 28, 28) </li> <li>FNN Input: (1, 28*28)</li> </ul> </li> <li>Clear gradient buffets</li> <li>Get output given inputs </li> <li>Get loss</li> <li>Get gradients w.r.t. parameters</li> <li>Update parameters using gradients<ul> <li><code>parameters = parameters - learning_rate * parameters_gradients</code></li> </ul> </li> <li>REPEAT</li> </ol> </li> </ul> <p>Same 7 step process for training models</p> <pre><code># Number of steps to unroll\nseq_dim = 28  \n\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        model.train()\n        # Load images as tensors with gradient accumulation abilities\n        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        # outputs.size() --&gt; 100, 10\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            model.eval()\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images to a Torch tensors with gradient accumulation abilities\n                images = images.view(-1, seq_dim, input_dim)\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Iteration: 500. Loss: 2.301494836807251. Accuracy: 12\nIteration: 1000. Loss: 2.2986037731170654. Accuracy: 14\nIteration: 1500. Loss: 2.278566598892212. Accuracy: 18\nIteration: 2000. Loss: 2.169614315032959. Accuracy: 21\nIteration: 2500. Loss: 1.1662731170654297. Accuracy: 51\nIteration: 3000. Loss: 0.9290509223937988. Accuracy: 71\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#model-b-2-hidden-layer-relu","title":"Model B: 2 Hidden Layer (ReLU)","text":"<ul> <li>Unroll 28 time steps<ul> <li>Each step input size: 28 x 1</li> <li>Total per unroll: 28 x 28<ul> <li>Feedforward Neural Network inpt size: 28 x 28 </li> </ul> </li> </ul> </li> <li>2 Hidden layer</li> <li>ReLU Activation Function</li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#steps_1","title":"Steps","text":"<ul> <li>Step 1: Load Dataset</li> <li>Step 2: Make Dataset Iterable</li> <li>Step 3: Create Model Class</li> <li>Step 4: Instantiate Model Class</li> <li>Step 5: Instantiate Loss Class</li> <li>Step 6: Instantiate Optimizer Class</li> <li>Step 7: Train Model</li> </ul> <p>2 Hidden Layer + ReLU</p> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n'''\nSTEP 1: LOADING DATASET\n'''\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\n\nclass RNNModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n        super(RNNModel, self).__init__()\n        # Hidden dimensions\n        self.hidden_dim = hidden_dim\n\n        # Number of hidden layers\n        self.layer_dim = layer_dim\n\n        # Building your RNN\n        # batch_first=True causes input/output tensors to be of shape\n        # (batch_dim, seq_dim, feature_dim)\n        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n\n        # Readout layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        # Initialize hidden state with zeros\n        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n\n        # We need to detach the hidden state to prevent exploding/vanishing gradients\n        # This is part of truncated backpropagation through time (BPTT)\n        out, hn = self.rnn(x, h0.detach())\n\n        # Index hidden state of last time step\n        # out.size() --&gt; 100, 28, 100\n        # out[:, -1, :] --&gt; 100, 100 --&gt; just want last time step hidden states! \n        out = self.fc(out[:, -1, :]) \n        # out.size() --&gt; 100, 10\n        return out\n\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28\nhidden_dim = 100\nlayer_dim = 2  # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER\noutput_dim = 10\n\nmodel = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)\n\n# JUST PRINTING MODEL &amp; PARAMETERS \nprint(model)\nprint(len(list(model.parameters())))\nfor i in range(len(list(model.parameters()))):\n    print(list(model.parameters())[i].size())\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.01\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\n\n# Number of steps to unroll\nseq_dim = 28  \n\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        model.train()\n        # Load images as tensors with gradient accumulation abilities\n        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        # outputs.size() --&gt; 100, 10\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            model.eval()\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Resize images\n                images = images.view(-1, seq_dim, input_dim)\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>RNNModel(\n  (rnn): RNN(28, 100, num_layers=2, batch_first=True)\n  (fc): Linear(in_features=100, out_features=10, bias=True)\n)\n\n10\n\ntorch.Size([100, 28])\ntorch.Size([100, 100])\ntorch.Size([100])\ntorch.Size([100])\ntorch.Size([100, 100])\ntorch.Size([100, 100])\ntorch.Size([100])\ntorch.Size([100])\ntorch.Size([10, 100])\ntorch.Size([10])\n\nIteration: 500. Loss: 2.3019518852233887. Accuracy: 11\nIteration: 1000. Loss: 2.299217700958252. Accuracy: 11\nIteration: 1500. Loss: 2.279090166091919. Accuracy: 14\nIteration: 2000. Loss: 2.126953125. Accuracy: 25\nIteration: 2500. Loss: 1.356347680091858. Accuracy: 57\nIteration: 3000. Loss: 0.7377720475196838. Accuracy: 69\n</code></pre> <ul> <li>10 sets of parameters</li> <li>First hidden Layer<ul> <li>\\(A_1 = [100, 28]\\)</li> <li>\\(A_3 = [100, 100]\\)</li> <li>\\(B_1 = [100]\\)</li> <li>\\(B_3 = [100]\\)</li> </ul> </li> <li>Second hidden layer<ul> <li>\\(A_2 = [100, 100]\\)</li> <li>\\(A_5 = [100, 100]\\)</li> <li>\\(B_2 = [100]\\)</li> <li>\\(B_5 = [100]\\)</li> </ul> </li> <li>Readout layer<ul> <li>\\(A_4 = [10, 100]\\)</li> <li>\\(B_4 = [10]\\)</li> </ul> </li> </ul> <p></p>"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#model-c-2-hidden-layer","title":"Model C: 2 Hidden Layer","text":"<ul> <li>Unroll 28 time steps<ul> <li>Each step input size: 28 x 1</li> <li>Total per unroll: 28 x 28<ul> <li>Feedforward Neural Network inpt size: 28 x 28 </li> </ul> </li> </ul> </li> <li>2 Hidden layer</li> <li>Tanh Activation Function</li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#steps_2","title":"Steps","text":"<ul> <li>Step 1: Load Dataset</li> <li>Step 2: Make Dataset Iterable</li> <li>Step 3: Create Model Class</li> <li>Step 4: Instantiate Model Class</li> <li>Step 5: Instantiate Loss Class</li> <li>Step 6: Instantiate Optimizer Class</li> <li>Step 7: Train Model</li> </ul> <p>!!! \"2 Hidden + ReLU\"     <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n'''\nSTEP 1: LOADING DATASET\n'''\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\n\nclass RNNModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n        super(RNNModel, self).__init__()\n        # Hidden dimensions\n        self.hidden_dim = hidden_dim\n\n        # Number of hidden layers\n        self.layer_dim = layer_dim\n\n        # Building your RNN\n        # batch_first=True causes input/output tensors to be of shape\n        # (batch_dim, seq_dim, feature_dim)\n        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='tanh')\n\n        # Readout layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        # Initialize hidden state with zeros\n        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n\n        # One time step\n        # We need to detach the hidden state to prevent exploding/vanishing gradients\n        # This is part of truncated backpropagation through time (BPTT)\n        out, hn = self.rnn(x, h0.detach())\n\n        # Index hidden state of last time step\n        # out.size() --&gt; 100, 28, 100\n        # out[:, -1, :] --&gt; 100, 100 --&gt; just want last time step hidden states! \n        out = self.fc(out[:, -1, :]) \n        # out.size() --&gt; 100, 10\n        return out\n\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28\nhidden_dim = 100\nlayer_dim = 2  # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER\noutput_dim = 10\n\nmodel = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)\n\n# JUST PRINTING MODEL &amp; PARAMETERS \nprint(model)\nprint(len(list(model.parameters())))\nfor i in range(len(list(model.parameters()))):\n    print(list(model.parameters())[i].size())\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\n\n# Number of steps to unroll\nseq_dim = 28  \n\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as tensors with gradient accumulation abilities\n        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        # outputs.size() --&gt; 100, 10\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Resize images\n                images = images.view(-1, seq_dim, input_dim)\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre></p> <pre><code>RNNModel(\n  (rnn): RNN(28, 100, num_layers=2, batch_first=True)\n  (fc): Linear(in_features=100, out_features=10, bias=True)\n)\n\n10\n\ntorch.Size([100, 28])\ntorch.Size([100, 100])\ntorch.Size([100])\ntorch.Size([100])\ntorch.Size([100, 100])\ntorch.Size([100, 100])\ntorch.Size([100])\ntorch.Size([100])\ntorch.Size([10, 100])\ntorch.Size([10])\nIteration: 500. Loss: 0.5943437218666077. Accuracy: 77\nIteration: 1000. Loss: 0.22048641741275787. Accuracy: 91\nIteration: 1500. Loss: 0.18479223549365997. Accuracy: 94\nIteration: 2000. Loss: 0.2723771929740906. Accuracy: 91\nIteration: 2500. Loss: 0.18817797303199768. Accuracy: 92\nIteration: 3000. Loss: 0.1685929149389267. Accuracy: 92\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#summary-of-results","title":"Summary of Results","text":"Model A Model B Model C ReLU ReLU Tanh 1 Hidden Layer 2 Hidden Layers 2 Hidden Layers 100 Hidden Units 100 Hidden Units 100 Hidden Units 92.48% 95.09% 95.54%"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#general-deep-learning-notes","title":"General Deep Learning Notes","text":"<ul> <li>2 ways to expand a recurrent neural network<ul> <li>More non-linear activation units (neurons)</li> <li>More hidden layers</li> </ul> </li> <li>Cons<ul> <li>Need a larger dataset<ul> <li>Curse of dimensionality</li> </ul> </li> <li>Does not necessarily mean higher accuracy</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#3-building-a-recurrent-neural-network-with-pytorch-gpu","title":"3. Building a Recurrent Neural Network with PyTorch (GPU)","text":""},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#model-c-2-hidden-layer-tanh","title":"Model C: 2 Hidden Layer (Tanh)","text":"<p>GPU: 2 things must be on GPU - <code>model</code> - <code>tensors</code></p>"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#steps_3","title":"Steps","text":"<ul> <li>Step 1: Load Dataset</li> <li>Step 2: Make Dataset Iterable</li> <li>Step 3: Create Model Class</li> <li>Step 4: Instantiate Model Class</li> <li>Step 5: Instantiate Loss Class</li> <li>Step 6: Instantiate Optimizer Class</li> <li>Step 7: Train Model</li> </ul> <p>2 Layer RNN + Tanh</p> <pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n'''\nSTEP 1: LOADING DATASET\n'''\ntrain_dataset = dsets.MNIST(root='./data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\n\nclass RNNModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n        super(RNNModel, self).__init__()\n        # Hidden dimensions\n        self.hidden_dim = hidden_dim\n\n        # Number of hidden layers\n        self.layer_dim = layer_dim\n\n        # Building your RNN\n        # batch_first=True causes input/output tensors to be of shape\n        # (batch_dim, seq_dim, feature_dim)\n        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='tanh')\n\n        # Readout layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        # Initialize hidden state with zeros\n        #######################\n        #  USE GPU FOR MODEL  #\n        #######################\n        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device)\n\n        # One time step\n        # We need to detach the hidden state to prevent exploding/vanishing gradients\n        # This is part of truncated backpropagation through time (BPTT)\n        out, hn = self.rnn(x, h0.detach())\n\n        # Index hidden state of last time step\n        # out.size() --&gt; 100, 28, 100\n        # out[:, -1, :] --&gt; 100, 100 --&gt; just want last time step hidden states! \n        out = self.fc(out[:, -1, :]) \n        # out.size() --&gt; 100, 10\n        return out\n\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28\nhidden_dim = 100\nlayer_dim = 2  # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER\noutput_dim = 10\n\nmodel = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)\n\n#######################\n#  USE GPU FOR MODEL  #\n#######################\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\n\n# Number of steps to unroll\nseq_dim = 28  \n\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as tensors with gradient accumulation abilities\n        #######################\n        #  USE GPU FOR MODEL  #\n        #######################\n        images = images.view(-1, seq_dim, input_dim).requires_grad_().to(device)\n        labels = labels.to(device)\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        # outputs.size() --&gt; 100, 10\n        outputs = model(images)\n\n        # Calculate Loss: softmax --&gt; cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                #######################\n                #  USE GPU FOR MODEL  #\n                #######################\n                images = images.view(-1, seq_dim, input_dim).to(device)\n\n                # Forward pass only to get logits/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                #######################\n                #  USE GPU FOR MODEL  #\n                #######################\n                if torch.cuda.is_available():\n                    correct += (predicted.cpu() == labels.cpu()).sum()\n                else:\n                    correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n</code></pre> <pre><code>Iteration: 500. Loss: 0.5983774662017822. Accuracy: 81\nIteration: 1000. Loss: 0.2960105836391449. Accuracy: 86\nIteration: 1500. Loss: 0.19428101181983948. Accuracy: 93\nIteration: 2000. Loss: 0.11918395012617111. Accuracy: 95\nIteration: 2500. Loss: 0.11246936023235321. Accuracy: 95\nIteration: 3000. Loss: 0.15849310159683228. Accuracy: 95\n</code></pre>"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#summary","title":"Summary","text":"<p>We've learnt to...</p> <p>Success</p> <ul> <li> Feedforward Neural Networks Transition to Recurrent Neural Networks</li> <li> RNN Models in PyTorch<ul> <li> Model A: 1 Hidden Layer RNN (ReLU)</li> <li> Model B: 2 Hidden Layer RNN (ReLU)</li> <li> Model C: 2 Hidden Layer RNN (Tanh)</li> </ul> </li> <li> Models Variation in Code<ul> <li> Modifying only step 4</li> </ul> </li> <li> Ways to Expand Model\u2019s Capacity<ul> <li> More non-linear activation units (neurons)</li> <li> More hidden layers</li> </ul> </li> <li> Cons of Expanding Capacity<ul> <li> Need more data</li> <li> Does not necessarily mean higher accuracy</li> </ul> </li> <li> GPU Code<ul> <li> 2 things on GPU<ul> <li> model</li> <li> tensors with gradient accumulation abilities</li> </ul> </li> <li> Modifying only Step 3, 4 and 7</li> </ul> </li> <li> 7 Step Model Building Recap<ul> <li> Step 1: Load Dataset</li> <li> Step 2: Make Dataset Iterable</li> <li> Step 3: Create Model Class</li> <li> Step 4: Instantiate Model Class</li> <li> Step 5: Instantiate Loss Class</li> <li> Step 6: Instantiate Optimizer Class</li> <li> Step 7: Train Model</li> <li> Step 7: Train Model</li> </ul> </li> </ul>"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#citation","title":"Citation","text":"<p>If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.</p> <p></p>"},{"location":"deep_learning/production_pytorch/speed_optimization_basics_numba/","title":"Speed Optimization Basics: Numba","text":""},{"location":"deep_learning/production_pytorch/speed_optimization_basics_numba/#when-to-use-numba","title":"When to use Numba","text":"<ul> <li>Numba works well when the code relies a lot on (1) numpy, (2) loops, and/or (2) cuda.</li> <li>Hence, we would like to maximize the use of numba in our code where possible where there are loops/numpy</li> </ul>"},{"location":"deep_learning/production_pytorch/speed_optimization_basics_numba/#numba-cpu-nopython","title":"Numba CPU: nopython","text":"<ul> <li>For a basic numba application, we can cecorate python function thus allowing it to run without python interpreter</li> <li>Essentially, it will compile the function with specific arguments once into machine code, then uses the cache subsequently</li> </ul>"},{"location":"deep_learning/production_pytorch/speed_optimization_basics_numba/#with-numba-no-python","title":"With Numba: no python","text":"<pre><code>from numba import jit, prange\nimport numpy as np\n\n# Numpy array of 10k elements\ninput_ndarray = np.random.rand(10000).reshape(10000)\n\n# This is the only extra line of code you need to add\n# which is a decorator\n@jit(nopython=True)\ndef go_fast(a):\n    trace = 0\n    for i in range(a.shape[0]):\n        trace += np.tanh(a[i])\n    return a + trace\n\n%timeit go_fast(input_ndarray)\n</code></pre> <pre><code>161 \u00b5s \u00b1 2.62 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n</code></pre>"},{"location":"deep_learning/production_pytorch/speed_optimization_basics_numba/#without-numba","title":"Without numba","text":"<ul> <li>This is much slower, time measured in the millisecond space rather than microsecond with <code>@jit(nopython=True)</code> or <code>@njit</code></li> </ul> <pre><code># Without numba: notice how this is really slow\ndef go_normal(a):\n    trace = 0\n    for i in range(a.shape[0]):\n        trace += np.tanh(a[i])\n    return a + trace\n\n%timeit go_normal(input_ndarray)\n</code></pre> <pre><code>10.5 ms \u00b1 163 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</code></pre>"},{"location":"deep_learning/production_pytorch/speed_optimization_basics_numba/#numba-cpu-parallel","title":"Numba CPU: parallel","text":"<ul> <li>Here, instead of the normal <code>range()</code> function we would use for loops, we would need to use <code>prange()</code> which allows us to execute the loops in parallel on separate threads</li> <li>As you can see, it's slightly faster than <code>@jit(nopython=True)</code></li> </ul> <pre><code>@jit(nopython=True, parallel=True)\ndef go_even_faster(a):\n    trace = 0\n    for i in prange(a.shape[0]):\n        trace += np.tanh(a[i])\n    return a + trace\n\n%timeit go_even_faster(input_ndarray)\n</code></pre> <pre><code>148 \u00b5s \u00b1 71.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</code></pre>"},{"location":"deep_learning/production_pytorch/speed_optimization_basics_numba/#numba-cpu-fastmath","title":"Numba CPU: fastmath","text":"<ul> <li>What if we relax our condition of strictly adhering to <code>IEEE 754</code>. </li> <li>We can have faster performance (depends)</li> <li>I would say this is the least additional speed-up unless you really dig into areas where <code>fastmath=True</code> thrives</li> </ul> <pre><code>@jit(nopython=True, parallel=True, fastmath=True)\ndef go_super_fast(a):\n    trace = 0\n    for i in prange(a.shape[0]):\n        trace += np.tanh(a[i])\n    return a + trace\n\n%timeit go_super_fast(input_ndarray)\n</code></pre> <pre><code>113 \u00b5s \u00b1 39.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</code></pre>"},{"location":"deep_learning/production_pytorch/speed_optimization_basics_numba/#summary","title":"Summary","text":"<ul> <li>When to use Numba<ul> <li>(1) numpy array or torch tensors,</li> <li>(2) loops, and/or</li> <li>(3) cuda</li> </ul> </li> <li>Numba CPU: nopython\u00b6</li> <li>Numba CPU: parallel </li> <li>Numba CPU: fastmath</li> </ul>"},{"location":"machine_learning/intro/","title":"Machine Learning","text":"<p>We'll be covering both CPU and GPU implementations of machine learning algorithms and data wrangling libraries. </p> Packages and Languages you will Learn to Use <ul> <li>Python</li> <li>RAPIDS cuDF</li> <li>RAPIDS cuML</li> <li>Scikit-learn</li> <li>Pandas</li> <li>NumPy</li> <li>CuPy</li> </ul> <p>Work in progress</p> <p>This open-source portion is still a work in progress. Stay tuned while we gradually upload our tutorials and notes. Feel free to contact Ritchie Ng if you would like to contribute via our Facebook page.</p>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/","title":"GFD: GPU Fractional Differencing","text":""},{"location":"machine_learning/gpu/gpu_fractional_differencing/#summary","title":"Summary","text":"<p>Typically we attempt to achieve some form of stationarity via a transformation on our time series through common methods including integer differencing. However, integer differencing unnecessarily removes too much memory to achieve stationarity. An alternative, fractional differencing, allows us to achieve stationarity while maintaining the maximum amount of memory compared to integer differencing. While existing CPU-based implementations are inefficient for running fractional differencing on many large-scale time series, our GPU-based implementation enables rapid fractional differencing.</p> <p>This tutorial walks you through our GPU implementation of fractional differencing (GFD). Also, this tutorial is a special collaboration between industry and academia led by me.</p>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#credits","title":"Credits","text":"<p>Before we start, I would like to thank and give credits for this particular post to: 1. NVIDIA (Ettikan, Chris and Nick), Amazon AWS, ensemblecap.ai, and NExT++ (NUS School of Computing) 2. Marcos Lopez de Prado  for his recent push on the use of fractional differencing of which this guide is based on. 3. Hosking for his paper in 1981 on fractional differencing.</p>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#links","title":"Links","text":"<ul> <li>Code Repository: https://github.com/ritchieng/fractional_differencing_gpu</li> <li>Presentation: https://www.researchgate.net/publication/335159299_GFD_GPU_Fractional_Differencing_for_Rapid_Large-scale_Stationarizing_of_Time_Series_Data_while_Minimizing_Memory_Loss</li> </ul>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#installation-of-libraries-optional","title":"Installation of Libraries (Optional)","text":"<ul> <li>These steps are for easily running RAPIDS in Google Colab. You can skip this section if you've RAPIDS installed.</li> </ul> <pre><code>import pynvml\n\npynvml.nvmlInit()\nhandle = pynvml.nvmlDeviceGetHandleByIndex(0)\ndevice_name = pynvml.nvmlDeviceGetName(handle)\n\nprint(f'Currently running on {device_name}')\n\nif device_name != b'Tesla T4':\n    raise Exception(\"\"\"\n    Unfortunately this instance does not have a T4 GPU.\n\n    Please make sure you've configured Colab to request a GPU instance type.\n\n    Sometimes Colab allocates a Tesla K80 instead of a T4. Resetting the instance.\n\n    If you get a K80 GPU, try Runtime -&gt; Reset all runtimes -&gt; Keep trying!\n  \"\"\")\nelse:\n    print('Woo! You got the right kind of GPU, a Tesla T4!')\n</code></pre> <pre><code>Currently running on b'Tesla T4'\nWoo! You got the right kind of GPU, a Tesla T4!\n</code></pre> <pre><code># intall miniconda\n!wget -c https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n!chmod +x Miniconda3-4.5.4-Linux-x86_64.sh\n!bash ./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local\n\n# install RAPIDS packages\n!conda install -q -y --prefix /usr/local -c conda-forge \\\n  -c rapidsai-nightly/label/cuda10.0 -c nvidia/label/cuda10.0 \\\n  cudf cuml\n\n# set environment vars\nimport sys, os, shutil\nsys.path.append('/usr/local/lib/python3.6/site-packages/')\nos.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\nos.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n\n# copy .so files to current working dir\nfor fn in ['libcudf.so', 'librmm.so']:\n    shutil.copy('/usr/local/lib/'+fn, os.getcwd())\n</code></pre> <pre><code>--2019-08-19 05:36:25--  https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\nResolving repo.continuum.io (repo.continuum.io)... 104.18.200.79, 104.18.201.79, 2606:4700::6812:c84f, ...\nConnecting to repo.continuum.io (repo.continuum.io)|104.18.200.79|:443... connected.\nHTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n\n    The file is already fully retrieved; nothing to do.\n\nPREFIX=/usr/local\ninstalling: python-3.6.5-hc3d631a_2 ...\nPython 3.6.5 :: Anaconda, Inc.\ninstalling: ca-certificates-2018.03.07-0 ...\ninstalling: conda-env-2.6.0-h36134e3_1 ...\ninstalling: libgcc-ng-7.2.0-hdf63c60_3 ...\ninstalling: libstdcxx-ng-7.2.0-hdf63c60_3 ...\ninstalling: libffi-3.2.1-hd88cf55_4 ...\ninstalling: ncurses-6.1-hf484d3e_0 ...\ninstalling: openssl-1.0.2o-h20670df_0 ...\ninstalling: tk-8.6.7-hc745277_3 ...\ninstalling: xz-5.2.4-h14c3975_4 ...\ninstalling: yaml-0.1.7-had09818_2 ...\ninstalling: zlib-1.2.11-ha838bed_2 ...\ninstalling: libedit-3.1.20170329-h6b74fdf_2 ...\ninstalling: readline-7.0-ha6073c6_4 ...\ninstalling: sqlite-3.23.1-he433501_0 ...\ninstalling: asn1crypto-0.24.0-py36_0 ...\ninstalling: certifi-2018.4.16-py36_0 ...\ninstalling: chardet-3.0.4-py36h0f667ec_1 ...\ninstalling: idna-2.6-py36h82fb2a8_1 ...\ninstalling: pycosat-0.6.3-py36h0a5515d_0 ...\ninstalling: pycparser-2.18-py36hf9f622e_1 ...\ninstalling: pysocks-1.6.8-py36_0 ...\ninstalling: ruamel_yaml-0.15.37-py36h14c3975_2 ...\ninstalling: six-1.11.0-py36h372c433_1 ...\ninstalling: cffi-1.11.5-py36h9745a5d_0 ...\ninstalling: setuptools-39.2.0-py36_0 ...\ninstalling: cryptography-2.2.2-py36h14c3975_0 ...\ninstalling: wheel-0.31.1-py36_0 ...\ninstalling: pip-10.0.1-py36_0 ...\ninstalling: pyopenssl-18.0.0-py36_0 ...\ninstalling: urllib3-1.22-py36hbe7ace6_0 ...\ninstalling: requests-2.18.4-py36he2e5f8d_1 ...\ninstalling: conda-4.5.4-py36_0 ...\nunlinking: ca-certificates-2019.6.16-hecc5488_0\nunlinking: certifi-2019.6.16-py36_1\nunlinking: conda-4.7.11-py36_0\nunlinking: cryptography-2.7-py36h72c5cf5_0\nunlinking: libgcc-ng-9.1.0-hdf63c60_0\nunlinking: libstdcxx-ng-9.1.0-hdf63c60_0\nunlinking: openssl-1.1.1c-h516909a_0\nunlinking: python-3.6.7-h381d211_1004\nunlinking: sqlite-3.28.0-h8b20d00_0\nunlinking: tk-8.6.9-hed695b0_1002\ninstallation finished.\nWARNING:\n    You currently have a PYTHONPATH environment variable set. This may cause\n    unexpected behavior when running the Python interpreter in Miniconda3.\n    For best results, please verify that your PYTHONPATH only points to\n    directories of packages that are compatible with the Python interpreter\n    in Miniconda3: /usr/local\nSolving environment: ...working... done\n\n## Package Plan ##\n\n  environment location: /usr/local\n\n  added / updated specs: \n    - cudf\n    - cuml\n\n\nThe following packages will be UPDATED:\n\n    ca-certificates: 2018.03.07-0         --&gt; 2019.6.16-hecc5488_0 conda-forge\n    certifi:         2018.4.16-py36_0     --&gt; 2019.6.16-py36_1     conda-forge\n    conda:           4.5.4-py36_0         --&gt; 4.7.11-py36_0        conda-forge\n    cryptography:    2.2.2-py36h14c3975_0 --&gt; 2.7-py36h72c5cf5_0   conda-forge\n    libgcc-ng:       7.2.0-hdf63c60_3     --&gt; 9.1.0-hdf63c60_0                \n    libstdcxx-ng:    7.2.0-hdf63c60_3     --&gt; 9.1.0-hdf63c60_0                \n    openssl:         1.0.2o-h20670df_0    --&gt; 1.1.1c-h516909a_0    conda-forge\n    python:          3.6.5-hc3d631a_2     --&gt; 3.6.7-h381d211_1004  conda-forge\n    sqlite:          3.23.1-he433501_0    --&gt; 3.28.0-h8b20d00_0    conda-forge\n    tk:              8.6.7-hc745277_3     --&gt; 8.6.9-hed695b0_1002  conda-forge\n\nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... done\n</code></pre> <p>You would need to do a <code>Runtime &gt; Restart and Run All</code> on your Google Colab notebook at this step.</p>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#imports","title":"Imports","text":"<pre><code># Critical imports for GPU cuDF\nimport nvstrings, nvcategory, cudf\n</code></pre> <pre><code># Other imports\nimport numpy as np\nimport pandas as pd\nimport time\nimport pandas_datareader.data as web\nfrom datetime import datetime\nfrom matplotlib import pyplot as plt\nfrom numba import cuda\n</code></pre>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#plotting-style","title":"Plotting Style","text":"<pre><code># Display settings: just to make it slightly prettier\n# There'll be a separate post on beautiful plots\n%matplotlib inline\nfigsize=(25, 6)\nplt.style.use('fivethirtyeight')\n</code></pre>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#necessity-of-fractional-differencing","title":"Necessity of Fractional Differencing","text":"<ul> <li>Before we get into why fractional differencing is critical, let us inspect a simple time series pulled from the Fed's database: S&amp;P 500.</li> </ul>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#pulling-processing-plotting-spx-time-series","title":"Pulling, Processing &amp; Plotting SPX Time Series","text":"<pre><code># Read SPX data 2010-2019 from FED database: https://fred.stlouisfed.org/categories/94\nasset_name = 'SPX'\nstart = datetime(2010, 1, 1)\nend = datetime(2019, 8, 1)\ndf_raw = web.DataReader('SP500', 'fred', start, end)\n\n# Basic clean up data: dropna (there are other more robust treatments)\ndf_raw = df_raw.dropna()\ndf_raw.columns = [asset_name]\n\ndf_raw.plot(figsize=figsize)\nplt.title('S&amp;P 500 Absolute Levels');\n</code></pre>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#integer-differencing-1-day-returns-spx-time-series","title":"Integer Differencing (1 Day Returns) SPX Time Series","text":"<ul> <li>Traditionally we typically might difference our time series by one day (daily returns) or more to reach some form of stationarity via tests like ADF, KPSS, PP and more.</li> <li>However, these forms of integer differencing causes more information than needed to be lost.</li> </ul> <pre><code># One day returns calculation through differencing by 1\ndifferencing_factor = 1\ndf_daily_returns = (df_raw - df_raw.shift(differencing_factor)) / df_raw.shift(differencing_factor) * 100\ndf_daily_returns.plot(figsize=figsize)\nplt.title(f'{asset_name} Daily Returns via Differencing 1 Day');\n</code></pre>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#integer-differencing-causes-unnecessary-memory-loss","title":"Integer Differencing Causes Unnecessary Memory Loss","text":"<ul> <li>Because of this reason, we propose fractional differencing to achieve stationarity while minimizing memory loss.</li> </ul>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#basic-fractional-differencing","title":"Basic Fractional Differencing","text":"<p>Fractional differencing allows us to achieve stationarity while maintaining the maximum amount of memory compared to integer differencing.</p> <p>This was originally introduced in 1981 in his paper \u201cFractional Differencing\u201d by J. R. M. Hosking1  and subsequent work by others concentrated on fast and efficient implementations for fractional differentiation for continuous stochastic processes. Recently, fractional differencing was introduced for financial time series through the fixed window fractional differencing instead of the expanding window method by Marcos Lopez de Prado2.</p>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#fractional-differencing-weight-function-and-plot","title":"Fractional Differencing Weight Function and Plot","text":"<ul> <li>Weights formula: \\(w_k = -w_{k-1} \\frac{d - k + 1}{k}\\)</li> <li>Weight converges to zero: \\(w_k \\rightarrow 0\\)</li> </ul> <pre><code>def get_weights(d, num_k):\nr\"\"\"Calculate weights ($w$) for each lag ($k$) through\n    $w_k = -w_{k-1} \\frac{d - k + 1}{k}$.\n\n    Args:\n        d (int): differencing value.\n        num_k (int): number of lags (typically length of timeseries) to calculate w.\n    \"\"\"\n    w_k = np.array([1])\n\n    for k in range(1, num_k):\n        w_k = np.append(w_k, -w_k[-1] * ((d - k + 1)) / k)\n\n    w_k = w_k.reshape(-1, 1) \n\n    return w_k\n\ndef plot_weights(range_d, num_k, num_d_interval):\nr\"\"\"Plot weights ($w$) for each lag ($k$) for varying differencing value ($d$).\n\n    Args:\n        range_d (list): range of differencing values to plot.\n        num_k (int): number of lags (typically length of timeseries) to plot.\n        num_d_interval (int): number of d interval.\n    \"\"\"\n    # Get differencing values\n    interval = np.linspace(range_d[0], range_d[1], num_d_interval)\n\n    # Dataframe of lags (rows) x number of differencing intervals (columns)\n    df_wk = pd.DataFrame(np.zeros((num_k, num_d_interval)))\n\n    # Get weights array per differencing value\n    for i, d in enumerate(interval):\n        df_wk[i] = get_weights(d, num_k)\n\n    # Rename columns for legend\n    df_wk.columns = [round(x, 1) for x in interval]\n\n    # Plot\n    df_wk.plot(figsize=figsize)\n    plt.title('Lag weights ($w_k$) for various differencing values (d)')\n    plt.legend(title='Differencing value (d)')\n    plt.ylabel('Weights (w)')\n    plt.xlabel('Lag (k)')\n    plt.show()\n\n    # Return weights\n    return df_wk\n\ndf_wk = plot_weights(range_d=[0, 1], num_k=7, num_d_interval=6)\n</code></pre>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#fractional-differencing-weight-function-table","title":"Fractional Differencing Weight Function Table","text":"<pre><code># Dataframe for lag weights (w) for various differencing values (d)\ndf_wk\n</code></pre> 0.0 0.2 0.4 0.6 0.8 1.0 0 1.0 1.000000 1.000000 1.000000 1.000000 1.0 1 -0.0 -0.200000 -0.400000 -0.600000 -0.800000 -1.0 2 -0.0 -0.080000 -0.120000 -0.120000 -0.080000 0.0 3 -0.0 -0.048000 -0.064000 -0.056000 -0.032000 0.0 4 -0.0 -0.033600 -0.041600 -0.033600 -0.017600 0.0 5 -0.0 -0.025536 -0.029952 -0.022848 -0.011264 0.0 6 -0.0 -0.020429 -0.022963 -0.016755 -0.007885 0.0"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#fractional-differencing-value-calculation-toy-example","title":"Fractional Differencing Value Calculation Toy Example","text":"<ul> <li>Assume spot have values 100, 99, 98, 97, 96, 95, 94 from k=0 to k=6</li> </ul>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#for-d-0-no-differencing","title":"For \\(d = 0\\) (no differencing)","text":"<pre><code>differenced_value = 100 * 1 + 99 * 0 + 98 * 0 + 97 * 0 + 96 * 0 + 95 * 0 + 94 * 0\nprint(f'Differenced value: {differenced_value}')\ndf_wk.iloc[:, 0]\n</code></pre> <pre><code>Differenced value: 100\n\n\n\n\n\n0    1.0\n1   -0.0\n2   -0.0\n3   -0.0\n4   -0.0\n5   -0.0\n6   -0.0\nName: 0.0, dtype: float64\n</code></pre>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#for-d-04-fractional-differencing","title":"For \\(d = 0.4\\) (fractional differencing)","text":"<pre><code>differenced_value = 100 * 1 + 99 * -0.400000 + 98 * -0.120000 + 97 * -0.064000 + 96 * -0.041600 + 95 * -0.041600 + 94 * -0.041600\nprint(f'Differenced value: {differenced_value}')\ndf_wk.iloc[:, 1]\n</code></pre> <pre><code>Differenced value: 30.576000000000004\n\n\n\n\n\n0    1.000000\n1   -0.200000\n2   -0.080000\n3   -0.048000\n4   -0.033600\n5   -0.025536\n6   -0.020429\nName: 0.2, dtype: float64\n</code></pre>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#for-d-1-integer-differencing","title":"For \\(d = 1\\) (integer differencing)","text":"<pre><code>differenced_value = 100 * 1 + 99 * -1 + 98 * 0 + 97 * 0 + 96 * 0 + 95 * 0 + 94 * 0\nprint(f'Differenced value: {differenced_value}')\ndf_wk.iloc[:, -1]\n</code></pre> <pre><code>Differenced value: 1\n\n\n\n\n\n0    1.0\n1   -1.0\n2    0.0\n3    0.0\n4    0.0\n5    0.0\n6    0.0\nName: 1.0, dtype: float64\n</code></pre>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#cumulative-sum-of-weights-behavior","title":"Cumulative Sum of Weights Behavior","text":"<ul> <li>Higher differencing value would accelerate cumulative sum weight decay</li> <li>This causes:<ol> <li>Less information taken from further lags</li> <li>More information loss from further lags</li> </ol> </li> </ul> <pre><code>df_wk.cumsum().plot(figsize=figsize)\nplt.title('Cumulative Sum of Lag weights ($\\sum_0^k w_k$) for various differencing values (d)')\nplt.legend(title='Differencing value ($d$)')\nplt.ylabel('Cumulative Sum of Weights ($\\sum_0^k w_k$)')\nplt.xlabel('Lag (k)');\n</code></pre>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#fixed-window-fractional-differencing-cpu","title":"Fixed Window Fractional Differencing (CPU)","text":""},{"location":"machine_learning/gpu/gpu_fractional_differencing/#floored-weights-function","title":"Floored Weights Function","text":"<ul> <li>For computational efficiency, we want to stop the calculation of weights when the weights are too small (below a certain small predetermined small value).</li> <li>Image we had a time series with 1,000,000 data points, we do not want to compute the weights up till 1m lag for our latest data point!  <ul> <li>We can simply calculate the weights till they're too small which is what this function does.</li> </ul> </li> </ul> <pre><code>def get_weights_floored(d, num_k, floor=1e-3):\nr\"\"\"Calculate weights ($w$) for each lag ($k$) through\n    $w_k = -w_{k-1} \\frac{d - k + 1}{k}$ provided weight above a minimum value\n    (floor) for the weights to prevent computation of weights for the entire\n    time series.\n\n    Args:\n        d (int): differencing value.\n        num_k (int): number of lags (typically length of timeseries) to calculate w.\n        floor (float): minimum value for the weights for computational efficiency.\n    \"\"\"\n    w_k = np.array([1])\n    k = 1\n\n    while k &lt; num_k:\n        w_k_latest = -w_k[-1] * ((d - k + 1)) / k\n        if abs(w_k_latest) &lt;= floor:\n            break\n\n        w_k = np.append(w_k, w_k_latest)\n\n        k += 1\n\n    w_k = w_k.reshape(-1, 1) \n\n    return w_k\n\n# Show large time series being stopped\n# Notice how max lag is at 9?\nd = 0.5\nnum_k = 1000\nweights = get_weights_floored(d=d, num_k=num_k)\npd.DataFrame(weights).plot(legend=False, figsize=figsize)\n\n# Plot\nplt.title(f'Lag weights ($w_k$) for differencing value $d={d}$')\nplt.ylabel('Weights (w)')\nplt.xlabel('Lag (k)');\n</code></pre>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#fixed-window-fractional-differencing-function-cpu","title":"Fixed Window Fractional Differencing Function (CPU)","text":"<ul> <li>Here, we use the new floored weights function compared to our normal  weights function</li> <li>Calculate our differenced values through the dot product of our transposed weights matrix and our original time series within that window<ul> <li>Keep repeating by shifting the window by one time step until the end of the time series</li> </ul> </li> </ul> <pre><code>def frac_diff(df, d, floor=1e-3):\nr\"\"\"Fractionally difference time series via CPU.\n\n    Args:\n        df (pd.DataFrame): dataframe of raw time series values.\n        d (float): differencing value from 0 to 1 where &gt; 1 has no FD.\n        floor (float): minimum value of weights, ignoring anything smaller.\n    \"\"\"\n    # Get weights window\n    weights = get_weights_floored(d=d, num_k=len(df), floor=floor)\n    weights_window_size = len(weights)\n\n    # Reverse weights\n    weights = weights[::-1]\n\n    # Blank fractionally differenced series to be filled\n    df_fd = []\n\n    # Slide window of time series, to calculated fractionally differenced values\n    # per window\n    for idx in range(weights_window_size, df.shape[0]):\n        # Dot product of weights and original values\n        # to get fractionally differenced values\n        date_idx = df.index[idx]\n        df_fd.append(np.dot(weights.T, df.iloc[idx - weights_window_size:idx]).item())\n\n    # Return FD values and weights\n    df_fd = pd.DataFrame(df_fd)\n\n    return df_fd, weights\n\n# Start timer\nstart = time.time()\n\ndf_raw_fd, weights = frac_diff(df_raw, d=0.5, floor=5e-5)\n\n# End timer\nend = time.time()\n\nprint(f'Time {end-start} s')\n\n# Plot\ndf_raw_fd.plot(figsize=figsize);\n</code></pre> <pre><code>Time 0.5417799949645996 s\n</code></pre> <pre><code>import multiprocessing\nmultiprocessing.cpu_count()\n</code></pre> <pre><code>4\n</code></pre> <p>Existing CPU-based implementations are inefficient for running fractional differencing on many large-scale time-series. GPU-based implementations provide an avenue to adapt to this century\u2019s big data requirements.</p>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#fixed-window-fractional-differencing-function-gpu","title":"Fixed Window Fractional Differencing Function (GPU)","text":"<ul> <li>This gives a slight 1.5x to 2x speed-up on an NVIDIA T4 on Google Colab for this tiny dataset<ul> <li>For larger time series in line with real-world situations,  it can easily be up to 100x to 1000x dataset (shown below).</li> </ul> </li> </ul> <pre><code>def moving_dot_product_kernel(in_data, out, window_size, weights):\n    # Set the first window_size-1 rows in each chunk to np.nan due \n    # insufficient history\n    for i in range(cuda.threadIdx.x, window_size - 1, cuda.blockDim.x):\n        out[i] = np.nan\n\n    # Compute dot product of preceding window_size rows\n    for i in range(cuda.threadIdx.x + window_size - 1, in_data.size, cuda.blockDim.x):\n        rolling_dot_product = 0.0\n\n        k = 0\n        for j in range(i - window_size + 1, i + 1):\n            rolling_dot_product += in_data[j] * weights[k][0]\n            k += 1\n\n        out[i] = rolling_dot_product \n\ndef frac_diff_gpu(df, d, floor=1e-3):\nr\"\"\"Fractionally difference time series via GPU.\n\n    Args:\n        df (pd.DataFrame): dataframe of raw time series values.\n        d (float): differencing value from 0 to 1 where &gt; 1 has no FD.\n        floor (float): minimum value of weights, ignoring anything smaller.\n    \"\"\"\n\n    # Bring dataframe to GPU, reset index for GPU dot product kernel\n    gdf_raw = cudf.from_pandas(df).reset_index(drop=True)\n    gdf_raw.columns = ['in_data']\n\n    # Get weights window\n    weights = get_weights_floored(d=d, num_k=len(gdf_raw), floor=floor)\n    weights_window_size = len(weights)\n\n    # Reverse weights and as contiguous\n    weights = np.ascontiguousarray(weights[::-1])\n\n    # Bring weights to GPU\n    gdf_weights = cudf.DataFrame()\n    gdf_weights[gdf_raw.columns[0]] = weights.reshape(-1)\n\n    # Length of data\n    data_length = len(gdf_raw)\n\n    # T4: max of 518 threads per block.\n    # V100: max 1024 threads per block\n    threads_per_block = 518\n\n    # Chunk size split\n    # This has to be improved, but as a v0.1, it's sufficient to show speed-up\n    # Up to easily 100 million data points\n    trunk_size = data_length\n\n    # Get fractionally differenced time series through GPU function\n    gdf_raw_fd = gdf_raw.apply_chunks(moving_dot_product_kernel,\n                                 incols=['in_data'],\n                                 outcols=dict(out=np.float64),\n                                 kwargs=dict(window_size=weights_window_size, weights=weights),\n                                 chunks=list(range(0, data_length, trunk_size)) + [data_length],\n                                 tpb=threads_per_block)\n\n    # Bring to CPU for normal manipulation\n    df_raw_fd = gdf_raw_fd.to_pandas().dropna().iloc[:-1, 1]\n\n    return df_raw_fd, weights\n\n\n# Start timer\nstart = time.time()\n\ndf_raw_fd_from_gpu, weights = frac_diff_gpu(df_raw, d=0.5, floor=5e-5)\n\n# End timer\nend = time.time()\n\nprint(f'Time {end-start} s')\n\n# Plot\ndf_raw_fd_from_gpu.plot(figsize=figsize);\n</code></pre> <pre><code>Time 0.27262187004089355 s\n</code></pre>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#check-values","title":"Check Values","text":"<pre><code># Compare tail values\nprint('Tail values check')\nprint(df_raw_fd_from_gpu.tail().values)\nprint(df_raw_fd.tail().values.reshape(-1,))\n</code></pre> <pre><code>Tail values check\n[107.00340988 133.85520208 117.92878691 109.44132697  79.32562638]\n[107.00340988 133.85520208 117.92878691 109.44132697  79.32562638]\n</code></pre> <pre><code># Compare tail values\nprint('Head values check')\nprint(df_raw_fd_from_gpu.head().values)\nprint(df_raw_fd.head().values.reshape(-1,))\n</code></pre> <pre><code>Head values check\n[56.74989213 52.56766288 47.32421832 45.89772154 37.74401501]\n[56.74989213 52.56766288 47.32421832 45.89772154 37.74401501]\n</code></pre> <pre><code>num_rows_true = (df_raw_fd.values.astype(np.float32) == df_raw_fd_from_gpu.values.astype(np.float32)).astype(int).sum()\ntotal_rows = df_raw_fd.shape[0]\n\nprint(f'Number of rows equal: {num_rows_true}')\nprint(f'Total number of rows: {total_rows}')\n</code></pre> <pre><code>Number of rows equal: 2093\nTotal number of rows: 2093\n</code></pre>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#stationarity","title":"Stationarity","text":"<ul> <li>\"A stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time\", Robert Nau, Duke University</li> <li>Essentially what we are trying to do here with fractional differencing is to attempt to have a stationary time series without losing too much memory.</li> <li>There are many ways to check if a time series is stationary, but we will be using 1 test here (they're more, but it's not the purpose of this tutorial)<ul> <li>Augmented Dickey\u2013Fuller (ADF) test: check for unit root</li> </ul> </li> </ul>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#adf-test","title":"ADF Test","text":"<pre><code># Import adf/kpss\nfrom statsmodels.tsa.stattools import adfuller\nimport warnings\nwarnings.filterwarnings(\"ignore\")  # Ignore deprecations for cleaner output\n</code></pre>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#adf-raw-data-d0","title":"ADF: raw data (d=0)","text":"<pre><code># ADF on raw data\n\nprint('Stationarity Test for SPX Absolute Levels (d=0)')\nprint('-'*50)\n\nresult = adfuller(df_raw['SPX'], regression='c')\n\nprint(f't-stat \\n\\t{result[0]:.2f}')\nprint(f'p-value \\n\\t{result[1]:.2f}')\nprint(f'Critical Values')\nfor key, value in result[4].items():\n    print(f'\\t{key}: {value:.2f}')\n\n# test statistic &gt; test statistic (1%) AND p-value &gt; 0.01\n# Fail to reject the null hypothesis that there's unit root at the 1% significance level\n# ==&gt; Fail to reject H0 ==&gt; Unit root ==&gt; Non-stationary\n</code></pre> <pre><code>Stationarity Test for SPX Absolute Levels (d=0)\n--------------------------------------------------\nt-stat \n    -0.11\np-value \n    0.95\nCritical Values\n    1%: -3.43\n    5%: -2.86\n    10%: -2.57\n</code></pre>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#adf-daily-return-data-d1","title":"ADF: daily return data (d=1)","text":"<pre><code># ADF on 1 day returns (d=1)\n\nprint('Stationarity Test for SPX Daily Returns (d=1)')\nprint('-'*50)\nresult = adfuller(df_raw['SPX'].pct_change(1).dropna(), regression='c')\n\nprint(f't-stat \\n\\t{result[0]:.2f}')\nprint(f'p-value \\n\\t{result[1]:.2f}')\nprint(f'Critical Values')\nfor key, value in result[4].items():\n    print(f'\\t{key}: {value:.2f}')\n\n# test statistic &lt; test statistic (1%) AND p-value &lt; 0.01\n# Reject the null hypothesis that there's unit root at the 1% significance level\n# ==&gt; Reject H0 ==&gt; No unit root ==&gt; Stationary\n</code></pre> <pre><code>Stationarity Test for SPX Daily Returns (d=1)\n--------------------------------------------------\nt-stat \n    -11.12\np-value \n    0.00\nCritical Values\n    1%: -3.43\n    5%: -2.86\n    10%: -2.57\n</code></pre>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#adf-fractionally-differenced-data-d05","title":"ADF: fractionally differenced data (d=0.5)","text":"<pre><code># ADF on fractionally differenced values\n\nprint('Stationarity Test (ADF) for SPX Fractionally Differenced (d=0.5)')\nprint('-'*50)\n\n\ndf_raw_fd_from_gpu, weights = frac_diff_gpu(df_raw, d=0.5, floor=5e-5)\n\nresult = adfuller(df_raw_fd_from_gpu, regression='c')\n\nprint(f't-stat \\n\\t{result[0]:.2f}')\nprint(f'p-value \\n\\t{result[1]:.2f}')\nprint(f'Critical Values')\nfor key, value in result[4].items():\n    print(f'\\t{key}: {value:.2f}')\n\n# test statistic &lt; test statistic (1%) AND p-value &lt; 0.01\n# Reject the null hypothesis that there's unit root at the 1% significance level\n# ==&gt; Reject H0 ==&gt; No unit root ==&gt; Stationary\n</code></pre> <pre><code>Stationarity Test (ADF) for SPX Fractionally Differenced (d=0.5)\n--------------------------------------------------\nt-stat \n    -3.86\np-value \n    0.00\nCritical Values\n    1%: -3.43\n    5%: -2.86\n    10%: -2.57\n</code></pre>"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#large-scale-rapid-fractional-differencing","title":"Large-scale Rapid Fractional Differencing","text":"<ul> <li>Here we test on a 100k, 1m, 10m and 100m datapoint dataframe.</li> </ul> <pre><code># Create 100m data points\nlarge_time_series_length = int(1e6)\nprint(large_time_series_length)\n</code></pre> <pre><code>1000000\n</code></pre> <pre><code># Start timer\nstart = time.time()\n\ndf_raw = pd.DataFrame(np.arange(large_time_series_length) * np.random.rand(large_time_series_length))\n\ndf_raw_fd_from_gpu, weights = frac_diff_gpu(df_raw, d=0.5, floor=5e-5)\n\n# End timer\nend = time.time()\n\nprint(f'Time {end-start} s')\n\n# Check\nprint('FD Shape', df_raw_fd_from_gpu.shape[0])\nprint('Correct Shape', df_raw.shape[0] - weights.shape[0])\nprint('Original Shape', df_raw.shape[0])\n</code></pre> <pre><code>Time 0.4671785831451416 s\nFD Shape 999682\nCorrect Shape 999682\nOriginal Shape 1000000\n</code></pre> <pre><code># Start timer\nstart = time.time()\n\n# Create 100m data points\ndf_raw = pd.DataFrame(np.arange(large_time_series_length) * np.random.rand(large_time_series_length))\n\ndf_raw_fd, weights = frac_diff(df_raw, d=0.5, floor=5e-5)\n\n# End timer\nend = time.time()\n\nprint(f'Time {end-start} s')\n\n# Check\nprint('FD Shape', df_raw_fd_from_gpu.shape[0])\nprint('Correct Shape', df_raw.shape[0] - weights.shape[0])\nprint('Original Shape', df_raw.shape[0])\n</code></pre> <pre><code>Time 128.29733324050903 s\nFD Shape 999682\nCorrect Shape 999682\nOriginal Shape 1000000\n</code></pre>"},{"location":"machine_learning/gpu/rapids_cudf/","title":"RAPIDS cuDF","text":""},{"location":"machine_learning/gpu/rapids_cudf/#environment-setup","title":"Environment Setup","text":""},{"location":"machine_learning/gpu/rapids_cudf/#check-version","title":"Check Version","text":""},{"location":"machine_learning/gpu/rapids_cudf/#python-version","title":"Python Version","text":"<pre><code># Check Python Version\n!python --version\n</code></pre> <pre><code>Python 3.8.16\n</code></pre>"},{"location":"machine_learning/gpu/rapids_cudf/#ubuntu-version","title":"Ubuntu Version","text":"<pre><code># Check Ubuntu Version\n!lsb_release -a\n</code></pre> <pre><code>No LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 18.04.6 LTS\nRelease:    18.04\nCodename:   bionic\n</code></pre>"},{"location":"machine_learning/gpu/rapids_cudf/#check-cuda-version","title":"Check CUDA Version","text":"<pre><code># Check CUDA/cuDNN Version\n!nvcc -V &amp;&amp; which nvcc\n</code></pre> <pre><code>nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2021 NVIDIA Corporation\nBuilt on Sun_Feb_14_21:12:58_PST_2021\nCuda compilation tools, release 11.2, V11.2.152\nBuild cuda_11.2.r11.2/compiler.29618528_0\n/usr/local/cuda/bin/nvcc\n</code></pre>"},{"location":"machine_learning/gpu/rapids_cudf/#check-gpu-version","title":"Check GPU Version","text":"<pre><code># Check GPU\n!nvidia-smi\n</code></pre> <pre><code>Wed Jan  4 19:14:22 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   48C    P0    29W /  70W |      0MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>"},{"location":"machine_learning/gpu/rapids_cudf/#setup","title":"Setup:","text":"<p>This set up script:</p> <ol> <li>Checks to make sure that the GPU is RAPIDS compatible</li> <li>Installs the current stable version of RAPIDSAI's core libraries using pip, which are:</li> <li>cuDF</li> <li>cuML</li> <li>cuGraph</li> <li>xgboost</li> </ol> <p>This will complete in about 3-4 minutes</p> <p>Please use the RAPIDS Conda Colab Template notebook if you need to install any of RAPIDS Extended libraries, such as: - cuSpatial - cuSignal - cuxFilter - cuCIM</p> <p>OR - nightly versions of any library </p> <pre><code># This get the RAPIDS-Colab install files and test check your GPU.  Run this and the next cell only.\n# Please read the output of this cell.  If your Colab Instance is not RAPIDS compatible, it will warn you and give you remediation steps.\n!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n!python rapidsai-csp-utils/colab/pip-install.py\n</code></pre> <pre><code>Cloning into 'rapidsai-csp-utils'...\nremote: Enumerating objects: 328, done.\u001b[K\nremote: Counting objects: 100% (157/157), done.\u001b[K\nremote: Compressing objects: 100% (102/102), done.\u001b[K\nremote: Total 328 (delta 92), reused 98 (delta 55), pack-reused 171\u001b[K\nReceiving objects: 100% (328/328), 94.64 KiB | 18.93 MiB/s, done.\nResolving deltas: 100% (154/154), done.\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting pynvml\n  Downloading pynvml-11.4.1-py3-none-any.whl (46 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 47.0/47.0 KB 6.1 MB/s eta 0:00:00\nInstalling collected packages: pynvml\nSuccessfully installed pynvml-11.4.1\n***********************************************************************\nWoo! Your instance has the right kind of GPU, a Tesla T4!\nWe will now install RAPIDS via pip!  Please stand by, should be quick...\n***********************************************************************\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://pypi.ngc.nvidia.com\nCollecting cudf-cu11\n  Downloading https://developer.download.nvidia.com/compute/redist/cudf-cu11/cudf_cu11-22.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442.8 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 442.8/442.8 MB 3.5 MB/s eta 0:00:00\nCollecting cuml-cu11\n  Downloading https://developer.download.nvidia.com/compute/redist/cuml-cu11/cuml_cu11-22.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1359.8 MB)\ntcmalloc: large alloc 1359798272 bytes == 0x3116000 @  0x7f53812b21e7 0x4d30a0 0x4d312c 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4997a2\ntcmalloc: large alloc 1699749888 bytes == 0x541e4000 @  0x7f53812b3615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941\ntcmalloc: large alloc 1359798272 bytes == 0x3116000 @  0x7f53812b21e7 0x4d30a0 0x5dede2 0x6758aa 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4fe318 0x5da092 0x62042c 0x5d8d8c 0x561f80 0x4fd2db 0x4997c7 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.4/1.4 GB 1.3 MB/s eta 0:00:00\nCollecting cugraph-cu11\n  Downloading https://developer.download.nvidia.com/compute/redist/cugraph-cu11/cugraph_cu11-22.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1028.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.0/1.0 GB 1.9 MB/s eta 0:00:00\nRequirement already satisfied: numba&gt;=0.56.2 in /usr/local/lib/python3.8/dist-packages (from cudf-cu11) (0.56.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from cudf-cu11) (1.21.6)\nCollecting ptxcompiler-cu11\n  Downloading https://developer.download.nvidia.com/compute/redist/ptxcompiler-cu11/ptxcompiler_cu11-0.7.0.post1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8.8/8.8 MB 99.1 MB/s eta 0:00:00\nCollecting cuda-python&lt;12.0,&gt;=11.7.1\n  Downloading cuda_python-11.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 16.2/16.2 MB 77.6 MB/s eta 0:00:00\nRequirement already satisfied: pyarrow==9.0.0 in /usr/local/lib/python3.8/dist-packages (from cudf-cu11) (9.0.0)\nRequirement already satisfied: pandas&lt;1.6.0dev0,&gt;=1.0 in /usr/local/lib/python3.8/dist-packages (from cudf-cu11) (1.3.5)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from cudf-cu11) (4.4.0)\nRequirement already satisfied: cupy-cuda11x in /usr/local/lib/python3.8/dist-packages (from cudf-cu11) (11.0.0)\nRequirement already satisfied: cachetools in /usr/local/lib/python3.8/dist-packages (from cudf-cu11) (5.2.0)\nRequirement already satisfied: fsspec&gt;=0.6.0 in /usr/local/lib/python3.8/dist-packages (from cudf-cu11) (2022.11.0)\nCollecting protobuf&lt;3.21.0a0,&gt;=3.20.1\n  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.0/1.0 MB 46.4 MB/s eta 0:00:00\nCollecting rmm-cu11\n  Downloading https://developer.download.nvidia.com/compute/redist/rmm-cu11/rmm_cu11-22.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.8/1.8 MB 61.1 MB/s eta 0:00:00\nCollecting cubinlinker-cu11\n  Downloading https://developer.download.nvidia.com/compute/redist/cubinlinker-cu11/cubinlinker_cu11-0.3.0.post1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8.8/8.8 MB 99.9 MB/s eta 0:00:00\nRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from cudf-cu11) (21.3)\nCollecting nvtx&gt;=0.2.1\n  Downloading nvtx-0.2.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 453.6/453.6 KB 28.4 MB/s eta 0:00:00\nRequirement already satisfied: seaborn in /usr/local/lib/python3.8/dist-packages (from cuml-cu11) (0.11.2)\nCollecting raft-dask-cu11\n  Downloading https://developer.download.nvidia.com/compute/redist/raft-dask-cu11/raft_dask_cu11-22.12.0.post1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210.5 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 210.5/210.5 MB 6.7 MB/s eta 0:00:00\nRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from cuml-cu11) (1.7.3)\nCollecting treelite==3.0.1\n  Downloading treelite-3.0.1-py3-none-manylinux2014_x86_64.whl (864 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 864.6/864.6 KB 38.0 MB/s eta 0:00:00\nCollecting treelite-runtime==3.0.1\n  Downloading treelite_runtime-3.0.1-py3-none-manylinux2014_x86_64.whl (191 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 191.9/191.9 KB 25.3 MB/s eta 0:00:00\nCollecting dask-cudf-cu11\n  Downloading https://developer.download.nvidia.com/compute/redist/dask-cudf-cu11/dask_cudf_cu11-22.12.0.post1-py3-none-any.whl (76 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 76.6/76.6 KB 12.2 MB/s eta 0:00:00\nCollecting pylibraft-cu11\n  Downloading https://developer.download.nvidia.com/compute/redist/pylibraft-cu11/pylibraft_cu11-22.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (580.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 580.3/580.3 MB 3.2 MB/s eta 0:00:00\nCollecting pylibcugraph-cu11\n  Downloading https://developer.download.nvidia.com/compute/redist/pylibcugraph-cu11/pylibcugraph_cu11-22.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1627.2 MB)\ntcmalloc: large alloc 1627185152 bytes == 0x541e8000 @  0x7f53812b21e7 0x4d30a0 0x4d312c 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4997a2\ntcmalloc: large alloc 2033983488 bytes == 0xb51b6000 @  0x7f53812b3615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.6/1.6 GB 1.1 MB/s eta 0:00:00\nCollecting dask-cuda\n  Downloading dask_cuda-22.12.0-py3-none-any.whl (121 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.1/121.1 KB 17.5 MB/s eta 0:00:00\nRequirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from cuda-python&lt;12.0,&gt;=11.7.1-&gt;cudf-cu11) (0.29.32)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba&gt;=0.56.2-&gt;cudf-cu11) (5.2.0)\nRequirement already satisfied: llvmlite&lt;0.40,&gt;=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba&gt;=0.56.2-&gt;cudf-cu11) (0.39.1)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba&gt;=0.56.2-&gt;cudf-cu11) (57.4.0)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas&lt;1.6.0dev0,&gt;=1.0-&gt;cudf-cu11) (2.8.2)\nRequirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas&lt;1.6.0dev0,&gt;=1.0-&gt;cudf-cu11) (2022.7)\nRequirement already satisfied: fastrlock&gt;=0.5 in /usr/local/lib/python3.8/dist-packages (from cupy-cuda11x-&gt;cudf-cu11) (0.8.1)\nRequirement already satisfied: zict&gt;=0.1.3 in /usr/local/lib/python3.8/dist-packages (from dask-cuda-&gt;cugraph-cu11) (2.2.0)\nCollecting distributed==2022.11.1\n  Downloading distributed-2022.11.1-py3-none-any.whl (923 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 923.4/923.4 KB 50.7 MB/s eta 0:00:00\nCollecting dask==2022.11.1\n  Downloading dask-2022.11.1-py3-none-any.whl (1.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.1/1.1 MB 52.6 MB/s eta 0:00:00\nRequirement already satisfied: pynvml&gt;=11.0.0 in /usr/local/lib/python3.8/dist-packages (from dask-cuda-&gt;cugraph-cu11) (11.4.1)\nRequirement already satisfied: click&gt;=7.0 in /usr/local/lib/python3.8/dist-packages (from dask==2022.11.1-&gt;dask-cuda-&gt;cugraph-cu11) (7.1.2)\nRequirement already satisfied: toolz&gt;=0.8.2 in /usr/local/lib/python3.8/dist-packages (from dask==2022.11.1-&gt;dask-cuda-&gt;cugraph-cu11) (0.12.0)\nRequirement already satisfied: partd&gt;=0.3.10 in /usr/local/lib/python3.8/dist-packages (from dask==2022.11.1-&gt;dask-cuda-&gt;cugraph-cu11) (1.3.0)\nRequirement already satisfied: cloudpickle&gt;=1.1.1 in /usr/local/lib/python3.8/dist-packages (from dask==2022.11.1-&gt;dask-cuda-&gt;cugraph-cu11) (1.5.0)\nRequirement already satisfied: pyyaml&gt;=5.3.1 in /usr/local/lib/python3.8/dist-packages (from dask==2022.11.1-&gt;dask-cuda-&gt;cugraph-cu11) (6.0)\nRequirement already satisfied: tornado&lt;6.2,&gt;=6.0.3 in /usr/local/lib/python3.8/dist-packages (from distributed==2022.11.1-&gt;dask-cuda-&gt;cugraph-cu11) (6.0.4)\nRequirement already satisfied: locket&gt;=1.0.0 in /usr/local/lib/python3.8/dist-packages (from distributed==2022.11.1-&gt;dask-cuda-&gt;cugraph-cu11) (1.0.0)\nRequirement already satisfied: tblib&gt;=1.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed==2022.11.1-&gt;dask-cuda-&gt;cugraph-cu11) (1.7.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from distributed==2022.11.1-&gt;dask-cuda-&gt;cugraph-cu11) (2.11.3)\nRequirement already satisfied: psutil&gt;=5.0 in /usr/local/lib/python3.8/dist-packages (from distributed==2022.11.1-&gt;dask-cuda-&gt;cugraph-cu11) (5.4.8)\nRequirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from distributed==2022.11.1-&gt;dask-cuda-&gt;cugraph-cu11) (1.24.3)\nRequirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.8/dist-packages (from distributed==2022.11.1-&gt;dask-cuda-&gt;cugraph-cu11) (2.4.0)\nRequirement already satisfied: msgpack&gt;=0.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed==2022.11.1-&gt;dask-cuda-&gt;cugraph-cu11) (1.0.4)\nRequirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging-&gt;cudf-cu11) (3.0.9)\nRequirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.8/dist-packages (from raft-dask-cu11-&gt;cuml-cu11) (1.2.0)\nCollecting ucx-py-cu11\n  Downloading https://developer.download.nvidia.com/compute/redist/ucx-py-cu11/ucx_py_cu11-0.29.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8.3/8.3 MB 72.8 MB/s eta 0:00:00\nRequirement already satisfied: matplotlib&gt;=2.2 in /usr/local/lib/python3.8/dist-packages (from seaborn-&gt;cuml-cu11) (3.2.2)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib&gt;=2.2-&gt;seaborn-&gt;cuml-cu11) (0.11.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib&gt;=2.2-&gt;seaborn-&gt;cuml-cu11) (1.4.4)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&lt;1.6.0dev0,&gt;=1.0-&gt;cudf-cu11) (1.15.0)\nRequirement already satisfied: heapdict in /usr/local/lib/python3.8/dist-packages (from zict&gt;=0.1.3-&gt;dask-cuda-&gt;cugraph-cu11) (1.0.1)\nRequirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata-&gt;numba&gt;=0.56.2-&gt;cudf-cu11) (3.11.0)\nRequirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2-&gt;distributed==2022.11.1-&gt;dask-cuda-&gt;cugraph-cu11) (2.0.1)\nInstalling collected packages: ptxcompiler-cu11, nvtx, cubinlinker-cu11, ucx-py-cu11, protobuf, cuda-python, treelite-runtime, treelite, dask, rmm-cu11, distributed, pylibraft-cu11, dask-cuda, cudf-cu11, raft-dask-cu11, pylibcugraph-cu11, dask-cudf-cu11, cuml-cu11, cugraph-cu11\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.19.6\n    Uninstalling protobuf-3.19.6:\n      Successfully uninstalled protobuf-3.19.6\n  Attempting uninstall: dask\n    Found existing installation: dask 2022.2.1\n    Uninstalling dask-2022.2.1:\n      Successfully uninstalled dask-2022.2.1\n  Attempting uninstall: distributed\n    Found existing installation: distributed 2022.2.1\n    Uninstalling distributed-2022.2.1:\n      Successfully uninstalled distributed-2022.2.1\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.9.2 requires protobuf&lt;3.20,&gt;=3.9.2, but you have protobuf 3.20.3 which is incompatible.\ntensorboard 2.9.1 requires protobuf&lt;3.20,&gt;=3.9.2, but you have protobuf 3.20.3 which is incompatible.\nSuccessfully installed cubinlinker-cu11-0.3.0.post1 cuda-python-11.8.1 cudf-cu11-22.12.0 cugraph-cu11-22.12.0 cuml-cu11-22.12.0 dask-2022.11.1 dask-cuda-22.12.0 dask-cudf-cu11-22.12.0.post1 distributed-2022.11.1 nvtx-0.2.5 protobuf-3.20.3 ptxcompiler-cu11-0.7.0.post1 pylibcugraph-cu11-22.12.0 pylibraft-cu11-22.12.0 raft-dask-cu11-22.12.0.post1 rmm-cu11-22.12.0 treelite-3.0.1 treelite-runtime-3.0.1 ucx-py-cu11-0.29.0\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting cupy-cuda11x\n  Downloading cupy_cuda11x-11.4.0-cp38-cp38-manylinux1_x86_64.whl (93.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 93.7/93.7 MB 10.7 MB/s eta 0:00:00\nRequirement already satisfied: numpy&lt;1.26,&gt;=1.20 in /usr/local/lib/python3.8/dist-packages (from cupy-cuda11x) (1.21.6)\nRequirement already satisfied: fastrlock&gt;=0.5 in /usr/local/lib/python3.8/dist-packages (from cupy-cuda11x) (0.8.1)\nInstalling collected packages: cupy-cuda11x\nSuccessfully installed cupy-cuda11x-11.4.0\n\n          ***********************************************************************\n          With the new pip install complete, please do not run any further installation \n          commands from the conda based installation methods!!!\n\n          In your personal files, you can delete these cells.\n\n          RAPIDSAI owned templates/notebooks should already be updated with no action needed.\n          ***********************************************************************\n</code></pre>"},{"location":"machine_learning/gpu/rapids_cudf/#critical-imports","title":"Critical Imports","text":"<pre><code># Critical imports\nimport cudf\nimport cuml\nimport os\nimport numpy as np\nimport pandas as pd\n</code></pre>"},{"location":"machine_learning/gpu/rapids_cudf/#creating","title":"Creating","text":""},{"location":"machine_learning/gpu/rapids_cudf/#create-a-series-of-integers","title":"Create a Series of integers","text":"<pre><code>gdf = cudf.Series([1, 2, 3, 4, 5, 6])\nprint(gdf)\nprint(type(gdf))\n</code></pre> <pre><code>0    1\n1    2\n2    3\n3    4\n4    5\n5    6\ndtype: int64\n&lt;class 'cudf.core.series.Series'&gt;\n</code></pre>"},{"location":"machine_learning/gpu/rapids_cudf/#create-a-series-of-floats","title":"Create a Series of floats","text":"<pre><code>gdf = cudf.Series([1., 2., 3., 4., 5., 6.])\nprint(gdf)\n</code></pre> <pre><code>0    1.0\n1    2.0\n2    3.0\n3    4.0\n4    5.0\n5    6.0\ndtype: float64\n</code></pre>"},{"location":"machine_learning/gpu/rapids_cudf/#create-a-series-of-strings","title":"Create a  Series of strings","text":"<pre><code>gdf = cudf.Series(['a', 'b', 'c'])\nprint(gdf)\n</code></pre> <pre><code>0    a\n1    b\n2    c\ndtype: object\n</code></pre>"},{"location":"machine_learning/gpu/rapids_cudf/#create-3-column-dataframe","title":"Create 3 column DataFrame","text":"<ul> <li>Consisting of dates, integers and floats</li> </ul> <pre><code># Import\nimport datetime as dt\n\n# Using a dictionary of key-value pairs\n# Each key in the dictionary represents a category\n# The key is the category's name\n# The value is a list of the values in that category\ngdf = cudf.DataFrame({\n    # Create 10 busindates ess from 1st January 2019 via pandas\n    'dates': pd.date_range('1/1/2019', periods=10, freq='B'),\n    # Integers\n    'integers': [i for i in range(10)],\n    # Floats\n    'floats': [float(i) for i in range(10)]\n})\n\n# Print dataframe\nprint(gdf)\n</code></pre> <pre><code>       dates  integers  floats\n0 2019-01-01         0     0.0\n1 2019-01-02         1     1.0\n2 2019-01-03         2     2.0\n3 2019-01-04         3     3.0\n4 2019-01-07         4     4.0\n5 2019-01-08         5     5.0\n6 2019-01-09         6     6.0\n7 2019-01-10         7     7.0\n8 2019-01-11         8     8.0\n9 2019-01-14         9     9.0\n</code></pre>"},{"location":"machine_learning/gpu/rapids_cudf/#create-2-column-dataframe","title":"Create 2 column Dataframe","text":"<ul> <li>Consisting of integers and string category</li> </ul> <pre><code># Using a dictionary\n# Each key in the dictionary represents a category\n# The key is the category's name\n# The value is a list of the values in that category\ngdf = cudf.DataFrame({\n    'integers': [1 ,2, 3, 4],\n    'string': ['a', 'b', 'c', 'd']\n})\n\nprint(gdf)\n</code></pre> <pre><code>   integers string\n0         1      a\n1         2      b\n2         3      c\n3         4      d\n</code></pre>"},{"location":"machine_learning/gpu/rapids_cudf/#create-a-2-column-dataframe-with-pandas-bridge","title":"Create a 2 Column  Dataframe with Pandas Bridge","text":"<ul> <li>Consisting of integers and string category</li> <li>For all string columns, you must convert them to type <code>category</code> for filtering functions to work intuitively (for now)</li> </ul> <pre><code># Create pandas dataframe\npandas_df = pd.DataFrame({\n    'integers': [1, 2, 3, 4], \n    'strings': ['a', 'b', 'c', 'd']\n})\n\n# Convert string column to category format\npandas_df['strings'] = pandas_df['strings'].astype('category')\n\n# Bridge from pandas to cudf\ngdf = cudf.DataFrame.from_pandas(pandas_df)\n\n# Print dataframe\nprint(gdf)\n</code></pre> <pre><code>   integers strings\n0         1       a\n1         2       b\n2         3       c\n3         4       d\n</code></pre>"},{"location":"machine_learning/gpu/rapids_cudf/#viewing","title":"Viewing","text":""},{"location":"machine_learning/gpu/rapids_cudf/#printing-column-names","title":"Printing Column Names","text":"<pre><code>gdf.columns\n</code></pre> <pre><code>Index(['integers', 'strings'], dtype='object')\n</code></pre>"},{"location":"machine_learning/gpu/rapids_cudf/#viewing-top-of-dataframe","title":"Viewing Top of DataFrame","text":"<pre><code>num_of_rows_to_view = 2 \nprint(gdf.head(num_of_rows_to_view))\n</code></pre> <pre><code>   integers strings\n0         1       a\n1         2       b\n</code></pre>"},{"location":"machine_learning/gpu/rapids_cudf/#viewing-bottom-of-dataframe","title":"Viewing Bottom of DataFrame","text":"<pre><code>num_of_rows_to_view = 3 \nprint(gdf.tail(num_of_rows_to_view))\n</code></pre> <pre><code>   integers strings\n1         2       b\n2         3       c\n3         4       d\n</code></pre>"},{"location":"machine_learning/gpu/rapids_cudf/#filtering","title":"Filtering","text":""},{"location":"machine_learning/gpu/rapids_cudf/#method-1-query","title":"Method 1: Query","text":""},{"location":"machine_learning/gpu/rapids_cudf/#filtering-integersfloats-by-column-values","title":"Filtering Integers/Floats by Column Values","text":"<ul> <li>This only works for floats and integers, not for strings</li> </ul> <pre><code># DO NOT RUN\n# TOFIX: `cffi` package version mismatch error\nprint(gdf.query('integers == 1'))\n</code></pre> <pre><code>   integers strings\n0         1       a\n</code></pre>"},{"location":"machine_learning/gpu/rapids_cudf/#filtering-strings-by-column-values","title":"Filtering Strings by Column Values","text":"<ul> <li>This only works for floats and integers, not for strings so this will return an error!</li> </ul> <pre><code>print(gdf.query('strings == a'))\n</code></pre> <pre><code>---------------------------------------------------------------------------\n\nKeyError                                  Traceback (most recent call last)\n\n/usr/local/lib/python3.8/dist-packages/cudf/core/dataframe.py in extract_col(df, col)\n   7558     try:\n-&gt; 7559         return df._data[col]\n   7560     except KeyError:\n\n\n/usr/local/lib/python3.8/dist-packages/cudf/core/column_accessor.py in __getitem__(self, key)\n    154     def __getitem__(self, key: Any) -&gt; ColumnBase:\n--&gt; 155         return self._data[key]\n    156\n\n\nKeyError: 'a'\n\n\nDuring handling of the above exception, another exception occurred:\n\n\nKeyError                                  Traceback (most recent call last)\n\n&lt;ipython-input-17-5cfd0345d51c&gt; in &lt;module&gt;\n----&gt; 1 print(gdf.query('strings == a'))\n\n\n/usr/local/lib/python3.8/dist-packages/cudf/core/dataframe.py in query(self, expr, local_dict)\n   4172             }\n   4173             # Run query\n-&gt; 4174             boolmask = queryutils.query_execute(self, expr, callenv)\n   4175             return self._apply_boolean_mask(boolmask)\n   4176\n\n\n/usr/local/lib/python3.8/dist-packages/cudf/utils/queryutils.py in query_execute(df, expr, callenv)\n    212 \n    213     # prepare col args\n--&gt; 214     colarrays = [cudf.core.dataframe.extract_col(df, col) for col in columns]\n    215 \n    216     # wait to check the types until we know which cols are used\n\n\n/usr/local/lib/python3.8/dist-packages/cudf/utils/queryutils.py in &lt;listcomp&gt;(.0)\n    212 \n    213     # prepare col args\n--&gt; 214     colarrays = [cudf.core.dataframe.extract_col(df, col) for col in columns]\n    215 \n    216     # wait to check the types until we know which cols are used\n\n\n/usr/local/lib/python3.8/dist-packages/cudf/core/dataframe.py in extract_col(df, col)\n   7565         ):\n   7566             return df.index._data.columns[0]\n-&gt; 7567         return df.index._data[col]\n   7568 \n   7569\n\n\n/usr/local/lib/python3.8/dist-packages/cudf/core/column_accessor.py in __getitem__(self, key)\n    153 \n    154     def __getitem__(self, key: Any) -&gt; ColumnBase:\n--&gt; 155         return self._data[key]\n    156 \n    157     def __setitem__(self, key: Any, value: Any):\n\n\nKeyError: 'a'\n</code></pre>"},{"location":"machine_learning/gpu/rapids_cudf/#method-2-simple-columns","title":"Method 2:  Simple Columns","text":""},{"location":"machine_learning/gpu/rapids_cudf/#filtering-strings-by-column-values_1","title":"Filtering Strings by Column Values","text":"<pre><code># Filtering based on the string column\nprint(gdf[gdf.strings == 'b'])\n</code></pre> <pre><code>   integers strings\n1         2       b\n</code></pre>"},{"location":"machine_learning/gpu/rapids_cudf/#filtering-integersfloats-by-column-values_1","title":"Filtering Integers/Floats by Column Values","text":"<pre><code># Filtering based on the string column\nprint(gdf[gdf.integers == 2])\n</code></pre> <pre><code>   integers strings\n1         2       b\n</code></pre>"},{"location":"machine_learning/gpu/rapids_cudf/#method-2-simple-rows","title":"Method 2:  Simple Rows","text":""},{"location":"machine_learning/gpu/rapids_cudf/#filtering-by-row-numbers","title":"Filtering by Row Numbers","text":"<pre><code># Filter rows 0 to 2 (not inclusive of the third row with the index 2)\nprint(gdf[0:2])\n</code></pre> <pre><code>   integers strings\n0         1       a\n1         2       b\n</code></pre>"},{"location":"machine_learning/gpu/rapids_cudf/#method-3-locrows-columns","title":"Method 3:  loc[rows, columns]","text":"<pre><code># The syntax is as follows loc[rows, columns] allowing you to choose rows and columns accordingly\n# The example allows us to filter the first 3 rows (inclusive) of the column integers\nprint(gdf.loc[0:2, ['integers']])\n</code></pre> <pre><code>   integers\n0         1\n1         2\n2         3\n</code></pre> <pre><code>\n</code></pre>"},{"location":"news/ammi_facebook_google_recap_2018_11_21/","title":"AMMI (AIMS) Recap Supported by Facebook and Google","text":""},{"location":"news/ammi_facebook_google_recap_2018_11_21/#to-the-future-of-ai-in-and-beyond-africa","title":"To the future of AI in and beyond Africa","text":"<p>Helped to teach these students from The African Masters of Machine Intelligence (AMMI) by The African Institute for Mathematical Sciences (AIMS) with Alfredo Canziani.</p> <p>A unique pedagogy is required for deep learning beginning with intuition, culminating in just enough math, and ending with programming. This approached has worked when I delivered my deep learning course to over 3000 students over 120 countries. And I continue to champion this pedagogy. I\u2019m very pleased that I share the same passion with Alfedo when he delivered his interactive lectures.</p> <p>Importantly, I see great potential when I personally interacted with students here. I see future deep learning researchers and applied machine intelligence experts who have the potential to make groundbreaking changes in the fields of healthcare, agriculture and education throughout Africa. With the right education, mentorship and opportunities, they would be pioneers. Simply put, deep learning talent is underexplored in this continent.</p> <p>I am thankful to Alfredo Canziani who has been my best pal and inspirational guy while I was there, Moustapha Cisse for creating these opportunities for young African students to thrive, #Facebook and #Google for supporting this program, and Pedro Antonio Mart\u00ednez Mediano and Marc Deisenroff. Coincidentally, I would also like to thank Yoshua Bengio for holding ICLR in Africa in 2020 for the first time that he announced recently. Finally, really appreciate how #PyTorch has made programming neural networks more approachable and it was made possible with the amazing PyTorch community sparked by Soumith Chintala.</p> <p>Speaking about community, I got to contribute to the deep learning community with the help from supportive groups and individuals from the global deep learning community. Likewise, we should continually help these young Africans like any other.</p> <p>To the future of AI in and beyond Africa. Looking forward to help Moustapha in any way I can to nurture AI talent \ud83d\ude42</p> <p>I'm being a fanboy here, but Yann Lecun liked my post on Facebook! </p> <p>Cheers, Ritchie</p>"},{"location":"news/dbs_gpu_rapids_nvidia_ensemble_frac_diff/","title":"GPU Fractional Differencing with NVIDIA RAPIDS","text":""},{"location":"news/dbs_gpu_rapids_nvidia_ensemble_frac_diff/#introduction-to-deep-learning","title":"Introduction to Deep Learning","text":"<p>I created and led a workshop on accelerating fractional differencing with NVIDIA's RAPIDS in DBS.</p> <p>This is our article on medium with NVIDIA on our work.</p> <p>You can check out our presentation and open-source code on Github.</p> <p></p> <p>Thanks to our industry partners DBS, NVIDIA and ensemblecap.ai</p> <p>Cheers, Ritchie</p>"},{"location":"news/deep_learning_wizard_1y_2018_06_01/","title":"Featured on PyTorch Website","text":""},{"location":"news/deep_learning_wizard_1y_2018_06_01/#pytorch-a-year-later","title":"PyTorch a Year Later","text":"<p>We are featured on PyTorch website's post </p> <p>I used PyTorch from day 1 and I fell in love with it. One year later, PyTorch's community has grown tremendously. I thank Facebook AI Research (FAIR) and everyone who contributed to building one of the best deep learning framework. In particular Soumith Chintala from FAIR where he is continually fixing bugs and making PyTorch better everyday.</p> <p>A year later, Deep Learning Wizard has taught more than 2000 students across 100+ countries, and we continue to strive to release new and updated content to enable anyone to build customized deep learning systems to solve many problems. And I'm trying to push for PyTorch workshops at NVIDIA Deep Learning Institute in Singapore and across the world, so stay tuned!</p> <p>A big shoutout for Alfredo Canziani who's currently a postdoc at NYU co-teaching with Yann LeCun on deep learning classes and he's releasing another technical course on PyTorch that is going to be awesome.</p> <p>To more great years ahead for PyTorch </p> <p>Cheers, Ritchie Ng</p>"},{"location":"news/deep_learning_wizard_nvidia_inception_2018_05_01/","title":"We Are an NVIDIA Inception Partner","text":""},{"location":"news/deep_learning_wizard_nvidia_inception_2018_05_01/#we-did-it","title":"We did it!","text":"<p>After almost a year, we are an NVIDIA Inception Partner now! </p> <p>\"NVIDIA Inception Partner</p> <p></p> <p>Deep Learning Wizard is proudly an NVIDIA Inception Partner, empowering people across the world to leverage on deep learning via open-source programming languages and frameworks to solve problems. </p> <p>Cheers, Ritchie Ng</p>"},{"location":"news/defence_and_science_technology_agency_dsta_nvidia_talk_2016_06/","title":"Deep Learning Demystified, Defence and Science Technology Agency (DSTA)","text":""},{"location":"news/defence_and_science_technology_agency_dsta_nvidia_talk_2016_06/#introduction-to-deep-learning","title":"Introduction to Deep Learning","text":"<p>I gave an introductory talk on deep learning to more than 1000 students at BrainHack held by DSTA in June 2019.</p> <p>I'm really excited for Singapore's progress in leveraging on deep learning for the defense sector. There are a lot of applications and areas of research.</p> <p> </p> <p>Special thanks to DSTA, NVIDIA and ensemblecap.ai</p> <p>Cheers, Ritchie</p>"},{"location":"news/detect_waterbone_debris_ai_for_social_good_icml_2019_06/","title":"Oral Presentation for AI for Social Good Workshop ICML","text":""},{"location":"news/detect_waterbone_debris_ai_for_social_good_icml_2019_06/#detecting-waterborne-debris-with-sim2real-and-randomization","title":"Detecting Waterborne Debris with Sim2Real and Randomization","text":"<p>We'll be presenting our work in ICML this June.</p> <p>Special thanks to everyone working on this project: </p> <p>Jie Fu (MILA, Polytechnic Montr\u00e9al); Ritchie Ng (National University of Singapore); Mirgahney Mohamed (The African Institute for Mathematical Sciences); Yi Tay (Nanyang Technological University); Kris Sankaran (Montreal Institute for Learning Algorithms); Shangbang Long (Peking University); Alfredo Canziani (New York University); Chris J Pal (MILA, Polytechnique Montr\u00e9al, Element AI); Moustapha Cisse (Google Research)</p> <p>Cheers, Ritchie</p>"},{"location":"news/facebook_pytorch_devcon_recap_2018_10_02/","title":"Recap of Facebook PyTorch Devcon","text":""},{"location":"news/facebook_pytorch_devcon_recap_2018_10_02/#woo","title":"Woo!","text":"<p>Proud of how far PyTorch has come. Thanks Soumith Chintala, it is a great journey and it just started. And thanks to Adam Paszke too!</p> <p>Finally got to catch up with Soumith Chintala, Alfredo Canziani and Marek Bardo\u0144ski and found Andrej Karpathy! Back to Singapore tonight \ud83e\udd17</p> <p>Here are some photos of my trip! </p> <p> </p> <p>Cheers, Ritchie</p>"},{"location":"news/facebook_pytorch_developer_conference_2018_09_05/","title":"Facebook PyTorch Developer Conference","text":""},{"location":"news/facebook_pytorch_developer_conference_2018_09_05/#we-are-heading-down","title":"We are heading down!","text":"<p>In barely 2 short years, PyTorch (Facebook) will be hosting their first PyTorch Developer Conference in San Francisco, USA.</p> <p>I will be heading down thanks to Soumith Chintala for the invite and arrangements. Looking forward to meet anyone there. </p> <p>The PyTorch ecosystem has grown tremendously from when I first started using it. To this date, I've taught more than 3000 students worldwide in 120+ countries and every single wizard has fallen in love with it!</p> <p>Cheers, Ritchie Ng</p>"},{"location":"news/it_youth_leader_2019_03_11/","title":"IT Youth Leader of the Year 2019 Award","text":""},{"location":"news/it_youth_leader_2019_03_11/#we-got-it","title":"We got it!","text":"<p>I am very happy to be the IT Youth Leader of the Year 2019 in Singapore for my industry and non-profit contribution in AI at Deep Learning Wizard, ensemblecap.ai, NUS,  and NVIDIA!</p> <p>I would like to thank the professor who changed my life, believed in me and nominated me for consideration, Professor Chua Tat-Seng from NExT Centre, NUS School of Computing. Big thanks to everyone who helped for this in NExT Centre particularly Choon Meng, Tek Min, Chua Yi Hao, and Eugene Tan.</p> <p>Special thanks to an important friend and associate who stuck by with me all the way, Jie Fu from our endless days of programming (and scribbling equations) during your PhD years to now as a postdoc in MILA \ud83d\ude42And Ilija Ilievski from those years till now too!</p> <p>Big thanks to the people who believed in the future of AI and asset management and made this happen with the original team Damien Loh, Atsuo Ogaki, and Samuel Chen from ensemblecap.ai</p> <p>Thanks to new found close friend Alfredo Canziani who motivated me on open-source education further and supporting each other \ud83d\ude42 And thanks to Moustapha Cisse for setting up an amazing initiative in Africa with #AMMI to kickstart a future of AI by Africa and trusted your students with our teaching. And I'm on my starting journey to help as many students of your as possible by guiding them on learning like Merghaney Mohammed and Enoch Tetteh.</p> <p>Thanks to Soumith Chintala for spearheading #PyTorch and it's now in production running critical functions in finance (like my firm) and tech.</p> <p>Thanks to Marek Bardo\u0144ski where our friendship started when you were in #NVIDIA and we've supported each other ever since in our learning journey.</p> <p>Thanks to #NVIDIA for allowing me to lead deep learning workshops under DLI in NUS all these years.</p> <p>Thanks to my mentors and everyone in #PhilipYeoInitiative like Abel Ang, Seok Lin Khoo, Kiren Kumar, Claire Cheong, Chong Siak Ching, and Kai Hoe Tan</p> <p>Last thanks to everyone in #SingaporeComputerSociety like Jason Chen for being there when I was nervous and Howie Lau for spearheading tech from start till now. And to a senior big award winner I've a lot to learn and also from NUS: Jeffrey Tiong!</p> <p>Thanks to forward-looking civil servants and minister like Minister S Iswaran for gracing the event and delivering a speech on digital readiness and skills empowerment for Singaporeans.</p> <p>Thanks to my family for being by my side Carine Chng, Ng William, and Adabelle Ng for giving me the freedom and support to pursue whatever I wanted even from young.</p> <p>To the future of AI!</p> <p>Cheers, Ritchie</p>"},{"location":"news/nanjing_next_nus_tsinghua_ai_finance_healthcare_2018_11_01/","title":"NExT++ AI in Finance and Healthcare Workshop 2018","text":""},{"location":"news/nanjing_next_nus_tsinghua_ai_finance_healthcare_2018_11_01/#recap","title":"Recap","text":"<p>This is NExT++\u2019s 3<sup>rd</sup> Workshop and this time we are looking at the applications and development of AI related technologies in the healthcare and finance verticals with the theme \u201cAI in Health and Finance\u201d</p> <p>Visual summary of my talk on AI and Unstructured Analytics in Fintech. Doesn't contain everything, but whatever the non-technical designers could come up with in real-time. Really amazing so kudos to them.</p> <p>Really thankful to the whole team in my former lab, NExT++ and a close friend, Choon Meng, for making all the arrangements. Also thanks to Tek Min, Yi Hao and everyone else.</p> <p>Super happy to catch up with Prof Tat-Seng Chua (KITHCT Chair Professor at the School of Computing), Prof Sun Maosong (Dean of Department of Computer Science and Technology, Tsinghua University), and Prof Dame Wendy Hall (Director of the Web Science Institute, University of Southampton).</p> <p></p> <p>Cheers, Ritchie</p>"},{"location":"news/news/","title":"Welcome to our Blog","text":"<p>Here, we post news related to Deep Learning Wizard's releases, features and achievements </p>"},{"location":"news/news/#notable-news","title":"Notable News","text":"<ul> <li> Oral Presentation for AI for Social Good Workshop ICML, Long Beach, Los Angeles, USA, June 2019</li> <li> IT Youth Leader of The Year, Singapore Computer Society (SCS), Singapore, March 2019</li> <li> Foundations of Deep Learning, African Masters of Machine Intelligence (AMMI), Google &amp; Facebook, Kigali, Rwanda, November 2018</li> <li> Invited to Facebook PyTorch Developer Conference, San Francisco, USA, September 2018</li> <li> Led NUS-MIT-NUHS Datathon NVIDIA Image Recognition Workshop, Singapore, July 2018</li> <li> Featured on PyTorch Website, January 2018</li> <li> Led NVIDIA Self-Driving Cars and Healthcare Talk, Singapore, June 2017</li> <li> Clinched NVIDIA Inception Partner Status, Singapore, May 2017</li> </ul>"},{"location":"news/nvidia_nus_mit_datathon_2018_07_05/","title":"NVIDIA Workshop at NUS-MIT-NUHS Datathon","text":""},{"location":"news/nvidia_nus_mit_datathon_2018_07_05/#image-recognition-workshop-by-ritchie-ng","title":"Image Recognition Workshop by Ritchie Ng","text":"<p>The NVIDIA Deep Learning Institute (DLI) trains developers, data scientists, and researchers on how to use artificial intelligence to solve real-world problems across a wide range of domains. In the deep learning courses, you\u2019ll learn how to train, optimize, and deploy neural networks using the latest tools, frameworks, and techniques for deep learning.</p> <p>In \u201cImage Classification with DIGITS\u201d mini-course, you will learn how to train a deep neural network to recognize handwritten digits by loading image data into a training environment, choosing and training a network, testing with new data, and iterating to improve performance.</p> <p>Link to the NUS-MIT-NUHS Datathon workshop.</p>"},{"location":"news/nvidia_self_driving_cars_talk_2017_06_21/","title":"NVIDIA Self-Driving Cars and Healthcare Workshop","text":""},{"location":"news/nvidia_self_driving_cars_talk_2017_06_21/#hosted-by-ritchie-ng","title":"Hosted by Ritchie Ng","text":"<p>A talk by Marek Bardo\u0144ski, Senior Deep Learning Research Engineer, NVIDIA Switzerland. This is hosted by Ritchie Ng, Deep Learning Researcher, NExT Search Centre, NUS.</p> <p>We will be touching on cutting-edge applications of deep learning for self-driving cars and medical diagnostics. Also there will be a tutorial followed by networking with deep learning researchers from NUS and NVIDIA.</p> <p>Details: Wednesday, June 21<sup>st</sup> 1:00 PM to 3:30 PM</p> <p>The Hangar by NUS Enterprise 21 Heng Mui Keng Terrace, Singapore 119613</p> <p></p>"},{"location":"ood/intro/","title":"Out-of-distribution Data","text":"<p>This is a persistent and critical production issue present in any machine learning and deep learning systems, no matter how good the models were trained.</p> <p>A single out-of-distribution sample data fed into a well-trained model can result in a few problematic outcomes, and in production-level systems they become fat-tailed critical risks that have outsized consequences.</p> Problematic consequences of out-of-distribution data <ol> <li>False positive &amp; False Negative: prediction does not match the ground truth.</li> <li>Unreliable True positive &amp; True Negative: prediction matches the ground truth but due to samples being out-of-distribution, the performance of these predictions becomes very erratic. </li> </ol>"},{"location":"programming/intro/","title":"Programming","text":"<p>We'll be covering C++, Python, Bash and more for end-to-end AI deployments.</p> Packages and Languages you will learn to use <ul> <li>C++</li> <li>Python</li> <li>Bash</li> <li>Matplotlib</li> <li>Javascript</li> <li>Electron</li> </ul> <p>Work in progress</p> <p>This open-source portion is still a work in progress. Stay tuned while we gradually upload our tutorials and notes. Feel free to contact Ritchie Ng if you would like to contribute via our Facebook page.</p>"},{"location":"programming/bash/bash/","title":"Bash","text":"<p>Run Bash Files</p> <p>You can find the bash code files for this section in this link.</p>"},{"location":"programming/bash/bash/#creating-and-running-a-bash-file","title":"Creating and Running a Bash File","text":""},{"location":"programming/bash/bash/#create-bash-file","title":"Create bash file","text":"<pre><code>touch hello_world.sh\n</code></pre>"},{"location":"programming/bash/bash/#edit-bash-file-with-hello-world","title":"Edit bash file with Hello World","text":"<p>You can edit with anything like Vim, Sublime, etc.</p> <pre><code>echo Hello World!\n</code></pre>"},{"location":"programming/bash/bash/#run-bash-file","title":"Run bash file","text":"<pre><code>bash hello_world.sh\n</code></pre> <p>This will print out, in your bash: <pre><code>Hello World!\n</code></pre></p>"},{"location":"programming/bash/bash/#calculating","title":"Calculating","text":"<pre><code># Add\necho $((10+10))\n\n# Subtract\necho $((10-10))\n\n# Divide\necho $((10/10))\n\n# Multiple\necho $((10*10))\n\n# Modulo\necho $((10%10))\n\n# Multiple Operations: Divide and Add\necho $((10/10 + 10))\n</code></pre> <pre><code>20\n0\n1\n100\n0\n11\n</code></pre>"},{"location":"programming/bash/bash/#loops-and-conditional","title":"Loops and Conditional","text":""},{"location":"programming/bash/bash/#for-loop","title":"For Loop","text":"<pre><code>for i in 'A' 'B' 'C'\ndo\necho $i\ndone\n</code></pre> <pre><code>A\nB\nC\n</code></pre>"},{"location":"programming/bash/bash/#for-loop-with-range","title":"For Loop With Range","text":"<p>This will echo the digits 0 to 10 without explicitly requiring to define the whole range of numbers/alphabets like above.</p> <pre><code>for ((i=0; i&lt;=10; i++));\ndo\necho $i\ndone\n</code></pre> <pre><code>0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n</code></pre>"},{"location":"programming/bash/bash/#if-else-conditional","title":"If Else Conditional","text":"<p>This is a simple if-else to check if the day of the week is 5, meaning if it is a Friday.</p> <pre><code>day=$(date +%u)\n\nif [ $day == 5 ];\nthen\necho \"Friday is here!\"\n\nelse\necho \"Friday is not here :(\"\necho \"Today is day $day of the week\"\nfi\n</code></pre>"},{"location":"programming/bash/bash/#sequentially-running-of-python-scripts","title":"Sequentially Running of Python Scripts","text":"<p>This snippet allows you to to run 3 python scripts sequentially, waiting for each to finish before proceeding to the next.</p> <pre><code>python script_1.py\nwait\n\npython script_2.py wait\n\npython script_3.py\nwait\n\necho \"Finished running all 3 scripts in sequence!\"\n</code></pre>"},{"location":"programming/bash/bash/#parallel-running-of-python-scripts","title":"Parallel Running of Python Scripts","text":"<pre><code>python script_1.py &amp;&amp; script_2.py &amp;&amp; script_3.py\nwait\n\necho \"Finished running all 3 scripts in parallel in sequence\"\n</code></pre>"},{"location":"programming/bash/bash/#reading-and-writing-operations","title":"Reading and Writing Operations","text":""},{"location":"programming/bash/bash/#reading-logs-and-texts","title":"Reading logs and texts","text":"<p>Create a text file called <code>random_text.txt</code> with the following contents</p> <pre><code>Row 1\nRow 2\nRow 3\nRow 4\nRow 5\nRow 6\nRow 7\nRow 8\nRow 9\nRow 10\n</code></pre> <p>Then run the following command to read it in bash then print it.</p> <pre><code>text_file=$(cat random_text.txt)\necho $text_file\n</code></pre> <pre><code>Row 1 Row 2 Row 3 Row 4 Row 5 Row 6 Row 7 Row 8 Row 9 Row 10\n</code></pre>"},{"location":"programming/bash/bash/#date-operations","title":"Date Operations","text":""},{"location":"programming/bash/bash/#getting-current-date","title":"Getting Current Date","text":"<p>This will return the date in the format YYYY-MM-DD for example 2019-06-03.</p> <pre><code>DATE=`date +%Y-%m-%d`\necho $DATE\n</code></pre>"},{"location":"programming/bash/bash/#getting-current-day-of-week","title":"Getting Current Day of Week","text":"<p>This will return 1, 2, 3, 4, 5, 6, 7 depending on the day of the week.</p> <pre><code>DAY=$(date +%u)\necho $DAY\n</code></pre>"},{"location":"programming/bash/bash/#changing-system-dates-by-1-day","title":"Changing System Dates By 1 Day","text":"<p>You can change system dates based on this. Surprisingly, you'll find it useful for testing an environment for deployments in the next day and then shifting it back to the actual day.</p> <pre><code>sudo date -s 'next day'\nsudo date -s 'yesterday'\n</code></pre> <p>If you are running some tests via bash and want to disable typing in password you can edit the sudoer file via <code>sudo visudo</code> and adding the following line. Only use <code>sudo visudo</code> and nothing else, as they've a special syntax.</p> <pre><code>&lt;username&gt; ALL=(ALL) NOPASSWD: /bin/date\n</code></pre> <p>To find out your username, simply just run the command <code>whoami</code>.</p>"},{"location":"programming/bash/bash/#jupyter-utility-commands","title":"Jupyter Utility Commands","text":""},{"location":"programming/bash/bash/#convert-notebook-to-htmlmarkdown","title":"Convert Notebook to HTML/Markdown","text":"<pre><code>jupyter nbconvert --to markdown python.ipynb\njupyter nbconvert --to html python.ipynb\n</code></pre>"},{"location":"programming/bash/bash/#bash-convenient-commands","title":"Bash Convenient Commands","text":""},{"location":"programming/bash/bash/#list-directories-only","title":"List directories only","text":"<p><code>ls -d */</code></p>"},{"location":"programming/bash/bash/#list-non-directories-only","title":"List non-directories only","text":"<p><code>ls -p | grep -v '/$'</code></p>"},{"location":"programming/bash/bash/#check-ip","title":"Check IP","text":"<p><code>ifconfig | sed -En \"s/127.0.0.1//;s/.*inet (addr:)?(([0-9]*\\.){3}[0-9]*).*/\\2/p\"</code></p>"},{"location":"programming/bash/bash/#check-internet-speed","title":"Check internet speed","text":"<p><code>ethtool eno1</code></p>"},{"location":"programming/bash/bash/#check-disk-space","title":"Check disk space","text":"<p><code>df -h</code></p>"},{"location":"programming/bash/bash/#check-ubuntu-version","title":"Check ubuntu version","text":"<p><code>lsb_release -a</code></p>"},{"location":"programming/bash/bash/#check-truncated-system-logs","title":"Check truncated system logs","text":"<p><code>tail /var/log/syslog</code></p>"},{"location":"programming/bash/bash/#check-cuda-version","title":"Check CUDA version","text":"<p><code>nvcc -V</code></p>"},{"location":"programming/bash/bash/#check-cudnn-version","title":"Check cuDNN version","text":"<p><code>cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2</code></p>"},{"location":"programming/bash/bash/#check-username","title":"Check username","text":"<p><code>whoami</code></p>"},{"location":"programming/bash/bash/#untar-file","title":"Untar file","text":"<p><code>tar -xvzf file_name</code></p>"},{"location":"programming/bash/bash/#open-pdf-file","title":"Open PDF file","text":"<p><code>gvfs-open file_name</code></p>"},{"location":"programming/bash/bash/#download-file-from-link-rapidly-with-aria","title":"Download file from link rapidly with aria","text":"<p><code>aria2c -x16 -c url_link</code></p>"},{"location":"programming/bash/bash/#kill-all-python-processes","title":"Kill all python processes","text":"<p><code>ps ax | grep python | cut -c1-5 | xargs kill -9</code></p>"},{"location":"programming/bash/bash/#install-deb-files","title":"Install .deb files","text":"<p><code>sudo apt-get install -f file_name.deb</code></p>"},{"location":"programming/bash/bash/#empty-thrash","title":"Empty thrash","text":"<pre><code>sudo apt-get install trash-cli\nthrash-empty\n</code></pre>"},{"location":"programming/bash/bash/#fix-git-permissions","title":"Fix Git Permissions","text":"<p>There are times whether you're on Windows (frequent), Mac, or Linux, you might encounter a permission error although you have obviously set it up correctly and it was last working. This is quick fix you can run. </p> <pre><code>eval `ssh-agent -s` &amp;&amp; ssh-add ~/.ssh/github\n</code></pre> <p>If you want, you can even make an alias <code>fixgit</code> and quickly call it in bash to make it easier to fix whenever you face this issue. It's a quick fix, there're more permnanent ways to fix it if you want. But this is a quick fix section. </p> <p>In your <code>.bashrc</code> or <code>.zshrc</code>, include the following:</p> <pre><code>alias fixgit='eval `ssh-agent -s` &amp;&amp; ssh-add ~/.ssh/github'\n</code></pre>"},{"location":"programming/bash/bash/#get-git-repo-information","title":"Get Git Repo Information","text":"<pre><code>curl https://api.github.com/&lt;user&gt;/&lt;repo_name&gt;\n</code></pre>"},{"location":"programming/bash/bash/#conda-commands","title":"Conda Commands","text":""},{"location":"programming/bash/bash/#check-conda-environment","title":"Check conda environment","text":"<p><code>conda env list</code></p>"},{"location":"programming/bash/bash/#create-conda-kernel","title":"Create conda kernel","text":"<pre><code>conda create -n kernel_name python=3.6\nsource activate kernel_name\n</code></pre>"},{"location":"programming/bash/bash/#install-conda-kernel","title":"Install conda kernel","text":"<pre><code>conda install ipykernel\nsource activate kernel_name\npython -m ipykernel install --user --name kernel_name --display-name kernel_name\n</code></pre>"},{"location":"programming/bash/bash/#remove-conda-kernel","title":"Remove conda kernel","text":"<p><code>conda env remove -n kernel_name</code></p>"},{"location":"programming/bash/bash/#recovering-problematic-conda-installation","title":"Recovering problematic conda installation","text":"<pre><code># Download miniconda according to your environment\n# Link: https://docs.conda.io/en/latest/miniconda.html\n\n# Backup existing miniconda environment that may have problems\nmv miniconda3 miniconda3_backup\n\n# Install miniconda\nbash Miniconda3-latest-Linux-x86_64.sh\n\n# Restore old environment settings\nrsync -a miniconda3_backup/ miniconda3/\n</code></pre>"},{"location":"programming/bash/bash/#internet-operations","title":"Internet Operations","text":""},{"location":"programming/bash/bash/#checking-internet-availability","title":"Checking Internet Availability","text":"<p>This script will return whether your internet is fine or not without using pings.</p> <p>Pings can often be rendered unusable when the network administrator disables ICMP to prevent the origination of ping floods from the data centre.</p> <pre><code>if nc -zw1 google.com 443;\nthen\necho \"INTERNET: OK\"\nelse\necho \"INTERNET: NOT OK\"\nfi\n</code></pre>"},{"location":"programming/bash/bash/#cron-operations","title":"Cron Operations","text":""},{"location":"programming/bash/bash/#edit-cron","title":"Edit Cron","text":"<p>Formatting follows this syntax with full credits on this beautiful diagram to fedorqui from Stack Overflow: <pre><code> \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59)\n \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23)\n \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500 day of month (1 - 31)\n \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500 month (1 - 12)\n \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500 day of week (0 - 6 =&gt; Sunday - Saturday, or\n \u2502 \u2502 \u2502 \u2502 \u2502                1 - 7 =&gt; Monday - Sunday)\n \u2193 \u2193 \u2193 \u2193 \u2193\n * * * * * command to be executed\n</code></pre></p> <p>Edit cron with this command: <pre><code>sudo crontab -e\n</code></pre></p>"},{"location":"programming/bash/bash/#list-cron","title":"List Cron","text":"<pre><code>sudo crontab -l\n</code></pre>"},{"location":"programming/bash/bash/#status-start-stop-and-restart","title":"Status, Start, Stop and Restart","text":"<pre><code>sudo service cron status\nsudo service cron stop\nsudo service cron start\nsudo service cron restart\n</code></pre>"},{"location":"programming/bash/bash/#cron-debugging","title":"Cron Debugging","text":"<p>Install postfix for local routing of errors (choose local option): <pre><code>sudo apt-get install postfix\n</code></pre></p> <p>Restart cron to see for any errors posted (if not errors, there will be no file, be patient before errors are posted): <pre><code>sudo cat /var/mail/root\n</code></pre></p>"},{"location":"programming/bash/bash/#cron-bash-fix","title":"Cron Bash Fix","text":"<p>Cron uses <code>/bin/sh</code> as the default shell. Typically you would realize you're using <code>/bin/bash</code> as your shell, so this typically needs to be rectified before you can use cron to schedule commands as if it were your usual bash.</p> <p>Edit your cron file via <code>sudo crontab -e</code> and paste the following lines at the end of the file prior to your command like so. Take note for <code>PATH</code>, you've to paste the output of <code>echo PATH=$PATH</code> in there instead!</p> <pre><code>SHELL=/bin/bash\nPATH=/usr/lib....\n# Your command schedule here!\n</code></pre>"},{"location":"programming/bash/bash/#cron-conda-environment","title":"Cron Conda Environment","text":"<p>This is an example of enabling an anaconda environment, for example the default <code>base</code>, and running a python script. </p> <p>Take note you need to put your python script in the right directory or simply navigate to that path with <code>cd</code> prior to <code>\"$(conda shell.bash hook)\"</code>. </p> <pre><code>SHELL=/bin/bash\nPATH=/usr/lib....\n* * * * 1-5 eval \"$(conda shell.bash hook)\" &amp;&amp; conda activate base &amp;&amp; python python_script_name.py\n</code></pre>"},{"location":"programming/bash/bash/#cron-running-processes","title":"Cron Running Processes","text":"<p>Some times you want to see the status of running tasks and may want to get the PID to end it. This is a very handy command.</p> <pre><code>ps -o pid,sess,cmd afx | egrep -A20 \"( |/)cron( -f)?$\"\n</code></pre> <p>You can get the PID of the cron process and then end it with <code>sudo pkill -s &lt;PID&gt;</code></p>"},{"location":"programming/bash/bash/#hardware-information","title":"Hardware Information","text":""},{"location":"programming/bash/bash/#comprehensive-cpu-information","title":"Comprehensive CPU Information","text":"<pre><code>cat /proc/cpuinfo\n</code></pre>"},{"location":"programming/bash/bash/#number-of-cpu-threads","title":"Number of CPU Threads","text":"<pre><code>!grep -c ^processor /proc/cpuinfo\n</code></pre> <p>or </p> <pre><code>nproc\n</code></pre>"},{"location":"programming/bash/bash/#cpu-model-name","title":"CPU Model Name","text":"<pre><code>!cat /proc/cpuinfo | grep \"model name\" </code></pre>"},{"location":"programming/bash/bash/#check-available-ram","title":"Check Available RAM","text":""},{"location":"programming/bash/bash/#in-mb","title":"In MB","text":"<pre><code>free -m\n</code></pre>"},{"location":"programming/bash/bash/#in-gb","title":"In GB","text":"<pre><code>free -g\n</code></pre>"},{"location":"programming/cpp/cpp/","title":"C++","text":"<p>Run Jupyter Notebook</p> <p>You can run the code for this section in this jupyter notebook link.</p>"},{"location":"programming/cpp/cpp/#installation-of-interactive-c17","title":"Installation of Interactive C++17","text":"<p>Xeus-Cling is a game-changer where similar to Python Jupyter Notebooks, we can run C++ Jupyter Notebooks now. Run the following bash commands in sequence to create a C++ kernel for Jupyter Notebook.</p> <pre><code>conda create -n cpp\nsource activate cpp\nconda install -c conda-forge xeus-cling\njupyter kernelspec install --user /home/ritchie/miniconda3/envs/cpp/share/jupyter/kernels/xcpp17\njupyter notebook\n</code></pre>"},{"location":"programming/cpp/cpp/#printing","title":"Printing","text":""},{"location":"programming/cpp/cpp/#printing-single-line","title":"Printing Single Line","text":"<pre><code>// When you compile, the preprocessor runs and acts on all line with the pound key first\n\n// This is a preprocessor instruction that\n// essentially places the file with the name iostream into this spot\n#include &lt;iostream&gt;\n</code></pre> <pre><code>// Print one line\nstd::cout &lt;&lt; \"Using Xeus Cling\" &lt;&lt; std::endl;\n</code></pre> <pre><code>Using Xeus Cling\n</code></pre>"},{"location":"programming/cpp/cpp/#printing-2-lines","title":"Printing 2 Lines","text":"<pre><code>// Print two lines\nstd::cout &lt;&lt; \"First Line \\nSecond Line\" &lt;&lt; std::endl;\n</code></pre> <pre><code>First Line \nSecond Line\n</code></pre>"},{"location":"programming/cpp/cpp/#printing-with-tabs","title":"Printing with Tabs","text":"<pre><code>// Print numbers with nicely formatted tabs\nstd::cout &lt;&lt; \"One hundred:\\t\";\nstd::cout &lt;&lt; (float) 1000/10 &lt;&lt; std::endl;\n\nstd::cout &lt;&lt; \"Two hundred:\\t\";\nstd::cout &lt;&lt; (double) 2000/10 &lt;&lt; std::endl;\n\nstd::cout &lt;&lt; \"Three hundred:\\t\";\nstd::cout &lt;&lt; (double) 3000/10 &lt;&lt; std::endl;\n</code></pre> <pre><code>One hundred:    100\nTwo hundred:    200\nThree hundred:  300\n</code></pre>"},{"location":"programming/cpp/cpp/#easier-printing-subjective-with-namespaces","title":"Easier Printing (subjective) with Namespaces","text":"<ul> <li>Gets irritating to use <code>std</code> in front of <code>cout</code> and <code>endl</code> to keep printing so we can use namespaces</li> </ul> <pre><code>using namespace std;\ncout &lt;&lt; \"No need for messy std::\" &lt;&lt; endl;\n</code></pre> <pre><code>No need for messy std::\n</code></pre>"},{"location":"programming/cpp/cpp/#variables","title":"Variables","text":""},{"location":"programming/cpp/cpp/#general-memory-management","title":"General memory management","text":"<p>In C++, you always need to determine each variable's type so the compiler will know how much memory (RAM) to allocate to the variable</p>"},{"location":"programming/cpp/cpp/#types-of-variables","title":"Types of Variables","text":"Type Bytes (Size) Range of Values char 1 0 to 255 or -127 to 127 unsigned char 1 -127 to 127 signed char 1 0 to 255 bool 1 True or False int 4 -2,147,483,648 to 2,147,483,647 unsigned int 4 0 to 4,294,967,295 signed int 4 -2,147,483,648 to 2,147,483,647 short int 2 -32,768 to 32,767 long int 4 -2,147,483,648 to 2,147,483,647 float 4 3.4e-38 to 3.4e38 double 8 1.7e-308 to 1.7e-08 long double 8 1.7e-308 to 1.7e-08"},{"location":"programming/cpp/cpp/#integer-sizes","title":"Integer sizes","text":"<ul> <li>Typical sizes<ul> <li>Short integer: 2 bytes (will be smaller than long)<ul> <li>Limitation is that the value of this short integer has a max value, you should be cautious of using short integers</li> </ul> </li> <li>Long integer: 4 bytes</li> <li>Integer: 2 or 4 bytes</li> </ul> </li> <li>Notes<ul> <li>Technically these sizes can vary depending on your processor (32/64 bit) and compiler</li> <li>You should not assume the sizes but the hierarchy of sizes will not change (short memory &lt; long memory)</li> </ul> </li> </ul>"},{"location":"programming/cpp/cpp/#integer-size","title":"Integer size","text":"<pre><code>cout &lt;&lt; \"Size of an integer: \" &lt;&lt; sizeof (int);\n</code></pre> <pre><code>Size of an integer: 4\n</code></pre>"},{"location":"programming/cpp/cpp/#short-integer-size","title":"Short integer size","text":"<pre><code>cout &lt;&lt; \"Size of a short integer: \" &lt;&lt; sizeof (short int);\n</code></pre> <pre><code>Size of a short integer: 2\n</code></pre>"},{"location":"programming/cpp/cpp/#long-integer-size","title":"Long integer size","text":"<pre><code>cout &lt;&lt; \"Size of an long integer: \" &lt;&lt; sizeof (long int);\n</code></pre> <pre><code>Size of an long integer: 8\n</code></pre>"},{"location":"programming/cpp/cpp/#unsigned-or-signed-integers","title":"Unsigned or signed integers","text":"<ul> <li>Unsigned integers: only can hold positive integers</li> <li>Signed integers: can hold positive/negative integers</li> </ul>"},{"location":"programming/cpp/cpp/#signed-short-integer","title":"Signed short integer","text":"<pre><code>cout &lt;&lt; \"Size of an signed short integer: \" &lt;&lt; sizeof (signed short int);\n</code></pre> <pre><code>Size of an signed short integer: 2\n</code></pre>"},{"location":"programming/cpp/cpp/#unsigned-short-integer","title":"Unsigned short integer","text":"<pre><code>cout &lt;&lt; \"Size of an unsigned short integer: \" &lt;&lt; sizeof (unsigned short int);\n</code></pre> <pre><code>Size of an unsigned short integer: 2\n</code></pre>"},{"location":"programming/cpp/cpp/#signed-long-integer","title":"Signed long integer","text":"<pre><code>cout &lt;&lt; \"Size of an signed long integer: \" &lt;&lt; sizeof (signed long int);\n</code></pre> <pre><code>Size of an signed long integer: 8\n</code></pre>"},{"location":"programming/cpp/cpp/#unsigned-long-integer","title":"Unsigned long integer","text":"<pre><code>cout &lt;&lt; \"Size of an unsigned long integer: \" &lt;&lt; sizeof (unsigned long int);\n</code></pre> <pre><code>Size of an unsigned long integer: 8\n</code></pre>"},{"location":"programming/cpp/cpp/#constants","title":"Constants","text":""},{"location":"programming/cpp/cpp/#literal-constants","title":"Literal Constants","text":"<pre><code>int varOne = 20;\nstd::cout &lt;&lt; varOne &lt;&lt; std::endl;\n</code></pre> <pre><code>20\n</code></pre>"},{"location":"programming/cpp/cpp/#enumerated-constants","title":"Enumerated Constants","text":"<p>This enables you to create a new type! In this example we create a new type <code>directions</code> containing <code>Up</code>, <code>Down</code>, <code>Left</code>, and <code>Right</code>.</p> <pre><code>enum directions {Up, Down, Left, Right};\n\ndirections goWhere;\ngoWhere = Right;\n\nif (goWhere == Right)\nstd::cout &lt;&lt; \"Go right\" &lt;&lt; std::endl;\n</code></pre> <pre><code>Go right\n</code></pre>"},{"location":"programming/cpp/cpp/#functions","title":"Functions","text":""},{"location":"programming/cpp/cpp/#function-without-return-value","title":"Function Without Return Value","text":"<p>Syntax generally follows <code>void FunctionName(argOne, argTwo)</code> to define the function followed by <code>FuntionName()</code> to call the function.</p> <pre><code>// Usage of void when the function does not return anything\n// In this example, this function prints out the multiplication result of two given numbers\nvoid MultiplyTwoNumbers(int firstNum, int secondNum)\n{\n// Define variable as integer type\nlong int value;\nvalue = firstNum * secondNum;\nstd::cout &lt;&lt; value &lt;&lt; std::endl; }\n</code></pre>"},{"location":"programming/cpp/cpp/#multiply-two-numbers-3-and-2","title":"Multiply Two Numbers 3 and 2","text":"<pre><code>MultiplyTwoNumbers(3, 2)\n</code></pre> <pre><code>6\n</code></pre>"},{"location":"programming/cpp/cpp/#multiply-two-numbers-6-and-2","title":"Multiply Two Numbers 6 and 2","text":"<pre><code>MultiplyTwoNumbers(6, 2)\n</code></pre> <pre><code>12\n</code></pre>"},{"location":"programming/cpp/cpp/#aliases-function","title":"Aliases Function","text":"<p>Say we want the variable <code>value</code> to be of type <code>unsigned short int</code> such that it's 2 bytes and can hold 2x the range of values compared to just <code>short int</code>. We can use <code>typedef</code> as an alias.</p> Type Bytes (Size) Range of Values short int 2 -32,768 to 32,767 unsigned short int 2 0 to 65,536 <pre><code>// Usage of void when the function does not return anything\n// In this exmaple, this function prints out the multiplication result of two given numbers\nvoid MultiplyTwoNumbersWithAlias(int firstNum, int secondNum)\n{\n// Using an alias\ntypedef unsigned short int ushortint;\n// initializing value variable with ushortint type\nushortint value;\nvalue = firstNum * secondNum;\nstd::cout &lt;&lt; value &lt;&lt; std::endl; }\n</code></pre>"},{"location":"programming/cpp/cpp/#multiply-two-numbers-10-and-10","title":"Multiply Two Numbers 10 and 10","text":"<pre><code>MultiplyTwoNumbersWithAlias(10, 10)\n</code></pre> <pre><code>100\n</code></pre>"},{"location":"programming/cpp/cpp/#multiply-two-numbers-1000-and-65","title":"Multiply Two Numbers 1000 and 65","text":"<pre><code>MultiplyTwoNumbersWithAlias(1000, 65)\n</code></pre> <pre><code>65000\n</code></pre>"},{"location":"programming/cpp/cpp/#multiply-two-numbers-1000-and-67","title":"Multiply Two Numbers 1000 and 67","text":"<ul> <li>Notice how you don't get 67,000? This is because our variable <code>value</code> of <code>ushortint</code> type can only hold values up to the integer 65,536.</li> <li>What this returns is the remainder of 67,000 - 65,536 = 1464</li> </ul> <pre><code>MultiplyTwoNumbersWithAlias(1000, 67)\n</code></pre> <pre><code>1464\n</code></pre> <pre><code>std::cout &lt;&lt; 67 * 1000 - 65536 &lt;&lt; std::endl;\n</code></pre> <pre><code>1464\n</code></pre>"},{"location":"programming/cpp/cpp/#function-with-return-value","title":"Function with Return Value","text":"<p>Unlike functions without return values where we use <code>void</code> to declare the function, here we use <code>int</code> to declare our function that returns values.</p> <pre><code>// In this exmaple, this function returns the value of the multiplication of two numbers\nint MultiplyTwoNumbersNoPrint(int firstNum, int secondNum)\n{\n// Define variable as integer type\nlong int value;\nvalue = firstNum * secondNum;\nreturn value;\n}\n</code></pre>"},{"location":"programming/cpp/cpp/#call-function","title":"Call Function","text":"<pre><code>// Declare variable with type\nlong int returnValue;\n// Call function\nreturnValue = MultiplyTwoNumbersNoPrint(10, 2);\n// Print variable\nstd::cout &lt;&lt; returnValue &lt;&lt; std::endl;\n</code></pre> <pre><code>20\n</code></pre>"},{"location":"programming/cpp/cpp/#function-inner-workings","title":"Function Inner Workings","text":"<ul> <li>Essentially our lines of codes translates to instruction pointers with unique memory addresses.</li> <li>Execution of instruction pointers operates on a \"LIFO\" basis, last in first out.<ul> <li>Oversimplifying here, in our example, the last line is taken off first and it follows up</li> </ul> </li> </ul> <pre><code>// Code Space\nint varOneTest = 10; // Instruction pointer 100\nstd::cout &lt;&lt; varOneTest &lt;&lt; std::endl; // Instruction Pointer 102\n</code></pre> <pre><code>10\n\n\n\n\n\n@0x7fa6b7de5460\n</code></pre>"},{"location":"programming/cpp/cpp/#arrays","title":"Arrays","text":"<p>An array contains a sequence of elements with the same data type. </p>"},{"location":"programming/cpp/cpp/#creating-an-array","title":"Creating an Array","text":"<pre><code>// This is how you declare an array of 50 elements each of type double\ndouble DoubleArray[50];\n</code></pre>"},{"location":"programming/cpp/cpp/#accessing-arrays-elements","title":"Accessing Array's Elements","text":""},{"location":"programming/cpp/cpp/#first-element","title":"First Element","text":"<pre><code>// This access the first array \nstd::cout &lt;&lt; DoubleArray[0] &lt;&lt; std::endl;\n</code></pre> <pre><code>0\n</code></pre>"},{"location":"programming/cpp/cpp/#last-element","title":"Last Element","text":"<pre><code>std::cout &lt;&lt; DoubleArray[49] &lt;&lt; std::endl;\n</code></pre> <pre><code>0\n</code></pre>"},{"location":"programming/cpp/cpp/#first-10-elements","title":"First 10 Elements","text":"<pre><code>// In steps of 1\nfor (int i=0; i&lt;10; i++)\n{\n// This is how you print a mix of characters and declared variables\nstd::cout &lt;&lt; \"Element \" &lt;&lt; i &lt;&lt; \" contains \" &lt;&lt;  DoubleArray[i] &lt;&lt; std::endl;\n}\n</code></pre> <pre><code>Element 0 contains 0\nElement 1 contains 0\nElement 2 contains 0\nElement 3 contains 0\nElement 4 contains 0\nElement 5 contains 0\nElement 6 contains 0\nElement 7 contains 0\nElement 8 contains 0\nElement 9 contains 0\n</code></pre> <pre><code>// In steps of 2\nfor (int i=0; i&lt;10; i+=2)\n{\n// This is how you print a mix of characters and declared variables\nstd::cout &lt;&lt; \"Element \" &lt;&lt; i &lt;&lt; \" contains \" &lt;&lt;  DoubleArray[i] &lt;&lt; std::endl;\n}\n</code></pre> <pre><code>Element 0 contains 0\nElement 2 contains 0\nElement 4 contains 0\nElement 6 contains 0\nElement 8 contains 0\n</code></pre>"},{"location":"programming/cpp/cpp/#going-beyond-the-arrays-length","title":"Going Beyond The Array's Length","text":"<p>This will return a warning that it's past the end of the array</p> <p><pre><code>std::cout &lt;&lt; DoubleArray[50] &lt;&lt; std::endl;\n</code></pre> <pre><code>input_line_36:2:15: warning: array index 50 is past the end of the array (which contains 50 elements)\n[-Warray-bounds]\nstd::cout &lt;&lt; DoubleArray[50] &lt;&lt; std::endl;\n              ^           ~~\ninput_line_32:3:1: note: array 'DoubleArray' declared here\ndouble DoubleArray[50];\n^\n4.94066e-323\n</code></pre></p>"},{"location":"programming/cpp/cpp/#arrays-with-enumeration","title":"Arrays with Enumeration","text":"<pre><code>enum directionsNew {up, down, left, right, individualDirections};\n\nint directionsArray[individualDirections] = {1, 2, 3, 4};\n\nstd::cout &lt;&lt; \"Up value:\\t\" &lt;&lt; directionsArray[up];\nstd::cout &lt;&lt; \"\\nDown value:\\t\" &lt;&lt; directionsArray[down];\nstd::cout &lt;&lt; \"\\nLeft value:\\t\" &lt;&lt; directionsArray[left];\nstd::cout &lt;&lt; \"\\nRight value:\\t\" &lt;&lt; directionsArray[right];\n// This is the number of elements in the array\nstd::cout &lt;&lt; \"\\nNum value:\\t\" &lt;&lt; sizeof(directionsArray) / sizeof(directionsArray[0]) &lt;&lt; std::endl;\n</code></pre> <pre><code>Up value:   1\nDown value: 2\nLeft value: 3\nRight value:    4\nNum value:  4\n</code></pre>"},{"location":"programming/cpp/cpp/#arrays-with-1-dimension-tensors","title":"Arrays with &gt;1 Dimension (Tensors)","text":""},{"location":"programming/cpp/cpp/#multi-dimension-array-with-numbers","title":"Multi Dimension Array with Numbers","text":"<pre><code>// This is how you declare a multi-dimensional array of 5x5 elements each of type double\ndouble multiDimArray[5][5] = {\n{1, 2, 3, 4, 5},\n{2, 2, 3, 4, 5},\n{3, 2, 3, 4, 5},\n{4, 2, 3, 4, 5},\n{5, 2, 3, 4, 5}\n};\n\n// Print each row of our 5x5 multi-dimensional array\nfor (int i=0; i&lt;5; i++)\n{\nfor (int j=0; j&lt;5; j++)\n{\nstd::cout &lt;&lt; multiDimArray[i][j];\n};\nstd::cout &lt;&lt; \"\\n\" &lt;&lt; std::endl;\n};\n</code></pre> <pre><code>12345\n\n22345\n\n32345\n\n42345\n\n52345\n</code></pre>"},{"location":"programming/cpp/cpp/#multi-dimension-array-with-characters","title":"Multi Dimension Array with Characters","text":"<pre><code>// This is how you declare a multi-dimensional array of 5x5 elements each of type char\nchar multiDimArrayChars[5][5] = {\n{'a', 'b', 'c', 'd', 'e'},\n{'b', 'b', 'c', 'd', 'e'},\n{'c', 'b', 'c', 'd', 'e'},\n{'d', 'b', 'c', 'd', 'e'},\n{'e', 'b', 'c', 'd', 'e'},\n};\n\n// Print each row of our 5x5 multi-dimensional array\nfor (int i=0; i&lt;5; i++)\n{\nfor (int j=0; j&lt;5; j++)\n{\nstd::cout &lt;&lt; multiDimArrayChars[i][j];\n};\nstd::cout &lt;&lt; \"\\n\" &lt;&lt; std::endl;\n};\n</code></pre> <pre><code>abcde\n\nbbcde\n\ncbcde\n\ndbcde\n\nebcde\n</code></pre>"},{"location":"programming/cpp/cpp/#copy-arrays","title":"Copy Arrays","text":""},{"location":"programming/cpp/cpp/#copy-number-arrays","title":"Copy Number Arrays","text":"<pre><code>double ArrayNumOne[] = {1, 2, 3, 4, 5};\ndouble ArrayNumTwo[5];\n\n// Use namespace std so it's cleaner\nusing namespace std;\n\n// Copy array with copy()\ncopy(begin(ArrayNumOne), end(ArrayNumOne), begin(ArrayNumTwo));\n\n// Print double type array with copy()\ncopy(begin(ArrayNumTwo), end(ArrayNumTwo), ostream_iterator&lt;double&gt;(cout, \"\\n\"));\n</code></pre> <pre><code>1\n2\n3\n4\n5\n</code></pre>"},{"location":"programming/cpp/cpp/#copy-string-arrays","title":"Copy String Arrays","text":"<pre><code>char ArrayCharOne[] = {'a', 'b', 'c', 'd', 'e'};\nchar ArrayCharTwo[5];\n\n// Use namespace std so it's cleaner\nusing namespace std;\n\n// Copy array with copy()\ncopy(begin(ArrayCharOne), end(ArrayCharOne), begin(ArrayCharTwo));\n\n// Print char type array with copy()\ncopy(begin(ArrayCharTwo), end(ArrayCharTwo), ostream_iterator&lt;char&gt;(cout, \"\\n\"));\n</code></pre> <pre><code>a\nb\nc\nd\ne\n</code></pre>"},{"location":"programming/cpp/cpp/#mathematical-operators","title":"Mathematical Operators","text":"<ul> <li>Add, subtract, multiply, divide and modulus</li> </ul>"},{"location":"programming/cpp/cpp/#add","title":"Add","text":"<pre><code>double addStatement = 5 + 5;\nstd::cout &lt;&lt; addStatement;\n</code></pre> <pre><code>10\n</code></pre>"},{"location":"programming/cpp/cpp/#subtract","title":"Subtract","text":"<pre><code>double subtractStatement = 5 - 5;\nstd::cout &lt;&lt; subtractStatement;\n</code></pre> <pre><code>0\n</code></pre>"},{"location":"programming/cpp/cpp/#divide","title":"Divide","text":"<pre><code>double divideStatement = 5 / 5;\nstd::cout &lt;&lt; divideStatement;\n</code></pre> <pre><code>1\n</code></pre>"},{"location":"programming/cpp/cpp/#multiply","title":"Multiply","text":"<pre><code>double multiplyStatement = 5 * 5;\nstd::cout &lt;&lt; multiplyStatement;\n</code></pre> <pre><code>25\n</code></pre>"},{"location":"programming/cpp/cpp/#modulus","title":"Modulus","text":"<ul> <li>Gets the remainder of the division</li> </ul> <pre><code>double modulusStatement = 8 % 5;\nstd::cout &lt;&lt; modulusStatement;\n</code></pre> <pre><code>3\n</code></pre>"},{"location":"programming/cpp/cpp/#exponent","title":"Exponent","text":"<ul> <li>Base to the power of something, this requires a new package called <code>&lt;cmath&gt;</code> that we want to include.</li> </ul> <pre><code>#include &lt;cmath&gt;\n\nvoid SquareNumber(int baseNum, int exponentNum)\n{\n// Square the locally scoped variable with 2\nint squaredNumber;\nsquaredNumber = pow(baseNum, exponentNum);\nstd::cout &lt;&lt; \"Base of 2 with exponent of 2 gives: \" &lt;&lt; squaredNumber &lt;&lt; std::endl;\n}\n\nSquareNumber(2, 2)\n</code></pre> <pre><code>Base of 2 with exponent of 2 gives: 4\n</code></pre>"},{"location":"programming/cpp/cpp/#incrementingdecrementing","title":"Incrementing/Decrementing","text":"<ul> <li>3 ways to do this, from least to most verbose</li> </ul>"},{"location":"programming/cpp/cpp/#methods","title":"Methods","text":""},{"location":"programming/cpp/cpp/#method-1","title":"Method 1","text":"<pre><code>double idx = 1;\nidx++;\nstd::cout &lt;&lt; idx;\n</code></pre> <pre><code>2\n</code></pre>"},{"location":"programming/cpp/cpp/#method-2","title":"Method 2","text":"<pre><code>idx += 1;\nstd::cout &lt;&lt; idx;\n</code></pre> <pre><code>3\n</code></pre>"},{"location":"programming/cpp/cpp/#method-3","title":"Method 3","text":"<pre><code>idx = idx + 1;\nstd::cout &lt;&lt; idx;\n</code></pre> <pre><code>4\n</code></pre>"},{"location":"programming/cpp/cpp/#prefixpostfix","title":"Prefix/Postfix","text":""},{"location":"programming/cpp/cpp/#prefix","title":"Prefix","text":"<ul> <li>This will change both incremented variable and the new variable you assign the incremented variable to</li> <li>Summary: both variables will have the same values</li> </ul> <pre><code>// Instantiate\ndouble a = 1;\ndouble b = 1;\n\n// Print original values\ncout &lt;&lt; \"Old a:\\t\" &lt;&lt; a &lt;&lt; \"\\n\";\ncout &lt;&lt; \"Old b:\\t\" &lt;&lt; b &lt;&lt; \"\\n\";\n\n// Prefix increment\na = ++b;\n\n// Print new values\ncout &lt;&lt; \"New a:\\t\" &lt;&lt; a &lt;&lt; \"\\n\";\ncout &lt;&lt; \"New b:\\t\" &lt;&lt; b &lt;&lt; \"\\n\";\n</code></pre> <pre><code>Old a:  1\nOld b:  1\nNew a:  2\nNew b:  2\n</code></pre>"},{"location":"programming/cpp/cpp/#postfix","title":"Postfix","text":"<ul> <li>This will change only the incremented variable but not the variable it's assigned to</li> <li>Summary: incremented variable will change but not the variable it was assigned to</li> </ul> <pre><code>// Instantiate\ndouble c = 2;\ndouble d = 2;\n\n// Print original values\ncout &lt;&lt; \"Old c:\\t\" &lt;&lt; c &lt;&lt; \"\\n\";\ncout &lt;&lt; \"Old d:\\t\" &lt;&lt; d &lt;&lt; \"\\n\";\n\n// Prefix increment\nc = d--;\n\n// Print new values, notice how only d decremented? c which is what d is assigned to doesn't change.\ncout &lt;&lt; \"New c:\\t\" &lt;&lt; c &lt;&lt; \"\\n\";\ncout &lt;&lt; \"New d:\\t\" &lt;&lt; d &lt;&lt; \"\\n\";\n</code></pre> <pre><code>Old c:  2\nOld d:  2\nNew c:  2\nNew d:  1\n</code></pre>"},{"location":"programming/cpp/cpp/#conditional-statements","title":"Conditional Statements","text":""},{"location":"programming/cpp/cpp/#if","title":"If","text":"<pre><code>int maxValue = 10;\n\n// Increment till 10\nfor (int i=0; i&lt;=10; i+=2)\n{\n// Stop if the number reaches 10 (inclusive)\nif (i == maxValue)\n{\ncout &lt;&lt; \"Reached max value!\";\ncout &lt;&lt; \"\\nValue is \" &lt;&lt; i &lt;&lt; endl;\n};\n};\n</code></pre> <pre><code>Reached max value!\nValue is 10\n</code></pre>"},{"location":"programming/cpp/cpp/#else","title":"Else","text":"<pre><code>int newMaxValue = 20;\n\n// Increment till 10\nfor (int i=0; i&lt;=10; i+=2)\n{\n// Stop if the number reaches 10 (inclusive)\nif (i == newMaxValue)\n{\ncout &lt;&lt; \"Reached max value!\";\ncout &lt;&lt; \"\\nValue is \" &lt;&lt; i &lt;&lt; endl;\n}\n// Else print current value\nelse\n{\ncout &lt;&lt; \"\\nCurrent Value is \" &lt;&lt; i &lt;&lt; endl;\n}\n}\n</code></pre> <pre><code>Current Value is 0\n\nCurrent Value is 2\n\nCurrent Value is 4\n\nCurrent Value is 6\n\nCurrent Value is 8\n\nCurrent Value is 10\n</code></pre>"},{"location":"programming/cpp/cpp/#logical-operators","title":"Logical Operators","text":""},{"location":"programming/cpp/cpp/#and","title":"And","text":"<pre><code>int varOneNew = 10;\nint varTwo = 10;\nint varCheckOne = 10;\nint varCheckTwo = 5;\n\n// This should print out\nif ((varOneNew == varCheckOne) &amp;&amp; (varTwo == varCheckOne))\n{\nstd::cout &lt;&lt; \"Both values equal to 10!\" &lt;&lt; std::endl;\n}\n\n// This should not print out as varTwo does not equal to 5\nif ((varOneNew == varCheckOne) &amp;&amp; (varTwo == varCheckTwo))\n{\nstd::cout &lt;&lt; \"VarOneNew equals to 10, VarTwo equals to 5\" &lt;&lt; std::endl;\n}\n</code></pre> <pre><code>Both values equal to 10!\n</code></pre>"},{"location":"programming/cpp/cpp/#or","title":"Or","text":"<pre><code>// On the contrary, this exact same statement would print out\n// as VarOne is equal to 10 and we are using an OR operator\nif ((varOneNew == varCheckOne) || (varTwo == varCheckTwo))\n{\nstd::cout &lt;&lt; \"VarOneNew equals to 10 or VarTwo equals to 5\" &lt;&lt; std::endl;\n}\n</code></pre> <pre><code>VarOneNew equals to 10 or VarTwo equals to 5\n</code></pre>"},{"location":"programming/cpp/cpp/#not","title":"Not","text":"<pre><code>// This would print out as VarTwo is not equal to 5\nif (varTwo != varCheckTwo)\n{\nstd::cout &lt;&lt; \"VarTwo (10) is not equal to VarCheckTwo (5).\" &lt;&lt; std::endl;\n}\n</code></pre> <pre><code>VarTwo (10) is not equal to VarCheckTwo (5).\n</code></pre>"},{"location":"programming/cpp/cpp/#getting-user-input","title":"Getting User Input","text":"<pre><code>using namespace std;\nlong double inputOne, inputTwo;\ncout &lt;&lt; \"This program multiplies 2 given numbers\\n\";\ncout &lt;&lt; \"Enter first number: \\n\";\ncin &gt;&gt; inputOne;\ncout &lt;&lt; \"Enter second number: \\n\";\ncin &gt;&gt; inputTwo;\ncout &lt;&lt; \"Multiplication value: \" &lt;&lt; inputOne * inputTwo &lt;&lt; endl;\n</code></pre> <pre><code>This program multiplies 2 given numbers\nEnter first number: \n10\nEnter second number: \n10\nMultiplication value: 100\n</code></pre>"},{"location":"programming/cpp/cpp/#loops","title":"Loops","text":""},{"location":"programming/cpp/cpp/#for-loop","title":"For Loop","text":"<pre><code>for (int i=0; i&lt;10; i+=1)\n{\ncout &lt;&lt; \"Value of i is: \" &lt;&lt; i &lt;&lt; endl;\n}\n</code></pre> <pre><code>Value of i is: 0\nValue of i is: 1\nValue of i is: 2\nValue of i is: 3\nValue of i is: 4\nValue of i is: 5\nValue of i is: 6\nValue of i is: 7\nValue of i is: 8\nValue of i is: 9\n</code></pre>"},{"location":"programming/cpp/cpp/#while-loop","title":"While Loop","text":"<pre><code>int idxWhile = 0;\n\nwhile (idxWhile &lt; 10)\n{\nidxWhile += 1;\ncout &lt;&lt; \"Value of while loop i is: \" &lt;&lt; idxWhile &lt;&lt; endl;\n}\n</code></pre> <pre><code>Value of while loop i is: 1\nValue of while loop i is: 2\nValue of while loop i is: 3\nValue of while loop i is: 4\nValue of while loop i is: 5\nValue of while loop i is: 6\nValue of while loop i is: 7\nValue of while loop i is: 8\nValue of while loop i is: 9\nValue of while loop i is: 10\n</code></pre>"},{"location":"programming/cpp/cpp/#while-loop-with-continuebreak","title":"While Loop with Continue/Break","text":"<pre><code>int idxWhileNew = 0;\n\nwhile (idxWhileNew &lt; 100)\n{\nidxWhileNew += 1;\ncout &lt;&lt; \"Value of while loop i is: \" &lt;&lt; idxWhile &lt;&lt; endl;\n\nif (idxWhileNew == 10)\n{\ncout &lt;&lt; \"Max value of 10 reached!\" &lt;&lt; endl;\nbreak;\n}\n}\n</code></pre> <pre><code>Value of while loop i is: 10\nValue of while loop i is: 10\nValue of while loop i is: 10\nValue of while loop i is: 10\nValue of while loop i is: 10\nValue of while loop i is: 10\nValue of while loop i is: 10\nValue of while loop i is: 10\nValue of while loop i is: 10\nValue of while loop i is: 10\nMax value of 10 reached!\n</code></pre>"},{"location":"programming/electron/electron/","title":"Electron","text":""},{"location":"programming/electron/electron/#why-electron","title":"Why Electron","text":"<p>We choose to cover electron as you can easily use it as a front-end application across any platforms (Windows, MacOS, Linux or even a mobile application) for your AI applications.</p>"},{"location":"programming/electron/electron/#installation-of-electron","title":"Installation of Electron","text":"<pre><code>npm i -D electron@latest\n</code></pre>"},{"location":"programming/electron/electron/#creating-electron-project","title":"Creating Electron Project","text":""},{"location":"programming/electron/electron/#critical-files","title":"Critical Files","text":"<p>You should have 3 base files <code>package.json</code>, <code>main.js</code> and <code>index.html</code> to have a basic application.</p> <pre><code>mkdir app\ncd app\n\nnpm init\n\ntouch main.js\ntouch index.html\n</code></pre>"},{"location":"programming/electron/electron/#edit-packagejson","title":"Edit package.json","text":"<ul> <li>When you run <code>npm init</code>, it should create a <code>package.json</code> file. But we need to make some tiny changes to leverage on electron.</li> <li>Key fields<ul> <li><code>name</code>: name of your app, can be anything</li> <li><code>version</code>: version of your app, can be anything</li> <li><code>main</code>: main javascript file, we recommend using <code>main.js</code></li> <li><code>scripts</code>: here you want to copy the whole <code>scripts</code> section to leverage on electron</li> <li><code>devDependencies</code>: electron version required</li> </ul> </li> </ul> <pre><code>{\n\"name\": \"dlw\",\n\"version\": \"0.1.0\",\n\"main\": \"main.js\",\n\"scripts\": {\n\"start\": \"electron .\"\n},\n\"devDependencies\": {\n\"electron\": \"^6.0.8\"\n}\n}\n</code></pre>"},{"location":"programming/electron/electron/#edit-mainjs","title":"Edit main.js","text":"<p>This beautiful boilerplate code is provided by Electron, full credits to them. </p> <pre><code>const { app, BrowserWindow } = require('electron')\n\n// Keep a global reference of the window object, if you don't, the window will\n// be closed automatically when the JavaScript object is garbage collected.\nlet win\n\nfunction createWindow () {\n// Create the browser window.\nwin = new BrowserWindow({\nwidth: 800,\nheight: 600,\nwebPreferences: {\nnodeIntegration: true\n}\n})\n\n// and load the index.html of the app.\nwin.loadFile('index.html')\n\n// Open the DevTools.\n// win.webContents.openDevTools()\n\n// Emitted when the window is closed.\nwin.on('closed', () =&gt; {\n// Dereference the window object, usually you would store windows\n// in an array if your app supports multi windows, this is the time\n// when you should delete the corresponding element.\nwin = null\n})\n}\n\n// This method will be called when Electron has finished\n// initialization and is ready to create browser windows.\n// Some APIs can only be used after this event occurs.\napp.on('ready', createWindow)\n\n// Quit when all windows are closed.\napp.on('window-all-closed', () =&gt; {\n// On macOS it is common for applications and their menu bar\n// to stay active until the user quits explicitly with Cmd + Q\nif (process.platform !== 'darwin') {\napp.quit()\n}\n})\n\napp.on('activate', () =&gt; {\n// On macOS it's common to re-create a window in the app when the\n// dock icon is clicked and there are no other windows open.\nif (win === null) {\ncreateWindow()\n}\n})\n\n// In this file you can include the rest of your app's specific main process\n// code. You can also put them in separate files and require them here.\n</code></pre>"},{"location":"programming/electron/electron/#edit-indexhtml","title":"Edit index.html","text":"<p>I modified this script from electron's boilerplate code where it will display critical dependencies' versions for your node, chrome and electron.</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n\n  &lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;title&gt;Dashboard&lt;/title&gt;\n  &lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;h1&gt;Dashboard&lt;/h1&gt;\n\n    &lt;h2&gt;Environment&lt;/h2&gt;\n        &lt;br/&gt;\n        Node: &lt;script&gt;document.write(process.versions.node)&lt;/script&gt;\n\n        &lt;br/&gt;\n        Chrome: &lt;script&gt;document.write(process.versions.chrome)&lt;/script&gt;\n\n        &lt;br/&gt;\n        Electron: &lt;script&gt;document.write(process.versions.electron)&lt;/script&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"programming/electron/electron/#starting-app","title":"Starting App","text":"<p>This will start your electron application.</p> <pre><code>npm start\n</code></pre>"},{"location":"programming/electron/electron/#packaging-electron-app","title":"Packaging Electron App","text":""},{"location":"programming/electron/electron/#wine","title":"Wine","text":"<p>The reason for installing Wine is being able to package Electron applications for the Windows platform, creating the executable file <code>app.exe</code> like any other application on Windows. The final aim of our tutorial is to package the app for Windows, MacOS and Ubuntu.</p>"},{"location":"programming/electron/electron/#installation-of-wine","title":"Installation of Wine","text":"<p>This assumes installation on Ubuntu 16.04 <code>xenial</code>, if you're on Ubuntu 18.04 or 19.04, change to <code>bionic</code> and <code>disco</code> respectively.</p> <p>Also, this works on 64-bit system architecture.</p> <pre><code>cd ~\nwget -qO - https://dl.winehq.org/wine-builds/winehq.key | sudo apt-key add -\nsudo apt-add-repository 'deb https://dl.winehq.org/wine-builds/ubuntu/ xenial main'\nsudo apt-get update\nsudo apt-get install --install-recommends winehq-stable\n\nsudo chown root:root ~/.wine\n</code></pre>"},{"location":"programming/electron/electron/#check-wine-version","title":"Check Wine Version","text":"<pre><code>wine --version\n</code></pre>"},{"location":"programming/electron/electron/#packaging-windows-application","title":"Packaging Windows Application","text":"<p>This packages the application churning the necessary files and the executable <code>app.exe</code> for windows 64 bit.</p> <pre><code>electron-packager ./app app --platform=win32 --arch=x64\n</code></pre>"},{"location":"programming/electron/electron/#python-scripts","title":"Python Scripts","text":""},{"location":"programming/electron/electron/#installing-python-node-package","title":"Installing Python Node Package","text":"<p>So we want to easily create Python scripts and run through Javascript in the Electron application. This can be done via <code>python-shell</code> npm package.</p> <pre><code>sudo npm install --save python-shell </code></pre>"},{"location":"programming/electron/electron/#creating-hello-from-js","title":"Creating \"Hello from JS\"","text":""},{"location":"programming/electron/electron/#javascript","title":"Javascript","text":"<p>In your <code>main.js</code> file, you would want to add the following code. </p> <p>This leverages on the <code>python-shell</code> package to send a message to <code>hello_world.py</code> and receive the message subsequently.</p> <pre><code>// Start Python shell\nlet {PythonShell} = require('python-shell')\n\n// Start shell for specific script for communicating\nlet pyshell = new PythonShell('./scripts/hello_world.py');\n\n// Send a message to the Python script via stdin\npyshell.send('Hello from JS');\n\n// Receive message from Python script\npyshell.on('message', function (message) {\nconsole.log(message);\n\n});\n\n// End the input stream and allow the process to exit\npyshell.end(function (err, code, signal) {\nif (err) throw err;\n//  console.log('The exit code was: ' + code);\n//  console.log('The exit signal was: ' + signal);\nconsole.log('finished');\n});\n</code></pre>"},{"location":"programming/electron/electron/#python","title":"Python","text":"<p>Create a folder <code>scripts</code> to hold all your Python scripts. Then create a Python file named <code>hello_world.py</code> with the following content.</p> <pre><code>import sys\n\nmsg_from_js = sys.stdin.read()\n\nprint(msg_from_js)\n</code></pre>"},{"location":"programming/electron/electron/#run-app","title":"Run App","text":"<p>Run via <code>npm start</code> and you'll see this in your bash output. Viola! We managed to call <code>hello_world.py</code> via <code>main.js</code> through the <code>python-shell</code> package. Next task, we will be passing this message to <code>index.html</code>.</p> <pre><code>Hello from JS\n\nfinished\n</code></pre>"},{"location":"programming/javascript/javascript/","title":"Javascript","text":"<p>Run Jupyter Notebook</p> <p>You can run the code for this section in this jupyter notebook link.</p>"},{"location":"programming/javascript/javascript/#installation-of-ijavascript","title":"Installation of iJavascript","text":"<p>This will enable the Javascript kernel to be installed in your jupyter notebook kernel list so you can play with Javascript easily.</p> <p>For now, the installation requires the ancient Python 2.7. Hopefully this changes in the future.</p> <pre><code>conda create -n py27 python=2.7\nconda activate py27\nsudo npm install -g --unsafe-perm ijavascript\nconda install jupyter\nijsinstall\njupyter notebook\n</code></pre>"},{"location":"programming/javascript/javascript/#variables-constants","title":"Variables &amp; Constants","text":""},{"location":"programming/javascript/javascript/#variable","title":"Variable","text":""},{"location":"programming/javascript/javascript/#declaring-variable","title":"Declaring Variable","text":"<pre><code>var randomNumber = 1.52\n</code></pre>"},{"location":"programming/javascript/javascript/#read-out-variable","title":"Read out Variable","text":"<pre><code>// Check value\nrandomNumber\n</code></pre> <pre><code>1.52\n</code></pre>"},{"location":"programming/javascript/javascript/#read-out-via-console","title":"Read out via Console","text":"<pre><code>// Using the console via log and error\nconsole.log(randomNumber)\nconsole.error(randomNumber)\n</code></pre> <pre><code>1.52\n\n\n1.52\n</code></pre>"},{"location":"programming/javascript/javascript/#constants","title":"Constants","text":""},{"location":"programming/javascript/javascript/#declaring-constant","title":"Declaring Constant","text":"<p>This is a weird one, because in Javascript when you declare a constant, you are essentially unable to overwrite the constant subsequently. It's useful when you want a fixed value.</p> <pre><code>const cannotOverwriteNumber = 2.22\n</code></pre>"},{"location":"programming/javascript/javascript/#read-out-constant","title":"Read out Constant","text":"<pre><code>cannotOverwriteNumber\n</code></pre> <pre><code>2.22\n</code></pre>"},{"location":"programming/javascript/javascript/#overwrite-constant-error","title":"Overwrite Constant (Error)","text":"<p>This will throw an error because this constant has been declared once and you cannot do it again.</p> <pre><code>const cannotOverwriteNumber = 1.52\n</code></pre> <pre><code>evalmachine.&lt;anonymous&gt;:1\n\nconst cannotOverwriteNumber = 1.52\n\n^\n\n\n\nSyntaxError: Identifier 'cannotOverwriteNumber' has already been declared\n\n    at evalmachine.&lt;anonymous&gt;:1:1\n\n    at Script.runInThisContext (vm.js:122:20)\n\n    at Object.runInThisContext (vm.js:329:38)\n\n    at run ([eval]:1054:15)\n\n    at onRunRequest ([eval]:888:18)\n\n    at onMessage ([eval]:848:13)\n\n    at process.emit (events.js:198:13)\n\n    at emit (internal/child_process.js:832:12)\n\n    at process._tickCallback (internal/process/next_tick.js:63:19)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"programming/numpycupy/linalg_numpy_cupy/","title":"Linear Algebra with NumPy and CuPy","text":"<p>In this section, we will be covering linear algebra and using numpy for CPU-based matrix manipulation and cupy for GPU-based matrix manipulation.</p>"},{"location":"programming/numpycupy/linalg_numpy_cupy/#types-of-matrices","title":"Types of Matrices","text":"<ul> <li>M x N Matrix<ul> <li>M rows and N columns</li> </ul> </li> <li>1 x N Matrix (Row Vector)<ul> <li>1 row and N columns</li> </ul> </li> <li>M x 1 Matrix (Column Vector)<ul> <li>M rows and 1 column</li> </ul> </li> <li>1 x 1 Matrix (Scalar)<ul> <li>1 row and 1 column</li> </ul> </li> <li>N x M Matrix from M x N Matrix (Transposed Matrix)<ul> <li>Swapping M rows to columns and N columns to rows where matrix \\(A\\) then becomes transposed matrix \\(A\\prime\\)</li> </ul> </li> <li>Symmetric Matrix<ul> <li>M x N Matrix is equal to N x M Transposed Matrix</li> </ul> </li> <li>M x M Matrix (Square Matrix)<ul> <li>M rows = N columns</li> </ul> </li> <li>Diagonal Matrix<ul> <li>all matrix elements are zeroes except for the diagonal</li> </ul> </li> <li>Identity matrix<ul> <li>all matrix elements are zeroes except for the diagonals being 1's </li> </ul> </li> </ul>"},{"location":"programming/numpycupy/linalg_numpy_cupy/#multiplication-and-subtraction","title":"Multiplication and Subtraction","text":"<ul> <li>Element-wise Matrix Addition/Subtraction<ul> <li>add/subtract every individual element \\(a_{ij}\\) of the first matrix \\(A_{M \\times N}\\) with \\(b_{ij}\\) of the second matrix \\(B_{M \\times N}\\)</li> </ul> </li> <li>Scalar Multiplication with Matrix<ul> <li>multiply scalar \\(\\lambda\\) with every individual element \\(a_{ij}\\) of the matrix \\(A_{M \\times N}\\)</li> </ul> </li> <li>Matrix Multiplication<ul> <li>can only be done where \\(A_{M \\times N}\\) and \\(B_{N \\times O}\\) have a coinciding dimension \\(N\\) which is the same to yield a new matrix \\(C_{N \\times O}\\) of \\(N \\times O\\) dimension</li> <li>new element \\(c_{ij} = \\sum _{q=1} ^N a_{iq} b_{qj}\\)</li> <li>essentially the first row and first column element of the new matrix is equal to the summation element-wise multiplication of the first row of matrix \\(A\\) with the first column of matrix \\(B\\)</li> </ul> </li> </ul>"},{"location":"programming/numpycupy/linalg_numpy_cupy/#determinant-of-square-matrix","title":"Determinant of Square Matrix","text":""},{"location":"programming/numpycupy/linalg_numpy_cupy/#determinant-of-2-x-2-square-matrix","title":"Determinant of 2 x 2 Square Matrix","text":"<p>The determinant of a 2x2 square matrix can be derived by multiplying all the elements on the main diagonal and subtracting the multiplication of all the elements in the other diagonal which will result in a scalar value.</p> \\[ A = \\begin{bmatrix}a &amp; b\\\\c &amp; d\\end{bmatrix} \\] \\[\\det(A) = | A | =  ad - bc \\]"},{"location":"programming/numpycupy/linalg_numpy_cupy/#determinant-of-3-x-3-square-matrix","title":"Determinant of 3 x 3 Square Matrix","text":"\\[ \\begin{bmatrix} a &amp; b &amp;c \\\\ d&amp; e &amp;f \\\\ g&amp; h &amp;i \\end{bmatrix} \\\\ = a (-1)^{(1+1)}\\det \\begin{bmatrix} e &amp; f\\\\ h &amp; i \\end{bmatrix} + b (-1)^{(1+2)} \\det \\begin{bmatrix} d &amp; f\\\\ g &amp; i \\end{bmatrix} + c (-1)^{(1+3)} \\det \\begin{bmatrix} d &amp; e\\\\ g &amp; h \\end{bmatrix} \\]"},{"location":"programming/numpycupy/linalg_numpy_cupy/#determinant-types","title":"Determinant Types","text":"<ul> <li>singular matrix: \\(\\det(A) = 0\\) where at least 2 rows/columns are linearly dependent</li> <li>non-singular: \\(\\det(A) \\neq 0\\) where no rows/columns are linearly dependent (matrix has full rank)</li> </ul>"},{"location":"programming/numpycupy/linalg_numpy_cupy/#inverse-of-a-non-singular-square-matrix","title":"Inverse of a Non-Singular Square Matrix","text":""},{"location":"programming/numpycupy/linalg_numpy_cupy/#unique-property","title":"Unique Property","text":"<p>The inverse can be calculated only if it is a non-singular square matrix hence it is useful to first calculate the determinant. Also the inverse of the matrix yields a useful and unique property where</p> \\[ AA^{-1} = A^{-1}A = I \\]"},{"location":"programming/numpycupy/linalg_numpy_cupy/#inverse-and-identity-of-2-x-2-non-singular-square-matrix","title":"Inverse and Identity of 2 x 2 Non-Singular Square Matrix","text":"\\[ A=\\begin{bmatrix}a &amp; b \\\\c &amp; d \\end{bmatrix} \\] \\[ A^{-1}=\\frac{1}{ad-bc}\\begin{bmatrix}d &amp; -b \\\\-c &amp; a \\end{bmatrix} \\] \\[ \\begin{array}{lcl}AA^{-1}&amp;=&amp;\\begin{bmatrix}a &amp; b \\\\c &amp; d \\end{bmatrix}\\frac{1}{ad-bc}\\begin{bmatrix}d &amp; -b \\\\-c &amp; a \\end{bmatrix}\\\\ &amp;=&amp;\\frac{1}{ad-bc}\\begin{bmatrix}ad+b(-c) &amp; a(-b)+b(a) \\\\cd+d(-c) &amp; c(-b)+d(a) \\end{bmatrix}\\\\ &amp;=&amp;\\begin{bmatrix}1 &amp; 0\\\\0 &amp; 1 \\end{bmatrix}\\end{array} \\]"},{"location":"programming/numpycupy/linalg_numpy_cupy/#linear-system-of-equations-in-matrices","title":"Linear System of Equations in Matrices","text":"<p>We can express many linear equations in the form of matrices for example \\(AX= B\\)</p> <p>Where we have \\(A\\) representing our parameters, \\(B\\) representing our input variables and \\(B\\) representing our constant/bias variables.</p> \\[ A = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1N} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2N} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{M1} &amp; a_{M2} &amp; \\cdots &amp; a_{MN} \\end{bmatrix}, X = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_M \\end{bmatrix}, B = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_M \\end{bmatrix} \\] <p>If we find \\(\\det (A)\\) to be non-zero where the matrix is a non-singular matrix, the system of equations would have a unique solution. Hence, given \\(A\\) and \\(B\\), we can find out the unique solution in \\(B\\).</p> \\[  AX = B \\\\ A^{-1}AX = A^{-1}B \\\\ IX = A^{-1}B \\\\ X = A^{-1}B \\]"},{"location":"programming/numpycupy/linalg_numpy_cupy/#solving-system-of-equations-for-square-and-non-singular","title":"Solving System of Equations for Square and Non-Singular","text":"<p>If the matrix A is square and \\(det(A) \\neq 0\\) (non-singular, inverse matrix can be calculated), then Cramer's rule can be applied to solve the linear system of equations.</p>"},{"location":"programming/numpycupy/linalg_numpy_cupy/#process-for-a-2-x-2-non-singular-matrix-a","title":"Process for a 2 x 2 Non-Singular Matrix A","text":"<ul> <li>Replace First Column of \\(A\\) with \\(B\\) to get \\(A_1\\) $$ x_1 = \\frac{\\det(A_1)}{\\det(A)} $$</li> <li>Replace Second Column of \\(A\\) with \\(B\\) to get \\(A_2\\) $$ x_2 = \\frac{\\det(A_2)}{\\det(A)} $$</li> <li>Repeat till all columns covered for bigger matrices</li> </ul>"},{"location":"programming/numpycupy/linalg_numpy_cupy/#eigenvalue-characteristic-root-and-eigenvalue-characteristic-value","title":"Eigenvalue (characteristic root) and eigenvalue (characteristic value)","text":"<p>Given a square matrix \\(A\\), eigenvalue \\(\\lambda\\) and eigenvector \\(v\\) where \\(v \\neq 0\\), we have:</p> \\[ A v = \\lambda v \\\\ A v = \\lambda I v \\\\ Av - \\lambda I v = 0 \\\\ (A - \\lambda I)v = 0 \\\\ \\] <p>Since  \\(v \\neq 0\\) then we have characteristic matrix \\((A - \\lambda I) = 0\\).</p>"},{"location":"programming/numpycupy/linalg_numpy_cupy/#solving-eigens","title":"Solving eigens","text":"<ul> <li>We can solve for the determinant of the characteristic matrix (characteristic polynomial) \\(|A - \\lambda I| = 0\\) through this characteristic equation.<ul> <li>We will get multiple values of \\(\\lambda\\)</li> </ul> </li> <li>Substitute \\(\\lambda\\) into \\((A - \\lambda I)v = 0\\), solve for \\(v\\)<ul> <li>If infinite solution (no constant values for x and y), impose uniqueness with \\(v\\prime v  = 1\\) in this case \\(\\begin{bmatrix} x &amp; y \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = x^2 + y^2 = 1\\)</li> <li>Substitute y into \\(v\\prime v  = 1\\) to solve for x, then solve for y</li> <li>Substitute x and y to solve for \\(v\\)</li> </ul> </li> <li>Simple eigenvector \\(\\lambda _1 \\neq \\lambda _2\\) </li> <li>Repeated/double eigenvector \\(\\lambda _1 = \\lambda _2\\)</li> </ul>"},{"location":"programming/numpycupy/linalg_numpy_cupy/#properties-of-eigenvalue-and-eigenvector","title":"Properties of eigenvalue and eigenvector","text":"<ul> <li>If \\(|A - \\lambda I| = 0\\), singular therefore infinite solutions, hence:<ul> <li>\\(\\lambda \\gt 0\\): positive definite</li> <li>\\(\\lambda \\ge 0\\): positive semi-definite</li> <li>\\(\\lambda \\lt 0\\): negative definite</li> <li>\\(\\lambda \\le 0\\): negative semi-definite</li> </ul> </li> </ul>"},{"location":"programming/plotting/intro/","title":"Plotting","text":"Packages you will learn to use <ol> <li>Fast professional static plots: ggplot2 and rpy2</li> <li>Complicated static plots: matplotlib</li> <li>Live stream and/or interactive plots: plotly</li> <li>Dashboard: streamlit</li> </ol> <p>Typically with tremendous mastery of matplotlib, you will be able to create any good looking plots static or interactive. However, it takes many more lines of code and is relatively complicated.</p> <p>Thus, before we go there, we will be covering the fastest way to create professional looking static plots with rpy2 and ggplot rapidly. It is unconventional but it yields the fastest result for time-sensitive learners. We'll then move backwards to then create more complicated plots with matplotlib. Subsequently we'll create live stream and interactive plots with plotly. Finally we will be bringing everything together with streamlit as a dashboard.</p> <p>As usual, you can go freestyle and just use any of our open-source material you think is useful and you do not need to follow the sequence we are suggesting.</p>"},{"location":"programming/python/python/","title":"Python","text":""},{"location":"programming/python/python/#lists","title":"Lists","text":""},{"location":"programming/python/python/#creating-list-manual-fill","title":"Creating List: Manual Fill","text":"<pre><code>lst = [0, 1, 2 ,3]\nprint(lst)\n</code></pre> <pre><code>[0, 1, 2, 3]\n</code></pre>"},{"location":"programming/python/python/#creating-list-list-comprehension","title":"Creating List: List Comprehension","text":"<pre><code>lst = [i for i in range(4)]\nprint(lst)\n</code></pre> <pre><code>[0, 1, 2, 3]\n</code></pre>"},{"location":"programming/python/python/#joining-list-with-blanks","title":"Joining List with Blanks","text":"<pre><code># To use .join(), your list needs to be of type string\nlst_to_string = list(map(str, lst))\n\n# Join the list of strings\nlst_join = ' '.join(lst_to_string)\nprint(lst_join)\n</code></pre> <pre><code>0 1 2 3\n</code></pre>"},{"location":"programming/python/python/#joining-list-with-comma","title":"Joining List with Comma","text":"<pre><code># Join the list of strings\nlst_join = ', '.join(lst_to_string)\nprint(lst_join)\n</code></pre> <pre><code>0, 1, 2, 3\n</code></pre>"},{"location":"programming/python/python/#checking-lists-equal-method-1","title":"Checking Lists Equal: Method 1","text":"<p>Returns <code>True</code> if equal, and <code>False</code> if unequal</p> <pre><code>lst_unequal = [1, 1, 2, 3, 4, 4]\nlst_equal = [0, 0, 0, 0, 0, 0]\n\nprint('-'*50)\nprint('Unequal List')\nprint('-'*50)\n\nprint(lst_unequal[1:])\nprint(lst_unequal[:-1])\nbool_equal = lst_unequal[1:] == lst_unequal[:-1]\nprint(bool_equal)\n\nprint('-'*50)\nprint('Equal List')\nprint('-'*50)\n\nprint(lst_equal[1:])\nprint(lst_equal[:-1])\nbool_equal = lst_equal[1:] == lst_equal[:-1]\nprint(bool_equal)\n</code></pre> <pre><code>--------------------------------------------------\nUnequal List\n--------------------------------------------------\n[1, 2, 3, 4, 4]\n[1, 1, 2, 3, 4]\nFalse\n--------------------------------------------------\nEqual List\n--------------------------------------------------\n[0, 0, 0, 0, 0]\n[0, 0, 0, 0, 0]\nTrue\n</code></pre>"},{"location":"programming/python/python/#checking-lists-equal-method-2","title":"Checking Lists Equal: Method 2","text":"<p>Returns <code>True</code> if equal, and <code>False</code> if unequal. Here, <code>all</code> essentially checks that there is no <code>False</code> in the list. </p> <pre><code>print('-'*50)\nprint('Unequal List')\nprint('-'*50)\n\nlst_check = [i == lst_unequal[0] for i in lst_unequal]\nbool_equal = all(lst_check)\nprint(bool_equal)\n\nprint('-'*50)\nprint('Equal List')\nprint('-'*50)\n\nlst_check = [i == lst_equal[0] for i in lst_equal]\nbool_equal = all(lst_check)\nprint(bool_equal)\n</code></pre> <pre><code>--------------------------------------------------\nUnequal List\n--------------------------------------------------\nFalse\n--------------------------------------------------\nEqual List\n--------------------------------------------------\nTrue\n</code></pre>"},{"location":"programming/python/python/#sets","title":"Sets","text":""},{"location":"programming/python/python/#removing-duplicate-from-list","title":"Removing Duplicate from List","text":"<p>Sets can be very useful for quickly removing duplicates from a list, essentially finding unique values</p> <pre><code>lst_one = [1, 2, 3, 5]\nlst_two = [1, 1, 2, 4]\nlst_both = lst_one + lst_two\nlst_no_duplicate = list(set(lst_both))\n\nprint(f'Original Combined List {lst_both}')\nprint(f'No Duplicated Combined List {lst_no_duplicate}')\n</code></pre> <pre><code>Original Combined List [1, 2, 3, 5, 1, 1, 2, 4]\nNo Duplicated Combined List [1, 2, 3, 4, 5]\n</code></pre>"},{"location":"programming/python/python/#lambda-map-filter-reduce-partial","title":"Lambda, map, filter, reduce, partial","text":""},{"location":"programming/python/python/#lambda","title":"Lambda","text":"<p>The syntax is simple <code>lambda your_variables: your_operation</code></p>"},{"location":"programming/python/python/#add-function","title":"Add Function","text":"<pre><code>add = lambda x, y: x + y\nadd(2, 3)\n</code></pre> <pre><code>5\n</code></pre>"},{"location":"programming/python/python/#multiply-function","title":"Multiply Function","text":"<pre><code>multiply = lambda x, y: x * y \nmultiply(2, 3)\n</code></pre> <pre><code>6\n</code></pre>"},{"location":"programming/python/python/#map","title":"Map","text":""},{"location":"programming/python/python/#create-list","title":"Create List","text":"<pre><code>lst = [i for i in range(11)]\nprint(lst)\n</code></pre> <pre><code>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n</code></pre>"},{"location":"programming/python/python/#map-square-function-to-list","title":"Map Square Function to List","text":"<pre><code>square_element = map(lambda x: x**2, lst)\n\n# This gives you a map object\nprint(square_element)\n\n# You need to explicitly return a list\nprint(list(square_element))\n</code></pre> <pre><code>&lt;map object at 0x7f08c8620438&gt;\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n</code></pre>"},{"location":"programming/python/python/#create-multiple-list","title":"Create Multiple List","text":"<pre><code>lst_1 = [1, 2, 3, 4]\nlst_2 = [2, 4, 6, 8]\nlst_3 = [3, 6, 9, 12]\n</code></pre>"},{"location":"programming/python/python/#map-add-function-to-multiple-lists","title":"Map Add Function to Multiple Lists","text":"<pre><code>add_elements = map(lambda x, y, z : x + y + z, lst_1, lst_2, lst_3)\nprint(list(add_elements))\n</code></pre> <pre><code>[6, 12, 18, 24]\n</code></pre>"},{"location":"programming/python/python/#filter","title":"Filter","text":""},{"location":"programming/python/python/#create-list_1","title":"Create List","text":"<pre><code>lst = [i for i in range(10)]\nprint(lst)\n</code></pre> <pre><code>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n</code></pre>"},{"location":"programming/python/python/#filter-multiples-of-3","title":"Filter multiples of 3","text":"<pre><code>multiples_of_three = filter(lambda x: x % 3 == 0, lst)\nprint(list(multiples_of_three))\n</code></pre> <pre><code>[0, 3, 6, 9]\n</code></pre>"},{"location":"programming/python/python/#reduce","title":"Reduce","text":"<p>The syntax is <code>reduce(function, sequence)</code>. The function is applied to the elements in the list in a sequential manner. Meaning if <code>lst = [1, 2, 3, 4]</code> and you have a sum function, you would arrive with <code>((1+2) + 3) + 4</code>.</p> <pre><code>from functools import reduce\nsum_all = reduce(lambda x, y: x + y, lst)\n# Here we've 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9\nprint(sum_all)\nprint(1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9)\n</code></pre> <pre><code>45\n45\n</code></pre>"},{"location":"programming/python/python/#partial","title":"Partial","text":"<p>Allows us to predefine and freeze a function's argument. Combined with lambda, it allows us to have more flexibility beyond lambda's restriction of a single line.</p> <pre><code>from functools import partial\n\ndef display_sum_three(a, b, c):\n    sum_all = a + b + c\n    print(f'Sum is {sum_all}')\n\nfixed_args_func = partial(display_sum_three, b=3, c=4)\n\n# Given fixed arguments b=3 and c=4\n# We add the new variable against the fixed arguments\nvar_int = 1\nfixed_args_func(var_int)\n\n# More advanced mapping with partial\n# Add a variable from 0 to 9 to the constants\nprint('-'*50)\n_ = list(map(fixed_args_func, list(range(10))))\n\n# How about using with lambda to modifying constants without\n# declaring your function again?\nprint('-'*50)\n_ = list(map(lambda x: fixed_args_func(x, b=2), list(range(10))))\n</code></pre> <pre><code>Sum is 8\n--------------------------------------------------\nSum is 7\nSum is 8\nSum is 9\nSum is 10\nSum is 11\nSum is 12\nSum is 13\nSum is 14\nSum is 15\nSum is 16\n--------------------------------------------------\nSum is 6\nSum is 7\nSum is 8\nSum is 9\nSum is 10\nSum is 11\nSum is 12\nSum is 13\nSum is 14\nSum is 15\n</code></pre>"},{"location":"programming/python/python/#generators","title":"Generators","text":"<ul> <li> <p>Why: <code>generators</code> are typically more memory-efficient than using simple <code>for loops</code></p> <ul> <li>Imagine wanting to sum digits 0 to 1 trillion, using a list containing those numbers and summing them would be very RAM memory-inefficient.</li> <li>Using a generator would allow you to sum one digit sequentially, staggering the RAM memory usage in steps.</li> </ul> </li> <li> <p>What: <code>generator</code> basically a function that returns an iterable object where we can iterate one bye one</p> </li> <li>Types: generator functions and generator expressions</li> <li>Dependencies: we need to install a memory profiler, so install via <code>pip install memory_profiler</code></li> </ul>"},{"location":"programming/python/python/#simple-custom-generator-function-example-sum-1-to-1000000","title":"Simple custom generator function example: sum 1 to 1,000,000","text":"<ul> <li>What: let's create a simple generator, allowing us to iterate through the digits 1 to 1,000,000 (inclusive) one by one with an increment of 1 at each step and summing them</li> <li>How: 2 step process with a <code>while</code> and a <code>yield</code></li> </ul> <pre><code># Load memory profiler\n%load_ext memory_profiler\n\n# Here we take a step from 1\ndef create_numbers(end_number):\n    current_number = 1\n\n    # Step 1: while\n    while current_number &lt;= end_number:\n        # Step 2: yield\n        yield current_number\n\n        # Add to current number\n        current_number += 1\n\n# Here we sum the digits 1 to 100 (inclusive) and time it\n%memit total = sum(create_numbers(1e6))\nprint(total)\n</code></pre> <pre><code>peak memory: 46.50 MiB, increment: 0.28 MiB\n500000500000\n</code></pre>"},{"location":"programming/python/python/#without-generator-function-sum-with-list","title":"Without generator function: sum with list","text":"<ul> <li>Say we don't use a generator, and have a list of digits 0 to 1,000,000 (inclusive) in memory then sum them. </li> <li>Notice how this is double the memory than using a generator!</li> </ul> <pre><code>%memit total = sum(list(range(int(1e6) + 1)))\nprint(total)\n</code></pre> <pre><code>peak memory: 85.14 MiB, increment: 38.38 MiB\n500000500000\n</code></pre>"},{"location":"programming/python/python/#without-generator-function-sum-with-for-loop","title":"Without generator function: sum with for loop","text":"<ul> <li>Say we don't use a generator and don't put all our numbers into a list </li> <li>Notiice how this is much better than summing a list but still worst than a generator in terms of memory?</li> </ul> <pre><code>def sum_with_loop(end_number):\n    total = 0\n    for i in range(end_number + 1):\n        i += 1\n        total += i\n\n    return total\n\n%memit total = sum_with_loop(int(1e6))\nprint(total)\n</code></pre> <pre><code>peak memory: 54.49 MiB, increment: 0.00 MiB\n500001500001\n</code></pre>"},{"location":"programming/python/python/#generator-expression","title":"Generator expression","text":"<ul> <li>Like list/dictionary expressions, we can have generator expressions too</li> <li>We can quickly create generators this way, allowing us to make computations on the fly rather than pre-compute on a whole list/array of numbers<ul> <li>This is more memory efficient</li> </ul> </li> </ul> <pre><code># Define the list\nlist_of_numbers = list(range(10))\n\n# Find square root using the list comprehension\nlist_of_results = [number ** 2 for number in list_of_numbers]\nprint(list_of_results)\n\n# Use generator expression to calculate the square root\ngenerator_of_results = (number ** 2 for number in list_of_numbers)\nprint(generator_of_results)\n\nfor idx in range(10):\n    print(next(generator_of_results))\n</code></pre> <pre><code>[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n&lt;generator object &lt;genexpr&gt; at 0x7f08c85aa4f8&gt;\n0\n1\n4\n9\n16\n25\n36\n49\n64\n81\n</code></pre>"},{"location":"programming/python/python/#decorators","title":"Decorators","text":"<ul> <li>This allows us to to modify our original function or even entirely replace it without changing the function's code. </li> <li>It sounds mind-boggling, but a simple case I would like to illustrate here is using decorators for consistent logging (formatted print statements).</li> <li>For us to understand decorators, we'll first need to understand:<ul> <li><code>first class objects</code></li> <li><code>*args</code></li> <li><code>*kwargs</code></li> </ul> </li> </ul>"},{"location":"programming/python/python/#first-class-objects","title":"First Class Objects","text":"<pre><code>def outer():\n    def inner():\n        print('Inside inner() function.')\n\n    # This returns a function.\n    return inner\n\n# Here, we are assigning `outer()` function to the object `call_outer`.\ncall_outer = outer()\n\n# Then we call `call_outer()` \ncall_outer()\n</code></pre> <pre><code>Inside inner() function.\n</code></pre>"},{"location":"programming/python/python/#args","title":"*args","text":"<ul> <li>This is used to indicate that positional arguments should be stored in the variable args</li> <li><code>*</code> is for iterables and positional parameters</li> </ul> <pre><code># Define dummy function\ndef dummy_func(*args):\n    print(args)\n\n# * allows us to extract positional variables from an iterable when we are calling a function\ndummy_func(*range(10))\n\n# If we do not use *, this would happen\ndummy_func(range(10))\n\n# See how we can have varying arguments?\ndummy_func(*range(2))\n</code></pre> <pre><code>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\n(range(0, 10),)\n(0, 1)\n</code></pre>"},{"location":"programming/python/python/#kwargs","title":"**kwargs","text":"<ul> <li><code>**</code> is for dictionaries &amp; key/value pairs</li> </ul> <pre><code># New dummy function\ndef dummy_func_new(**kwargs):\n    print(kwargs)\n\n# Call function with no arguments\ndummy_func_new()\n\n# Call function with 2 arguments\ndummy_func_new(a=0, b=1)\n\n# Again, there's no limit to the number of arguments.\ndummy_func_new(a=0, b=1, c=2)\n\n# Or we can just pass the whole dictionary object if we want\nnew_dict = {'a': 0, 'b': 1, 'c': 2, 'd': 3}\ndummy_func_new(**new_dict)\n</code></pre> <pre><code>{}\n{'a': 0, 'b': 1}\n{'a': 0, 'b': 1, 'c': 2}\n{'a': 0, 'b': 1, 'c': 2, 'd': 3}\n</code></pre>"},{"location":"programming/python/python/#decorators-as-logger-and-debugging","title":"Decorators as Logger and Debugging","text":"<ul> <li>A simple way to remember the power of decorators is that the decorator (the nested function illustrated below) can<ul> <li>(1) access the passed arguments of the decorated function and</li> <li>(2) access the decorated function</li> </ul> </li> <li>Therefore this allows us to modify the decorated function without changing the decorated function</li> </ul> <pre><code># Create a nested function that will be our decorator\ndef function_inspector(func):\n    def inner(*args, **kwargs):\n        result = func(*args, **kwargs)\n        print(f'Function args: {args}')\n        print(f'Function kwargs: {kwargs}')\n        print(f'Function return result: {result}')\n        return result\n    return inner\n\n# Decorate our multiply function with our logger for easy logging\n# Of arguments pass to the function and results returned\n@function_inspector\ndef multiply_func(num_one, num_two):\n    return num_one * num_two\n\nmultiply_result = multiply_func(num_one=1, num_two=2)\n</code></pre> <pre><code>Function args: ()\nFunction kwargs: {'num_one': 1, 'num_two': 2}\nFunction return result: 2\n</code></pre>"},{"location":"programming/python/python/#dates","title":"Dates","text":""},{"location":"programming/python/python/#get-current-date","title":"Get Current Date","text":"<pre><code>import datetime\nnow = datetime.datetime.now()\nprint(now)\n</code></pre> <pre><code>2019-08-12 14:20:45.604849\n</code></pre>"},{"location":"programming/python/python/#get-clean-string-current-date","title":"Get Clean String Current Date","text":"<pre><code># YYYY-MM-DD\nnow.date().strftime('20%y-%m-%d')\n</code></pre> <pre><code>'2019-08-12'\n</code></pre>"},{"location":"programming/python/python/#count-business-days","title":"Count Business Days","text":"<pre><code># Number of business days in a month from Jan 2019 to Feb 2019\nimport numpy as np\ndays = np.busday_count('2019-01', '2019-02')\nprint(days)\n</code></pre> <pre><code>23\n</code></pre>"},{"location":"programming/python/python/#progress-bars","title":"Progress Bars","text":""},{"location":"programming/python/python/#tqdm","title":"TQDM","text":"<p>Simple progress bar via <code>pip install tqdm</code></p> <pre><code>from tqdm import tqdm\nimport time\nfor i in tqdm(range(100)):\n    time.sleep(0.1)\n    pass\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:10&lt;00:00,  9.91it/s]\n</code></pre>"},{"location":"programming/python/python/#check-paths","title":"Check Paths","text":""},{"location":"programming/python/python/#check-path-exists","title":"Check Path Exists","text":"<ul> <li>Check if directory exists</li> </ul> <pre><code>import os\ndirectory='new_dir'\nprint(os.path.exists(directory))\n\n# Magic function to list all folders\n!ls -d */\n</code></pre> <pre><code>False\nls: cannot access '*/': No such file or directory\n</code></pre>"},{"location":"programming/python/python/#check-path-exists-otherwise-create-folder","title":"Check Path Exists Otherwise Create Folder","text":"<ul> <li>Check if directory exists, otherwise make folder</li> </ul> <pre><code>if not os.path.exists(directory):\n    os.makedirs(directory)\n\n# Magic function to list all folders\n!ls -d */\n\n# Remove directory\n!rmdir new_dir\n</code></pre> <pre><code>new_dir/\n</code></pre>"},{"location":"programming/python/python/#exception-handling","title":"Exception Handling","text":""},{"location":"programming/python/python/#try-except-finally-error","title":"Try, Except, Finally: Error","text":"<ul> <li>This is very handy and often exploited to patch up (save) poorly written code</li> <li>You can use general exceptions or specific ones like <code>ValueError</code>, <code>KeyboardInterrupt</code> and <code>MemoryError</code> to name a few</li> </ul> <pre><code>value_one = 'a'\nvalue_two = 2\n\n# Try the following line of code\ntry:\n    final_sum = value_one / value_two\n    print('Code passed!')\n# If the code above fails, code nested under except will be executed\nexcept:\n    print('Code failed!')\n# This will run no matter whether the nested code in try or except is executed\nfinally:\n    print('Ran code block regardless of error or not.')\n</code></pre> <pre><code>Code failed!\nRan code block regardless of error or not.\n</code></pre>"},{"location":"programming/python/python/#try-except-finally-no-error","title":"Try, Except, Finally: No Error","text":"<ul> <li>There won't be errors because you can divide 4 with 2</li> </ul> <pre><code>value_one = 4\nvalue_two = 2\n\n# Try the following line of code\ntry:\n    final_sum = value_one / value_two\n    print('Code passed!')\n# If the code above fails, code nested under except will be executed\nexcept:\n    print('Code failed!')\n# This will run no matter whether the nested code in try or except is executed\nfinally:\n    print('Ran code block regardless of error or not.')\n</code></pre> <pre><code>Code passed!\nRan code block regardless of error or not.\n</code></pre>"},{"location":"programming/python/python/#assertion","title":"Assertion","text":"<ul> <li>This comes in handy when you want to enforce strict requirmenets of a certain value, shape, value type, or others</li> </ul> <pre><code>for i in range(10):\n    assert i &lt;= 5, 'Value is more than 5, rejected'\n    print(f'Passed assertion for value {i}')\n</code></pre> <pre><code>Passed assertion for value 0\nPassed assertion for value 1\nPassed assertion for value 2\nPassed assertion for value 3\nPassed assertion for value 4\nPassed assertion for value 5\n\n\n\n---------------------------------------------------------------------------\n\nAssertionError                            Traceback (most recent call last)\n\n&lt;ipython-input-2-d9d077e139a9&gt; in &lt;module&gt;\n      1 for i in range(10):\n----&gt; 2     assert i &lt;= 5, 'Value is more than 5, rejected'\n      3     print(f'Passed assertion for value {i}')\n\n\nAssertionError: Value is more than 5, rejected\n</code></pre>"},{"location":"programming/python/python/#asynchronous","title":"Asynchronous","text":""},{"location":"programming/python/python/#concurrency-parallelism-asynchronous","title":"Concurrency, Parallelism, Asynchronous","text":"<ul> <li>Concurrency (single CPU core): multiple threads on a single core running in sequence, only 1 thread is making progress at any point<ul> <li>Think of 1 human, packing a box then wrapping the box</li> </ul> </li> <li>Parallelism (mutliple GPU cores): multiple threads on multiple cores running in parallel, multiple threads can be making progress<ul> <li>Think of 2 humans, one packing a box, another wrapping the box</li> </ul> </li> <li>Asynchronous: concurrency but with a more dynamic system that moves amongst threads more efficiently rather than waiting for a task to finish then moving to the next task<ul> <li>Python's <code>asyncio</code> allows us to code asynchronously</li> <li>Benefits:<ul> <li>Scales better if you need to wait on a lot of processes<ul> <li>Less memory (easier in this sense) to wait on thousands of co-routines than running on thousands of threads</li> </ul> </li> <li>Good for IO bound uses like reading/saving from databases while subsequently running other computation</li> <li>Easier management than multi-thread processing like in parallel programming<ul> <li>In the sense that everything operates sequentially in the same memory space</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"programming/python/python/#asynchronous-key-components","title":"Asynchronous Key Components","text":"<ul> <li>The three main parts are (1) coroutines and subroutines, (2) event loops, and (3) future.<ul> <li>Co-routine and subroutines<ul> <li>Subroutine: the usual function</li> <li>Coroutine: this allows us to maintain states with memory of where things stopped so we can swap amongst subroutines<ul> <li><code>async</code> declares a function as a coroutine</li> <li><code>await</code> to call a coroutine</li> </ul> </li> </ul> </li> <li>Event loops</li> <li>Future</li> </ul> </li> </ul>"},{"location":"programming/python/python/#synchronous-2-function-calls","title":"Synchronous 2 Function Calls","text":"<pre><code>import timeit\ndef add_numbers(num_1, num_2):\n    print('Adding')\n    time.sleep(1)\n    return num_1 + num_2\n\ndef display_sum(num_1, num_2):\n    total_sum = add_numbers(num_1, num_2)\n    print(f'Total sum {total_sum}')\n\ndef main():\n    display_sum(2, 2)\n    display_sum(2, 2)\n\nstart = timeit.default_timer()\n\nmain()\n\nend = timeit.default_timer()\ntotal_time = end - start\n\nprint(f'Total time {total_time:.2f}s')\n</code></pre> <pre><code>Adding\nTotal sum 4\nAdding\nTotal sum 4\nTotal time 2.00s\n</code></pre>"},{"location":"programming/python/python/#parallel-2-function-calls","title":"Parallel 2 Function Calls","text":"<pre><code>from multiprocessing import Pool\nfrom functools import partial\n\nstart = timeit.default_timer()\n\npool = Pool()\nresult = pool.map(partial(display_sum, num_2=2), [2, 2]) \n\nend = timeit.default_timer()\ntotal_time = end - start\n\nprint(f'Total time {total_time:.2f}s')\n</code></pre> <pre><code>Adding\nAdding\nTotal sum 4\nTotal sum 4\nTotal time 1.08s\n</code></pre>"},{"location":"programming/python/python/#asynchronous-2-function-calls","title":"Asynchronous 2 Function Calls","text":"<p>For this use case, it'll take half the time compared to a synchronous application and slightly faster than parallel application (although not always true for parallel except in this case)</p> <pre><code>import asyncio\nimport timeit\nimport time\n\nasync def add_numbers(num_1, num_2):\n    print('Adding')\n    await asyncio.sleep(1)\n    return num_1 + num_2 \n\nasync def display_sum(num_1, num_2):\n    total_sum = await add_numbers(num_1, num_2)\n    print(f'Total sum {total_sum}')\n\nasync def main():\n    # .gather allows us to group subroutines\n    await asyncio.gather(display_sum(2, 2), \n                         display_sum(2, 2))\n\nstart = timeit.default_timer()\n\n# For .ipynb, event loop already done\nawait main()\n\n# For .py\n# asyncio.run(main())\n\nend = timeit.default_timer()\ntotal_time = end - start\n\nprint(f'Total time {total_time:.4f}s')\n</code></pre> <pre><code>Adding\nAdding\nTotal sum 4\nTotal sum 4\nTotal time 1.0021s\n</code></pre>"},{"location":"programming/r/r/","title":"R","text":""},{"location":"programming/r/r/#quick-start-basics","title":"Quick Start Basics","text":""},{"location":"programming/r/r/#variable-object","title":"Variable Object","text":"<pre><code>var_a &lt;- 2\nprint(var_a)\n</code></pre> <pre><code>[1] 2\n</code></pre> <pre><code>var_b = 3\nprint(var_b)\n</code></pre> <pre><code>[1] 3\n</code></pre>"},{"location":"programming/r/r/#check-object-type","title":"Check Object Type","text":"<pre><code># In this case, it's a numeric type\nclass(var_a)\n</code></pre> <p>'numeric'</p>"},{"location":"programming/r/r/#list-vector","title":"List / Vector","text":"<pre><code>list_a &lt;- c(0, 1, 2)\nlist_a\n</code></pre> <ol><li>0</li><li>1</li><li>2</li></ol> <pre><code># It's inclusive start till end, unlike Python which leaves the last out\nlist_b &lt;- c(0:6)\nlist_b\n</code></pre> <ol><li>0</li><li>1</li><li>2</li><li>3</li><li>4</li><li>5</li><li>6</li></ol> <pre><code># Similar to a Python dictionary\nlist_c &lt;- list(john_height=120, doe_height=150)\nlist_c\n</code></pre> $john_height 120 $doe_height 150 <pre><code># Get keys only via names()\nnames(list_c)\n</code></pre> <ol><li>'john_height'</li><li>'doe_height'</li></ol> <pre><code># Get values only via uname()\nunname(list_c)\n</code></pre> <ol> <li>120</li> <li>150</li> </ol>"},{"location":"programming/r/r/#matrix","title":"Matrix","text":"<pre><code># Create 2 vectors\nvect_a &lt;- c(1, 2)\nvect_b &lt;- c(3, 4)\n\nvect_a\nvect_b\n</code></pre> <ol><li>1</li><li>2</li></ol> <ol><li>3</li><li>4</li></ol> <pre><code># Bind as row\nmat_c &lt;- rbind(vect_a, vect_b)\nmat_c\n</code></pre> A matrix: 2 \u00d7 2 of type dbl vect_a12 vect_b34 <pre><code># Bind as column\nmat_d &lt;- cbind(vect_a, vect_b)\nmat_d\n</code></pre> A matrix: 2 \u00d7 2 of type dbl vect_avect_b 13 24 <pre><code># Create matrix\nmat_e &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow=2, ncol=3)\n\nmat_e\n</code></pre> A matrix: 2 \u00d7 3 of type dbl 135 246 <pre><code># Create matrix with byrow option\nmat_g &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow=2, ncol=3, byrow=FALSE)\n\nmat_g\n</code></pre> A matrix: 2 \u00d7 3 of type dbl 135 246 <pre><code># Create same matrix with byrow option\nmat_f &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow=2, ncol=3, byrow=TRUE)\n\nmat_f\n</code></pre> A matrix: 2 \u00d7 3 of type dbl 123 456 <pre><code># Index matrix of first row and first column\nmat_f[1,1]\n# Index matrix of second row and first column\nmat_f[2,1]\n# Index matrix of second row and third column\nmat_f[2,3]\n# Select full first row\nmat_f[1,]\n# Select full first column\nmat_f[,1]\n</code></pre> <p>1</p> <p>4</p> <p>6</p> <ol><li>1</li><li>2</li><li>3</li></ol> <ol><li>1</li><li>4</li></ol> <pre><code># Find transpose of matrix: rows to cols\nt(mat_f)\n</code></pre> A matrix: 3 \u00d7 2 of type dbl 14 25 36 <pre><code># Add matrix\nmat_1 &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow=2, ncol=3)\nmat_2 &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow=2, ncol=3)\nmat_new &lt;- mat_1 + mat_2\nmat_1\nmat_2\nmat_new\n</code></pre> A matrix: 2 \u00d7 3 of type dbl 135 246 A matrix: 2 \u00d7 3 of type dbl 135 246 A matrix: 2 \u00d7 3 of type dbl 2610 4812 <pre><code># Subtract matrix\nmat_1 &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow=2, ncol=3)\nmat_2 &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow=2, ncol=3)\nmat_new &lt;- mat_1 - mat_2\nmat_1\nmat_2\nmat_new\n</code></pre> A matrix: 2 \u00d7 3 of type dbl 135 246 A matrix: 2 \u00d7 3 of type dbl 135 246 A matrix: 2 \u00d7 3 of type dbl 000 000 <pre><code># Element-wise multiplication / Hadamard product\nmat_1 &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow=2, ncol=3)\nmat_2 &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow=2, ncol=3)\nmat_new &lt;- mat_1 * mat_2\nmat_1\nmat_2\nmat_new\n</code></pre> A matrix: 2 \u00d7 3 of type dbl 135 246 A matrix: 2 \u00d7 3 of type dbl 135 246 A matrix: 2 \u00d7 3 of type dbl 1 925 41636 <pre><code># Matrix multiplication \nmat_1 &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow=2, ncol=3)\nmat_2 &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow=3, ncol=2)\nmat_new &lt;- mat_1 %*% mat_2\nmat_1\nmat_2\nmat_new\n</code></pre> A matrix: 2 \u00d7 3 of type dbl 135 246 A matrix: 3 \u00d7 2 of type dbl 14 25 36 A matrix: 2 \u00d7 2 of type dbl 2249 2864 <pre><code># Element-wise division\nmat_1 &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow=2, ncol=3)\nmat_2 &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow=2, ncol=3)\nmat_new &lt;- mat_1 / mat_2\nmat_1\nmat_2\nmat_new\n</code></pre> A matrix: 2 \u00d7 3 of type dbl 135 246 A matrix: 2 \u00d7 3 of type dbl 135 246 A matrix: 2 \u00d7 3 of type dbl 111 111 <pre><code># Scalar multiplication with matrix\nmat_1 &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow=2, ncol=3)\nmat_new &lt;- 10 * mat_1\nmat_1\nmat_new\n</code></pre> A matrix: 2 \u00d7 3 of type dbl 135 246 A matrix: 2 \u00d7 3 of type dbl 103050 204060 <pre><code># Broadcast vector of 1x3 to matrix of 2x3\nvec_1 &lt;- c(1, 2, 3)\nmat_1 &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow=2, ncol=3)\nvec_1 + mat_1\n</code></pre> A matrix: 2 \u00d7 3 of type dbl 267 459 <pre><code># Check for determinant of matrix\nmat_A &lt;- matrix(c(1, 2, 3, 4), nrow=2, ncol=2)\nmat_A\n# Here det != 0, hence inverse exist\ndet_of_mat_A = det(mat_A)\ncat(det_of_mat_A, 'is non-zero hence the inverse exist')\n</code></pre> A matrix: 2 \u00d7 2 of type dbl 13 24 <pre><code>-2 is non-zero hence the inverse exist\n</code></pre> <pre><code># Invert of Matrix: mat'\nmat_A_inv = solve(mat_A)\nmat_A_inv\n</code></pre> A matrix: 2 \u00d7 2 of type dbl -2 1.5  1-0.5 <pre><code># I = A'A = AA'\nmat_A_identity = mat_A_inv %*% mat_A\nmat_A_identity\nmat_A_inv %*% mat_A\nmat_A %*% mat_A_inv\n</code></pre> A matrix: 2 \u00d7 2 of type dbl 10 01 A matrix: 2 \u00d7 2 of type dbl 10 01 A matrix: 2 \u00d7 2 of type dbl 10 01 <pre><code># IA = AI = A\nmat_A\nmat_A_identity %*% mat_A\nmat_A %*% mat_A_identity\n</code></pre> A matrix: 2 \u00d7 2 of type dbl 13 24 A matrix: 2 \u00d7 2 of type dbl 13 24 A matrix: 2 \u00d7 2 of type dbl 13 24"},{"location":"programming/r/r/#dataframes","title":"DataFrames","text":"<pre><code>price_var &lt;- c(100, 105, 120)\ndate_var &lt;- as.Date(c('2021-01-11','2021-01-12','2021-01-13'))\ndf &lt;- data.frame(price_var, date_var)\ndf\n</code></pre> A data.frame: 3 \u00d7 2 price_vardate_var &lt;dbl&gt;&lt;date&gt; 1002021-01-11 1052021-01-12 1202021-01-13"},{"location":"programming/r/r/#for-loop","title":"For Loop","text":"<pre><code>for (i in c(0:3)) {\nprint(i)\n}\n</code></pre> <pre><code>[1] 0\n[1] 1\n[1] 2\n[1] 3\n</code></pre>"},{"location":"programming/r/r/#while-loop","title":"While Loop","text":"<pre><code># Be careful, you might end up in an infinite loop\n# if say i = -1\ni &lt;- 0\nwhile (i &lt; 3) {\nstdout_str = paste('Yet to reach 3, i is: ', i)\nprint(stdout_str)\ni &lt;- i + 1\n}\n\nprint('Reached 3!')\n</code></pre> <pre><code>[1] \"Yet to reach 3, i is:  0\"\n[1] \"Yet to reach 3, i is:  1\"\n[1] \"Yet to reach 3, i is:  2\"\n[1] \"Reached 3!\"\n</code></pre>"},{"location":"programming/r/r/#math","title":"Math","text":"<pre><code># Multiplication\n2 * 2\n</code></pre> <p>4</p> <pre><code># Division\n2 / 2\n</code></pre> <p>1</p> <pre><code># Addition\n2 + 2\n</code></pre> <p>4</p> <pre><code># Subtraction\n2 - 2\n</code></pre> <p>0</p> <pre><code># Exponentiation\nexp(10)\n</code></pre> <p>22026.4657948067</p> <pre><code># Inbuilt value pi\n2*pi\n</code></pre> <p>6.28318530717959</p> <pre><code># Natural log: ln\nlog(2)\n</code></pre> <p>0.693147180559945</p> <pre><code># Log with base 2\nvar_a = log(5, base=2)\nvar_a\n</code></pre> <p>2.32192809488736</p> <pre><code># Power to go back to 2\n2**var_a\n</code></pre> <p>5</p> <pre><code># Power\n2**2\n</code></pre> <p>4</p> <pre><code># Square root\nsqrt(4)\n\n# Alternative square root via power yiled same results\n4**0.5\n</code></pre> <p>2</p> <p>2</p> <pre><code># We can square root a complex number\nsqrt(-25+0i)\n\n# But we cannot square root a real number\ncat(sprintf('---------------'))\ncat(sprintf('This will fail!'))\ncat(sprintf('---------------'))\n\nsqrt(-25)\n</code></pre> <p>0+5i</p> <pre><code>---------------This will fail!---------------\n\nWarning message in sqrt(-25):\n\u201cNaNs produced\u201d\n</code></pre> <p>NaN</p> <pre><code># Print as fractions rather than decimal\nlibrary(MASS)\nfractions(1 - 0.25)\n</code></pre> <p>\u00be</p>"},{"location":"programming/r/r/#statistics","title":"Statistics","text":"<pre><code># Create list\nlist_a &lt;- c(1, 2, 3, 4, 5, 6)\n\n# Get standard deviation\ncat('standard deviation:', sd(list_a))\n\n# Get variance\ncat('\\nvariance:', var(list_a))\n\n# Get mean\ncat('\\nmedian:', mean(list_a))\n\n# Get median\ncat('\\nmean:', median(list_a))\n\n# Get summary statistics\nsummary(list_a)\n</code></pre> <pre><code>standard deviation: 1.870829\nvariance: 3.5\nmedian: 3.5\nmean: 3.5\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    2.25    3.50    3.50    4.75    6.00\n</code></pre> <pre><code># t-test\nvar_test = t.test(list_a)\nvar_test\n</code></pre> <pre><code>    One Sample t-test\n\ndata:  list_a\nt = 4.5826, df = 5, p-value = 0.005934\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 1.536686 5.463314\nsample estimates:\nmean of x \n      3.5\n</code></pre> <pre><code># This shows what you can call if we assign it to an object\nnames(t.test(list_a))\n</code></pre> <ol><li>'statistic'</li><li>'parameter'</li><li>'p.value'</li><li>'conf.int'</li><li>'estimate'</li><li>'null.value'</li><li>'stderr'</li><li>'alternative'</li><li>'method'</li><li>'data.name'</li></ol> <pre><code># Get confidence interval\nvar_test$conf.int\n</code></pre> <ol><li>1.53668569301968</li><li>5.46331430698032</li></ol> <pre><code># Mid-point of confidence interval is the mean\nmean(var_test$conf.int)\n</code></pre> <p>3.5</p> <pre><code># Get p-value\nvar_test$p.value\n</code></pre> <p>0.00593354451759226</p> <pre><code># Plot histogram, hist() doesn't work all the time\nbarplot(table(list_a))\n</code></pre> <p></p> <pre><code># Plot dot plot\nplot(list_a)\n</code></pre> <p></p> <pre><code># Plot dot plot\n# type: p for point, l for line, b for both\n# col: cyan, blue, green, red\nplot(list_a, xlab='variable x', ylab='variable y', main='X-Y Plot', type='b', col='blue')\n</code></pre> <p></p> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"programming/sympy/calculus_sympy/","title":"Calculus wth Sympy","text":"<p>In this section, we will be covering basic calculus theory and using sympy to solve the related equations.</p>"},{"location":"programming/sympy/calculus_sympy/#calculus-differentiation-theory","title":"Calculus: Differentiation (Theory)","text":""},{"location":"programming/sympy/calculus_sympy/#basic-rules","title":"Basic Rules","text":"\\[ f(x) = x^n \\\\ f\\prime(x) = nx^{x-1} \\] \\[ f(x) = c \\\\ f\\prime(x) = 0 \\] \\[ f(x) = \\cos(x) \\\\ f\\prime(x) = -\\sin(x) \\] \\[ f(x) = \\cosh(x) \\\\ f\\prime(x) = \\sinh(x) \\] \\[ f(x) = \\sin(x) \\\\ f\\prime(x) = \\cos(x) \\] \\[ f(x) = \\sinh(x) \\\\ f\\prime(x) = \\cosh(x) \\] \\[ f(x) = \\ln (x) \\\\ f\\prime(x) = \\frac{1}{x} \\]"},{"location":"programming/sympy/calculus_sympy/#sum-difference-rule","title":"Sum Difference Rule","text":"<p>Given</p> \\[ f(x) = g(x) \\pm h(x) \\] <p>Then</p> \\[ f\\prime(x) = g\\prime(x) \\pm h\\prime(x) \\]"},{"location":"programming/sympy/calculus_sympy/#product-rule","title":"Product Rule","text":"<p>Given a function with two parts</p> \\[ f(x) = g(x) \\cdot h(x) \\] <p>First derivative</p> \\[ f\\prime(x) = g\\prime(x) \\cdot h(x) + g(x) \\cdot h\\prime(x) \\] <p>Given three parts, we always use the product rule to keep it to 2 parts</p> \\[ f(x) = g(x) \\cdot h(x) \\cdot i(x) \\] \\[ f'(x) = \\big[g(x) \\cdot h(x)\\big]\\prime \\cdot i(x) + \\big[g(x) \\cdot h(x) \\big] \\cdot i\\prime(x) \\]"},{"location":"programming/sympy/calculus_sympy/#quotient-rule","title":"Quotient Rule","text":"<p>Given</p> \\[ f(x) = \\frac{g(x)}{h(x)} \\] <p>Then</p> \\[ f\\prime(x) = \\frac{g\\prime(x) \\cdot h(x) - g(x) \\cdot h\\prime(x)}{h^2(x)} \\]"},{"location":"programming/sympy/calculus_sympy/#logarithmic-rule","title":"Logarithmic Rule","text":"<p>Given</p> \\[f(x) = ln(g(x))\\] <p>Then</p> \\[ f\\prime(x) = \\frac{g\\prime(x)}{g(x)} \\]"},{"location":"programming/sympy/calculus_sympy/#exponential-rule","title":"Exponential Rule","text":"<p>Given</p> \\[ f(x) = \\exp \\big(g(x)\\big) \\] <p>Then</p> \\[ f\\prime(x) = g\\prime(x) \\exp \\big(g(x)\\big) \\]"},{"location":"programming/sympy/calculus_sympy/#partial-derivative","title":"Partial Derivative","text":"<p>Given</p> \\[ f(x) = g(x_{i}, x_{i+1}, \\dots, x_n) \\] <p>When computing the partial derivative of \\(x_i\\) with respect to (w.r.t.) the multivariable function \\(f(x_i \\dots x_n)\\), treat the rest of the variables as constant.</p>"},{"location":"programming/sympy/calculus_sympy/#calculus-integration-theory","title":"Calculus: Integration (Theory)","text":""},{"location":"programming/sympy/calculus_sympy/#basic-rules_1","title":"Basic Rules","text":"<p>Given function \\(f(x)\\) and integral of that function \\(F(x)\\), then basic rules include</p> \\[ f(x) = x^n \\\\ F(x) = \\frac{x^{n+1}}{n+1} + c \\] \\[ f(x) = 0 \\\\ F(x) = c \\] \\[ f(x) = e^x \\\\ F(x) = e^x + c \\] \\[ f(x) = a^x \\\\ F(x) = \\frac{a^x}{\\ln a} + c \\] \\[ f(x) = \\sin(x) \\\\ F(x) = -\\cos(x) + c \\] \\[ f(x) = \\sinh(x) \\\\ F(x) = \\cosh(x) + c \\] \\[ f(x) = \\cos(x) \\\\ F(x) = \\sin(x) + c \\] \\[ f(x) = \\cosh(x) \\\\ F(x) = \\sinh(x) + c \\] \\[ f(x) = \\frac{1}{x} \\\\ F(x) = \\ln\\left|x\\right| + c \\] \\[ f(x) = m \\\\ F(x) = mx + c \\] \\[ f(x) = \\frac{1}{x^2 + a^2} \\\\ F(x) = (\\frac{1}{a}) \\cdot \\arctan(\\frac{x}{a}) + c \\] \\[ f(x) = \\frac{1}{\\sqrt{a^2 - x^2}} \\\\ F(x) = \\arcsin(\\frac{x}{a}) + c \\]"},{"location":"programming/sympy/calculus_sympy/#sum-difference-rule_1","title":"Sum Difference Rule","text":"<p>Given</p> \\[ f(x) = g(x) \\pm h(x) \\] <p>Then</p> \\[ F(x) = \\int \\big( g(x) \\pm h(x) \\big) \\, dx\\]"},{"location":"programming/sympy/calculus_sympy/#integration-by-parts","title":"Integration by Parts","text":"\\[ F(x) = \\int u\\ v\\prime \\ dx = u\\ v - \\int v\\ u\\prime \\ dx \\]"},{"location":"programming/sympy/calculus_sympy/#definite-integral-rules","title":"Definite Integral Rules","text":""},{"location":"programming/sympy/calculus_sympy/#zero-integral-when-not-moving-integration-point","title":"Zero integral when not moving integration point","text":"\\[ \\int _a ^a f(x) \\ dx = 0 \\]"},{"location":"programming/sympy/calculus_sympy/#definite-integrals-interval-switching-becomes-negative","title":"Definite integral's interval switching becomes negative","text":"\\[ \\int _a ^b f(x) \\ dx = - \\int _b ^a f(x) \\ dx  \\]"},{"location":"programming/sympy/calculus_sympy/#definite-integral-decomposed-into-parts","title":"Definite integral decomposed into parts","text":"\\[ \\int _a ^b f(x) \\ dx = \\int _a ^c f(x) \\ dx - \\int _b ^c f(x) \\ dx \\]"},{"location":"programming/sympy/calculus_sympy/#substitution-method-for-solving-definite-integrals","title":"Substitution method for solving definite integrals","text":""},{"location":"programming/sympy/calculus_sympy/#substitution-example-1","title":"Substitution Example 1","text":"<p>Given $$ f(x) = x \\cos(x^2 + 1) $$</p> <p>Then definite integral from -1 to 1 is  $$ F(x)_{-1}^1 = \\int _{-1} ^ 1 x \\cos(x^2 + 1)  dx $$</p> <p>Modifying the equation to make \\(du\\) and \\(u\\) substitution method results in $$ F(x)_{-1}^1 = \\frac{1}{2} \\int _{-1} ^ 1 2x \\cos(x^2 + 1)  dx $$</p> <p>Let \\(u\\) be $$ u = x^2 +1  $$</p> <p>Let \\(du\\) be $$ du = 2x $$</p> <p>New limits given -1 and 1 $$ u = (-1)^2 + 1 = 2 $$ $$ u = (1)^2 + 1 = 2 $$</p> <p>Then by zero integral rule $$ F(x)_{-1}^1 = \\int _2 ^2 cos(u) du = 0 $$</p>"},{"location":"programming/sympy/calculus_sympy/#substitution-example-2","title":"Substitution Example 2","text":"<p>Given $$ h(x) = x(x+3)^{\\frac{1}{2}} $$</p> <p>Then definite integral from -1 to 1 is $$ H(x)_{-1}^1 = \\int _{-1} ^ 1 x(x+3)^{\\frac{1}{2}} $$</p> <p>Let \\(u\\) be $$ u = x + 3 $$ $$ x = u - 3 $$</p> <p>Let \\(du\\) be $$ du = 1 $$</p> <p>New limits given -1 and 1 $$ u = -1 + 3 = 2 $$ $$ u = 1 + 3 = 4 $$</p> <p>Then</p> \\[ H(x)_{-1}^1 = \\int _{2} ^ 4 (u-3)(u)^{\\frac{1}{2}} \\ du \\] \\[ H(x)_{-1}^1 = \\int _{2} ^ 4 u^{\\frac{3}{2}} - 3u^{\\frac{1}{2}} \\ du \\] \\[ H(x)_{-1}^1 = \\big[ \\frac{u^{\\frac{5}{2}}}{\\frac{5}{2}} - \\frac{3u^{\\frac{3}{2}}}{\\frac{3}{2}} \\big]_2 ^4 \\] \\[ H(x)_{-1}^1 = \\frac{2}{5} [-8 + 6\\sqrt(2)] \\]"},{"location":"programming/sympy/calculus_sympy/#substitution-example-3","title":"Substitution Example 3","text":"<p>Keep on substituting!</p> <p>You can keep running the substitution rule multiple times, where you can go from \\(u\\) to \\(z\\) to etc. to solve the definite integral. It is important to take note that your limits change every time you run the substitution rule once!</p>"},{"location":"programming/sympy/calculus_sympy/#multiple-integrals","title":"Multiple Integrals","text":"<p>This is used for functions with multiple variables. Hence, instead of the usual single variable where the definite integral represents the area under the curve, multiple integrals calculate hypervolumes with multiple dimensional functions.</p> <p>The key is in integrating step by step, when integrating with respect to each variable, the rest of the variables act as constants.</p>"},{"location":"programming/sympy/calculus_sympy/#double-integral","title":"Double Integral","text":"\\[V = \\int _{x=a}^{x=b} \\int _{y=c}^{y=d} f(x,y) \\ dy \\ dx\\]"},{"location":"programming/sympy/calculus_sympy/#triple-integral","title":"Triple Integral","text":"\\[V = \\int _{x=a}^{x=b} \\int _{y=c}^{y=d} \\int _{z=e}^{z=f} f(x,y,z) \\ dz \\ dy \\ dx\\]"},{"location":"programming/sympy/calculus_sympy/#calculus-taylor-expansion-theory","title":"Calculus: Taylor Expansion (Theory)","text":""},{"location":"programming/sympy/calculus_sympy/#taylor-expansion-single-variable","title":"Taylor Expansion: Single Variable","text":"<p>Given a function that has continuous derivatives up to \\((n+1)\\) order, the function can be expanded with the following equation:</p> \\[ F(x) = \\sum^{\\infty}_{n=0} \\frac{F^{(n)}(a)}{n!}\\ (x-a)^n \\] \\[ F(x) = F(a) + F\\prime (x)(x-a) + \\frac{F\\prime\\prime(a)}{2!}(x-a)^2 + \\cdots + \\frac{F(n)(a)}{n!} + R_n(x) \\] <p>If not expanded till \\(\\infty\\) then there will be a remainder of \\(R_n(x)\\)</p>"},{"location":"programming/sympy/calculus_sympy/#taylor-expansion-multiple-variables","title":"Taylor Expansion: Multiple Variables","text":"<p>In the case of 2 variables \\((x, y)\\), we can expand the multivariate equation with the generalized Taylor expansion equation:</p> \\[ F(x, y) = \\\\ F(a, b) + \\big[ \\frac{\\partial}{\\partial x} F(x, y)_{a,b} (x-a) + \\frac{\\partial}{\\partial y} F(x, y)_{a,b} ) (y - b)  \\big] \\\\ + \\frac{1}{2!} \\big[ \\frac{\\partial ^2}{\\partial x^2} F(x, y)_{a,b} (x-a)^2 + \\frac{\\partial ^2}{\\partial y^2} F(x, y)_{a,b}(y-b)^2 \\big] \\\\ + \\frac{\\partial ^2}{\\partial x \\partial y} F(x, y)_{a,b} (x-a)(y-b) + R_2 (x,y) \\]"},{"location":"programming/sympy/calculus_sympy/#maclaurin-expansion","title":"Maclaurin Expansion","text":"<p>It is simply a Taylor expansion about the point 0.</p>"}]}