{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"about/","text":"About Us \u00b6 We deploy a top-down approach that enables you to grasp deep learning and deep reinforcement learning theories and code easily and quickly. We have open-sourced all our materials through our Deep Learning Wizard Tutorials . For visual learners, feel free to sign up for our video course and join over 6000 deep learning wizards. To this date, we have taught thousands of students across more than 120+ countries from students in high school to postgraduates and professionals in leading MNCs and research institutions around the world. Experienced Research and Applied Core Team \u00b6 Ritchie Ng Currently in Hessian Matrix, NUS, NVIDIA (1) I lead as Chief Investment Officer, Head of AI, at Hessian Matrix where we develop systematic strategies with deep Learning, reinforcement learning and bayesian learning for thin-tailed and fat-tailed distributions. We are a systematic global hedge fund led by a team with deep experience formerly from GIC, NUS, and NVIDIA. (2) I am also an NVIDIA Deep Learning Institute instructor leading all deep learning industry workshops in NUS, Singapore and conducting workshops across Southeast Asia. (3) I\u2019m into applied research for systematic trading strategies with deep learning at NUS where I am a research scholar in NExT (NUS) ) and a PYI Fellow. (4) My passion for enabling anyone to leverage on deep learning has led to the creation of Deep Learning Wizard where I have taught and still continue to teach more than 3000 undergraduates, graduates and professionals in over 60 countries around the world. Previously in AI Quant Hedge Fund: ensemblecap.ai I spent 2.5 years leading artificial intelligence with my colleagues in ensemblecap.ai , an AI hedge fund based in Singapore comprising research scientists, engineers, quants, and traders from NVIDIA and JP Morgan. I have built the whole AI tech stack in a production environment with rigorous time-sensitive and fail-safe software testing powering multi-million dollar trades daily. Additionally, I led, as portfolio manager, our deep learning systematic portfolio, delivering high positive returns in highly volatile years like 2018 (US-China trade war initiation) and 2020 YTD (covid-19 virus) Previously in AI Academia: NUS I was previously conducting research in meta-learning for hyperparameter optimization for deep learning algorithms in NExT Search Centre that is jointly setup between National University of Singapore (NUS), Tsinghua University and University of Southampton led by co-directors Prof Tat-Seng Chua (KITHCT Chair Professor at the School of Computing), Prof Sun Maosong (Dean of Department of Computer Science and Technology, Tsinghua University), and Prof Dame Wendy Hall (Director of the Web Science Institute, University of Southampton). I graduated from NUS where I was an NUS Global Merit Scholar, Chua Thian Poh Community Leadership Programme Fellow, Philip Yeo Innovation Associate, and NUS Enterprise I&E Praticum Award recipient. I was awarded the IT Youth Leader of the Year Award in 2019. Check out my profile link at ritchieng.com Jie Fu I am undergoing my postdoc journey at Montreal Institute for Learning Algorithms (MILA) to prepare for the coming AI winter, as Eddard Stark said \"He won't be a boy forever and winter is coming\" -- Game of Thrones. I am privileged to work with Christopher Pal. I earned my PhD degree from National University of Singapore (NUS), and was fortunately under the supervision of Tat-Seng Chua and Huan Xu, also closely working with Jiashi Feng and Kian Hsiang Low. I am interested in machine learning. More specifically, my research is focused on deep learning, probabilistic reasoning, reinforcement learning and neural abstract machines. I am especially excited about reducing the gap between theoretical and practical algorithms in a principled and efficient manner. Check out my profile link at bigaidream.github.io Supporters \u00b6 Alfredo Canziani Alfredo is the main person who inspired Ritchie to open-source Deep Learning Wizard's materials. We are very grateful for his advice particular in the space of open-source projects, deep learning and PyTorch. He is currently working as a Post-Doctoral Deep Learning Research Scientist and Lecturer at Courant Institute of Mathematical Sciences, under the supervision of professors KyungHyun Cho and Yann LeCun. Do check out his latest mini-course on PyTorch that was held in Princeton University and was delivered with Ritchie Ng at AMMI (AIMS) Kigali, Rwanda supported by Google and Facebook. Marek Bardonski Since graduation, Marek has been working on state-of-the-art research. He was involved in inventing the system that diagnoses problems with Ubuntu packages using Machine Learning and in improving the detection accuracy of breast cancer using Deep Learning, each time leading 40 highly skilled Machine Learning/Deep Learning engineers for over 6 months. NASA noticed his unique skills and asked him to manage a project aiming to improve the International Space Station rotation algorithm that would allow them to collect enough power from solar panels while preventing them from overheating. The project was successful and generated a huge ROI for NASA. Marek has been honored by multiple Harvard professors and the Director of Advanced Exploration Systems at NASA, who signed an official Letter of Recommendation. Since his first summer of university coursework, Marek has participated in four internships at NVIDIA HQ and Microsoft HQ. He also spent one year in NVIDIA Switzerland optimizing Deep Learning inference for self-driving cars and, by implementing very efficient Deep Learning algorithms in the GPU assembly language, improving the speed of Tesla\u2019s cars self-driving engine inference. He's currently the Head of AI at Sigmoidal . \"NVIDIA Inception Partner Deep Learning Wizard is proudly an NVIDIA Inception Partner, empowering people across the world to leverage on deep learning via open-source programming languages and frameworks to solve problems. Amazon AWS Activate Programme Startup Deep Learning Wizard is also supported by the Amazon AWS Activate Programme, Portfolio Plus, allowing us to have the infrastructure to scale and grow. Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"About Us"},{"location":"about/#about-us","text":"We deploy a top-down approach that enables you to grasp deep learning and deep reinforcement learning theories and code easily and quickly. We have open-sourced all our materials through our Deep Learning Wizard Tutorials . For visual learners, feel free to sign up for our video course and join over 6000 deep learning wizards. To this date, we have taught thousands of students across more than 120+ countries from students in high school to postgraduates and professionals in leading MNCs and research institutions around the world.","title":"About Us"},{"location":"about/#experienced-research-and-applied-core-team","text":"Ritchie Ng Currently in Hessian Matrix, NUS, NVIDIA (1) I lead as Chief Investment Officer, Head of AI, at Hessian Matrix where we develop systematic strategies with deep Learning, reinforcement learning and bayesian learning for thin-tailed and fat-tailed distributions. We are a systematic global hedge fund led by a team with deep experience formerly from GIC, NUS, and NVIDIA. (2) I am also an NVIDIA Deep Learning Institute instructor leading all deep learning industry workshops in NUS, Singapore and conducting workshops across Southeast Asia. (3) I\u2019m into applied research for systematic trading strategies with deep learning at NUS where I am a research scholar in NExT (NUS) ) and a PYI Fellow. (4) My passion for enabling anyone to leverage on deep learning has led to the creation of Deep Learning Wizard where I have taught and still continue to teach more than 3000 undergraduates, graduates and professionals in over 60 countries around the world. Previously in AI Quant Hedge Fund: ensemblecap.ai I spent 2.5 years leading artificial intelligence with my colleagues in ensemblecap.ai , an AI hedge fund based in Singapore comprising research scientists, engineers, quants, and traders from NVIDIA and JP Morgan. I have built the whole AI tech stack in a production environment with rigorous time-sensitive and fail-safe software testing powering multi-million dollar trades daily. Additionally, I led, as portfolio manager, our deep learning systematic portfolio, delivering high positive returns in highly volatile years like 2018 (US-China trade war initiation) and 2020 YTD (covid-19 virus) Previously in AI Academia: NUS I was previously conducting research in meta-learning for hyperparameter optimization for deep learning algorithms in NExT Search Centre that is jointly setup between National University of Singapore (NUS), Tsinghua University and University of Southampton led by co-directors Prof Tat-Seng Chua (KITHCT Chair Professor at the School of Computing), Prof Sun Maosong (Dean of Department of Computer Science and Technology, Tsinghua University), and Prof Dame Wendy Hall (Director of the Web Science Institute, University of Southampton). I graduated from NUS where I was an NUS Global Merit Scholar, Chua Thian Poh Community Leadership Programme Fellow, Philip Yeo Innovation Associate, and NUS Enterprise I&E Praticum Award recipient. I was awarded the IT Youth Leader of the Year Award in 2019. Check out my profile link at ritchieng.com Jie Fu I am undergoing my postdoc journey at Montreal Institute for Learning Algorithms (MILA) to prepare for the coming AI winter, as Eddard Stark said \"He won't be a boy forever and winter is coming\" -- Game of Thrones. I am privileged to work with Christopher Pal. I earned my PhD degree from National University of Singapore (NUS), and was fortunately under the supervision of Tat-Seng Chua and Huan Xu, also closely working with Jiashi Feng and Kian Hsiang Low. I am interested in machine learning. More specifically, my research is focused on deep learning, probabilistic reasoning, reinforcement learning and neural abstract machines. I am especially excited about reducing the gap between theoretical and practical algorithms in a principled and efficient manner. Check out my profile link at bigaidream.github.io","title":"Experienced Research and Applied Core Team"},{"location":"about/#supporters","text":"Alfredo Canziani Alfredo is the main person who inspired Ritchie to open-source Deep Learning Wizard's materials. We are very grateful for his advice particular in the space of open-source projects, deep learning and PyTorch. He is currently working as a Post-Doctoral Deep Learning Research Scientist and Lecturer at Courant Institute of Mathematical Sciences, under the supervision of professors KyungHyun Cho and Yann LeCun. Do check out his latest mini-course on PyTorch that was held in Princeton University and was delivered with Ritchie Ng at AMMI (AIMS) Kigali, Rwanda supported by Google and Facebook. Marek Bardonski Since graduation, Marek has been working on state-of-the-art research. He was involved in inventing the system that diagnoses problems with Ubuntu packages using Machine Learning and in improving the detection accuracy of breast cancer using Deep Learning, each time leading 40 highly skilled Machine Learning/Deep Learning engineers for over 6 months. NASA noticed his unique skills and asked him to manage a project aiming to improve the International Space Station rotation algorithm that would allow them to collect enough power from solar panels while preventing them from overheating. The project was successful and generated a huge ROI for NASA. Marek has been honored by multiple Harvard professors and the Director of Advanced Exploration Systems at NASA, who signed an official Letter of Recommendation. Since his first summer of university coursework, Marek has participated in four internships at NVIDIA HQ and Microsoft HQ. He also spent one year in NVIDIA Switzerland optimizing Deep Learning inference for self-driving cars and, by implementing very efficient Deep Learning algorithms in the GPU assembly language, improving the speed of Tesla\u2019s cars self-driving engine inference. He's currently the Head of AI at Sigmoidal . \"NVIDIA Inception Partner Deep Learning Wizard is proudly an NVIDIA Inception Partner, empowering people across the world to leverage on deep learning via open-source programming languages and frameworks to solve problems. Amazon AWS Activate Programme Startup Deep Learning Wizard is also supported by the Amazon AWS Activate Programme, Portfolio Plus, allowing us to have the infrastructure to scale and grow.","title":"Supporters"},{"location":"about/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"consultancy/","text":"Training, Consultancy & Deployment \u00b6 We've empowered hundreds of clients from large start-ups, to MNCs, to educational institutions, and to government organizations with deep learning. If you require on-site training, consultancy or deployment on deep learning, deep reinforcement learning, machine learning, and/or general data science projects, please feel free to contact us at ritchie@deeplearningwizard.com .","title":"Consultancy"},{"location":"consultancy/#training-consultancy-deployment","text":"We've empowered hundreds of clients from large start-ups, to MNCs, to educational institutions, and to government organizations with deep learning. If you require on-site training, consultancy or deployment on deep learning, deep reinforcement learning, machine learning, and/or general data science projects, please feel free to contact us at ritchie@deeplearningwizard.com .","title":"Training, Consultancy &amp; Deployment"},{"location":"home/","text":"Learn The Whole AI Pipeline With \u00b6 You will the learn the suite of tools to build an end-to-end deep learning pipeline. Tools You Will Learn To Use PyTorch (CPU/GPU) Scikit-learn (CPU) RAPIDS cuML (GPU) Gym (CPU) Python (scripting) C++ (programming) Bash (scripting) NumPy (CPU) CuPy (GPU) Pandas (CPU) RAPIDS cuDF (GPU) Matplotlib (Plot) Plotly (Plot) Streamlit (Dashboard) CassandraDB (CPU DB) BlazingSQL (GPU DB) We deploy a top-down approach that enables you to grasp deep learning and deep reinforcement learning theories and code easily and quickly. We have open-sourced all our materials through our Deep Learning Wizard Tutorials . For visual learners, feel free to sign up for our video course and join over 6000 deep learning wizards. To this date, we have taught thousands of students across more than 120+ countries from students in high school to postgraduates and professionals in leading MNCs and research institutions around the world. Simulation of deep reinforcement learning agent mastering games like Super Mario Bros , Flappy Bird and PacMan . These games have APIs for algorithms to interact with the environment, and they are created by talented people so feel free to check out their respective repositories with the links given. \u21a9","title":"Intro"},{"location":"home/#learn-the-whole-ai-pipeline-with","text":"You will the learn the suite of tools to build an end-to-end deep learning pipeline. Tools You Will Learn To Use PyTorch (CPU/GPU) Scikit-learn (CPU) RAPIDS cuML (GPU) Gym (CPU) Python (scripting) C++ (programming) Bash (scripting) NumPy (CPU) CuPy (GPU) Pandas (CPU) RAPIDS cuDF (GPU) Matplotlib (Plot) Plotly (Plot) Streamlit (Dashboard) CassandraDB (CPU DB) BlazingSQL (GPU DB) We deploy a top-down approach that enables you to grasp deep learning and deep reinforcement learning theories and code easily and quickly. We have open-sourced all our materials through our Deep Learning Wizard Tutorials . For visual learners, feel free to sign up for our video course and join over 6000 deep learning wizards. To this date, we have taught thousands of students across more than 120+ countries from students in high school to postgraduates and professionals in leading MNCs and research institutions around the world. Simulation of deep reinforcement learning agent mastering games like Super Mario Bros , Flappy Bird and PacMan . These games have APIs for algorithms to interact with the environment, and they are created by talented people so feel free to check out their respective repositories with the links given. \u21a9","title":"Learn The Whole AI Pipeline With"},{"location":"pipeline/","text":"CPU to GPU Production-level Pipeline for AI \u00b6 At Deep Learning Wizard, we cover the basics of some parts of the whole tech stack for production-level CPU/GPU-powered AI. This AI pipeline is entirely based on open-source distributions. This stack would get you started, and enable you to adjust the stack according to your needs.","title":"AI Pipeline"},{"location":"pipeline/#cpu-to-gpu-production-level-pipeline-for-ai","text":"At Deep Learning Wizard, we cover the basics of some parts of the whole tech stack for production-level CPU/GPU-powered AI. This AI pipeline is entirely based on open-source distributions. This stack would get you started, and enable you to adjust the stack according to your needs.","title":"CPU to GPU Production-level Pipeline for AI"},{"location":"review/","text":"Reviews \u00b6 To this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world. These are just some of the hundreds of reviews we've had and will be updated when we have more time or contributors. Roberto Trevi\u00f1o Cervantes Congratulations! You have achieved the unachievable, teaching a 15 year old boy how to get a machine to estimate a function, just what he had been trying to do for almost a year. Muktabh Mayank This course helped me understand idiomatic pytorch and avoiding translating theano-to-torch. Charles Neiswender I really feel like I can take the techniques I learned and apply them to deep learning projects of my choosing. Ian Lipton This was a good overview of the different types of neural networks. I like that the instructor started with linear and logistic regression before diving into the neural networks. That background helped make sense of the neural network functionality. I like that this gave enough detail on the mathematics so that you can understand the basic functioning of the models but didn't overwhelm you with the nitty gritty math. And check out hundreds of more reviews for our video course !","title":"Reviews"},{"location":"review/#reviews","text":"To this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world. These are just some of the hundreds of reviews we've had and will be updated when we have more time or contributors. Roberto Trevi\u00f1o Cervantes Congratulations! You have achieved the unachievable, teaching a 15 year old boy how to get a machine to estimate a function, just what he had been trying to do for almost a year. Muktabh Mayank This course helped me understand idiomatic pytorch and avoiding translating theano-to-torch. Charles Neiswender I really feel like I can take the techniques I learned and apply them to deep learning projects of my choosing. Ian Lipton This was a good overview of the different types of neural networks. I like that the instructor started with linear and logistic regression before diving into the neural networks. That background helped make sense of the neural network functionality. I like that this gave enough detail on the mathematics so that you can understand the basic functioning of the models but didn't overwhelm you with the nitty gritty math. And check out hundreds of more reviews for our video course !","title":"Reviews"},{"location":"database/intro/","text":"Scalable Database for Deep Learning \u00b6 Tools You Will Learn To Use Python C++ Apache CassandraDB BlazingSQL This is a critical part of production deployment of deep learning algorithms. Reasons for Using Apache Cassandra \u00b6 It is is an open-source NoSQL database management system (DBMS) that is designed to handle large amounts of data across many servers, providing high availability with no single point of failure. Meaning if one server fails, everything goes as per normal as data is shared amongst nodes (servers). Compared to other master/slave models like MongoDB, Cassandra has a master-less model. It has extremely fast read/write speeds. You read/write millions of rows of data easily. Work in progress This open-source portion is still a work in progress. Stay tuned while we gradually upload our tutorials and notes. Feel free to contact Ritchie Ng if you would like to contribute via our Facebook page.","title":"Introduction"},{"location":"database/intro/#scalable-database-for-deep-learning","text":"Tools You Will Learn To Use Python C++ Apache CassandraDB BlazingSQL This is a critical part of production deployment of deep learning algorithms.","title":"Scalable Database for Deep Learning"},{"location":"database/intro/#reasons-for-using-apache-cassandra","text":"It is is an open-source NoSQL database management system (DBMS) that is designed to handle large amounts of data across many servers, providing high availability with no single point of failure. Meaning if one server fails, everything goes as per normal as data is shared amongst nodes (servers). Compared to other master/slave models like MongoDB, Cassandra has a master-less model. It has extremely fast read/write speeds. You read/write millions of rows of data easily. Work in progress This open-source portion is still a work in progress. Stay tuned while we gradually upload our tutorials and notes. Feel free to contact Ritchie Ng if you would like to contribute via our Facebook page.","title":"Reasons for Using Apache Cassandra"},{"location":"database/setting_up_cluster/","text":"Setting up Cassandra Multi-node Cluster \u00b6 Install Apache Cassandra \u00b6 Patience is key initially It will be quite tedious to many people in setting up a multi-node Cassandra cluster. You are likely to face a lot of problems. But with a strong community and this tried-and-proven guide, you should be ok! Just be patient. Remember when you're asking for help on StackOverflow, here or anywhere else, do paste the logs to make everyone's life easier to help you debug quickly. All you need to do is to run this base command and paste the logs. cat /var/log/cassandra/system.log Here, we will install all of the required Debian packages. To ensure you've the latest version of Cassandra, please use the official link at cassandra.apache.org/download . Bash Commands echo \"deb http://www.apache.org/dist/cassandra/debian 311x main\" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list curl https://www.apache.org/dist/cassandra/KEYS | sudo apt-key add - sudo apt-get update sudo apt-get install cassandra This would get you up and running with Cassandra v3.11. Install DataStax Python Cassandra Driver \u00b6 We need to install the Python Cassandra Driver so we can easily interact with our Cassandra Database without using the CQLSH (this is Cassandra's shell to run CQL commands to perform CRUD operations) commandline directly. What is CRUD? CRUD is the acronym of CREATE, READ, UPDATE, and DELETE. For databases, we typically have these 4 categories of operations so we can create new data points, update, read or delete them in our database. Bash Commands pip install cassandra-driver This is strictly for installation on Linux platforms, refer to the official website for more details. Single Node Cassandra Cluster \u00b6 Before we venture into the cool world of multi-node cluster requiring many servers, we will start with a single node cluster that only requires a single server (desktop/laptop). Once you've installed everything so far, you should run the following. Bash commands sudo service cassandra start sudo nodetool status And this will print out something like that: Datacenter: datacenter1 ============================= Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN 127.0.0.1 318.78 KiB 256 100.0% g5ac4c9-99b7-65d-24cfd82524f9 rack1 That's it, we've built our single node Cassandra database. Multi-node Cassandra Cluster \u00b6 Remember it's standard to have at least 3 nodes, and in a basic 3 separate server configuration (3 separate desktops for non-enterprise users). Because of this, we will base this tutorial on setting up a 3-node cluster. But the same steps apply if you want to even do 10000 nodes. Installing Sublime critical Sublime is a text editor, and it would help if you're not familiar with VIM which I use frequently. Honestly using Sublime to edit all the following files we will edit is much easier, trust me! To install Sublime run the following bash commands in sequence: wget -qO - https://download.sublimetext.com/sublimehq-pub.gpg | sudo apt-key add - echo \"deb https://download.sublimetext.com/ apt/stable/\" | sudo tee /etc/apt/sources.list.d/sublime-text.list sudo apt-get update sudo apt-get install sublime-text Now check if it works by running this command in your bash: subl This should open Sublime text editor! You're now ready. Before we move on to the steps to set up each node, we need the IP address of all 3 servers. This is simple, just run the following bash command: ifconfig | sed -En 's/127.0.0.1//;s/.*inet (addr:)?(([0-9]*\\.){3}[0-9]*).*/\\2/p' This would return the server's IP address that we need for example: server_1_ip: 112.522.6.61 server_2_ip: 112.522.6.62 server_3_ip: 112.522.6.63 We will be using these IPs as a base for configuration in the subsequent sections. Take note yours would differ and you need to change accordingly. Steps Per Node \u00b6 Critical Section You Need To Repeat This is a critcal section. On EVERY server, you need to repeat all the steps shown in this section. In our case of a 3 node cluster, using 3 servers, we need to repeat this 3 times. Step 1: Modify Cassandra Configuration Settings \u00b6 Run the following bash to edit the configuration file for server 1 (112.522.6.61): cd /etc/cassandra subl cassandra.yaml Now you need to find the following fields and change them accordingly. cluster_name: 'CassandraDBCluster' seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: - seeds: \"112.522.6.61, 112.522.6.62, 112.522.6.63\" listen_address: 112.522.6.61 rpc_address: 112.522.6.61 endpoint_snitch: GossipingPropertyFileSnitch auto_bootstrap: true Step 2: Modify Rack Details \u00b6 Run the following bash command to edit the rack details: cd /etc/cassandra subl cassandra-rackdc.properties And change the following fields to this: dc=datacenter1 Step 3: Check Status \u00b6 Restart service with the following bash commands: sudo rm -rf /var/lib/cassandra/data/system/* sudo service cassandra restart sudo nodetool status Here you would see an output like this: Datacenter: datacenter ============================= Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN 112.522.6.61 318.78 KiB 256 100.0% f5c84c9-99b7-45d-8856-24cfd82523f9 rack1 UN 112.522.6.62 206.42 KiB 256 100.0% ac2f24da-1b2c-4e3-8a75-0e28ec366a4 rack1 UN 112.522.6.63 338.34 KiB 256 100.0% 55a227d-ffbe-45d5-8698-60c374b3a6b rack1 Step 4: Configure Firewall IP Settings \u00b6 Run the following bash commands sudo apt-get install iptables-persistent sudo iptables -A INPUT -p tcp -s 112.522.6.62 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT sudo iptables -A INPUT -p tcp -s 112.522.6.63 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT sudo bash -c \"iptables-save > /etc/iptables/rules.v4\" sudo netfilter-persistent reload sudo nodetool status Repeat Steps 1 to 4 \u00b6 You have to repeat all 4 steps for the other 2 servers but step 1 and step 4 requires slightly different settings. For the other servers, you need to basically change the IP to the server's IP. And for the IP firewall settings, you need to allow entry from the other 2 servers instead of itself. Server 2 Example for Step 2 \u00b6 cluster_name: 'CassandraDBCluster' seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: - seeds: \"112.522.6.61, 112.522.6.62, 112.522.6.63\" # ONLY THIS LINE CHANGES listen_address: 112.522.6.62 # ONLY THIS LINE CHANGES rpc_address: 112.522.6.62 endpoint_snitch: GossipingPropertyFileSnitch auto_bootstrap: true Server 3 Example for Step 2 \u00b6 cluster_name: 'CassandraDBCluster' seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: - seeds: \"112.522.6.61, 112.522.6.62, 112.522.6.63\" # ONLY THIS LINE CHANGES listen_address: 112.522.6.63 # ONLY THIS LINE CHANGES rpc_address: 112.522.6.63 endpoint_snitch: GossipingPropertyFileSnitch auto_bootstrap: true Server 2 Example for Step 4 \u00b6 sudo apt-get install iptables-persistent # ONLY THESE LINES CHANGE TO ALLOW COMMUNICATION WITH SERVER 1 AND 3 sudo iptables -A INPUT -p tcp -s 112.522.6.61 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT sudo iptables -A INPUT -p tcp -s 112.522.6.63 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT sudo bash -c \"iptables-save > /etc/iptables/rules.v4\" sudo netfilter-persistent reload sudo nodetool status Server 3 Example for Step 4 \u00b6 sudo apt-get install iptables-persistent # ONLY THESE LINES CHANGE TO ALLOW COMMUNICATION WITH SERVER 1 AND 2 sudo iptables -A INPUT -p tcp -s 112.522.6.61 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT sudo iptables -A INPUT -p tcp -s 112.522.6.62 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT sudo bash -c \"iptables-save > /etc/iptables/rules.v4\" sudo netfilter-persistent reload sudo nodetool status Dead Node Fix \u00b6 When your server/desktop restarts, you may face a dead node. You need replace the address with the same IP for it to work. cd /etc/cassandra subtl cassandra-env.sh Add the following line in the last row assuming server 1 is dead where the IP is 112.522.6.61 JVM_OPTS=\"$JVM_OPTS -Dcassandra.replace_address=112.522.6.61\" Then run the following bash commands sudo rm -rf /var/lib/cassandra/data/system/* sudo service cassandra restart sudo nodetool status You might have to wait awhile and re-run sudo nodetool status for the DB to get up and running. Summary \u00b6 We have successfully set up a 3-node Cassandra cluster DB after all these steps. We will move on to interacting with the cluster with CQLSH and the Python Driver in subsequent guides. Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Cassandra Cluster Setup"},{"location":"database/setting_up_cluster/#setting-up-cassandra-multi-node-cluster","text":"","title":"Setting up Cassandra Multi-node Cluster"},{"location":"database/setting_up_cluster/#install-apache-cassandra","text":"Patience is key initially It will be quite tedious to many people in setting up a multi-node Cassandra cluster. You are likely to face a lot of problems. But with a strong community and this tried-and-proven guide, you should be ok! Just be patient. Remember when you're asking for help on StackOverflow, here or anywhere else, do paste the logs to make everyone's life easier to help you debug quickly. All you need to do is to run this base command and paste the logs. cat /var/log/cassandra/system.log Here, we will install all of the required Debian packages. To ensure you've the latest version of Cassandra, please use the official link at cassandra.apache.org/download . Bash Commands echo \"deb http://www.apache.org/dist/cassandra/debian 311x main\" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list curl https://www.apache.org/dist/cassandra/KEYS | sudo apt-key add - sudo apt-get update sudo apt-get install cassandra This would get you up and running with Cassandra v3.11.","title":"Install Apache Cassandra"},{"location":"database/setting_up_cluster/#install-datastax-python-cassandra-driver","text":"We need to install the Python Cassandra Driver so we can easily interact with our Cassandra Database without using the CQLSH (this is Cassandra's shell to run CQL commands to perform CRUD operations) commandline directly. What is CRUD? CRUD is the acronym of CREATE, READ, UPDATE, and DELETE. For databases, we typically have these 4 categories of operations so we can create new data points, update, read or delete them in our database. Bash Commands pip install cassandra-driver This is strictly for installation on Linux platforms, refer to the official website for more details.","title":"Install DataStax Python Cassandra Driver"},{"location":"database/setting_up_cluster/#single-node-cassandra-cluster","text":"Before we venture into the cool world of multi-node cluster requiring many servers, we will start with a single node cluster that only requires a single server (desktop/laptop). Once you've installed everything so far, you should run the following. Bash commands sudo service cassandra start sudo nodetool status And this will print out something like that: Datacenter: datacenter1 ============================= Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN 127.0.0.1 318.78 KiB 256 100.0% g5ac4c9-99b7-65d-24cfd82524f9 rack1 That's it, we've built our single node Cassandra database.","title":"Single Node Cassandra Cluster"},{"location":"database/setting_up_cluster/#multi-node-cassandra-cluster","text":"Remember it's standard to have at least 3 nodes, and in a basic 3 separate server configuration (3 separate desktops for non-enterprise users). Because of this, we will base this tutorial on setting up a 3-node cluster. But the same steps apply if you want to even do 10000 nodes. Installing Sublime critical Sublime is a text editor, and it would help if you're not familiar with VIM which I use frequently. Honestly using Sublime to edit all the following files we will edit is much easier, trust me! To install Sublime run the following bash commands in sequence: wget -qO - https://download.sublimetext.com/sublimehq-pub.gpg | sudo apt-key add - echo \"deb https://download.sublimetext.com/ apt/stable/\" | sudo tee /etc/apt/sources.list.d/sublime-text.list sudo apt-get update sudo apt-get install sublime-text Now check if it works by running this command in your bash: subl This should open Sublime text editor! You're now ready. Before we move on to the steps to set up each node, we need the IP address of all 3 servers. This is simple, just run the following bash command: ifconfig | sed -En 's/127.0.0.1//;s/.*inet (addr:)?(([0-9]*\\.){3}[0-9]*).*/\\2/p' This would return the server's IP address that we need for example: server_1_ip: 112.522.6.61 server_2_ip: 112.522.6.62 server_3_ip: 112.522.6.63 We will be using these IPs as a base for configuration in the subsequent sections. Take note yours would differ and you need to change accordingly.","title":"Multi-node Cassandra Cluster"},{"location":"database/setting_up_cluster/#steps-per-node","text":"Critical Section You Need To Repeat This is a critcal section. On EVERY server, you need to repeat all the steps shown in this section. In our case of a 3 node cluster, using 3 servers, we need to repeat this 3 times.","title":"Steps Per Node"},{"location":"database/setting_up_cluster/#step-1-modify-cassandra-configuration-settings","text":"Run the following bash to edit the configuration file for server 1 (112.522.6.61): cd /etc/cassandra subl cassandra.yaml Now you need to find the following fields and change them accordingly. cluster_name: 'CassandraDBCluster' seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: - seeds: \"112.522.6.61, 112.522.6.62, 112.522.6.63\" listen_address: 112.522.6.61 rpc_address: 112.522.6.61 endpoint_snitch: GossipingPropertyFileSnitch auto_bootstrap: true","title":"Step 1: Modify Cassandra Configuration Settings"},{"location":"database/setting_up_cluster/#step-2-modify-rack-details","text":"Run the following bash command to edit the rack details: cd /etc/cassandra subl cassandra-rackdc.properties And change the following fields to this: dc=datacenter1","title":"Step 2: Modify Rack Details"},{"location":"database/setting_up_cluster/#step-3-check-status","text":"Restart service with the following bash commands: sudo rm -rf /var/lib/cassandra/data/system/* sudo service cassandra restart sudo nodetool status Here you would see an output like this: Datacenter: datacenter ============================= Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN 112.522.6.61 318.78 KiB 256 100.0% f5c84c9-99b7-45d-8856-24cfd82523f9 rack1 UN 112.522.6.62 206.42 KiB 256 100.0% ac2f24da-1b2c-4e3-8a75-0e28ec366a4 rack1 UN 112.522.6.63 338.34 KiB 256 100.0% 55a227d-ffbe-45d5-8698-60c374b3a6b rack1","title":"Step 3: Check Status"},{"location":"database/setting_up_cluster/#step-4-configure-firewall-ip-settings","text":"Run the following bash commands sudo apt-get install iptables-persistent sudo iptables -A INPUT -p tcp -s 112.522.6.62 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT sudo iptables -A INPUT -p tcp -s 112.522.6.63 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT sudo bash -c \"iptables-save > /etc/iptables/rules.v4\" sudo netfilter-persistent reload sudo nodetool status","title":"Step 4: Configure Firewall IP Settings"},{"location":"database/setting_up_cluster/#repeat-steps-1-to-4","text":"You have to repeat all 4 steps for the other 2 servers but step 1 and step 4 requires slightly different settings. For the other servers, you need to basically change the IP to the server's IP. And for the IP firewall settings, you need to allow entry from the other 2 servers instead of itself.","title":"Repeat Steps 1 to 4"},{"location":"database/setting_up_cluster/#server-2-example-for-step-2","text":"cluster_name: 'CassandraDBCluster' seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: - seeds: \"112.522.6.61, 112.522.6.62, 112.522.6.63\" # ONLY THIS LINE CHANGES listen_address: 112.522.6.62 # ONLY THIS LINE CHANGES rpc_address: 112.522.6.62 endpoint_snitch: GossipingPropertyFileSnitch auto_bootstrap: true","title":"Server 2 Example for Step 2"},{"location":"database/setting_up_cluster/#server-3-example-for-step-2","text":"cluster_name: 'CassandraDBCluster' seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: - seeds: \"112.522.6.61, 112.522.6.62, 112.522.6.63\" # ONLY THIS LINE CHANGES listen_address: 112.522.6.63 # ONLY THIS LINE CHANGES rpc_address: 112.522.6.63 endpoint_snitch: GossipingPropertyFileSnitch auto_bootstrap: true","title":"Server 3 Example for Step 2"},{"location":"database/setting_up_cluster/#server-2-example-for-step-4","text":"sudo apt-get install iptables-persistent # ONLY THESE LINES CHANGE TO ALLOW COMMUNICATION WITH SERVER 1 AND 3 sudo iptables -A INPUT -p tcp -s 112.522.6.61 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT sudo iptables -A INPUT -p tcp -s 112.522.6.63 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT sudo bash -c \"iptables-save > /etc/iptables/rules.v4\" sudo netfilter-persistent reload sudo nodetool status","title":"Server 2 Example for Step 4"},{"location":"database/setting_up_cluster/#server-3-example-for-step-4","text":"sudo apt-get install iptables-persistent # ONLY THESE LINES CHANGE TO ALLOW COMMUNICATION WITH SERVER 1 AND 2 sudo iptables -A INPUT -p tcp -s 112.522.6.61 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT sudo iptables -A INPUT -p tcp -s 112.522.6.62 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT sudo bash -c \"iptables-save > /etc/iptables/rules.v4\" sudo netfilter-persistent reload sudo nodetool status","title":"Server 3 Example for Step 4"},{"location":"database/setting_up_cluster/#dead-node-fix","text":"When your server/desktop restarts, you may face a dead node. You need replace the address with the same IP for it to work. cd /etc/cassandra subtl cassandra-env.sh Add the following line in the last row assuming server 1 is dead where the IP is 112.522.6.61 JVM_OPTS=\"$JVM_OPTS -Dcassandra.replace_address=112.522.6.61\" Then run the following bash commands sudo rm -rf /var/lib/cassandra/data/system/* sudo service cassandra restart sudo nodetool status You might have to wait awhile and re-run sudo nodetool status for the DB to get up and running.","title":"Dead Node Fix"},{"location":"database/setting_up_cluster/#summary","text":"We have successfully set up a 3-node Cassandra cluster DB after all these steps. We will move on to interacting with the cluster with CQLSH and the Python Driver in subsequent guides.","title":"Summary"},{"location":"database/setting_up_cluster/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/course_progression/","text":"Course Progression \u00b6 If you would like a smooth transition in learning deep learning concepts, you need to follow the materials in a sequential order. Some sections are still pending as I am working on them, and they will have the icon beside them. 1. Practical Deep Learning with PyTorch \u00b6 Matrices Gradients Linear Regression Logistic Regression Feedforward Neural Networks (FNN) Convolutional Neural Networks (CNN) Recurrent Neural Networks (RNN) Long Short Term Memory Neural Networks (LSTM) Autoencoders (AE) Fully-connected Overcomplete Autoencoder (AE) Variational Autoencoders (VAE) Adversarial Autoencoders (AAE) Generative Adversarial Networks (GAN) Transformers 2. Boosting Deep Learning Models with PyTorch \u00b6 Derivatives, Gradients and Jacobian Gradient Descent and Backpropagation (From Scratch FNN Regression) Learning Rate Scheduling Optimizers Advanced Learning Rate Optimization Weight Initializations and Activation Functions Overfitting Prevention Loss, Accuracy and Weight Visualizations Data Preprocessing for Images and Videos Data Preprocessing for Time Series 3. Deep Model-Free Reinforcement Learning with PyTorch \u00b6 Supervised Learning to Reinforcement Learning Markov Decision Processes and Bellman Equations Dynamic Programming Monte Carlo Approach Temporal-Difference Policy Gradient: REINFORCE Policy Gradient: Actor-Critic Policy Gradient: A2C/A3C Policy Gradient: ACKTR Policy Gradient: PPO Policy Gradient: DPG Policy Gradient: DDPG (DQN & DPG) 4. From Scratch with Python and PyTorch \u00b6 From Scratch Logistic Regression Classification From Scratch FNN Classification From Scratch CNN Classification From Scratch RNN Classification From Scratch LSTM Classification From Scratch AE","title":"Course Progression"},{"location":"deep_learning/course_progression/#course-progression","text":"If you would like a smooth transition in learning deep learning concepts, you need to follow the materials in a sequential order. Some sections are still pending as I am working on them, and they will have the icon beside them.","title":"Course Progression"},{"location":"deep_learning/course_progression/#1-practical-deep-learning-with-pytorch","text":"Matrices Gradients Linear Regression Logistic Regression Feedforward Neural Networks (FNN) Convolutional Neural Networks (CNN) Recurrent Neural Networks (RNN) Long Short Term Memory Neural Networks (LSTM) Autoencoders (AE) Fully-connected Overcomplete Autoencoder (AE) Variational Autoencoders (VAE) Adversarial Autoencoders (AAE) Generative Adversarial Networks (GAN) Transformers","title":"1. Practical Deep Learning with PyTorch"},{"location":"deep_learning/course_progression/#2-boosting-deep-learning-models-with-pytorch","text":"Derivatives, Gradients and Jacobian Gradient Descent and Backpropagation (From Scratch FNN Regression) Learning Rate Scheduling Optimizers Advanced Learning Rate Optimization Weight Initializations and Activation Functions Overfitting Prevention Loss, Accuracy and Weight Visualizations Data Preprocessing for Images and Videos Data Preprocessing for Time Series","title":"2. Boosting Deep Learning Models with PyTorch"},{"location":"deep_learning/course_progression/#3-deep-model-free-reinforcement-learning-with-pytorch","text":"Supervised Learning to Reinforcement Learning Markov Decision Processes and Bellman Equations Dynamic Programming Monte Carlo Approach Temporal-Difference Policy Gradient: REINFORCE Policy Gradient: Actor-Critic Policy Gradient: A2C/A3C Policy Gradient: ACKTR Policy Gradient: PPO Policy Gradient: DPG Policy Gradient: DDPG (DQN & DPG)","title":"3. Deep Model-Free Reinforcement Learning with PyTorch"},{"location":"deep_learning/course_progression/#4-from-scratch-with-python-and-pytorch","text":"From Scratch Logistic Regression Classification From Scratch FNN Classification From Scratch CNN Classification From Scratch RNN Classification From Scratch LSTM Classification From Scratch AE","title":"4. From Scratch with Python and PyTorch"},{"location":"deep_learning/intro/","text":"Deep Learning and Deep Reinforcement Learning Theory and Programming Tutorials \u00b6 We'll be covering both CPU and GPU implementations of deep learning and deep reinforcement learning algorithms Packages and Languages you will Learn to Use Python PyTorch NumPy Gym If you would like a more visual and guided experience, feel free to take our video course . Work in progress This open-source portion is still a work in progress, it may be sparse in explanation as traditionally all our explanation are done via video. Stay tuned while we gradually upload our tutorials and notes. Feel free to contact Ritchie Ng if you would like to contribute via our Facebook page. Also take note that these notes are best used as a referral. This is because we have yet to expand it comprehensively to be a stand-alone guide. Go head and take our video course that provides a much easier, proven-to-work, experience. All of our code allows you to run in a notebook for this deep learning section. Please use a jupyter notebook and run the examples from the start of the page to the end. Remember to Right mouse click > Open image in new tab if you would like to zoom into the diagrams if you find them too small. Clarifications These are some clarifications we would like to highlight. When we state linear function , more specifically we meant affine function that comprises a linear function and a constant. We did this initially to make it easier as \"linear function\" was easier to digest. For all diagrams that says valid padding , they refer to no padding such that your output size will be smaller than your input size. For all diagrams that says same padding , they refer to zero padding (padding your input with zeroes) such that your output size will be equal to your input size. Errors to be corrected As we are rapidly prototyping there may be some errors. For these errors stated here, they will be corrected soon. Feel free to report bugs, corrections or improvements on our Github repository . If you did not go through all the materials, you would not be familiar with these, so go through them and come back to review these changes. For all diagrams that says dot product , they refer to matrix product .","title":"Introduction"},{"location":"deep_learning/intro/#deep-learning-and-deep-reinforcement-learning-theory-and-programming-tutorials","text":"We'll be covering both CPU and GPU implementations of deep learning and deep reinforcement learning algorithms Packages and Languages you will Learn to Use Python PyTorch NumPy Gym If you would like a more visual and guided experience, feel free to take our video course . Work in progress This open-source portion is still a work in progress, it may be sparse in explanation as traditionally all our explanation are done via video. Stay tuned while we gradually upload our tutorials and notes. Feel free to contact Ritchie Ng if you would like to contribute via our Facebook page. Also take note that these notes are best used as a referral. This is because we have yet to expand it comprehensively to be a stand-alone guide. Go head and take our video course that provides a much easier, proven-to-work, experience. All of our code allows you to run in a notebook for this deep learning section. Please use a jupyter notebook and run the examples from the start of the page to the end. Remember to Right mouse click > Open image in new tab if you would like to zoom into the diagrams if you find them too small. Clarifications These are some clarifications we would like to highlight. When we state linear function , more specifically we meant affine function that comprises a linear function and a constant. We did this initially to make it easier as \"linear function\" was easier to digest. For all diagrams that says valid padding , they refer to no padding such that your output size will be smaller than your input size. For all diagrams that says same padding , they refer to zero padding (padding your input with zeroes) such that your output size will be equal to your input size. Errors to be corrected As we are rapidly prototyping there may be some errors. For these errors stated here, they will be corrected soon. Feel free to report bugs, corrections or improvements on our Github repository . If you did not go through all the materials, you would not be familiar with these, so go through them and come back to review these changes. For all diagrams that says dot product , they refer to matrix product .","title":"Deep Learning and Deep Reinforcement Learning Theory and Programming Tutorials"},{"location":"deep_learning/readings/","text":"Info This is a list of growing number of papers and implementations I think are interesting. Long Tailed Recognition \u00b6 Large-Scale Long-Tailed Recognition in an Open World Frequently in real world scenario there're new unseen classes or samples within the tail classes This tackles the problem with dynamic embedding to bring associative memory to aid prediction of long-tailed classes The model essentially combines direct image features with embeddings from other classes Better Generalization (Overfitting Prevention or Regularization) \u00b6 Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World They propose to use domain randomization to train deep learning algorithms on synthetic data and transferring to real-world data The idea is that with sufficient variability in the textures of synthetic data, real-world data becomes another variation of the synthetic data It works surprisingly well and it's a simple technique of varying image textures essentially enabling CNNs to be more robust to variations in image textures Optimization \u00b6 Online Learning Rate Adaptation with Hypergradient Descent Reduces the need for learning rate scheduling for SGD, SGD and nesterov momentum, and Adam Uses the concept of hypergradients (gradients w.r.t. learning rate) obtained via reverse-mode automatic differentiation to dynamically update learning rates in real-time alongside weight updates Little additional computation because just needs just one additional copy of original gradients store in memory Severely under-appreciated paper Network Compression \u00b6 Energy-constrained Compression for Deep Neural Networks via Weighted Sparse Projection and Layer Input Masking More production applications of DNN require low-energy consumption environment like self-driving cars, VR goggles, and drones As such it's critical to optimize DNN not for its primary performance (accuracy etc.) but for its energy consumption performance too In the DNN training, this paper introduces an energy budget constraint on top of other optimization objectives This allows optimization of multiple objectives simultaneously (top-1 accuracy and energy consumption for example) It's done through weighted sparse projection and layer input masking Architecture Search \u00b6 DARTS: Differentiable Architecture Search Neural search algorithm based on gradient descent and continuous relaxation in the architecture space. A good move towards automatic architecture designs of neural networks. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks Scales all dimensions of a CNN, resolution/depth/width using compound coefficient Uses neural architecture search Network Pruning \u00b6 EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis Compared to existing Hessian-based methods, this works on the KFE Reported 10x reduction in model size and 8x reduction in FLOPs on Wide ResNet32 (WRN32) Bayesian Deep Learning \u00b6 Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam Variational Adam (Vadam), an alternative to varianal inference via dropout. Vadam perturbs the network's weights when backpropagating, allowing low computation cost uncertainty estimates. Not as good as dropout in terms of performance, but a good direction for computationally cheaper options. Explainability \u00b6 A Unified Approach to Intepreting Model Predictions Introduces SHAP (SHapley Additive exPlanations) \"SHAP assigns each feature an importance value for a particular prediction\" Higher positive SHAP values (red) = increase the probability of the class Higher negative SHAP values (blue) = decrease the probability of the class Hierarchical interpretations for neural network predictions Given a prediction from the deep neural network, agglomerative contextual decomposition (ACD) produces a hierarchical clusters of input features alongside cluster-wise contribution to the final prediction. The hierarchical clustering is then optimized to identify learned clusters driving the DNN's predictions. Cautious \u00b6 Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing Shows through scatterplots that multiple toy datasets although visually very different can have similar summary statistics like mean, standard deviation and pearson correlation This paper emphasises the need to always visualize your data Visualization \u00b6 Netron Easily visualize your saved deep learning models (PyTorch .pth, TensorFlow .pb, MXNet .model, ONNX, and more) You can even check out each node's documentation quickly in the interface Missing Values \u00b6 BRITS If you face problems in missing data in your time series and you use existing imputation methods, there is an alternative called BRITS where it learns missing values in time series via a bidirectional recurrency dynamical system Correlation \u00b6 DCCA: Deep Canonical Correlation Analysis Learn non-linear complex transformations such that resulting transformed data have high linear correlation Alternative to non-parametric methods like kernel canonical correlation analysis (KCCA) and non-linear extension of canonical correlation analysis (CCA) Shown to learn higher correlation representations than CCA/KCCA Deep Reinforcement Learning \u00b6 An Empirical Analysis of Proximal Policy Optimization with Kronecker-factored Natural Gradients Shows 2 SOTA for deep RL currently (2018 / early 2019): PPO and ACKTR Attempts to combined PPO objective with K-FAC natural gradient optimization: PPOKFAC Does not improve sample complexity, stick with either PPO/ACKTR for now","title":"Additional Readings"},{"location":"deep_learning/readings/#long-tailed-recognition","text":"Large-Scale Long-Tailed Recognition in an Open World Frequently in real world scenario there're new unseen classes or samples within the tail classes This tackles the problem with dynamic embedding to bring associative memory to aid prediction of long-tailed classes The model essentially combines direct image features with embeddings from other classes","title":"Long Tailed Recognition"},{"location":"deep_learning/readings/#better-generalization-overfitting-prevention-or-regularization","text":"Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World They propose to use domain randomization to train deep learning algorithms on synthetic data and transferring to real-world data The idea is that with sufficient variability in the textures of synthetic data, real-world data becomes another variation of the synthetic data It works surprisingly well and it's a simple technique of varying image textures essentially enabling CNNs to be more robust to variations in image textures","title":"Better Generalization (Overfitting Prevention or Regularization)"},{"location":"deep_learning/readings/#optimization","text":"Online Learning Rate Adaptation with Hypergradient Descent Reduces the need for learning rate scheduling for SGD, SGD and nesterov momentum, and Adam Uses the concept of hypergradients (gradients w.r.t. learning rate) obtained via reverse-mode automatic differentiation to dynamically update learning rates in real-time alongside weight updates Little additional computation because just needs just one additional copy of original gradients store in memory Severely under-appreciated paper","title":"Optimization"},{"location":"deep_learning/readings/#network-compression","text":"Energy-constrained Compression for Deep Neural Networks via Weighted Sparse Projection and Layer Input Masking More production applications of DNN require low-energy consumption environment like self-driving cars, VR goggles, and drones As such it's critical to optimize DNN not for its primary performance (accuracy etc.) but for its energy consumption performance too In the DNN training, this paper introduces an energy budget constraint on top of other optimization objectives This allows optimization of multiple objectives simultaneously (top-1 accuracy and energy consumption for example) It's done through weighted sparse projection and layer input masking","title":"Network Compression"},{"location":"deep_learning/readings/#architecture-search","text":"DARTS: Differentiable Architecture Search Neural search algorithm based on gradient descent and continuous relaxation in the architecture space. A good move towards automatic architecture designs of neural networks. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks Scales all dimensions of a CNN, resolution/depth/width using compound coefficient Uses neural architecture search","title":"Architecture Search"},{"location":"deep_learning/readings/#network-pruning","text":"EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis Compared to existing Hessian-based methods, this works on the KFE Reported 10x reduction in model size and 8x reduction in FLOPs on Wide ResNet32 (WRN32)","title":"Network Pruning"},{"location":"deep_learning/readings/#bayesian-deep-learning","text":"Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam Variational Adam (Vadam), an alternative to varianal inference via dropout. Vadam perturbs the network's weights when backpropagating, allowing low computation cost uncertainty estimates. Not as good as dropout in terms of performance, but a good direction for computationally cheaper options.","title":"Bayesian Deep Learning"},{"location":"deep_learning/readings/#explainability","text":"A Unified Approach to Intepreting Model Predictions Introduces SHAP (SHapley Additive exPlanations) \"SHAP assigns each feature an importance value for a particular prediction\" Higher positive SHAP values (red) = increase the probability of the class Higher negative SHAP values (blue) = decrease the probability of the class Hierarchical interpretations for neural network predictions Given a prediction from the deep neural network, agglomerative contextual decomposition (ACD) produces a hierarchical clusters of input features alongside cluster-wise contribution to the final prediction. The hierarchical clustering is then optimized to identify learned clusters driving the DNN's predictions.","title":"Explainability"},{"location":"deep_learning/readings/#cautious","text":"Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing Shows through scatterplots that multiple toy datasets although visually very different can have similar summary statistics like mean, standard deviation and pearson correlation This paper emphasises the need to always visualize your data","title":"Cautious"},{"location":"deep_learning/readings/#visualization","text":"Netron Easily visualize your saved deep learning models (PyTorch .pth, TensorFlow .pb, MXNet .model, ONNX, and more) You can even check out each node's documentation quickly in the interface","title":"Visualization"},{"location":"deep_learning/readings/#missing-values","text":"BRITS If you face problems in missing data in your time series and you use existing imputation methods, there is an alternative called BRITS where it learns missing values in time series via a bidirectional recurrency dynamical system","title":"Missing Values"},{"location":"deep_learning/readings/#correlation","text":"DCCA: Deep Canonical Correlation Analysis Learn non-linear complex transformations such that resulting transformed data have high linear correlation Alternative to non-parametric methods like kernel canonical correlation analysis (KCCA) and non-linear extension of canonical correlation analysis (CCA) Shown to learn higher correlation representations than CCA/KCCA","title":"Correlation"},{"location":"deep_learning/readings/#deep-reinforcement-learning","text":"An Empirical Analysis of Proximal Policy Optimization with Kronecker-factored Natural Gradients Shows 2 SOTA for deep RL currently (2018 / early 2019): PPO and ACKTR Attempts to combined PPO objective with K-FAC natural gradient optimization: PPOKFAC Does not improve sample complexity, stick with either PPO/ACKTR for now","title":"Deep Reinforcement Learning"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/","text":"Derivative, Gradient and Jacobian \u00b6 Run Jupyter Notebook You can run the code for this section in this jupyter notebook link . Simplified Equation \u00b6 This is the simplified equation we have been using on how we update our parameters to reach good values (good local or global minima) \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our tensors with gradient accumulation abilities) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : gradients of loss with respect to the model's parameters Even simplier equation in English: parameters = parameters - learning_rate * parameters_gradients This process can be broken down into 2 sequential parts Backpropagation Gradient descent Simplified Equation Breakdown \u00b6 Our simplified equation can be broken down into 2 parts Backpropagation: getting our gradients Our partial derivatives of loss (scalar number) with respect to (w.r.t.) our model's parameters and w.r.t. our input Backpropagation gets us \\nabla_\\theta \\nabla_\\theta which is our gradient Gradient descent: using our gradients to update our parameters Somehow, the terms backpropagation and gradient descent are often mixed together. But they're totally different. Gradient descent relates to using our gradients obtained from backpropagation to update our weights. Gradient descent: \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta Steps \u00b6 Derivatives Partial Derivatives Gradients Gradient, Jacobian and Generalized Jacobian Differences Backpropagation: computing gradients Gradient descent: using gradients to update parameters Derivative \u00b6 Given a simple cubic equation: f(x) = 2x^3 + 5 f(x) = 2x^3 + 5 Calculating the derivative \\frac{df(x)}{dx} \\frac{df(x)}{dx} is simply calculating the difference in values of y y for an extremely small (infinitesimally) change in value of x x which is frequently labelled as h h \\frac{df(x)}{dx} = \\displaystyle{\\lim_{h \\to 0}} \\frac{f(x + h) - f(x)}{h} \\frac{df(x)}{dx} = \\displaystyle{\\lim_{h \\to 0}} \\frac{f(x + h) - f(x)}{h} \\frac{f(x + h) - f(x)}{h} \\frac{f(x + h) - f(x)}{h} is the slope formula similar to what you may be familiar with: Change in y y over change in x x : \\frac{\\Delta y}{\\Delta x} \\frac{\\Delta y}{\\Delta x} And the derivative is the slope when h \\rightarrow 0 h \\rightarrow 0 , in essence a super teeny small h h Let's break down \\frac{df}{dx} = \\displaystyle{\\lim_{h \\to 0}} \\frac{f(x + h) - f(x)}{h} \\frac{df}{dx} = \\displaystyle{\\lim_{h \\to 0}} \\frac{f(x + h) - f(x)}{h} \\displaystyle{\\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{(2(x+h)^3 + 5) - 2x^3 + 5}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{(2(x+h)^3 + 5) - 2x^3 + 5}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{2(x^2 + 2xh + h^2)(x+h) - 2x^3}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{2(x^2 + 2xh + h^2)(x+h) - 2x^3}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{2(x^3 + 2x^2h + h^3 + x^2h + 2xh^2 + h^3) - 2x^3}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{2(x^3 + 2x^2h + h^3 + x^2h + 2xh^2 + h^3) - 2x^3}{h}} \\displaystyle{\\lim_{h \\to 0}\\frac{2(x^3 + 3x^2h + h^3 + 2xh^2) - 2x^3}{h}} \\displaystyle{\\lim_{h \\to 0}\\frac{2(x^3 + 3x^2h + h^3 + 2xh^2) - 2x^3}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{6x^2h + h^3 + 2xh^2}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{6x^2h + h^3 + 2xh^2}{h}} \\displaystyle{\\lim_{h \\to 0} 6x^2 + h^2 + 2xh} = 6x^2 \\displaystyle{\\lim_{h \\to 0} 6x^2 + h^2 + 2xh} = 6x^2 Partial Derivative \u00b6 Ok, it's simple to calculate our derivative when we've only one variable in our function. If we've more than one (as with our parameters in our models), we need to calculate our partial derivatives of our function with respect to our variables Given a simple equation f(x, z) = 4x^4z^3 f(x, z) = 4x^4z^3 , let us get our partial derivatives 2 parts: partial derivative of our function w.r.t. x and z Partial derivative of our function w.r.t. x: \\frac{\\delta f(x, z)}{\\delta x} \\frac{\\delta f(x, z)}{\\delta x} Let z z term be a constant, a a f(x, z) = 4x^4a f(x, z) = 4x^4a \\frac{\\delta f(x, z)}{\\delta x} = 16x^3a \\frac{\\delta f(x, z)}{\\delta x} = 16x^3a Now we substitute a a with our z term, a = z^3 a = z^3 \\frac{\\delta f(x, z)}{\\delta x} = 16x^3z^3 \\frac{\\delta f(x, z)}{\\delta x} = 16x^3z^3 Partial derivative of our function w.r.t. z: \\frac{\\delta f(x, z)}{\\delta z} \\frac{\\delta f(x, z)}{\\delta z} Let x x term be a constant, a a f(x, z) = 4az^3 f(x, z) = 4az^3 \\frac{\\delta f(x, z)}{\\delta z} = 12az^2 \\frac{\\delta f(x, z)}{\\delta z} = 12az^2 Now we substitute a a with our x x term, a = x^4 a = x^4 \\frac{\\delta f(x, z)}{\\delta z} = 12x^4z^2 \\frac{\\delta f(x, z)}{\\delta z} = 12x^4z^2 Ta da! We made it, we calculated our partial derivatives of our function w.r.t. the different variables Gradient \u00b6 We can now put all our partial derivatives into a vector of partial derivatives Also called \"gradient\" Represented by \\nabla_{(x,z)} \\nabla_{(x,z)} \\nabla_{(x,z)} = \\begin{bmatrix} \\frac{df(x,z)}{dx} \\\\ \\frac{df(x,z)}{dz} \\end{bmatrix} = \\begin{bmatrix} 16x^3z^3 \\\\ 12x^4z^2 \\end{bmatrix} \\nabla_{(x,z)} = \\begin{bmatrix} \\frac{df(x,z)}{dx} \\\\ \\frac{df(x,z)}{dz} \\end{bmatrix} = \\begin{bmatrix} 16x^3z^3 \\\\ 12x^4z^2 \\end{bmatrix} It is critical to note that the term gradient applies for f : \\mathbb{R}^N \\rightarrow \\mathbb{R} f : \\mathbb{R}^N \\rightarrow \\mathbb{R} Where our function maps a vector input to a scalar output: in deep learning, our loss function that produces a scalar loss Gradient, Jacobian, and Generalized Jacobian \u00b6 In the case where we have non-scalar outputs, these are the right terms of matrices or vectors containing our partial derivatives Gradient: vector input to scalar output f : \\mathbb{R}^N \\rightarrow \\mathbb{R} f : \\mathbb{R}^N \\rightarrow \\mathbb{R} Jacobian: vector input to vector output f : \\mathbb{R}^N \\rightarrow \\mathbb{R}^M f : \\mathbb{R}^N \\rightarrow \\mathbb{R}^M Generalized Jacobian: tensor input to tensor output In this case, a tensor can be any number of dimensions. Summary \u00b6 We've learnt to... Success Calculate derivatives Calculate partial derivatives Get gradients Differentiate the concepts amongst gradients, Jacobian and Generalized Jacobian Now it is time to move on to backpropagation and gradient descent for a simple 1 hidden layer FNN with all these concepts in mind. Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Derivative, Gradient and Jacobian"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#derivative-gradient-and-jacobian","text":"Run Jupyter Notebook You can run the code for this section in this jupyter notebook link .","title":"Derivative, Gradient and Jacobian"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#simplified-equation","text":"This is the simplified equation we have been using on how we update our parameters to reach good values (good local or global minima) \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our tensors with gradient accumulation abilities) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : gradients of loss with respect to the model's parameters Even simplier equation in English: parameters = parameters - learning_rate * parameters_gradients This process can be broken down into 2 sequential parts Backpropagation Gradient descent","title":"Simplified Equation"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#simplified-equation-breakdown","text":"Our simplified equation can be broken down into 2 parts Backpropagation: getting our gradients Our partial derivatives of loss (scalar number) with respect to (w.r.t.) our model's parameters and w.r.t. our input Backpropagation gets us \\nabla_\\theta \\nabla_\\theta which is our gradient Gradient descent: using our gradients to update our parameters Somehow, the terms backpropagation and gradient descent are often mixed together. But they're totally different. Gradient descent relates to using our gradients obtained from backpropagation to update our weights. Gradient descent: \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta","title":"Simplified Equation Breakdown"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#steps","text":"Derivatives Partial Derivatives Gradients Gradient, Jacobian and Generalized Jacobian Differences Backpropagation: computing gradients Gradient descent: using gradients to update parameters","title":"Steps"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#derivative","text":"Given a simple cubic equation: f(x) = 2x^3 + 5 f(x) = 2x^3 + 5 Calculating the derivative \\frac{df(x)}{dx} \\frac{df(x)}{dx} is simply calculating the difference in values of y y for an extremely small (infinitesimally) change in value of x x which is frequently labelled as h h \\frac{df(x)}{dx} = \\displaystyle{\\lim_{h \\to 0}} \\frac{f(x + h) - f(x)}{h} \\frac{df(x)}{dx} = \\displaystyle{\\lim_{h \\to 0}} \\frac{f(x + h) - f(x)}{h} \\frac{f(x + h) - f(x)}{h} \\frac{f(x + h) - f(x)}{h} is the slope formula similar to what you may be familiar with: Change in y y over change in x x : \\frac{\\Delta y}{\\Delta x} \\frac{\\Delta y}{\\Delta x} And the derivative is the slope when h \\rightarrow 0 h \\rightarrow 0 , in essence a super teeny small h h Let's break down \\frac{df}{dx} = \\displaystyle{\\lim_{h \\to 0}} \\frac{f(x + h) - f(x)}{h} \\frac{df}{dx} = \\displaystyle{\\lim_{h \\to 0}} \\frac{f(x + h) - f(x)}{h} \\displaystyle{\\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{(2(x+h)^3 + 5) - 2x^3 + 5}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{(2(x+h)^3 + 5) - 2x^3 + 5}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{2(x^2 + 2xh + h^2)(x+h) - 2x^3}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{2(x^2 + 2xh + h^2)(x+h) - 2x^3}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{2(x^3 + 2x^2h + h^3 + x^2h + 2xh^2 + h^3) - 2x^3}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{2(x^3 + 2x^2h + h^3 + x^2h + 2xh^2 + h^3) - 2x^3}{h}} \\displaystyle{\\lim_{h \\to 0}\\frac{2(x^3 + 3x^2h + h^3 + 2xh^2) - 2x^3}{h}} \\displaystyle{\\lim_{h \\to 0}\\frac{2(x^3 + 3x^2h + h^3 + 2xh^2) - 2x^3}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{6x^2h + h^3 + 2xh^2}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{6x^2h + h^3 + 2xh^2}{h}} \\displaystyle{\\lim_{h \\to 0} 6x^2 + h^2 + 2xh} = 6x^2 \\displaystyle{\\lim_{h \\to 0} 6x^2 + h^2 + 2xh} = 6x^2","title":"Derivative"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#partial-derivative","text":"Ok, it's simple to calculate our derivative when we've only one variable in our function. If we've more than one (as with our parameters in our models), we need to calculate our partial derivatives of our function with respect to our variables Given a simple equation f(x, z) = 4x^4z^3 f(x, z) = 4x^4z^3 , let us get our partial derivatives 2 parts: partial derivative of our function w.r.t. x and z Partial derivative of our function w.r.t. x: \\frac{\\delta f(x, z)}{\\delta x} \\frac{\\delta f(x, z)}{\\delta x} Let z z term be a constant, a a f(x, z) = 4x^4a f(x, z) = 4x^4a \\frac{\\delta f(x, z)}{\\delta x} = 16x^3a \\frac{\\delta f(x, z)}{\\delta x} = 16x^3a Now we substitute a a with our z term, a = z^3 a = z^3 \\frac{\\delta f(x, z)}{\\delta x} = 16x^3z^3 \\frac{\\delta f(x, z)}{\\delta x} = 16x^3z^3 Partial derivative of our function w.r.t. z: \\frac{\\delta f(x, z)}{\\delta z} \\frac{\\delta f(x, z)}{\\delta z} Let x x term be a constant, a a f(x, z) = 4az^3 f(x, z) = 4az^3 \\frac{\\delta f(x, z)}{\\delta z} = 12az^2 \\frac{\\delta f(x, z)}{\\delta z} = 12az^2 Now we substitute a a with our x x term, a = x^4 a = x^4 \\frac{\\delta f(x, z)}{\\delta z} = 12x^4z^2 \\frac{\\delta f(x, z)}{\\delta z} = 12x^4z^2 Ta da! We made it, we calculated our partial derivatives of our function w.r.t. the different variables","title":"Partial Derivative"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#gradient","text":"We can now put all our partial derivatives into a vector of partial derivatives Also called \"gradient\" Represented by \\nabla_{(x,z)} \\nabla_{(x,z)} \\nabla_{(x,z)} = \\begin{bmatrix} \\frac{df(x,z)}{dx} \\\\ \\frac{df(x,z)}{dz} \\end{bmatrix} = \\begin{bmatrix} 16x^3z^3 \\\\ 12x^4z^2 \\end{bmatrix} \\nabla_{(x,z)} = \\begin{bmatrix} \\frac{df(x,z)}{dx} \\\\ \\frac{df(x,z)}{dz} \\end{bmatrix} = \\begin{bmatrix} 16x^3z^3 \\\\ 12x^4z^2 \\end{bmatrix} It is critical to note that the term gradient applies for f : \\mathbb{R}^N \\rightarrow \\mathbb{R} f : \\mathbb{R}^N \\rightarrow \\mathbb{R} Where our function maps a vector input to a scalar output: in deep learning, our loss function that produces a scalar loss","title":"Gradient"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#gradient-jacobian-and-generalized-jacobian","text":"In the case where we have non-scalar outputs, these are the right terms of matrices or vectors containing our partial derivatives Gradient: vector input to scalar output f : \\mathbb{R}^N \\rightarrow \\mathbb{R} f : \\mathbb{R}^N \\rightarrow \\mathbb{R} Jacobian: vector input to vector output f : \\mathbb{R}^N \\rightarrow \\mathbb{R}^M f : \\mathbb{R}^N \\rightarrow \\mathbb{R}^M Generalized Jacobian: tensor input to tensor output In this case, a tensor can be any number of dimensions.","title":"Gradient, Jacobian, and Generalized Jacobian"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#summary","text":"We've learnt to... Success Calculate derivatives Calculate partial derivatives Get gradients Differentiate the concepts amongst gradients, Jacobian and Generalized Jacobian Now it is time to move on to backpropagation and gradient descent for a simple 1 hidden layer FNN with all these concepts in mind.","title":"Summary"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/boosting_models_pytorch/forwardpropagation_backpropagation_gradientdescent/","text":"Forwardpropagation, Backpropagation and Gradient Descent with PyTorch \u00b6 Run Jupyter Notebook You can run the code for this section in this jupyter notebook link . Transiting to Backpropagation \u00b6 Let's go back to our simple FNN to put things in perspective Let us ignore non-linearities for now to keep it simpler, but it's just a tiny change subsequently Given a linear transformation on our input (for simplicity instead of an affine transformation that includes a bias): \\hat y = \\theta x \\hat y = \\theta x \\theta \\theta is our parameters x x is our input \\hat y \\hat y is our prediction Then we have our MSE loss function L = \\frac{1}{2} (\\hat y - y)^2 L = \\frac{1}{2} (\\hat y - y)^2 We need to calculate our partial derivatives of our loss w.r.t. our parameters to update our parameters: \\nabla_{\\theta} = \\frac{\\delta L}{\\delta \\theta} \\nabla_{\\theta} = \\frac{\\delta L}{\\delta \\theta} With chain rule we have \\frac{\\delta L}{\\delta \\theta} = \\frac{\\delta L}{\\delta \\hat y} \\frac{\\delta \\hat y}{\\delta \\theta} \\frac{\\delta L}{\\delta \\theta} = \\frac{\\delta L}{\\delta \\hat y} \\frac{\\delta \\hat y}{\\delta \\theta} \\frac{\\delta L}{\\delta \\hat y} = (\\hat y - y) \\frac{\\delta L}{\\delta \\hat y} = (\\hat y - y) \\frac{\\delta \\hat y}{\\delta \\theta} \\frac{\\delta \\hat y}{\\delta \\theta} is our partial derivatives of y y w.r.t. our parameters (our gradient) as we have covered previously Forward Propagation, Backward Propagation and Gradient Descent \u00b6 All right, now let's put together what we have learnt on backpropagation and apply it on a simple feedforward neural network (FNN) Let us assume the following simple FNN architecture and take note that we do not have bias here to keep things simple FNN architecture Linear function: hidden size = 32 Non-linear function: sigmoid Linear function: output size = 1 Non-linear function: sigmoid We will be going through a binary classification problem classifying 2 types of flowers Output size: 1 (represented by 0 or 1 depending on the flower) Input size: 2 (features of the flower) Number of training samples: 100 Load 3-class dataset We want to set a seed to encourage reproducibility so you can match our loss numbers. import torch import torch.nn as nn # Set manual seed torch . manual_seed ( 2 ) Here we want to load our flower classification dataset of 150 samples. There are 2 features, hence the input size would be 150x2. There is no one-hot encoding so the output would not be a size of 150x3 but a size of 150x1. from sklearn import datasets from sklearn import preprocessing iris = datasets . load_iris () X = torch . tensor ( preprocessing . normalize ( iris . data [:, : 2 ]), dtype = torch . float ) y = torch . tensor ( iris . target . reshape ( - 1 , 1 ), dtype = torch . float ) print ( X . size ()) print ( y . size ()) torch . Size ([ 150 , 2 ]) torch . Size ([ 150 , 1 ]) From 3 class dataset to 2 class dataset We only want 2 classes because we want a binary classification problem. As mentioned, there is no one-hot encoding, so each class is represented by 0, 1, or 2. All we need to do is to filter out all samples with a label of 2 to have 2 classes. # We only take 2 classes to make a binary classification problem X = X [: y [ y < 2 ] . size ()[ 0 ]] y = y [: y [ y < 2 ] . size ()[ 0 ]] ```` ``` python print ( X . size ()) print ( y . size ()) torch . Size ([ 100 , 2 ]) torch . Size ([ 100 , 1 ]) Building our FNN model class from scratch class FNN ( nn . Module ): def __init__ ( self , ): super () . __init__ () # Dimensions for input, hidden and output self . input_dim = 2 self . hidden_dim = 32 self . output_dim = 1 # Learning rate definition self . learning_rate = 0.001 # Our parameters (weights) # w1: 2 x 32 self . w1 = torch . randn ( self . input_dim , self . hidden_dim ) # w2: 32 x 1 self . w2 = torch . randn ( self . hidden_dim , self . output_dim ) def sigmoid ( self , s ): return 1 / ( 1 + torch . exp ( - s )) def sigmoid_first_order_derivative ( self , s ): return s * ( 1 - s ) # Forward propagation def forward ( self , X ): # First linear layer self . y1 = torch . matmul ( X , self . w1 ) # 3 X 3 \".dot\" does not broadcast in PyTorch # First non-linearity self . y2 = self . sigmoid ( self . y1 ) # Second linear layer self . y3 = torch . matmul ( self . y2 , self . w2 ) # Second non-linearity y4 = self . sigmoid ( self . y3 ) return y4 # Backward propagation def backward ( self , X , l , y4 ): # Derivative of binary cross entropy cost w.r.t. final output y4 self . dC_dy4 = y4 - l ''' Gradients for w2: partial derivative of cost w.r.t. w2 dC/dw2 ''' self . dy4_dy3 = self . sigmoid_first_order_derivative ( y4 ) self . dy3_dw2 = self . y2 # Y4 delta: dC_dy4 dy4_dy3 self . y4_delta = self . dC_dy4 * self . dy4_dy3 # This is our gradients for w1: dC_dy4 dy4_dy3 dy3_dw2 self . dC_dw2 = torch . matmul ( torch . t ( self . dy3_dw2 ), self . y4_delta ) ''' Gradients for w1: partial derivative of cost w.r.t w1 dC/dw1 ''' self . dy3_dy2 = self . w2 self . dy2_dy1 = self . sigmoid_first_order_derivative ( self . y2 ) # Y2 delta: (dC_dy4 dy4_dy3) dy3_dy2 dy2_dy1 self . y2_delta = torch . matmul ( self . y4_delta , torch . t ( self . dy3_dy2 )) * self . dy2_dy1 # Gradients for w1: (dC_dy4 dy4_dy3) dy3_dy2 dy2_dy1 dy1_dw1 self . dC_dw1 = torch . matmul ( torch . t ( X ), self . y2_delta ) # Gradient descent on the weights from our 2 linear layers self . w1 -= self . learning_rate * self . dC_dw1 self . w2 -= self . learning_rate * self . dC_dw2 def train ( self , X , l ): # Forward propagation y4 = self . forward ( X ) # Backward propagation and gradient descent self . backward ( X , l , y4 ) Training our FNN model # Instantiate our model class and assign it to our model object model = FNN () # Loss list for plotting of loss behaviour loss_lst = [] # Number of times we want our FNN to look at all 100 samples we have, 100 implies looking through 100x num_epochs = 101 # Let's train our model with 100 epochs for epoch in range ( num_epochs ): # Get our predictions y_hat = model ( X ) # Cross entropy loss, remember this can never be negative by nature of the equation # But it does not mean the loss can't be negative for other loss functions cross_entropy_loss = - ( y * torch . log ( y_hat ) + ( 1 - y ) * torch . log ( 1 - y_hat )) # We have to take cross entropy loss over all our samples, 100 in this 2-class iris dataset mean_cross_entropy_loss = torch . mean ( cross_entropy_loss ) . detach () . item () # Print our mean cross entropy loss if epoch % 20 == 0 : print ( 'Epoch {} | Loss: {} ' . format ( epoch , mean_cross_entropy_loss )) loss_lst . append ( mean_cross_entropy_loss ) # (1) Forward propagation: to get our predictions to pass to our cross entropy loss function # (2) Back propagation: get our partial derivatives w.r.t. parameters (gradients) # (3) Gradient Descent: update our weights with our gradients model . train ( X , y ) Epoch 0 | Loss : 0.9228229522705078 Epoch 20 | Loss : 0.6966760754585266 Epoch 40 | Loss : 0.6714916229248047 Epoch 60 | Loss : 0.6686137914657593 Epoch 80 | Loss : 0.666690468788147 Epoch 100 | Loss : 0.6648102402687073 Our loss is decreasing gradually, so it's learning. It has a possibility of reducing to almost 0 (overfitting) with sufficient model capacity (more layers or wider layers). We will explore overfitting and learning rate optimization subsequently. Summary \u00b6 We've learnt... Success The math behind forwardpropagation, backwardpropagation and gradient descent for FNN Implement a basic FNN from scratch with PyTorch Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Forward- and Backward-propagation and Gradient Descent (From Scratch FNN Regression)"},{"location":"deep_learning/boosting_models_pytorch/forwardpropagation_backpropagation_gradientdescent/#forwardpropagation-backpropagation-and-gradient-descent-with-pytorch","text":"Run Jupyter Notebook You can run the code for this section in this jupyter notebook link .","title":"Forwardpropagation, Backpropagation and Gradient Descent with PyTorch"},{"location":"deep_learning/boosting_models_pytorch/forwardpropagation_backpropagation_gradientdescent/#transiting-to-backpropagation","text":"Let's go back to our simple FNN to put things in perspective Let us ignore non-linearities for now to keep it simpler, but it's just a tiny change subsequently Given a linear transformation on our input (for simplicity instead of an affine transformation that includes a bias): \\hat y = \\theta x \\hat y = \\theta x \\theta \\theta is our parameters x x is our input \\hat y \\hat y is our prediction Then we have our MSE loss function L = \\frac{1}{2} (\\hat y - y)^2 L = \\frac{1}{2} (\\hat y - y)^2 We need to calculate our partial derivatives of our loss w.r.t. our parameters to update our parameters: \\nabla_{\\theta} = \\frac{\\delta L}{\\delta \\theta} \\nabla_{\\theta} = \\frac{\\delta L}{\\delta \\theta} With chain rule we have \\frac{\\delta L}{\\delta \\theta} = \\frac{\\delta L}{\\delta \\hat y} \\frac{\\delta \\hat y}{\\delta \\theta} \\frac{\\delta L}{\\delta \\theta} = \\frac{\\delta L}{\\delta \\hat y} \\frac{\\delta \\hat y}{\\delta \\theta} \\frac{\\delta L}{\\delta \\hat y} = (\\hat y - y) \\frac{\\delta L}{\\delta \\hat y} = (\\hat y - y) \\frac{\\delta \\hat y}{\\delta \\theta} \\frac{\\delta \\hat y}{\\delta \\theta} is our partial derivatives of y y w.r.t. our parameters (our gradient) as we have covered previously","title":"Transiting to Backpropagation"},{"location":"deep_learning/boosting_models_pytorch/forwardpropagation_backpropagation_gradientdescent/#forward-propagation-backward-propagation-and-gradient-descent","text":"All right, now let's put together what we have learnt on backpropagation and apply it on a simple feedforward neural network (FNN) Let us assume the following simple FNN architecture and take note that we do not have bias here to keep things simple FNN architecture Linear function: hidden size = 32 Non-linear function: sigmoid Linear function: output size = 1 Non-linear function: sigmoid We will be going through a binary classification problem classifying 2 types of flowers Output size: 1 (represented by 0 or 1 depending on the flower) Input size: 2 (features of the flower) Number of training samples: 100 Load 3-class dataset We want to set a seed to encourage reproducibility so you can match our loss numbers. import torch import torch.nn as nn # Set manual seed torch . manual_seed ( 2 ) Here we want to load our flower classification dataset of 150 samples. There are 2 features, hence the input size would be 150x2. There is no one-hot encoding so the output would not be a size of 150x3 but a size of 150x1. from sklearn import datasets from sklearn import preprocessing iris = datasets . load_iris () X = torch . tensor ( preprocessing . normalize ( iris . data [:, : 2 ]), dtype = torch . float ) y = torch . tensor ( iris . target . reshape ( - 1 , 1 ), dtype = torch . float ) print ( X . size ()) print ( y . size ()) torch . Size ([ 150 , 2 ]) torch . Size ([ 150 , 1 ]) From 3 class dataset to 2 class dataset We only want 2 classes because we want a binary classification problem. As mentioned, there is no one-hot encoding, so each class is represented by 0, 1, or 2. All we need to do is to filter out all samples with a label of 2 to have 2 classes. # We only take 2 classes to make a binary classification problem X = X [: y [ y < 2 ] . size ()[ 0 ]] y = y [: y [ y < 2 ] . size ()[ 0 ]] ```` ``` python print ( X . size ()) print ( y . size ()) torch . Size ([ 100 , 2 ]) torch . Size ([ 100 , 1 ]) Building our FNN model class from scratch class FNN ( nn . Module ): def __init__ ( self , ): super () . __init__ () # Dimensions for input, hidden and output self . input_dim = 2 self . hidden_dim = 32 self . output_dim = 1 # Learning rate definition self . learning_rate = 0.001 # Our parameters (weights) # w1: 2 x 32 self . w1 = torch . randn ( self . input_dim , self . hidden_dim ) # w2: 32 x 1 self . w2 = torch . randn ( self . hidden_dim , self . output_dim ) def sigmoid ( self , s ): return 1 / ( 1 + torch . exp ( - s )) def sigmoid_first_order_derivative ( self , s ): return s * ( 1 - s ) # Forward propagation def forward ( self , X ): # First linear layer self . y1 = torch . matmul ( X , self . w1 ) # 3 X 3 \".dot\" does not broadcast in PyTorch # First non-linearity self . y2 = self . sigmoid ( self . y1 ) # Second linear layer self . y3 = torch . matmul ( self . y2 , self . w2 ) # Second non-linearity y4 = self . sigmoid ( self . y3 ) return y4 # Backward propagation def backward ( self , X , l , y4 ): # Derivative of binary cross entropy cost w.r.t. final output y4 self . dC_dy4 = y4 - l ''' Gradients for w2: partial derivative of cost w.r.t. w2 dC/dw2 ''' self . dy4_dy3 = self . sigmoid_first_order_derivative ( y4 ) self . dy3_dw2 = self . y2 # Y4 delta: dC_dy4 dy4_dy3 self . y4_delta = self . dC_dy4 * self . dy4_dy3 # This is our gradients for w1: dC_dy4 dy4_dy3 dy3_dw2 self . dC_dw2 = torch . matmul ( torch . t ( self . dy3_dw2 ), self . y4_delta ) ''' Gradients for w1: partial derivative of cost w.r.t w1 dC/dw1 ''' self . dy3_dy2 = self . w2 self . dy2_dy1 = self . sigmoid_first_order_derivative ( self . y2 ) # Y2 delta: (dC_dy4 dy4_dy3) dy3_dy2 dy2_dy1 self . y2_delta = torch . matmul ( self . y4_delta , torch . t ( self . dy3_dy2 )) * self . dy2_dy1 # Gradients for w1: (dC_dy4 dy4_dy3) dy3_dy2 dy2_dy1 dy1_dw1 self . dC_dw1 = torch . matmul ( torch . t ( X ), self . y2_delta ) # Gradient descent on the weights from our 2 linear layers self . w1 -= self . learning_rate * self . dC_dw1 self . w2 -= self . learning_rate * self . dC_dw2 def train ( self , X , l ): # Forward propagation y4 = self . forward ( X ) # Backward propagation and gradient descent self . backward ( X , l , y4 ) Training our FNN model # Instantiate our model class and assign it to our model object model = FNN () # Loss list for plotting of loss behaviour loss_lst = [] # Number of times we want our FNN to look at all 100 samples we have, 100 implies looking through 100x num_epochs = 101 # Let's train our model with 100 epochs for epoch in range ( num_epochs ): # Get our predictions y_hat = model ( X ) # Cross entropy loss, remember this can never be negative by nature of the equation # But it does not mean the loss can't be negative for other loss functions cross_entropy_loss = - ( y * torch . log ( y_hat ) + ( 1 - y ) * torch . log ( 1 - y_hat )) # We have to take cross entropy loss over all our samples, 100 in this 2-class iris dataset mean_cross_entropy_loss = torch . mean ( cross_entropy_loss ) . detach () . item () # Print our mean cross entropy loss if epoch % 20 == 0 : print ( 'Epoch {} | Loss: {} ' . format ( epoch , mean_cross_entropy_loss )) loss_lst . append ( mean_cross_entropy_loss ) # (1) Forward propagation: to get our predictions to pass to our cross entropy loss function # (2) Back propagation: get our partial derivatives w.r.t. parameters (gradients) # (3) Gradient Descent: update our weights with our gradients model . train ( X , y ) Epoch 0 | Loss : 0.9228229522705078 Epoch 20 | Loss : 0.6966760754585266 Epoch 40 | Loss : 0.6714916229248047 Epoch 60 | Loss : 0.6686137914657593 Epoch 80 | Loss : 0.666690468788147 Epoch 100 | Loss : 0.6648102402687073 Our loss is decreasing gradually, so it's learning. It has a possibility of reducing to almost 0 (overfitting) with sufficient model capacity (more layers or wider layers). We will explore overfitting and learning rate optimization subsequently.","title":"Forward Propagation, Backward Propagation and Gradient Descent"},{"location":"deep_learning/boosting_models_pytorch/forwardpropagation_backpropagation_gradientdescent/#summary","text":"We've learnt... Success The math behind forwardpropagation, backwardpropagation and gradient descent for FNN Implement a basic FNN from scratch with PyTorch","title":"Summary"},{"location":"deep_learning/boosting_models_pytorch/forwardpropagation_backpropagation_gradientdescent/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/","text":"Learning Rate Scheduling \u00b6 Run Jupyter Notebook You can run the code for this section in this jupyter notebook link . Optimization Algorithm: Mini-batch Stochastic Gradient Descent (SGD) \u00b6 We will be using mini-batch gradient descent in all our examples here when scheduling our learning rate Combination of batch gradient descent & stochastic gradient descent \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) Characteristics Compute the gradient of the lost function w.r.t. parameters for n sets of training sample (n input and n label) , \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) Use this to update our parameters at every iteration Typically in deep learning, some variation of mini-batch gradient is used where the batch size is a hyperparameter to be determined Learning Intuition Recap \u00b6 Learning process Original parameters \\rightarrow \\rightarrow given input, get output \\rightarrow \\rightarrow compare with labels \\rightarrow \\rightarrow get loss with comparison of input/output \\rightarrow \\rightarrow get gradients of loss w.r.t parameters \\rightarrow \\rightarrow update parameters so model can churn output closer to labels \\rightarrow \\rightarrow repeat For a detailed mathematical account of how this works and how to implement from scratch in Python and PyTorch, you can read our forward- and back-propagation and gradient descent post . Learning Rate Pointers \u00b6 Update parameters so model can churn output closer to labels, lower loss \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) If we set \\eta \\eta to be a large value \\rightarrow \\rightarrow learn too much (rapid learning) Unable to converge to a good local minima (unable to effectively gradually decrease your loss, overshoot the local lowest value) If we set \\eta \\eta to be a small value \\rightarrow \\rightarrow learn too little (slow learning) May take too long or unable to convert to a good local minima Need for Learning Rate Schedules \u00b6 Benefits Converge faster Higher accuracy Top Basic Learning Rate Schedules \u00b6 Step-wise Decay Reduce on Loss Plateau Decay Step-wise Learning Rate Decay \u00b6 Step-wise Decay: Every Epoch \u00b6 At every epoch, \\eta_t = \\eta_{t-1}\\gamma \\eta_t = \\eta_{t-1}\\gamma \\gamma = 0.1 \\gamma = 0.1 Optimization Algorithm 4: SGD Nesterov Modification of SGD Momentum v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Practical example Given \\eta_t = 0.1 \\eta_t = 0.1 and $ \\gamma = 0.01$ Epoch 0: \\eta_t = 0.1 \\eta_t = 0.1 Epoch 1: \\eta_{t+1} = 0.1 (0.1) = 0.01 \\eta_{t+1} = 0.1 (0.1) = 0.01 Epoch 2: \\eta_{t+2} = 0.1 (0.1)^2 = 0.001 \\eta_{t+2} = 0.1 (0.1)^2 = 0.001 Epoch n: \\eta_{t+n} = 0.1 (0.1)^n \\eta_{t+n} = 0.1 (0.1)^n Code for step-wise learning rate decay at every epoch import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) # Where to add a new import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 1 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 1 , gamma = 0.1 ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500. Loss : 0.15292978286743164 . Accuracy : 96 Epoch : 1 LR : [ 0.010000000000000002 ] Iteration : 1000. Loss : 0.1207798570394516 . Accuracy : 97 Epoch : 2 LR : [ 0.0010000000000000002 ] Iteration : 1500. Loss : 0.12287932634353638 . Accuracy : 97 Epoch : 3 LR : [ 0.00010000000000000003 ] Iteration : 2000. Loss : 0.05614742264151573 . Accuracy : 97 Epoch : 4 LR : [ 1.0000000000000003e-05 ] Iteration : 2500. Loss : 0.06775809079408646 . Accuracy : 97 Iteration : 3000. Loss : 0.03737065941095352 . Accuracy : 97 Step-wise Decay: Every 2 Epochs \u00b6 At every 2 epoch, \\eta_t = \\eta_{t-1}\\gamma \\eta_t = \\eta_{t-1}\\gamma \\gamma = 0.1 \\gamma = 0.1 Optimization Algorithm 4: SGD Nesterov Modification of SGD Momentum v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Practical example Given \\eta_t = 0.1 \\eta_t = 0.1 and \\gamma = 0.01 \\gamma = 0.01 Epoch 0: \\eta_t = 0.1 \\eta_t = 0.1 Epoch 1: \\eta_{t+1} = 0.1 \\eta_{t+1} = 0.1 Epoch 2: \\eta_{t+2} = 0.1 (0.1) = 0.01 \\eta_{t+2} = 0.1 (0.1) = 0.01 Code for step-wise learning rate decay at every 2 epoch import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) # Where to add a new import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 2 , gamma = 0.1 ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500. Loss : 0.15292978286743164 . Accuracy : 96 Epoch : 1 LR : [ 0.1 ] Iteration : 1000. Loss : 0.11253029108047485 . Accuracy : 96 Epoch : 2 LR : [ 0.010000000000000002 ] Iteration : 1500. Loss : 0.14498558640480042 . Accuracy : 97 Epoch : 3 LR : [ 0.010000000000000002 ] Iteration : 2000. Loss : 0.03691177815198898 . Accuracy : 97 Epoch : 4 LR : [ 0.0010000000000000002 ] Iteration : 2500. Loss : 0.03511016443371773 . Accuracy : 97 Iteration : 3000. Loss : 0.029424520209431648 . Accuracy : 97 Step-wise Decay: Every Epoch, Larger Gamma \u00b6 At every epoch, \\eta_t = \\eta_{t-1}\\gamma \\eta_t = \\eta_{t-1}\\gamma \\gamma = 0.96 \\gamma = 0.96 Optimization Algorithm 4: SGD Nesterov Modification of SGD Momentum v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Practical example Given \\eta_t = 0.1 \\eta_t = 0.1 and \\gamma = 0.96 \\gamma = 0.96 Epoch 1: \\eta_t = 0.1 \\eta_t = 0.1 Epoch 2: \\eta_{t+1} = 0.1 (0.96) = 0.096 \\eta_{t+1} = 0.1 (0.96) = 0.096 Epoch 3: \\eta_{t+2} = 0.1 (0.96)^2 = 0.092 \\eta_{t+2} = 0.1 (0.96)^2 = 0.092 Epoch n: \\eta_{t+n} = 0.1 (0.96)^n \\eta_{t+n} = 0.1 (0.96)^n Code for step-wise learning rate decay at every epoch with larger gamma import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) # Where to add a new import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 2 , gamma = 0.96 ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500. Loss : 0.15292978286743164 . Accuracy : 96 Epoch : 1 LR : [ 0.1 ] Iteration : 1000. Loss : 0.11253029108047485 . Accuracy : 96 Epoch : 2 LR : [ 0.096 ] Iteration : 1500. Loss : 0.11864850670099258 . Accuracy : 97 Epoch : 3 LR : [ 0.096 ] Iteration : 2000. Loss : 0.030942382290959358 . Accuracy : 97 Epoch : 4 LR : [ 0.09216 ] Iteration : 2500. Loss : 0.04521659016609192 . Accuracy : 97 Iteration : 3000. Loss : 0.027839098125696182 . Accuracy : 97 Pointers on Step-wise Decay \u00b6 You would want to decay your LR gradually when you're training more epochs Converge too fast, to a crappy loss/accuracy, if you decay rapidly To decay slower Larger \\gamma \\gamma Larger interval of decay Reduce on Loss Plateau Decay \u00b6 Reduce on Loss Plateau Decay, Patience=0, Factor=0.1 \u00b6 Reduce learning rate whenever loss plateaus Patience: number of epochs with no improvement after which learning rate will be reduced Patience = 0 Factor: multiplier to decrease learning rate, lr = lr*factor = \\gamma lr = lr*factor = \\gamma Factor = 0.1 Optimization Algorithm: SGD Nesterov Modification of SGD Momentum v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Code for reduce on loss plateau learning rate decay of factor 0.1 and 0 patience import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) # Where to add a new import from torch.optim.lr_scheduler import ReduceLROnPlateau ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 6000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # lr = lr * factor # mode='max': look for the maximum validation accuracy to track # patience: number of epochs - 1 where loss plateaus before decreasing LR # patience = 0, after 1 bad epoch, reduce LR # factor = decaying factor scheduler = ReduceLROnPlateau ( optimizer , mode = 'max' , factor = 0.1 , patience = 0 , verbose = True ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions # Without .item(), it is a uint8 tensor which will not work when you pass this number to the scheduler correct += ( predicted == labels ) . sum () . item () accuracy = 100 * correct / total # Print Loss # print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy)) # Decay Learning Rate, pass validation accuracy for tracking at every epoch print ( 'Epoch {} completed' . format ( epoch )) print ( 'Loss: {} . Accuracy: {} ' . format ( loss . item (), accuracy )) print ( '-' * 20 ) scheduler . step ( accuracy ) Epoch 0 completed Loss : 0.17087846994400024 . Accuracy : 96.26 -------------------- Epoch 1 completed Loss : 0.11688263714313507 . Accuracy : 96.96 -------------------- Epoch 2 completed Loss : 0.035437121987342834 . Accuracy : 96.78 -------------------- Epoch 2 : reducing learning rate of group 0 to 1.0000e-02 . Epoch 3 completed Loss : 0.0324370414018631 . Accuracy : 97.7 -------------------- Epoch 4 completed Loss : 0.022194599732756615 . Accuracy : 98.02 -------------------- Epoch 5 completed Loss : 0.007145566865801811 . Accuracy : 98.03 -------------------- Epoch 6 completed Loss : 0.01673538237810135 . Accuracy : 98.05 -------------------- Epoch 7 completed Loss : 0.025424446910619736 . Accuracy : 98.01 -------------------- Epoch 7 : reducing learning rate of group 0 to 1.0000e-03 . Epoch 8 completed Loss : 0.014696130529046059 . Accuracy : 98.05 -------------------- Epoch 8 : reducing learning rate of group 0 to 1.0000e-04 . Epoch 9 completed Loss : 0.00573748117312789 . Accuracy : 98.04 -------------------- Epoch 9 : reducing learning rate of group 0 to 1.0000e-05 . Reduce on Loss Plateau Decay, Patience=0, Factor=0.5 \u00b6 Reduce learning rate whenever loss plateaus Patience: number of epochs with no improvement after which learning rate will be reduced Patience = 0 Factor: multiplier to decrease learning rate, lr = lr*factor = \\gamma lr = lr*factor = \\gamma Factor = 0.5 Optimization Algorithm 4: SGD Nesterov Modification of SGD Momentum v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Code for reduce on loss plateau learning rate decay with factor 0.5 and 0 patience import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) # Where to add a new import from torch.optim.lr_scheduler import ReduceLROnPlateau ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 6000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # lr = lr * factor # mode='max': look for the maximum validation accuracy to track # patience: number of epochs - 1 where loss plateaus before decreasing LR # patience = 0, after 1 bad epoch, reduce LR # factor = decaying factor scheduler = ReduceLROnPlateau ( optimizer , mode = 'max' , factor = 0.5 , patience = 0 , verbose = True ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions # Without .item(), it is a uint8 tensor which will not work when you pass this number to the scheduler correct += ( predicted == labels ) . sum () . item () accuracy = 100 * correct / total # Print Loss # print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy)) # Decay Learning Rate, pass validation accuracy for tracking at every epoch print ( 'Epoch {} completed' . format ( epoch )) print ( 'Loss: {} . Accuracy: {} ' . format ( loss . item (), accuracy )) print ( '-' * 20 ) scheduler . step ( accuracy ) Epoch 0 completed Loss : 0.17087846994400024 . Accuracy : 96.26 -------------------- Epoch 1 completed Loss : 0.11688263714313507 . Accuracy : 96.96 -------------------- Epoch 2 completed Loss : 0.035437121987342834 . Accuracy : 96.78 -------------------- Epoch 2 : reducing learning rate of group 0 to 5.0000e-02 . Epoch 3 completed Loss : 0.04893001914024353 . Accuracy : 97.62 -------------------- Epoch 4 completed Loss : 0.020584167912602425 . Accuracy : 97.86 -------------------- Epoch 5 completed Loss : 0.006022400688380003 . Accuracy : 97.95 -------------------- Epoch 6 completed Loss : 0.028374142944812775 . Accuracy : 97.87 -------------------- Epoch 6 : reducing learning rate of group 0 to 2.5000e-02 . Epoch 7 completed Loss : 0.013204765506088734 . Accuracy : 98.0 -------------------- Epoch 8 completed Loss : 0.010137186385691166 . Accuracy : 97.95 -------------------- Epoch 8 : reducing learning rate of group 0 to 1.2500e-02 . Epoch 9 completed Loss : 0.0035198689438402653 . Accuracy : 98.01 -------------------- Pointers on Reduce on Loss Pleateau Decay \u00b6 In these examples, we used patience=1 because we are running few epochs You should look at a larger patience such as 5 if for example you ran 500 epochs. You should experiment with 2 properties Patience Decay factor Summary \u00b6 We've learnt... Success Learning Rate Intuition Update parameters so model can churn output closer to labels Gradual parameter updates Learning Rate Pointers If we set \\eta \\eta to be a large value \\rightarrow \\rightarrow learn too much (rapid learning) If we set \\eta \\eta to be a small value \\rightarrow \\rightarrow learn too little (slow learning) Learning Rate Schedules Step-wise Decay Reduce on Loss Plateau Decay Step-wise Decay Every 1 epoch Every 2 epoch Every 1 epoch, larger gamma Step-wise Decay Pointers Decay LR gradually Larger \\gamma \\gamma Larger interval of decay (increase epoch) Reduce on Loss Plateau Decay Patience=0, Factor=1 Patience=0, Factor=0.5 Pointers on Reduce on Loss Plateau Decay Larger patience with more epochs 2 hyperparameters to experiment Patience Decay factor Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Learning Rate Scheduling"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#learning-rate-scheduling","text":"Run Jupyter Notebook You can run the code for this section in this jupyter notebook link .","title":"Learning Rate Scheduling"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#optimization-algorithm-mini-batch-stochastic-gradient-descent-sgd","text":"We will be using mini-batch gradient descent in all our examples here when scheduling our learning rate Combination of batch gradient descent & stochastic gradient descent \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) Characteristics Compute the gradient of the lost function w.r.t. parameters for n sets of training sample (n input and n label) , \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) Use this to update our parameters at every iteration Typically in deep learning, some variation of mini-batch gradient is used where the batch size is a hyperparameter to be determined","title":"Optimization Algorithm: Mini-batch Stochastic Gradient Descent (SGD)"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#learning-intuition-recap","text":"Learning process Original parameters \\rightarrow \\rightarrow given input, get output \\rightarrow \\rightarrow compare with labels \\rightarrow \\rightarrow get loss with comparison of input/output \\rightarrow \\rightarrow get gradients of loss w.r.t parameters \\rightarrow \\rightarrow update parameters so model can churn output closer to labels \\rightarrow \\rightarrow repeat For a detailed mathematical account of how this works and how to implement from scratch in Python and PyTorch, you can read our forward- and back-propagation and gradient descent post .","title":"Learning Intuition Recap"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#learning-rate-pointers","text":"Update parameters so model can churn output closer to labels, lower loss \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) If we set \\eta \\eta to be a large value \\rightarrow \\rightarrow learn too much (rapid learning) Unable to converge to a good local minima (unable to effectively gradually decrease your loss, overshoot the local lowest value) If we set \\eta \\eta to be a small value \\rightarrow \\rightarrow learn too little (slow learning) May take too long or unable to convert to a good local minima","title":"Learning Rate Pointers"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#need-for-learning-rate-schedules","text":"Benefits Converge faster Higher accuracy","title":"Need for Learning Rate Schedules"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#top-basic-learning-rate-schedules","text":"Step-wise Decay Reduce on Loss Plateau Decay","title":"Top Basic Learning Rate Schedules"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#step-wise-learning-rate-decay","text":"","title":"Step-wise Learning Rate Decay"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#step-wise-decay-every-epoch","text":"At every epoch, \\eta_t = \\eta_{t-1}\\gamma \\eta_t = \\eta_{t-1}\\gamma \\gamma = 0.1 \\gamma = 0.1 Optimization Algorithm 4: SGD Nesterov Modification of SGD Momentum v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Practical example Given \\eta_t = 0.1 \\eta_t = 0.1 and $ \\gamma = 0.01$ Epoch 0: \\eta_t = 0.1 \\eta_t = 0.1 Epoch 1: \\eta_{t+1} = 0.1 (0.1) = 0.01 \\eta_{t+1} = 0.1 (0.1) = 0.01 Epoch 2: \\eta_{t+2} = 0.1 (0.1)^2 = 0.001 \\eta_{t+2} = 0.1 (0.1)^2 = 0.001 Epoch n: \\eta_{t+n} = 0.1 (0.1)^n \\eta_{t+n} = 0.1 (0.1)^n Code for step-wise learning rate decay at every epoch import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) # Where to add a new import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 1 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 1 , gamma = 0.1 ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500. Loss : 0.15292978286743164 . Accuracy : 96 Epoch : 1 LR : [ 0.010000000000000002 ] Iteration : 1000. Loss : 0.1207798570394516 . Accuracy : 97 Epoch : 2 LR : [ 0.0010000000000000002 ] Iteration : 1500. Loss : 0.12287932634353638 . Accuracy : 97 Epoch : 3 LR : [ 0.00010000000000000003 ] Iteration : 2000. Loss : 0.05614742264151573 . Accuracy : 97 Epoch : 4 LR : [ 1.0000000000000003e-05 ] Iteration : 2500. Loss : 0.06775809079408646 . Accuracy : 97 Iteration : 3000. Loss : 0.03737065941095352 . Accuracy : 97","title":"Step-wise Decay: Every Epoch"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#step-wise-decay-every-2-epochs","text":"At every 2 epoch, \\eta_t = \\eta_{t-1}\\gamma \\eta_t = \\eta_{t-1}\\gamma \\gamma = 0.1 \\gamma = 0.1 Optimization Algorithm 4: SGD Nesterov Modification of SGD Momentum v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Practical example Given \\eta_t = 0.1 \\eta_t = 0.1 and \\gamma = 0.01 \\gamma = 0.01 Epoch 0: \\eta_t = 0.1 \\eta_t = 0.1 Epoch 1: \\eta_{t+1} = 0.1 \\eta_{t+1} = 0.1 Epoch 2: \\eta_{t+2} = 0.1 (0.1) = 0.01 \\eta_{t+2} = 0.1 (0.1) = 0.01 Code for step-wise learning rate decay at every 2 epoch import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) # Where to add a new import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 2 , gamma = 0.1 ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500. Loss : 0.15292978286743164 . Accuracy : 96 Epoch : 1 LR : [ 0.1 ] Iteration : 1000. Loss : 0.11253029108047485 . Accuracy : 96 Epoch : 2 LR : [ 0.010000000000000002 ] Iteration : 1500. Loss : 0.14498558640480042 . Accuracy : 97 Epoch : 3 LR : [ 0.010000000000000002 ] Iteration : 2000. Loss : 0.03691177815198898 . Accuracy : 97 Epoch : 4 LR : [ 0.0010000000000000002 ] Iteration : 2500. Loss : 0.03511016443371773 . Accuracy : 97 Iteration : 3000. Loss : 0.029424520209431648 . Accuracy : 97","title":"Step-wise Decay: Every 2 Epochs"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#step-wise-decay-every-epoch-larger-gamma","text":"At every epoch, \\eta_t = \\eta_{t-1}\\gamma \\eta_t = \\eta_{t-1}\\gamma \\gamma = 0.96 \\gamma = 0.96 Optimization Algorithm 4: SGD Nesterov Modification of SGD Momentum v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Practical example Given \\eta_t = 0.1 \\eta_t = 0.1 and \\gamma = 0.96 \\gamma = 0.96 Epoch 1: \\eta_t = 0.1 \\eta_t = 0.1 Epoch 2: \\eta_{t+1} = 0.1 (0.96) = 0.096 \\eta_{t+1} = 0.1 (0.96) = 0.096 Epoch 3: \\eta_{t+2} = 0.1 (0.96)^2 = 0.092 \\eta_{t+2} = 0.1 (0.96)^2 = 0.092 Epoch n: \\eta_{t+n} = 0.1 (0.96)^n \\eta_{t+n} = 0.1 (0.96)^n Code for step-wise learning rate decay at every epoch with larger gamma import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) # Where to add a new import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 2 , gamma = 0.96 ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500. Loss : 0.15292978286743164 . Accuracy : 96 Epoch : 1 LR : [ 0.1 ] Iteration : 1000. Loss : 0.11253029108047485 . Accuracy : 96 Epoch : 2 LR : [ 0.096 ] Iteration : 1500. Loss : 0.11864850670099258 . Accuracy : 97 Epoch : 3 LR : [ 0.096 ] Iteration : 2000. Loss : 0.030942382290959358 . Accuracy : 97 Epoch : 4 LR : [ 0.09216 ] Iteration : 2500. Loss : 0.04521659016609192 . Accuracy : 97 Iteration : 3000. Loss : 0.027839098125696182 . Accuracy : 97","title":"Step-wise Decay: Every Epoch, Larger Gamma"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#pointers-on-step-wise-decay","text":"You would want to decay your LR gradually when you're training more epochs Converge too fast, to a crappy loss/accuracy, if you decay rapidly To decay slower Larger \\gamma \\gamma Larger interval of decay","title":"Pointers on Step-wise Decay"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#reduce-on-loss-plateau-decay","text":"","title":"Reduce on Loss Plateau Decay"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#reduce-on-loss-plateau-decay-patience0-factor01","text":"Reduce learning rate whenever loss plateaus Patience: number of epochs with no improvement after which learning rate will be reduced Patience = 0 Factor: multiplier to decrease learning rate, lr = lr*factor = \\gamma lr = lr*factor = \\gamma Factor = 0.1 Optimization Algorithm: SGD Nesterov Modification of SGD Momentum v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Code for reduce on loss plateau learning rate decay of factor 0.1 and 0 patience import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) # Where to add a new import from torch.optim.lr_scheduler import ReduceLROnPlateau ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 6000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # lr = lr * factor # mode='max': look for the maximum validation accuracy to track # patience: number of epochs - 1 where loss plateaus before decreasing LR # patience = 0, after 1 bad epoch, reduce LR # factor = decaying factor scheduler = ReduceLROnPlateau ( optimizer , mode = 'max' , factor = 0.1 , patience = 0 , verbose = True ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions # Without .item(), it is a uint8 tensor which will not work when you pass this number to the scheduler correct += ( predicted == labels ) . sum () . item () accuracy = 100 * correct / total # Print Loss # print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy)) # Decay Learning Rate, pass validation accuracy for tracking at every epoch print ( 'Epoch {} completed' . format ( epoch )) print ( 'Loss: {} . Accuracy: {} ' . format ( loss . item (), accuracy )) print ( '-' * 20 ) scheduler . step ( accuracy ) Epoch 0 completed Loss : 0.17087846994400024 . Accuracy : 96.26 -------------------- Epoch 1 completed Loss : 0.11688263714313507 . Accuracy : 96.96 -------------------- Epoch 2 completed Loss : 0.035437121987342834 . Accuracy : 96.78 -------------------- Epoch 2 : reducing learning rate of group 0 to 1.0000e-02 . Epoch 3 completed Loss : 0.0324370414018631 . Accuracy : 97.7 -------------------- Epoch 4 completed Loss : 0.022194599732756615 . Accuracy : 98.02 -------------------- Epoch 5 completed Loss : 0.007145566865801811 . Accuracy : 98.03 -------------------- Epoch 6 completed Loss : 0.01673538237810135 . Accuracy : 98.05 -------------------- Epoch 7 completed Loss : 0.025424446910619736 . Accuracy : 98.01 -------------------- Epoch 7 : reducing learning rate of group 0 to 1.0000e-03 . Epoch 8 completed Loss : 0.014696130529046059 . Accuracy : 98.05 -------------------- Epoch 8 : reducing learning rate of group 0 to 1.0000e-04 . Epoch 9 completed Loss : 0.00573748117312789 . Accuracy : 98.04 -------------------- Epoch 9 : reducing learning rate of group 0 to 1.0000e-05 .","title":"Reduce on Loss Plateau Decay, Patience=0, Factor=0.1"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#reduce-on-loss-plateau-decay-patience0-factor05","text":"Reduce learning rate whenever loss plateaus Patience: number of epochs with no improvement after which learning rate will be reduced Patience = 0 Factor: multiplier to decrease learning rate, lr = lr*factor = \\gamma lr = lr*factor = \\gamma Factor = 0.5 Optimization Algorithm 4: SGD Nesterov Modification of SGD Momentum v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Code for reduce on loss plateau learning rate decay with factor 0.5 and 0 patience import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) # Where to add a new import from torch.optim.lr_scheduler import ReduceLROnPlateau ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 6000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # lr = lr * factor # mode='max': look for the maximum validation accuracy to track # patience: number of epochs - 1 where loss plateaus before decreasing LR # patience = 0, after 1 bad epoch, reduce LR # factor = decaying factor scheduler = ReduceLROnPlateau ( optimizer , mode = 'max' , factor = 0.5 , patience = 0 , verbose = True ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions # Without .item(), it is a uint8 tensor which will not work when you pass this number to the scheduler correct += ( predicted == labels ) . sum () . item () accuracy = 100 * correct / total # Print Loss # print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy)) # Decay Learning Rate, pass validation accuracy for tracking at every epoch print ( 'Epoch {} completed' . format ( epoch )) print ( 'Loss: {} . Accuracy: {} ' . format ( loss . item (), accuracy )) print ( '-' * 20 ) scheduler . step ( accuracy ) Epoch 0 completed Loss : 0.17087846994400024 . Accuracy : 96.26 -------------------- Epoch 1 completed Loss : 0.11688263714313507 . Accuracy : 96.96 -------------------- Epoch 2 completed Loss : 0.035437121987342834 . Accuracy : 96.78 -------------------- Epoch 2 : reducing learning rate of group 0 to 5.0000e-02 . Epoch 3 completed Loss : 0.04893001914024353 . Accuracy : 97.62 -------------------- Epoch 4 completed Loss : 0.020584167912602425 . Accuracy : 97.86 -------------------- Epoch 5 completed Loss : 0.006022400688380003 . Accuracy : 97.95 -------------------- Epoch 6 completed Loss : 0.028374142944812775 . Accuracy : 97.87 -------------------- Epoch 6 : reducing learning rate of group 0 to 2.5000e-02 . Epoch 7 completed Loss : 0.013204765506088734 . Accuracy : 98.0 -------------------- Epoch 8 completed Loss : 0.010137186385691166 . Accuracy : 97.95 -------------------- Epoch 8 : reducing learning rate of group 0 to 1.2500e-02 . Epoch 9 completed Loss : 0.0035198689438402653 . Accuracy : 98.01 --------------------","title":"Reduce on Loss Plateau Decay, Patience=0, Factor=0.5"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#pointers-on-reduce-on-loss-pleateau-decay","text":"In these examples, we used patience=1 because we are running few epochs You should look at a larger patience such as 5 if for example you ran 500 epochs. You should experiment with 2 properties Patience Decay factor","title":"Pointers on Reduce on Loss Pleateau Decay"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#summary","text":"We've learnt... Success Learning Rate Intuition Update parameters so model can churn output closer to labels Gradual parameter updates Learning Rate Pointers If we set \\eta \\eta to be a large value \\rightarrow \\rightarrow learn too much (rapid learning) If we set \\eta \\eta to be a small value \\rightarrow \\rightarrow learn too little (slow learning) Learning Rate Schedules Step-wise Decay Reduce on Loss Plateau Decay Step-wise Decay Every 1 epoch Every 2 epoch Every 1 epoch, larger gamma Step-wise Decay Pointers Decay LR gradually Larger \\gamma \\gamma Larger interval of decay (increase epoch) Reduce on Loss Plateau Decay Patience=0, Factor=1 Patience=0, Factor=0.5 Pointers on Reduce on Loss Plateau Decay Larger patience with more epochs 2 hyperparameters to experiment Patience Decay factor","title":"Summary"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/boosting_models_pytorch/optimizers/","text":"Optimization Algorithms \u00b6 Run Jupyter Notebook You can run the code for this section in this jupyter notebook link . Introduction to Gradient-descent Optimizers \u00b6 Model Recap: 1 Hidden Layer Feedforward Neural Network (ReLU Activation) \u00b6 Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.3440718352794647 . Accuracy : 91 Iteration : 1000 . Loss : 0.2057694047689438 . Accuracy : 93 Iteration : 1500 . Loss : 0.2646750807762146 . Accuracy : 94 Iteration : 2000 . Loss : 0.17563636600971222 . Accuracy : 94 Iteration : 2500 . Loss : 0.1361844837665558 . Accuracy : 95 Iteration : 3000 . Loss : 0.11089023947715759 . Accuracy : 95 Non-Technical Process \u00b6 Convert inputs/labels to variables Clear gradient buffers Get output given inputs Get loss by comparing with labels Get gradients w.r.t. parameters (backpropagation) Update parameters using gradients (gradient descent) parameters = parameters - learning_rate * parameters_gradients REPEAT Why is it called Gradient Descent? \u00b6 Use gradients (calculated through backpropagation) \\rightarrow \\rightarrow update parameters to minimize our loss (descent) \\rightarrow \\rightarrow better predictive accuracy Mathematical Interpretation of Gradient Descent \u00b6 Model's parameters: \\theta \\in \u211d^d \\theta \\in \u211d^d Loss function: J(\\theta) J(\\theta) Gradient w.r.t. parameters: \\nabla J(\\theta) \\nabla J(\\theta) Learning rate: \\eta \\eta Batch Gradient descent: \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta) Optimization Algorithm 1: Batch Gradient Descent \u00b6 What we've covered so far: batch gradient descent \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta) Characteristics Compute the gradient of the lost function w.r.t. parameters for the entire training data, \\nabla J(\\theta) \\nabla J(\\theta) Use this to update our parameters at every iteration Problems Unable to fit whole datasets in memory Computationally slow as we attempt to compute a large gradient matrix \\rightarrow \\rightarrow first order derivative, \\nabla J(\\theta) \\nabla J(\\theta) Conceptually easy to understand \\rightarrow \\rightarrow rarely used Optimization Algorithm 2: Stochastic Gradient Descent \u00b6 Modification of batch gradient descent \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i}, y^{i}) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i}, y^{i}) Characteristics Compute the gradient of the lost function w.r.t. parameters for the one set of training sample (1 input and 1 label) , \\nabla J(\\theta, x^{i}, y^{i}) \\nabla J(\\theta, x^{i}, y^{i}) Use this to update our parameters at every iteration Benefits Able to fit large datasets Computationally faster \\rightarrow \\rightarrow instead gradients w.r.t to the whole training data, we get the gradients w.r.t. training sample Problems Updating very frequently \\rightarrow \\rightarrow huge variance in parameter updates \\rightarrow \\rightarrow may overshoot local minima Can be solved by carefully decaying your learning rate \\rightarrow \\rightarrow take smaller steps in incorporating gradients to improve the parameters Optimization Algorithm 3: Mini-batch Gradient Descent \u00b6 Combination of batch gradient descent & stochastic gradient descent \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) Characteristics Compute the gradient of the lost function w.r.t. parameters for n sets of training sample (n input and n label) , \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) Use this to update our parameters at every iteration Benefits Able to fit large datasets Computationally faster \\rightarrow \\rightarrow instead gradients w.r.t to the whole training data, we get the gradients w.r.t. training sample Lower variance of parameter updates This is often called SGD in deep learning frameworks .__. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.3440718352794647 . Accuracy : 91 Iteration : 1000 . Loss : 0.2057694047689438 . Accuracy : 93 Iteration : 1500 . Loss : 0.2646750807762146 . Accuracy : 94 Iteration : 2000 . Loss : 0.17563636600971222 . Accuracy : 94 Iteration : 2500 . Loss : 0.1361844837665558 . Accuracy : 95 Iteration : 3000 . Loss : 0.11089023947715759 . Accuracy : 95 Optimization Algorithm 4: SGD Momentum \u00b6 Modification of SGD v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Characteristics Compute the gradient of the lost function w.r.t. parameters for n sets of training sample (n input and n label) , \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) Use this to add to the previous update vector v_{t-1} v_{t-1} Momentum, usually set to \\gamma = 0.9 \\gamma = 0.9 Parameters updated with update vector, v_t v_t that incorporates previous update vector \\gamma v_{t} \\gamma v_{t} increases if gradient same sign/direction as v_{t-1} v_{t-1} Gives SGD the push when it is going in the right direction (minimizing loss) Accelerated convergence \\gamma v_{t} \\gamma v_{t} decreases if gradient different sign/direction as v_{t-1} v_{t-1} Dampens SGD when it is going in a different direction Lower variation in loss minimization Problems It might go the wrong direction (higher loss) \\rightarrow \\rightarrow continue to be accelerated to the wrong direction (higher loss) import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.16120098531246185 . Accuracy : 96 Iteration : 1000 . Loss : 0.15727552771568298 . Accuracy : 96 Iteration : 1500 . Loss : 0.1303034871816635 . Accuracy : 96 Iteration : 2000 . Loss : 0.022178759798407555 . Accuracy : 97 Iteration : 2500 . Loss : 0.07027597725391388 . Accuracy : 97 Iteration : 3000 . Loss : 0.02519878000020981 . Accuracy : 97 Optimization Algorithm 4: SGD Nesterov \u00b6 Modification of SGD Momentum v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Characteristics Compute the gradient of the lost function w.r.t. future approximate parameters for n sets of training sample (n input and n label) , \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) Use this to add to the previous update vector v_{t-1} v_{t-1} Momentum, usually set to \\gamma = 0.9 \\gamma = 0.9 Gradients w.r.t. future approximate parameters \\rightarrow \\rightarrow sense of where we will be \\rightarrow \\rightarrow anticipate if we are going in the wrong direction in the next step \\rightarrow \\rightarrow slow down accordingly import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.15292978286743164 . Accuracy : 96 Iteration : 1000 . Loss : 0.11253029108047485 . Accuracy : 96 Iteration : 1500 . Loss : 0.11986596137285233 . Accuracy : 96 Iteration : 2000 . Loss : 0.016192540526390076 . Accuracy : 97 Iteration : 2500 . Loss : 0.06744947284460068 . Accuracy : 97 Iteration : 3000 . Loss : 0.03692319989204407 . Accuracy : 97 Optimization Algorithm 4: Adam \u00b6 Adaptive Learning Rates m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t Keeping track of decaying gradient Estimate of the mean of gradients v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)g_t^2 v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)g_t^2 Keeping track of decaying squared gradient Estimate of the variance of gradients When m_t, v_t m_t, v_t initializes as 0, m_t, v_t \\rightarrow 0 m_t, v_t \\rightarrow 0 initially when decay rates small, \\beta_1, \\beta_2 \\rightarrow 1 \\beta_1, \\beta_2 \\rightarrow 1 Need to correct this with: \\hat m_t = \\frac{m_t}{1- \\beta_1} \\hat m_t = \\frac{m_t}{1- \\beta_1} \\hat v_t = \\frac{v_t}{1- \\beta_2} \\hat v_t = \\frac{v_t}{1- \\beta_2} \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat v_t} + \\epsilon}\\hat m_t \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat v_t} + \\epsilon}\\hat m_t Default recommended values \\beta_1 = 0.9 \\beta_1 = 0.9 \\beta_2 = 0.999 \\beta_2 = 0.999 \\epsilon = 10^{-8} \\epsilon = 10^{-8} Instead of learning rate \\rightarrow \\rightarrow equations account for estimates of mean/variance of gradients to determine the next learning rate import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' # learning_rate = 0.001 optimizer = torch . optim . Adam ( model . parameters ()) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.2703690826892853 . Accuracy : 93 Iteration : 1000 . Loss : 0.15547044575214386 . Accuracy : 95 Iteration : 1500 . Loss : 0.17266806960105896 . Accuracy : 95 Iteration : 2000 . Loss : 0.0865858644247055 . Accuracy : 96 Iteration : 2500 . Loss : 0.07156120240688324 . Accuracy : 96 Iteration : 3000 . Loss : 0.04664849117398262 . Accuracy : 97 Other Adaptive Algorithms \u00b6 Other adaptive algorithms (like Adam, adapting learning rates) Adagrad Adadelta Adamax RMSProp Optimization Algorithm 5: Adagrad \u00b6 import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' # learning_rate = 0.001 optimizer = torch . optim . Adagrad ( model . parameters ()) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.2757369875907898 . Accuracy : 92 Iteration : 1000 . Loss : 0.1992958039045334 . Accuracy : 93 Iteration : 1500 . Loss : 0.2227272093296051 . Accuracy : 94 Iteration : 2000 . Loss : 0.18628711998462677 . Accuracy : 94 Iteration : 2500 . Loss : 0.1470586657524109 . Accuracy : 95 Iteration : 3000 . Loss : 0.11748368293046951 . Accuracy : 95 Optimization Algorithm 6: Adadelta \u00b6 import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' # learning_rate = 0.001 optimizer = torch . optim . Adadelta ( model . parameters ()) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = Variable ( images . view ( - 1 , 28 * 28 )) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . data [ 0 ], accuracy )) Iteration : 500 . Loss : 0.26303035020828247 . Accuracy : 93.95 Iteration : 1000 . Loss : 0.08731874823570251 . Accuracy : 95.83 Iteration : 1500 . Loss : 0.11502093076705933 . Accuracy : 96.87 Iteration : 2000 . Loss : 0.03550947830080986 . Accuracy : 97.12 Iteration : 2500 . Loss : 0.042649827897548676 . Accuracy : 97.54 Iteration : 3000 . Loss : 0.03061559610068798 . Accuracy : 97.45 Optimization Algorithm 6: Adamax \u00b6 import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' # learning_rate = 0.001 optimizer = torch . optim . Adamax ( model . parameters ()) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.29930350184440613 . Accuracy : 92 Iteration : 1000 . Loss : 0.18749120831489563 . Accuracy : 93 Iteration : 1500 . Loss : 0.21887679398059845 . Accuracy : 95 Iteration : 2000 . Loss : 0.14390651881694794 . Accuracy : 95 Iteration : 2500 . Loss : 0.10771607607603073 . Accuracy : 96 Iteration : 3000 . Loss : 0.0839928686618805 . Accuracy : 96 Optimization Algorithm 7: RMSProp \u00b6 import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' # learning_rate = 0.001 optimizer = torch . optim . RMSprop ( model . parameters ()) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.25550296902656555 . Accuracy : 95 Iteration : 1000 . Loss : 0.17357593774795532 . Accuracy : 93 Iteration : 1500 . Loss : 0.10597744584083557 . Accuracy : 96 Iteration : 2000 . Loss : 0.03807783126831055 . Accuracy : 96 Iteration : 2500 . Loss : 0.10654022544622421 . Accuracy : 96 Iteration : 3000 . Loss : 0.05745543912053108 . Accuracy : 96 Summary of Optimization Algorithms Performance \u00b6 SGD: 95.78% SGD Momentum: 97.69% SGD Nesterov: 97.58% Adam: 97.20% Adagrad: 95.51% Adadelta: 97.45% Adamax: 96.58% RMSProp: 97.1% Performance is not definitive here I have used a seed to ensure you can reproduce results here. However, if you change the seed number you would realize that the performance of these optimization algorithms would change. A solution is to run each optimization on many seeds and get the average performance. Then you can compare the mean performance across all optimization algorithms. There are a lot of other factors like how Adam and SGD Momentum may have different ideal starting learning rates and require different learning rate scheduling. But off the hand, SGD and Adam are very robust optimization algorithms that you can rely on. Subsequently, we will look into more advanced optimization algorithms that are based mainly on SGD and Adam. Simple Suggestions \u00b6 Momentum/Nesterov Powerful if we control the learning rate schedule Adam Lazy to control the learning rate schedule Summary \u00b6 We've learnt... Success Recap of 7 step process Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Step 6 Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients Gradient descent Using gradients (error signals from loss class) to update parameters Mathematical interpretation: \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta) Optimisation Algorithms Batch gradient descent Stochastic gradient descent Mini-batch gradient descent (SGD) SGD + Momentum SGD + Nesterov Adam Other adaptive algorithms: adagrad, adamax, adadelta, RMSProp Recommendations SGD+M SGD+N Adam Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Optimization Algorithms"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithms","text":"Run Jupyter Notebook You can run the code for this section in this jupyter notebook link .","title":"Optimization Algorithms"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#introduction-to-gradient-descent-optimizers","text":"","title":"Introduction to Gradient-descent Optimizers"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#model-recap-1-hidden-layer-feedforward-neural-network-relu-activation","text":"","title":"Model Recap: 1 Hidden Layer Feedforward Neural Network (ReLU Activation)"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#steps","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.3440718352794647 . Accuracy : 91 Iteration : 1000 . Loss : 0.2057694047689438 . Accuracy : 93 Iteration : 1500 . Loss : 0.2646750807762146 . Accuracy : 94 Iteration : 2000 . Loss : 0.17563636600971222 . Accuracy : 94 Iteration : 2500 . Loss : 0.1361844837665558 . Accuracy : 95 Iteration : 3000 . Loss : 0.11089023947715759 . Accuracy : 95","title":"Steps"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#non-technical-process","text":"Convert inputs/labels to variables Clear gradient buffers Get output given inputs Get loss by comparing with labels Get gradients w.r.t. parameters (backpropagation) Update parameters using gradients (gradient descent) parameters = parameters - learning_rate * parameters_gradients REPEAT","title":"Non-Technical Process"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#why-is-it-called-gradient-descent","text":"Use gradients (calculated through backpropagation) \\rightarrow \\rightarrow update parameters to minimize our loss (descent) \\rightarrow \\rightarrow better predictive accuracy","title":"Why is it called Gradient Descent?"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#mathematical-interpretation-of-gradient-descent","text":"Model's parameters: \\theta \\in \u211d^d \\theta \\in \u211d^d Loss function: J(\\theta) J(\\theta) Gradient w.r.t. parameters: \\nabla J(\\theta) \\nabla J(\\theta) Learning rate: \\eta \\eta Batch Gradient descent: \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta)","title":"Mathematical Interpretation of Gradient Descent"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-1-batch-gradient-descent","text":"What we've covered so far: batch gradient descent \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta) Characteristics Compute the gradient of the lost function w.r.t. parameters for the entire training data, \\nabla J(\\theta) \\nabla J(\\theta) Use this to update our parameters at every iteration Problems Unable to fit whole datasets in memory Computationally slow as we attempt to compute a large gradient matrix \\rightarrow \\rightarrow first order derivative, \\nabla J(\\theta) \\nabla J(\\theta) Conceptually easy to understand \\rightarrow \\rightarrow rarely used","title":"Optimization Algorithm 1: Batch Gradient Descent"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-2-stochastic-gradient-descent","text":"Modification of batch gradient descent \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i}, y^{i}) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i}, y^{i}) Characteristics Compute the gradient of the lost function w.r.t. parameters for the one set of training sample (1 input and 1 label) , \\nabla J(\\theta, x^{i}, y^{i}) \\nabla J(\\theta, x^{i}, y^{i}) Use this to update our parameters at every iteration Benefits Able to fit large datasets Computationally faster \\rightarrow \\rightarrow instead gradients w.r.t to the whole training data, we get the gradients w.r.t. training sample Problems Updating very frequently \\rightarrow \\rightarrow huge variance in parameter updates \\rightarrow \\rightarrow may overshoot local minima Can be solved by carefully decaying your learning rate \\rightarrow \\rightarrow take smaller steps in incorporating gradients to improve the parameters","title":"Optimization Algorithm 2: Stochastic Gradient Descent"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-3-mini-batch-gradient-descent","text":"Combination of batch gradient descent & stochastic gradient descent \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) Characteristics Compute the gradient of the lost function w.r.t. parameters for n sets of training sample (n input and n label) , \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) Use this to update our parameters at every iteration Benefits Able to fit large datasets Computationally faster \\rightarrow \\rightarrow instead gradients w.r.t to the whole training data, we get the gradients w.r.t. training sample Lower variance of parameter updates This is often called SGD in deep learning frameworks .__. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.3440718352794647 . Accuracy : 91 Iteration : 1000 . Loss : 0.2057694047689438 . Accuracy : 93 Iteration : 1500 . Loss : 0.2646750807762146 . Accuracy : 94 Iteration : 2000 . Loss : 0.17563636600971222 . Accuracy : 94 Iteration : 2500 . Loss : 0.1361844837665558 . Accuracy : 95 Iteration : 3000 . Loss : 0.11089023947715759 . Accuracy : 95","title":"Optimization Algorithm 3: Mini-batch Gradient Descent"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-4-sgd-momentum","text":"Modification of SGD v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Characteristics Compute the gradient of the lost function w.r.t. parameters for n sets of training sample (n input and n label) , \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) Use this to add to the previous update vector v_{t-1} v_{t-1} Momentum, usually set to \\gamma = 0.9 \\gamma = 0.9 Parameters updated with update vector, v_t v_t that incorporates previous update vector \\gamma v_{t} \\gamma v_{t} increases if gradient same sign/direction as v_{t-1} v_{t-1} Gives SGD the push when it is going in the right direction (minimizing loss) Accelerated convergence \\gamma v_{t} \\gamma v_{t} decreases if gradient different sign/direction as v_{t-1} v_{t-1} Dampens SGD when it is going in a different direction Lower variation in loss minimization Problems It might go the wrong direction (higher loss) \\rightarrow \\rightarrow continue to be accelerated to the wrong direction (higher loss) import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.16120098531246185 . Accuracy : 96 Iteration : 1000 . Loss : 0.15727552771568298 . Accuracy : 96 Iteration : 1500 . Loss : 0.1303034871816635 . Accuracy : 96 Iteration : 2000 . Loss : 0.022178759798407555 . Accuracy : 97 Iteration : 2500 . Loss : 0.07027597725391388 . Accuracy : 97 Iteration : 3000 . Loss : 0.02519878000020981 . Accuracy : 97","title":"Optimization Algorithm 4: SGD Momentum"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-4-sgd-nesterov","text":"Modification of SGD Momentum v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Characteristics Compute the gradient of the lost function w.r.t. future approximate parameters for n sets of training sample (n input and n label) , \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) Use this to add to the previous update vector v_{t-1} v_{t-1} Momentum, usually set to \\gamma = 0.9 \\gamma = 0.9 Gradients w.r.t. future approximate parameters \\rightarrow \\rightarrow sense of where we will be \\rightarrow \\rightarrow anticipate if we are going in the wrong direction in the next step \\rightarrow \\rightarrow slow down accordingly import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.15292978286743164 . Accuracy : 96 Iteration : 1000 . Loss : 0.11253029108047485 . Accuracy : 96 Iteration : 1500 . Loss : 0.11986596137285233 . Accuracy : 96 Iteration : 2000 . Loss : 0.016192540526390076 . Accuracy : 97 Iteration : 2500 . Loss : 0.06744947284460068 . Accuracy : 97 Iteration : 3000 . Loss : 0.03692319989204407 . Accuracy : 97","title":"Optimization Algorithm 4: SGD Nesterov"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-4-adam","text":"Adaptive Learning Rates m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t Keeping track of decaying gradient Estimate of the mean of gradients v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)g_t^2 v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)g_t^2 Keeping track of decaying squared gradient Estimate of the variance of gradients When m_t, v_t m_t, v_t initializes as 0, m_t, v_t \\rightarrow 0 m_t, v_t \\rightarrow 0 initially when decay rates small, \\beta_1, \\beta_2 \\rightarrow 1 \\beta_1, \\beta_2 \\rightarrow 1 Need to correct this with: \\hat m_t = \\frac{m_t}{1- \\beta_1} \\hat m_t = \\frac{m_t}{1- \\beta_1} \\hat v_t = \\frac{v_t}{1- \\beta_2} \\hat v_t = \\frac{v_t}{1- \\beta_2} \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat v_t} + \\epsilon}\\hat m_t \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat v_t} + \\epsilon}\\hat m_t Default recommended values \\beta_1 = 0.9 \\beta_1 = 0.9 \\beta_2 = 0.999 \\beta_2 = 0.999 \\epsilon = 10^{-8} \\epsilon = 10^{-8} Instead of learning rate \\rightarrow \\rightarrow equations account for estimates of mean/variance of gradients to determine the next learning rate import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' # learning_rate = 0.001 optimizer = torch . optim . Adam ( model . parameters ()) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.2703690826892853 . Accuracy : 93 Iteration : 1000 . Loss : 0.15547044575214386 . Accuracy : 95 Iteration : 1500 . Loss : 0.17266806960105896 . Accuracy : 95 Iteration : 2000 . Loss : 0.0865858644247055 . Accuracy : 96 Iteration : 2500 . Loss : 0.07156120240688324 . Accuracy : 96 Iteration : 3000 . Loss : 0.04664849117398262 . Accuracy : 97","title":"Optimization Algorithm 4: Adam"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#other-adaptive-algorithms","text":"Other adaptive algorithms (like Adam, adapting learning rates) Adagrad Adadelta Adamax RMSProp","title":"Other Adaptive Algorithms"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-5-adagrad","text":"import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' # learning_rate = 0.001 optimizer = torch . optim . Adagrad ( model . parameters ()) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.2757369875907898 . Accuracy : 92 Iteration : 1000 . Loss : 0.1992958039045334 . Accuracy : 93 Iteration : 1500 . Loss : 0.2227272093296051 . Accuracy : 94 Iteration : 2000 . Loss : 0.18628711998462677 . Accuracy : 94 Iteration : 2500 . Loss : 0.1470586657524109 . Accuracy : 95 Iteration : 3000 . Loss : 0.11748368293046951 . Accuracy : 95","title":"Optimization Algorithm 5: Adagrad"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-6-adadelta","text":"import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' # learning_rate = 0.001 optimizer = torch . optim . Adadelta ( model . parameters ()) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = Variable ( images . view ( - 1 , 28 * 28 )) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . data [ 0 ], accuracy )) Iteration : 500 . Loss : 0.26303035020828247 . Accuracy : 93.95 Iteration : 1000 . Loss : 0.08731874823570251 . Accuracy : 95.83 Iteration : 1500 . Loss : 0.11502093076705933 . Accuracy : 96.87 Iteration : 2000 . Loss : 0.03550947830080986 . Accuracy : 97.12 Iteration : 2500 . Loss : 0.042649827897548676 . Accuracy : 97.54 Iteration : 3000 . Loss : 0.03061559610068798 . Accuracy : 97.45","title":"Optimization Algorithm 6: Adadelta"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-6-adamax","text":"import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' # learning_rate = 0.001 optimizer = torch . optim . Adamax ( model . parameters ()) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.29930350184440613 . Accuracy : 92 Iteration : 1000 . Loss : 0.18749120831489563 . Accuracy : 93 Iteration : 1500 . Loss : 0.21887679398059845 . Accuracy : 95 Iteration : 2000 . Loss : 0.14390651881694794 . Accuracy : 95 Iteration : 2500 . Loss : 0.10771607607603073 . Accuracy : 96 Iteration : 3000 . Loss : 0.0839928686618805 . Accuracy : 96","title":"Optimization Algorithm 6: Adamax"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-7-rmsprop","text":"import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' # learning_rate = 0.001 optimizer = torch . optim . RMSprop ( model . parameters ()) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.25550296902656555 . Accuracy : 95 Iteration : 1000 . Loss : 0.17357593774795532 . Accuracy : 93 Iteration : 1500 . Loss : 0.10597744584083557 . Accuracy : 96 Iteration : 2000 . Loss : 0.03807783126831055 . Accuracy : 96 Iteration : 2500 . Loss : 0.10654022544622421 . Accuracy : 96 Iteration : 3000 . Loss : 0.05745543912053108 . Accuracy : 96","title":"Optimization Algorithm 7: RMSProp"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#summary-of-optimization-algorithms-performance","text":"SGD: 95.78% SGD Momentum: 97.69% SGD Nesterov: 97.58% Adam: 97.20% Adagrad: 95.51% Adadelta: 97.45% Adamax: 96.58% RMSProp: 97.1% Performance is not definitive here I have used a seed to ensure you can reproduce results here. However, if you change the seed number you would realize that the performance of these optimization algorithms would change. A solution is to run each optimization on many seeds and get the average performance. Then you can compare the mean performance across all optimization algorithms. There are a lot of other factors like how Adam and SGD Momentum may have different ideal starting learning rates and require different learning rate scheduling. But off the hand, SGD and Adam are very robust optimization algorithms that you can rely on. Subsequently, we will look into more advanced optimization algorithms that are based mainly on SGD and Adam.","title":"Summary of Optimization Algorithms Performance"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#simple-suggestions","text":"Momentum/Nesterov Powerful if we control the learning rate schedule Adam Lazy to control the learning rate schedule","title":"Simple Suggestions"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#summary","text":"We've learnt... Success Recap of 7 step process Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Step 6 Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients Gradient descent Using gradients (error signals from loss class) to update parameters Mathematical interpretation: \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta) Optimisation Algorithms Batch gradient descent Stochastic gradient descent Mini-batch gradient descent (SGD) SGD + Momentum SGD + Nesterov Adam Other adaptive algorithms: adagrad, adamax, adadelta, RMSProp Recommendations SGD+M SGD+N Adam","title":"Summary"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/","text":"Weight Initializations & Activation Functions \u00b6 Run Jupyter Notebook You can run the code for this section in this jupyter notebook link . Recap of Logistic Regression \u00b6 Recap of Feedforward Neural Network Activation Function \u00b6 Sigmoid (Logistic) \u00b6 \\sigma(x) = \\frac{1}{1 + e^{-x}} \\sigma(x) = \\frac{1}{1 + e^{-x}} Input number \\rightarrow \\rightarrow [0, 1] Large negative number \\rightarrow \\rightarrow 0 Large positive number \\rightarrow \\rightarrow 1 Cons: Activation saturates at 0 or 1 with gradients \\approx \\approx 0 No signal to update weights \\rightarrow \\rightarrow cannot learn Solution: Have to carefully initialize weights to prevent this import matplotlib.pyplot as plt % matplotlib inline import numpy as np def sigmoid ( x ): a = [] for item in x : a . append ( 1 / ( 1 + np . exp ( - item ))) return a x = np . arange ( - 10. , 10. , 0.2 ) sig = sigmoid ( x ) plt . style . use ( 'ggplot' ) plt . plot ( x , sig , linewidth = 3.0 ) Tanh \u00b6 \\tanh(x) = 2 \\sigma(2x) -1 \\tanh(x) = 2 \\sigma(2x) -1 A scaled sigmoid function Input number \\rightarrow \\rightarrow [-1, 1] Cons: Activation saturates at 0 or 1 with gradients \\approx \\approx 0 No signal to update weights \\rightarrow \\rightarrow cannot learn Solution : Have to carefully initialize weights to prevent this x = np . arange ( - 10. , 10. , 0.2 ) tanh = np . dot ( 2 , sigmoid ( np . dot ( 2 , x ))) - 1 plt . plot ( x , tanh , linewidth = 3.0 ) ReLUs \u00b6 f(x) = \\max(0, x) f(x) = \\max(0, x) Pros: Accelerates convergence \\rightarrow \\rightarrow train faster Less computationally expensive operation compared to Sigmoid/Tanh exponentials Cons: Many ReLU units \"die\" \\rightarrow \\rightarrow gradients = 0 forever Solution : careful learning rate and weight initialization choice x = np . arange ( - 10. , 10. , 0.2 ) relu = np . maximum ( x , 0 ) plt . plot ( x , relu , linewidth = 3.0 ) Why do we need weight initializations or new activation functions? \u00b6 To prevent vanishing/exploding gradients Case 1: Sigmoid/Tanh \u00b6 Problem If variance of input too large: gradients = 0 (vanishing gradients) If variance of input too small: linear \\rightarrow \\rightarrow gradients = constant value Solutions Want a constant variance of input to achieve non-linearity \\rightarrow \\rightarrow unique gradients for unique updates Xavier Initialization (good constant variance for Sigmoid/Tanh) ReLU or Leaky ReLU Case 2: ReLU \u00b6 Solution to Case 1 Regardless of variance of input: gradients = 0 or 1 Problem But those with 0: no updates (\"dead ReLU units\") Has unlimited output size with input > 0 (explodes gradients subsequently) Solutions He Initialization (good constant variance) Leaky ReLU Case 3: Leaky ReLU \u00b6 Solution to Case 2 Solves the 0 signal issue when input < 0 Problem Has unlimited output size with input > 0 (explodes) Solution He Initialization (good constant variance) Summary of weight initialization solutions to activations \u00b6 Tanh/Sigmoid vanishing gradients can be solved with Xavier initialization Good range of constant variance ReLU/Leaky ReLU exploding gradients can be solved with He initialization Good range of constant variance Types of weight intializations \u00b6 Zero Initialization: set all weights to 0 \u00b6 Every neuron in the network computes the same output \\rightarrow \\rightarrow computes the same gradient \\rightarrow \\rightarrow same parameter updates Normal Initialization: set all weights to random small numbers \u00b6 Every neuron in the network computes different output \\rightarrow \\rightarrow computes different gradient \\rightarrow \\rightarrow different parameter updates \"Symmetry breaking\" Problem: variance that grows with the number of inputs Lecun Initialization: normalize variance \u00b6 Solves growing variance with the number of inputs \\rightarrow \\rightarrow constant variance Look at a simple feedforward neural network Equations for Lecun Initialization \u00b6 Y = AX + B Y = AX + B y = a_1x_1 + a_2x_2 + \\cdot + a_n x_n + b y = a_1x_1 + a_2x_2 + \\cdot + a_n x_n + b Var(y) = Var(a_1x_1 + a_2x_2 + \\cdot + a_n x_n + b) Var(y) = Var(a_1x_1 + a_2x_2 + \\cdot + a_n x_n + b) Var(a_i x_i) = E(x_i)^2 Var(a_i) + E(a_i)^2Var(x_i) + Var(a_i)Var(x_i) Var(a_i x_i) = E(x_i)^2 Var(a_i) + E(a_i)^2Var(x_i) + Var(a_i)Var(x_i) General term, you might be more familiar with the following Var(XY) = E(X)^2 Var(Y) + E(Y)^2Var(X) + Var(X)Var(Y) Var(XY) = E(X)^2 Var(Y) + E(Y)^2Var(X) + Var(X)Var(Y) E(x_i) E(x_i) : expectation/mean of x_i x_i E(a_i) E(a_i) : expectation/mean of a_i a_i Assuming inputs/weights drawn i.i.d. with Gaussian distribution of mean=0 E(x_i) = E(a_i) = 0 E(x_i) = E(a_i) = 0 Var(a_i x_i) = Var(a_i)Var(x_i) Var(a_i x_i) = Var(a_i)Var(x_i) Var(y) = Var(a_1)Var(x_1) + \\cdot + Var(a_n)Var(x_n) Var(y) = Var(a_1)Var(x_1) + \\cdot + Var(a_n)Var(x_n) Since the bias, b, is a constant, Var(b) = 0 Var(b) = 0 Since i.i.d. Var(y) = n \\times Var(a_i)Var(x_i) Var(y) = n \\times Var(a_i)Var(x_i) Since we want constant variance where Var(y) = Var(x_i) Var(y) = Var(x_i) 1 = nVar(a_i) 1 = nVar(a_i) Var(a_i) = \\frac{1}{n} Var(a_i) = \\frac{1}{n} This is essentially Lecun initialization, from his paper titled \"Efficient Backpropagation\" We draw our weights i.i.d. with mean=0 and variance = \\frac{1}{n} \\frac{1}{n} Where n n is the number of input units in the weight tensor Improvements to Lecun Intialization \u00b6 They are essentially slight modifications to Lecun'98 initialization Xavier Intialization Works better for layers with Sigmoid activations var(a_i) = \\frac{1}{n_{in} + n_{out}} var(a_i) = \\frac{1}{n_{in} + n_{out}} Where n_{in} n_{in} and n_{out} n_{out} are the number of input and output units in the weight tensor respectively Kaiming Initialization Works better for layers with ReLU or LeakyReLU activations var(a_i) = \\frac{2}{n_{in}} var(a_i) = \\frac{2}{n_{in}} Summary of weight initializations \u00b6 Normal Distribution Lecun Normal Distribution Xavier (Glorot) Normal Distribution Kaiming (He) Normal Distribution Weight Initializations with PyTorch \u00b6 Normal Initialization: Tanh Activation \u00b6 import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets from torch.autograd import Variable # Set seed torch . manual_seed ( 0 ) # Scheduler import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Linear weight, W, Y = WX + B nn . init . normal_ ( self . fc1 . weight , mean = 0 , std = 1 ) # Non-linearity self . tanh = nn . Tanh () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) nn . init . normal_ ( self . fc2 . weight , mean = 0 , std = 1 ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . tanh ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 1 , gamma = 0.96 ) ''' STEP 8: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted . type ( torch . FloatTensor ) . cpu () == labels . type ( torch . FloatTensor )) . sum () accuracy = 100. * correct . item () / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500 . Loss : 0.5192779302597046 . Accuracy : 87.9 Epoch : 1 LR : [ 0.096 ] Iteration : 1000 . Loss : 0.4060308337211609 . Accuracy : 90.15 Epoch : 2 LR : [ 0.09216 ] Iteration : 1500 . Loss : 0.2880493104457855 . Accuracy : 90.71 Epoch : 3 LR : [ 0.08847359999999999 ] Iteration : 2000 . Loss : 0.23173095285892487 . Accuracy : 91.99 Epoch : 4 LR : [ 0.084934656 ] Iteration : 2500 . Loss : 0.23814399540424347 . Accuracy : 92.32 Iteration : 3000 . Loss : 0.19513173401355743 . Accuracy : 92.55 Lecun Initialization: Tanh Activation \u00b6 By default, PyTorch uses Lecun initialization, so nothing new has to be done here compared to using Normal, Xavier or Kaiming initialization. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets from torch.autograd import Variable # Set seed torch . manual_seed ( 0 ) # Scheduler import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . tanh = nn . Tanh () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . tanh ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 1 , gamma = 0.96 ) ''' STEP 8: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted . type ( torch . FloatTensor ) . cpu () == labels . type ( torch . FloatTensor )) . sum () accuracy = 100. * correct . item () / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500 . Loss : 0.20123475790023804 . Accuracy : 95.63 Epoch : 1 LR : [ 0.096 ] Iteration : 1000 . Loss : 0.10885068774223328 . Accuracy : 96.48 Epoch : 2 LR : [ 0.09216 ] Iteration : 1500 . Loss : 0.1296212077140808 . Accuracy : 97.22 Epoch : 3 LR : [ 0.08847359999999999 ] Iteration : 2000 . Loss : 0.05178885534405708 . Accuracy : 97.36 Epoch : 4 LR : [ 0.084934656 ] Iteration : 2500 . Loss : 0.02619408629834652 . Accuracy : 97.61 Iteration : 3000 . Loss : 0.02096685953438282 . Accuracy : 97.7 Xavier Initialization: Tanh Activation \u00b6 import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets from torch.autograd import Variable # Set seed torch . manual_seed ( 0 ) # Scheduler import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Linear weight, W, Y = WX + B nn . init . xavier_normal_ ( self . fc1 . weight ) # Non-linearity self . tanh = nn . Tanh () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) nn . init . xavier_normal_ ( self . fc2 . weight ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . tanh ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 1 , gamma = 0.96 ) ''' STEP 8: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted . type ( torch . FloatTensor ) . cpu () == labels . type ( torch . FloatTensor )) . sum () accuracy = 100. * correct . item () / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500 . Loss : 0.14800140261650085 . Accuracy : 95.43 Epoch : 1 LR : [ 0.096 ] Iteration : 1000 . Loss : 0.17138008773326874 . Accuracy : 96.58 Epoch : 2 LR : [ 0.09216 ] Iteration : 1500 . Loss : 0.07987994700670242 . Accuracy : 96.95 Epoch : 3 LR : [ 0.08847359999999999 ] Iteration : 2000 . Loss : 0.07756654918193817 . Accuracy : 97.23 Epoch : 4 LR : [ 0.084934656 ] Iteration : 2500 . Loss : 0.05563584715127945 . Accuracy : 97.6 Iteration : 3000 . Loss : 0.07122127711772919 . Accuracy : 97.49 Xavier Initialization: ReLU Activation \u00b6 import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets from torch.autograd import Variable # Set seed torch . manual_seed ( 0 ) # Scheduler import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Linear weight, W, Y = WX + B nn . init . xavier_normal_ ( self . fc1 . weight ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) nn . init . xavier_normal_ ( self . fc2 . weight ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 1 , gamma = 0.96 ) ''' STEP 8: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted . type ( torch . FloatTensor ) . cpu () == labels . type ( torch . FloatTensor )) . sum () accuracy = 100. * correct . item () / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500 . Loss : 0.1245984435081482 . Accuracy : 95.82 Epoch : 1 LR : [ 0.096 ] Iteration : 1000 . Loss : 0.14348150789737701 . Accuracy : 96.72 Epoch : 2 LR : [ 0.09216 ] Iteration : 1500 . Loss : 0.10421314090490341 . Accuracy : 97.3 Epoch : 3 LR : [ 0.08847359999999999 ] Iteration : 2000 . Loss : 0.04693891853094101 . Accuracy : 97.29 Epoch : 4 LR : [ 0.084934656 ] Iteration : 2500 . Loss : 0.06869587302207947 . Accuracy : 97.61 Iteration : 3000 . Loss : 0.056865859776735306 . Accuracy : 97.48 He Initialization: ReLU Activation \u00b6 import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets from torch.autograd import Variable # Set seed torch . manual_seed ( 0 ) # Scheduler import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Linear weight, W, Y = WX + B nn . init . kaiming_normal_ ( self . fc1 . weight ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) nn . init . kaiming_normal_ ( self . fc2 . weight ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 1 , gamma = 0.96 ) ''' STEP 8: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted . type ( torch . FloatTensor ) . cpu () == labels . type ( torch . FloatTensor )) . sum () accuracy = 100. * correct . item () / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500 . Loss : 0.11658752709627151 . Accuracy : 95.7 Epoch : 1 LR : [ 0.096 ] Iteration : 1000 . Loss : 0.15525035560131073 . Accuracy : 96.65 Epoch : 2 LR : [ 0.09216 ] Iteration : 1500 . Loss : 0.09970294684171677 . Accuracy : 97.07 Epoch : 3 LR : [ 0.08847359999999999 ] Iteration : 2000 . Loss : 0.04063304886221886 . Accuracy : 97.23 Epoch : 4 LR : [ 0.084934656 ] Iteration : 2500 . Loss : 0.0719323456287384 . Accuracy : 97.7 Iteration : 3000 . Loss : 0.04470040276646614 . Accuracy : 97.39 Initialization Performance \u00b6 Initialization: Activation Test Accuracy Normal: Tanh 92.55 Lecun: Tanh 97.7 Xavier: Tanh 97.49 Xavier: ReLU 97.48 He: ReLU 97.39 Interpreting the Validation Accuracy Table Take note that these numbers would fluctuate slightly when you change seeds. However, the key point here is that all the other intializations are clearly much better than a basic normal distribution. Whether He, Xavier, or Lecun intialization is better or any other initializations depends on the overall model's architecture (RNN/LSTM/CNN/FNN etc.), activation functions (ReLU, Sigmoid, Tanh etc.) and more. For example, more advanced initializations we will cover subsequently is orthogonal initialization that works better for RNN/LSTM. But due to the math involved in that, we will be covering such advanced initializations in a separate section. Summary \u00b6 We've learnt... Success Recap of LG Recap of FNN Recap of Activation Functions Sigmoid (Logistic) Tanh ReLU Need for Weight Initializations Sigmoid/Tanh: vanishing gradients Constant Variance initialization with Lecun or Xavier ReLU: exploding gradients with dead units He Initialization Leaky ReLU: exploding gradients only He Initialization Types of weight initialisations Zero Normal: growing weight variance Lecun: constant variance Xavier: constant variance for Sigmoid/Tanh Kaiming He: constant variance for ReLU activations PyTorch implementation Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Weight Initialization and Activation Functions"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#weight-initializations-activation-functions","text":"Run Jupyter Notebook You can run the code for this section in this jupyter notebook link .","title":"Weight Initializations &amp; Activation Functions"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#recap-of-logistic-regression","text":"","title":"Recap of Logistic Regression"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#recap-of-feedforward-neural-network-activation-function","text":"","title":"Recap of Feedforward Neural Network Activation Function"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#sigmoid-logistic","text":"\\sigma(x) = \\frac{1}{1 + e^{-x}} \\sigma(x) = \\frac{1}{1 + e^{-x}} Input number \\rightarrow \\rightarrow [0, 1] Large negative number \\rightarrow \\rightarrow 0 Large positive number \\rightarrow \\rightarrow 1 Cons: Activation saturates at 0 or 1 with gradients \\approx \\approx 0 No signal to update weights \\rightarrow \\rightarrow cannot learn Solution: Have to carefully initialize weights to prevent this import matplotlib.pyplot as plt % matplotlib inline import numpy as np def sigmoid ( x ): a = [] for item in x : a . append ( 1 / ( 1 + np . exp ( - item ))) return a x = np . arange ( - 10. , 10. , 0.2 ) sig = sigmoid ( x ) plt . style . use ( 'ggplot' ) plt . plot ( x , sig , linewidth = 3.0 )","title":"Sigmoid (Logistic)"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#tanh","text":"\\tanh(x) = 2 \\sigma(2x) -1 \\tanh(x) = 2 \\sigma(2x) -1 A scaled sigmoid function Input number \\rightarrow \\rightarrow [-1, 1] Cons: Activation saturates at 0 or 1 with gradients \\approx \\approx 0 No signal to update weights \\rightarrow \\rightarrow cannot learn Solution : Have to carefully initialize weights to prevent this x = np . arange ( - 10. , 10. , 0.2 ) tanh = np . dot ( 2 , sigmoid ( np . dot ( 2 , x ))) - 1 plt . plot ( x , tanh , linewidth = 3.0 )","title":"Tanh"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#relus","text":"f(x) = \\max(0, x) f(x) = \\max(0, x) Pros: Accelerates convergence \\rightarrow \\rightarrow train faster Less computationally expensive operation compared to Sigmoid/Tanh exponentials Cons: Many ReLU units \"die\" \\rightarrow \\rightarrow gradients = 0 forever Solution : careful learning rate and weight initialization choice x = np . arange ( - 10. , 10. , 0.2 ) relu = np . maximum ( x , 0 ) plt . plot ( x , relu , linewidth = 3.0 )","title":"ReLUs"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#why-do-we-need-weight-initializations-or-new-activation-functions","text":"To prevent vanishing/exploding gradients","title":"Why do we need weight initializations or new activation functions?"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#case-1-sigmoidtanh","text":"Problem If variance of input too large: gradients = 0 (vanishing gradients) If variance of input too small: linear \\rightarrow \\rightarrow gradients = constant value Solutions Want a constant variance of input to achieve non-linearity \\rightarrow \\rightarrow unique gradients for unique updates Xavier Initialization (good constant variance for Sigmoid/Tanh) ReLU or Leaky ReLU","title":"Case 1: Sigmoid/Tanh"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#case-2-relu","text":"Solution to Case 1 Regardless of variance of input: gradients = 0 or 1 Problem But those with 0: no updates (\"dead ReLU units\") Has unlimited output size with input > 0 (explodes gradients subsequently) Solutions He Initialization (good constant variance) Leaky ReLU","title":"Case 2: ReLU"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#case-3-leaky-relu","text":"Solution to Case 2 Solves the 0 signal issue when input < 0 Problem Has unlimited output size with input > 0 (explodes) Solution He Initialization (good constant variance)","title":"Case 3: Leaky ReLU"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#summary-of-weight-initialization-solutions-to-activations","text":"Tanh/Sigmoid vanishing gradients can be solved with Xavier initialization Good range of constant variance ReLU/Leaky ReLU exploding gradients can be solved with He initialization Good range of constant variance","title":"Summary of weight initialization solutions to activations"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#types-of-weight-intializations","text":"","title":"Types of weight intializations"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#zero-initialization-set-all-weights-to-0","text":"Every neuron in the network computes the same output \\rightarrow \\rightarrow computes the same gradient \\rightarrow \\rightarrow same parameter updates","title":"Zero Initialization: set all weights to 0"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#normal-initialization-set-all-weights-to-random-small-numbers","text":"Every neuron in the network computes different output \\rightarrow \\rightarrow computes different gradient \\rightarrow \\rightarrow different parameter updates \"Symmetry breaking\" Problem: variance that grows with the number of inputs","title":"Normal Initialization: set all weights to random small numbers"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#lecun-initialization-normalize-variance","text":"Solves growing variance with the number of inputs \\rightarrow \\rightarrow constant variance Look at a simple feedforward neural network","title":"Lecun Initialization: normalize variance"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#equations-for-lecun-initialization","text":"Y = AX + B Y = AX + B y = a_1x_1 + a_2x_2 + \\cdot + a_n x_n + b y = a_1x_1 + a_2x_2 + \\cdot + a_n x_n + b Var(y) = Var(a_1x_1 + a_2x_2 + \\cdot + a_n x_n + b) Var(y) = Var(a_1x_1 + a_2x_2 + \\cdot + a_n x_n + b) Var(a_i x_i) = E(x_i)^2 Var(a_i) + E(a_i)^2Var(x_i) + Var(a_i)Var(x_i) Var(a_i x_i) = E(x_i)^2 Var(a_i) + E(a_i)^2Var(x_i) + Var(a_i)Var(x_i) General term, you might be more familiar with the following Var(XY) = E(X)^2 Var(Y) + E(Y)^2Var(X) + Var(X)Var(Y) Var(XY) = E(X)^2 Var(Y) + E(Y)^2Var(X) + Var(X)Var(Y) E(x_i) E(x_i) : expectation/mean of x_i x_i E(a_i) E(a_i) : expectation/mean of a_i a_i Assuming inputs/weights drawn i.i.d. with Gaussian distribution of mean=0 E(x_i) = E(a_i) = 0 E(x_i) = E(a_i) = 0 Var(a_i x_i) = Var(a_i)Var(x_i) Var(a_i x_i) = Var(a_i)Var(x_i) Var(y) = Var(a_1)Var(x_1) + \\cdot + Var(a_n)Var(x_n) Var(y) = Var(a_1)Var(x_1) + \\cdot + Var(a_n)Var(x_n) Since the bias, b, is a constant, Var(b) = 0 Var(b) = 0 Since i.i.d. Var(y) = n \\times Var(a_i)Var(x_i) Var(y) = n \\times Var(a_i)Var(x_i) Since we want constant variance where Var(y) = Var(x_i) Var(y) = Var(x_i) 1 = nVar(a_i) 1 = nVar(a_i) Var(a_i) = \\frac{1}{n} Var(a_i) = \\frac{1}{n} This is essentially Lecun initialization, from his paper titled \"Efficient Backpropagation\" We draw our weights i.i.d. with mean=0 and variance = \\frac{1}{n} \\frac{1}{n} Where n n is the number of input units in the weight tensor","title":"Equations for Lecun Initialization"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#improvements-to-lecun-intialization","text":"They are essentially slight modifications to Lecun'98 initialization Xavier Intialization Works better for layers with Sigmoid activations var(a_i) = \\frac{1}{n_{in} + n_{out}} var(a_i) = \\frac{1}{n_{in} + n_{out}} Where n_{in} n_{in} and n_{out} n_{out} are the number of input and output units in the weight tensor respectively Kaiming Initialization Works better for layers with ReLU or LeakyReLU activations var(a_i) = \\frac{2}{n_{in}} var(a_i) = \\frac{2}{n_{in}}","title":"Improvements to Lecun Intialization"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#summary-of-weight-initializations","text":"Normal Distribution Lecun Normal Distribution Xavier (Glorot) Normal Distribution Kaiming (He) Normal Distribution","title":"Summary of  weight initializations"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#weight-initializations-with-pytorch","text":"","title":"Weight Initializations with PyTorch"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#normal-initialization-tanh-activation","text":"import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets from torch.autograd import Variable # Set seed torch . manual_seed ( 0 ) # Scheduler import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Linear weight, W, Y = WX + B nn . init . normal_ ( self . fc1 . weight , mean = 0 , std = 1 ) # Non-linearity self . tanh = nn . Tanh () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) nn . init . normal_ ( self . fc2 . weight , mean = 0 , std = 1 ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . tanh ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 1 , gamma = 0.96 ) ''' STEP 8: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted . type ( torch . FloatTensor ) . cpu () == labels . type ( torch . FloatTensor )) . sum () accuracy = 100. * correct . item () / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500 . Loss : 0.5192779302597046 . Accuracy : 87.9 Epoch : 1 LR : [ 0.096 ] Iteration : 1000 . Loss : 0.4060308337211609 . Accuracy : 90.15 Epoch : 2 LR : [ 0.09216 ] Iteration : 1500 . Loss : 0.2880493104457855 . Accuracy : 90.71 Epoch : 3 LR : [ 0.08847359999999999 ] Iteration : 2000 . Loss : 0.23173095285892487 . Accuracy : 91.99 Epoch : 4 LR : [ 0.084934656 ] Iteration : 2500 . Loss : 0.23814399540424347 . Accuracy : 92.32 Iteration : 3000 . Loss : 0.19513173401355743 . Accuracy : 92.55","title":"Normal Initialization: Tanh Activation"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#lecun-initialization-tanh-activation","text":"By default, PyTorch uses Lecun initialization, so nothing new has to be done here compared to using Normal, Xavier or Kaiming initialization. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets from torch.autograd import Variable # Set seed torch . manual_seed ( 0 ) # Scheduler import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . tanh = nn . Tanh () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . tanh ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 1 , gamma = 0.96 ) ''' STEP 8: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted . type ( torch . FloatTensor ) . cpu () == labels . type ( torch . FloatTensor )) . sum () accuracy = 100. * correct . item () / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500 . Loss : 0.20123475790023804 . Accuracy : 95.63 Epoch : 1 LR : [ 0.096 ] Iteration : 1000 . Loss : 0.10885068774223328 . Accuracy : 96.48 Epoch : 2 LR : [ 0.09216 ] Iteration : 1500 . Loss : 0.1296212077140808 . Accuracy : 97.22 Epoch : 3 LR : [ 0.08847359999999999 ] Iteration : 2000 . Loss : 0.05178885534405708 . Accuracy : 97.36 Epoch : 4 LR : [ 0.084934656 ] Iteration : 2500 . Loss : 0.02619408629834652 . Accuracy : 97.61 Iteration : 3000 . Loss : 0.02096685953438282 . Accuracy : 97.7","title":"Lecun Initialization: Tanh Activation"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#xavier-initialization-tanh-activation","text":"import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets from torch.autograd import Variable # Set seed torch . manual_seed ( 0 ) # Scheduler import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Linear weight, W, Y = WX + B nn . init . xavier_normal_ ( self . fc1 . weight ) # Non-linearity self . tanh = nn . Tanh () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) nn . init . xavier_normal_ ( self . fc2 . weight ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . tanh ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 1 , gamma = 0.96 ) ''' STEP 8: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted . type ( torch . FloatTensor ) . cpu () == labels . type ( torch . FloatTensor )) . sum () accuracy = 100. * correct . item () / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500 . Loss : 0.14800140261650085 . Accuracy : 95.43 Epoch : 1 LR : [ 0.096 ] Iteration : 1000 . Loss : 0.17138008773326874 . Accuracy : 96.58 Epoch : 2 LR : [ 0.09216 ] Iteration : 1500 . Loss : 0.07987994700670242 . Accuracy : 96.95 Epoch : 3 LR : [ 0.08847359999999999 ] Iteration : 2000 . Loss : 0.07756654918193817 . Accuracy : 97.23 Epoch : 4 LR : [ 0.084934656 ] Iteration : 2500 . Loss : 0.05563584715127945 . Accuracy : 97.6 Iteration : 3000 . Loss : 0.07122127711772919 . Accuracy : 97.49","title":"Xavier Initialization: Tanh Activation"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#xavier-initialization-relu-activation","text":"import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets from torch.autograd import Variable # Set seed torch . manual_seed ( 0 ) # Scheduler import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Linear weight, W, Y = WX + B nn . init . xavier_normal_ ( self . fc1 . weight ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) nn . init . xavier_normal_ ( self . fc2 . weight ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 1 , gamma = 0.96 ) ''' STEP 8: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted . type ( torch . FloatTensor ) . cpu () == labels . type ( torch . FloatTensor )) . sum () accuracy = 100. * correct . item () / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500 . Loss : 0.1245984435081482 . Accuracy : 95.82 Epoch : 1 LR : [ 0.096 ] Iteration : 1000 . Loss : 0.14348150789737701 . Accuracy : 96.72 Epoch : 2 LR : [ 0.09216 ] Iteration : 1500 . Loss : 0.10421314090490341 . Accuracy : 97.3 Epoch : 3 LR : [ 0.08847359999999999 ] Iteration : 2000 . Loss : 0.04693891853094101 . Accuracy : 97.29 Epoch : 4 LR : [ 0.084934656 ] Iteration : 2500 . Loss : 0.06869587302207947 . Accuracy : 97.61 Iteration : 3000 . Loss : 0.056865859776735306 . Accuracy : 97.48","title":"Xavier Initialization: ReLU Activation"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#he-initialization-relu-activation","text":"import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets from torch.autograd import Variable # Set seed torch . manual_seed ( 0 ) # Scheduler import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Linear weight, W, Y = WX + B nn . init . kaiming_normal_ ( self . fc1 . weight ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) nn . init . kaiming_normal_ ( self . fc2 . weight ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 1 , gamma = 0.96 ) ''' STEP 8: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted . type ( torch . FloatTensor ) . cpu () == labels . type ( torch . FloatTensor )) . sum () accuracy = 100. * correct . item () / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500 . Loss : 0.11658752709627151 . Accuracy : 95.7 Epoch : 1 LR : [ 0.096 ] Iteration : 1000 . Loss : 0.15525035560131073 . Accuracy : 96.65 Epoch : 2 LR : [ 0.09216 ] Iteration : 1500 . Loss : 0.09970294684171677 . Accuracy : 97.07 Epoch : 3 LR : [ 0.08847359999999999 ] Iteration : 2000 . Loss : 0.04063304886221886 . Accuracy : 97.23 Epoch : 4 LR : [ 0.084934656 ] Iteration : 2500 . Loss : 0.0719323456287384 . Accuracy : 97.7 Iteration : 3000 . Loss : 0.04470040276646614 . Accuracy : 97.39","title":"He Initialization: ReLU Activation"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#initialization-performance","text":"Initialization: Activation Test Accuracy Normal: Tanh 92.55 Lecun: Tanh 97.7 Xavier: Tanh 97.49 Xavier: ReLU 97.48 He: ReLU 97.39 Interpreting the Validation Accuracy Table Take note that these numbers would fluctuate slightly when you change seeds. However, the key point here is that all the other intializations are clearly much better than a basic normal distribution. Whether He, Xavier, or Lecun intialization is better or any other initializations depends on the overall model's architecture (RNN/LSTM/CNN/FNN etc.), activation functions (ReLU, Sigmoid, Tanh etc.) and more. For example, more advanced initializations we will cover subsequently is orthogonal initialization that works better for RNN/LSTM. But due to the math involved in that, we will be covering such advanced initializations in a separate section.","title":"Initialization Performance"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#summary","text":"We've learnt... Success Recap of LG Recap of FNN Recap of Activation Functions Sigmoid (Logistic) Tanh ReLU Need for Weight Initializations Sigmoid/Tanh: vanishing gradients Constant Variance initialization with Lecun or Xavier ReLU: exploding gradients with dead units He Initialization Leaky ReLU: exploding gradients only He Initialization Types of weight initialisations Zero Normal: growing weight variance Lecun: constant variance Xavier: constant variance for Sigmoid/Tanh Kaiming He: constant variance for ReLU activations PyTorch implementation","title":"Summary"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/","text":"Markov Decision Processes (MDPs) \u00b6 Typically we can frame all RL tasks as MDPs 1 Intuitively, it's sort of a way to frame RL tasks such that we can solve them in a \"principled\" manner. We will go into the specifics throughout this tutorial The key in MDPs is the Markov Property Essentially the future depends on the present and not the past More specifically, the future is independent of the past given the present There's an assumption the present state encapsulates past information. This is not always true, see the note below. Putting into the context of what we have covered so far: our agent can (1) control its action based on its current (2) completely known state Back to the \"driving to avoid puppy\" example: given we know there is a dog in front of the car as the current state and the car is always moving forward (no reverse driving), the agent can decide to take a left/right turn to avoid colliding with the puppy in front Two main characteristics for MDPs \u00b6 Control over state transitions States completely observable Other Markov Models Permutations of whether there is presence of the two main characteristics would lead to different Markov models. These are not important now, but it gives you an idea of what other frameworks we can use besides MDPs. Types of Markov Models \u00b6 Control over state transitions and completely observable states: MDPs Control over state transitions and partially observable states : Partially Observable MDPs (POMDPs) No control over state transitions and completely observable states: Markov Chain No control over state transitions and partially observable states: Hidden Markov Model POMDPs \u00b6 Imagine our driving example where we don't know if the car is going forward/backward in its state, but only know there is a puppy in the center lane in front, this is a partially observable state There are ways to counter this Use the complete history to construct the current state Represent the current state as a probability distribution ( bayesian approach ) of what the agent perceives of the current state Using a RNN to form the current state that encapsulates the past 2 5 Components of MDPs \u00b6 \\mathcal{S} \\mathcal{S} : set of states \\mathcal{A} \\mathcal{A} : set of actions \\mathcal{R} \\mathcal{R} : reward function \\mathcal{P} \\mathcal{P} : transition probability function \\gamma \\gamma : discount for future rewards Remembering 5 components with a mnemonic A mnemonic I use to remember the 5 components is the acronym \"SARPY\" (sar-py). I know \\gamma \\gamma is not \\mathcal{Y} \\mathcal{Y} but it looks like a y so there's that. Moving From MDPs to Optimal Policy \u00b6 We have an agent acting in an environment The way the environment reacts to the agent's actions ( a a ) is dictated by a model The agent can take actions ( a a ) to move from one state ( s s ) to another new state ( s') s') When the agent has transited to a new state ( s') s') , there will a reward ( r r ) We may or may not know our model Model-based RL : this is where we can clearly define our (1) transition probabilities and/or (2) reward function A global minima can be attained via Dynamic Programming (DP) Model-free RL : this is where we cannot clearly define our (1) transition probabilities and/or (2) reward function Most real-world problems are under this category so we will mostly place our attention on this category How the agent acts ( a a ) in its current state ( s s ) is specified by its policy ( \\pi(s) \\pi(s) ) It can either be deterministic or stochastic Deterministic policy : a = \\pi(s) a = \\pi(s) Stochastic policy : \\mathbb{P}_\\pi [A=a \\vert S=s] = \\pi(a | s) \\mathbb{P}_\\pi [A=a \\vert S=s] = \\pi(a | s) This is the proability of taking an action given the current state under the policy \\mathcal{A} \\mathcal{A} : set of all actions \\mathcal{S} \\mathcal{S} : set of all states When the agent acts given its state under the policy ( \\pi(a | s) \\pi(a | s) ) , the transition probability function \\mathcal{P} \\mathcal{P} determines the subsequent state ( s' s' ) \\mathcal{P}_{ss'}^a = \\mathcal{P}(s' \\vert s, a) = \\mathbb{P} [S_{t+1} = s' \\vert S_t = s, A_t = a] \\mathcal{P}_{ss'}^a = \\mathcal{P}(s' \\vert s, a) = \\mathbb{P} [S_{t+1} = s' \\vert S_t = s, A_t = a] When the agent act based on its policy ( \\pi(a | s) \\pi(a | s) ) and transited to the new state determined by the transition probability function \\mathcal{P}_{ss'}^a \\mathcal{P}_{ss'}^a it gets a reward based on the reward function as a feedback \\mathcal{R}_s^a = \\mathbb{E} [\\mathcal{R}_{t+1} \\vert S_t = s, A_t = a] \\mathcal{R}_s^a = \\mathbb{E} [\\mathcal{R}_{t+1} \\vert S_t = s, A_t = a] Rewards are short-term, given as feedback after the agent takes an action and transits to a new state. Summing all future rewards and discounting them would lead to our return \\mathcal{G} \\mathcal{G} \\mathcal{G}_t = \\sum_{i=0}^{N} \\gamma^k \\mathcal{R}_{t+1+i} \\mathcal{G}_t = \\sum_{i=0}^{N} \\gamma^k \\mathcal{R}_{t+1+i} \\gamma \\gamma , our discount factor which ranges from 0 to 1 (inclusive) reduces the weightage of future rewards allowing us to balance between short-term and long-term goals With our return \\mathcal{G} \\mathcal{G} , we then have our state-value function \\mathcal{V}_{\\pi} \\mathcal{V}_{\\pi} (how good to stay in that state) and our action-value or q-value function \\mathcal{Q}_{\\pi} \\mathcal{Q}_{\\pi} (how good to take the action) \\mathcal{V}_{\\pi}(s) = \\mathbb{E}_{\\pi}[\\mathcal{G}_t \\vert \\mathcal{S}_t = s] \\mathcal{V}_{\\pi}(s) = \\mathbb{E}_{\\pi}[\\mathcal{G}_t \\vert \\mathcal{S}_t = s] \\mathcal{Q}_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[\\mathcal{G}_t \\vert \\mathcal{S}_t = s, \\mathcal{A}_t = a] \\mathcal{Q}_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[\\mathcal{G}_t \\vert \\mathcal{S}_t = s, \\mathcal{A}_t = a] The advantage function is simply the difference between the two functions \\mathcal{A}_{\\pi}(s, a) = \\mathcal{Q}_{\\pi}(s, a) - \\mathcal{V}_{\\pi}(s) \\mathcal{A}_{\\pi}(s, a) = \\mathcal{Q}_{\\pi}(s, a) - \\mathcal{V}_{\\pi}(s) Seems useless at this stage, but this advantage function will be used in some key algorithms we are covering Since our policy determines how our agent acts given its state, achieving an optimal policy \\pi_* \\pi_* would mean achieving optimal actions that is exactly what we want! Basic Categories of Approaches We've covered state-value functions, action-value functions, model-free RL and model-based RL. They form general overarching categories of how we design our agent. Categories of Design \u00b6 State-value based: search for the optimal state-value function (goodness of action in the state) Action-value based: search for the optimal action-value function (goodness of policy) Actor-critic based: using both state-value and action-value function Model based: attempts to model the environment to find the best policy Model-free based: trial and error to optimize for the best policy to get the most rewards instead of modelling the environment explicitly Optimal Policy \u00b6 Optimal policy \\pi_* \\pi_* \u2192 optimal state-value and action-value functions \u2192 max return \u2192 argmax of value functions \\pi_{*} = \\arg\\max_{\\pi} \\mathcal{V}_{\\pi}(s) = \\arg\\max_{\\pi} \\mathcal{Q}_{\\pi}(s, a) \\pi_{*} = \\arg\\max_{\\pi} \\mathcal{V}_{\\pi}(s) = \\arg\\max_{\\pi} \\mathcal{Q}_{\\pi}(s, a) To calculate argmax of value functions \u2192 we need max return \\mathcal{G}_t \\mathcal{G}_t \u2192 need max sum of rewards \\mathcal{R}_s^a \\mathcal{R}_s^a To get max sum of rewards \\mathcal{R}_s^a \\mathcal{R}_s^a we will rely on the Bellman Equations. 3 Bellman Equation \u00b6 Essentially, the Bellman Equation breaks down our value functions into two parts Immediate reward Discounted future value function State-value function can be broken into: \\begin{aligned} \\mathcal{V}_{\\pi}(s) &= \\mathbb{E}[\\mathcal{G}_t \\vert \\mathcal{S}_t = s] \\\\ &= \\mathbb{E} [\\mathcal{R}_{t+1} + \\gamma \\mathcal{R}_{t+2} + \\gamma^2 \\mathcal{R}_{t+3} + \\dots \\vert \\mathcal{S}_t = s] \\\\ &= \\mathbb{E} [\\mathcal{R}_{t+1} + \\gamma (\\mathcal{R}_{t+2} + \\gamma \\mathcal{R}_{t+3} + \\dots) \\vert \\mathcal{S}_t = s] \\\\ &= \\mathbb{E} [\\mathcal{R}_{t+1} + \\gamma \\mathcal{G}_{t+1} \\vert \\mathcal{S}_t = s] \\\\ &= \\mathbb{E} [\\mathcal{R}_{t+1} + \\gamma \\mathcal{V}_{\\pi}(\\mathcal{s}_{t+1}) \\vert \\mathcal{S}_t = s] \\end{aligned} \\begin{aligned} \\mathcal{V}_{\\pi}(s) &= \\mathbb{E}[\\mathcal{G}_t \\vert \\mathcal{S}_t = s] \\\\ &= \\mathbb{E} [\\mathcal{R}_{t+1} + \\gamma \\mathcal{R}_{t+2} + \\gamma^2 \\mathcal{R}_{t+3} + \\dots \\vert \\mathcal{S}_t = s] \\\\ &= \\mathbb{E} [\\mathcal{R}_{t+1} + \\gamma (\\mathcal{R}_{t+2} + \\gamma \\mathcal{R}_{t+3} + \\dots) \\vert \\mathcal{S}_t = s] \\\\ &= \\mathbb{E} [\\mathcal{R}_{t+1} + \\gamma \\mathcal{G}_{t+1} \\vert \\mathcal{S}_t = s] \\\\ &= \\mathbb{E} [\\mathcal{R}_{t+1} + \\gamma \\mathcal{V}_{\\pi}(\\mathcal{s}_{t+1}) \\vert \\mathcal{S}_t = s] \\end{aligned} Action-value function can be broken into: \\mathcal{Q}_{\\pi}(s, a) = \\mathbb{E} [\\mathcal{R}_{t+1} + \\gamma \\mathcal{Q}_{\\pi}(\\mathcal{s}_{t+1}, \\mathcal{a}_{t+1}) \\vert \\mathcal{S}_t = s, \\mathcal{A} = a] \\mathcal{Q}_{\\pi}(s, a) = \\mathbb{E} [\\mathcal{R}_{t+1} + \\gamma \\mathcal{Q}_{\\pi}(\\mathcal{s}_{t+1}, \\mathcal{a}_{t+1}) \\vert \\mathcal{S}_t = s, \\mathcal{A} = a] Key Recap on Value Functions \u00b6 \\mathcal{V}_{\\pi}(s) = \\mathbb{E}_{\\pi}[\\mathcal{G}_t \\vert \\mathcal{S}_t = s] \\mathcal{V}_{\\pi}(s) = \\mathbb{E}_{\\pi}[\\mathcal{G}_t \\vert \\mathcal{S}_t = s] State-value function: tells us how good to be in that state \\mathcal{Q}_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[\\mathcal{G}_t \\vert \\mathcal{S}_t = s, \\mathcal{A}_t = a] \\mathcal{Q}_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[\\mathcal{G}_t \\vert \\mathcal{S}_t = s, \\mathcal{A}_t = a] Action-value function: tells us how good to take actions given state Bellman Expectation Equations \u00b6 Now we can move from Bellman Equations into Bellman Expectation Equations Basic: State-value function \\mathcal{V}_{\\pi}(s) \\mathcal{V}_{\\pi}(s) Current state \\mathcal{S} \\mathcal{S} Multiple possible actions determined by stochastic policy \\pi(a | s) \\pi(a | s) Each possible action is associated with a action-value function \\mathcal{Q}_{\\pi}(s, a) \\mathcal{Q}_{\\pi}(s, a) returning a value of that particular action Multiplying the possible actions with the action-value function and summing them gives us an indication of how good it is to be in that state Final equation: \\mathcal{V}_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a | s) \\mathcal{Q}(s, a) \\mathcal{V}_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a | s) \\mathcal{Q}(s, a) Loose intuitive interpretation : state-value = sum(policy determining actions * respective action-values) Basic: Action-value function \\mathcal{Q}_{\\pi}(s, a) \\mathcal{Q}_{\\pi}(s, a) With a list of possible multiple actions, there is a list of possible subsequent states s' s' associated with: state value function \\mathcal{V}_{\\pi}(s') \\mathcal{V}_{\\pi}(s') transition probability function \\mathcal{P}_{ss'}^a \\mathcal{P}_{ss'}^a determining where the agent could land in based on the action reward \\mathcal{R}_s^a \\mathcal{R}_s^a for taking the action Summing the reward and the transition probability function associated with the state-value function gives us an indication of how good it is to take the actions given our state Final equation: \\mathcal{Q}_{\\pi}(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a {V}_{\\pi}(s') \\mathcal{Q}_{\\pi}(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a {V}_{\\pi}(s') Loose intuitive interpretation : action-value = reward + sum(transition outcomes determining states * respective state-values) Expanded functions (substitution) Substituting action-value function into the state-value function \\mathcal{V}_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a | s) (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a {V}_{\\pi}(s')) \\mathcal{V}_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a | s) (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a {V}_{\\pi}(s')) Substituting state-value function into action-value function \\mathcal{Q}_{\\pi}(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\sum_{a' \\in \\mathcal{A}} \\pi(a' | s') \\mathcal{Q}(s', a') \\mathcal{Q}_{\\pi}(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\sum_{a' \\in \\mathcal{A}} \\pi(a' | s') \\mathcal{Q}(s', a') Bellman Optimality Equations \u00b6 Remember optimal policy \\pi_* \\pi_* \u2192 optimal state-value and action-value functions \u2192 argmax of value functions \\pi_{*} = \\arg\\max_{\\pi} \\mathcal{V}_{\\pi}(s) = \\arg\\max_{\\pi} \\mathcal{Q}_{\\pi}(s, a) \\pi_{*} = \\arg\\max_{\\pi} \\mathcal{V}_{\\pi}(s) = \\arg\\max_{\\pi} \\mathcal{Q}_{\\pi}(s, a) Finally with Bellman Expectation Equations derived from Bellman Equations, we can derive the equations for the argmax of our value functions Optimal state-value function \\mathcal{V}_*(s) = \\arg\\max_{\\pi} \\mathcal{V}_{\\pi}(s) \\mathcal{V}_*(s) = \\arg\\max_{\\pi} \\mathcal{V}_{\\pi}(s) Given \\mathcal{V}_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a | s) (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a {V}_{\\pi}(s')) \\mathcal{V}_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a | s) (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a {V}_{\\pi}(s')) We have \\mathcal{V}_*(s) = \\max_{a \\in \\mathcal{A}} (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a {V}_{*}(s'))) \\mathcal{V}_*(s) = \\max_{a \\in \\mathcal{A}} (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a {V}_{*}(s'))) Optimal action-value function \\mathcal{Q}_*(s) = \\arg\\max_{\\pi} \\mathcal{Q}_{\\pi}(s) \\mathcal{Q}_*(s) = \\arg\\max_{\\pi} \\mathcal{Q}_{\\pi}(s) Given \\mathcal{Q}_{\\pi}(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\sum_{a' \\in \\mathcal{A}} \\pi(a' | s') \\mathcal{Q}(s', a') \\mathcal{Q}_{\\pi}(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\sum_{a' \\in \\mathcal{A}} \\pi(a' | s') \\mathcal{Q}(s', a') We have \\mathcal{Q}_{*}(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a max_{a' \\in \\mathcal{A}} \\mathcal{Q}_{*}(s', a') \\mathcal{Q}_{*}(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a max_{a' \\in \\mathcal{A}} \\mathcal{Q}_{*}(s', a') Optimal Action-value and State-value functions \u00b6 If the entire environment is known, such that we know our reward function and transition probability function, then we can solve for the optimal action-value and state-value functions via Dynamic Programming like Policy evaluation, policy improvement, and policy iteration However, typically we don't know the environment entirely then there is not closed form solution in getting optimal action-value and state-value functions. Hence, we need other iterative approaches like Monte-Carlo methods Temporal difference learning (model-free and learns with episodes) On-policy TD: SARSA Off-policy TD: Q-Learning and Deep Q-Learning (DQN) Policy gradient REINFORCE Actor-Critic A2C/A3C ACKTR PPO DPG DDPG (DQN + DPG) Closed form solution If there is a closed form solution, then the variables' values can be obtained with a finite number of mathematical operations (for example add, subtract, divide, and multiply). For example, solving 2x = 8 - 6x 2x = 8 - 6x would yield 8x = 8 8x = 8 by adding 6x 6x on both sides of the equation and finally yielding the value of x=1 x=1 by dividing both sides of the equation by 8 8 . These finite 2 steps of mathematical operations allowed us to solve for the value of x as the equation has a closed-form solution. However, many cases in deep learning and reinforcement learning there are no closed-form solutions which requires all the iterative methods mentioned above. Bellman, R. A Markovian Decision Process. Journal of Mathematics and Mechanics. 1957. \u21a9 Matthew J. Hausknecht and Peter Stone. Deep Recurrent Q-Learning for Partially Observable MDPs . 2015. \u21a9 R Bellman. On the Theory of Dynamic Programming. Proceedings of the National Academy of Sciences. 1952. \u21a9","title":"Markov Decision Processes (MDP) and Bellman Equations"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#markov-decision-processes-mdps","text":"Typically we can frame all RL tasks as MDPs 1 Intuitively, it's sort of a way to frame RL tasks such that we can solve them in a \"principled\" manner. We will go into the specifics throughout this tutorial The key in MDPs is the Markov Property Essentially the future depends on the present and not the past More specifically, the future is independent of the past given the present There's an assumption the present state encapsulates past information. This is not always true, see the note below. Putting into the context of what we have covered so far: our agent can (1) control its action based on its current (2) completely known state Back to the \"driving to avoid puppy\" example: given we know there is a dog in front of the car as the current state and the car is always moving forward (no reverse driving), the agent can decide to take a left/right turn to avoid colliding with the puppy in front","title":"Markov Decision Processes (MDPs)"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#two-main-characteristics-for-mdps","text":"Control over state transitions States completely observable Other Markov Models Permutations of whether there is presence of the two main characteristics would lead to different Markov models. These are not important now, but it gives you an idea of what other frameworks we can use besides MDPs.","title":"Two main characteristics for MDPs"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#types-of-markov-models","text":"Control over state transitions and completely observable states: MDPs Control over state transitions and partially observable states : Partially Observable MDPs (POMDPs) No control over state transitions and completely observable states: Markov Chain No control over state transitions and partially observable states: Hidden Markov Model","title":"Types of Markov Models"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#pomdps","text":"Imagine our driving example where we don't know if the car is going forward/backward in its state, but only know there is a puppy in the center lane in front, this is a partially observable state There are ways to counter this Use the complete history to construct the current state Represent the current state as a probability distribution ( bayesian approach ) of what the agent perceives of the current state Using a RNN to form the current state that encapsulates the past 2","title":"POMDPs"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#5-components-of-mdps","text":"\\mathcal{S} \\mathcal{S} : set of states \\mathcal{A} \\mathcal{A} : set of actions \\mathcal{R} \\mathcal{R} : reward function \\mathcal{P} \\mathcal{P} : transition probability function \\gamma \\gamma : discount for future rewards Remembering 5 components with a mnemonic A mnemonic I use to remember the 5 components is the acronym \"SARPY\" (sar-py). I know \\gamma \\gamma is not \\mathcal{Y} \\mathcal{Y} but it looks like a y so there's that.","title":"5 Components of MDPs"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#moving-from-mdps-to-optimal-policy","text":"We have an agent acting in an environment The way the environment reacts to the agent's actions ( a a ) is dictated by a model The agent can take actions ( a a ) to move from one state ( s s ) to another new state ( s') s') When the agent has transited to a new state ( s') s') , there will a reward ( r r ) We may or may not know our model Model-based RL : this is where we can clearly define our (1) transition probabilities and/or (2) reward function A global minima can be attained via Dynamic Programming (DP) Model-free RL : this is where we cannot clearly define our (1) transition probabilities and/or (2) reward function Most real-world problems are under this category so we will mostly place our attention on this category How the agent acts ( a a ) in its current state ( s s ) is specified by its policy ( \\pi(s) \\pi(s) ) It can either be deterministic or stochastic Deterministic policy : a = \\pi(s) a = \\pi(s) Stochastic policy : \\mathbb{P}_\\pi [A=a \\vert S=s] = \\pi(a | s) \\mathbb{P}_\\pi [A=a \\vert S=s] = \\pi(a | s) This is the proability of taking an action given the current state under the policy \\mathcal{A} \\mathcal{A} : set of all actions \\mathcal{S} \\mathcal{S} : set of all states When the agent acts given its state under the policy ( \\pi(a | s) \\pi(a | s) ) , the transition probability function \\mathcal{P} \\mathcal{P} determines the subsequent state ( s' s' ) \\mathcal{P}_{ss'}^a = \\mathcal{P}(s' \\vert s, a) = \\mathbb{P} [S_{t+1} = s' \\vert S_t = s, A_t = a] \\mathcal{P}_{ss'}^a = \\mathcal{P}(s' \\vert s, a) = \\mathbb{P} [S_{t+1} = s' \\vert S_t = s, A_t = a] When the agent act based on its policy ( \\pi(a | s) \\pi(a | s) ) and transited to the new state determined by the transition probability function \\mathcal{P}_{ss'}^a \\mathcal{P}_{ss'}^a it gets a reward based on the reward function as a feedback \\mathcal{R}_s^a = \\mathbb{E} [\\mathcal{R}_{t+1} \\vert S_t = s, A_t = a] \\mathcal{R}_s^a = \\mathbb{E} [\\mathcal{R}_{t+1} \\vert S_t = s, A_t = a] Rewards are short-term, given as feedback after the agent takes an action and transits to a new state. Summing all future rewards and discounting them would lead to our return \\mathcal{G} \\mathcal{G} \\mathcal{G}_t = \\sum_{i=0}^{N} \\gamma^k \\mathcal{R}_{t+1+i} \\mathcal{G}_t = \\sum_{i=0}^{N} \\gamma^k \\mathcal{R}_{t+1+i} \\gamma \\gamma , our discount factor which ranges from 0 to 1 (inclusive) reduces the weightage of future rewards allowing us to balance between short-term and long-term goals With our return \\mathcal{G} \\mathcal{G} , we then have our state-value function \\mathcal{V}_{\\pi} \\mathcal{V}_{\\pi} (how good to stay in that state) and our action-value or q-value function \\mathcal{Q}_{\\pi} \\mathcal{Q}_{\\pi} (how good to take the action) \\mathcal{V}_{\\pi}(s) = \\mathbb{E}_{\\pi}[\\mathcal{G}_t \\vert \\mathcal{S}_t = s] \\mathcal{V}_{\\pi}(s) = \\mathbb{E}_{\\pi}[\\mathcal{G}_t \\vert \\mathcal{S}_t = s] \\mathcal{Q}_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[\\mathcal{G}_t \\vert \\mathcal{S}_t = s, \\mathcal{A}_t = a] \\mathcal{Q}_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[\\mathcal{G}_t \\vert \\mathcal{S}_t = s, \\mathcal{A}_t = a] The advantage function is simply the difference between the two functions \\mathcal{A}_{\\pi}(s, a) = \\mathcal{Q}_{\\pi}(s, a) - \\mathcal{V}_{\\pi}(s) \\mathcal{A}_{\\pi}(s, a) = \\mathcal{Q}_{\\pi}(s, a) - \\mathcal{V}_{\\pi}(s) Seems useless at this stage, but this advantage function will be used in some key algorithms we are covering Since our policy determines how our agent acts given its state, achieving an optimal policy \\pi_* \\pi_* would mean achieving optimal actions that is exactly what we want! Basic Categories of Approaches We've covered state-value functions, action-value functions, model-free RL and model-based RL. They form general overarching categories of how we design our agent.","title":"Moving From MDPs to Optimal Policy"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#categories-of-design","text":"State-value based: search for the optimal state-value function (goodness of action in the state) Action-value based: search for the optimal action-value function (goodness of policy) Actor-critic based: using both state-value and action-value function Model based: attempts to model the environment to find the best policy Model-free based: trial and error to optimize for the best policy to get the most rewards instead of modelling the environment explicitly","title":"Categories of Design"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#optimal-policy","text":"Optimal policy \\pi_* \\pi_* \u2192 optimal state-value and action-value functions \u2192 max return \u2192 argmax of value functions \\pi_{*} = \\arg\\max_{\\pi} \\mathcal{V}_{\\pi}(s) = \\arg\\max_{\\pi} \\mathcal{Q}_{\\pi}(s, a) \\pi_{*} = \\arg\\max_{\\pi} \\mathcal{V}_{\\pi}(s) = \\arg\\max_{\\pi} \\mathcal{Q}_{\\pi}(s, a) To calculate argmax of value functions \u2192 we need max return \\mathcal{G}_t \\mathcal{G}_t \u2192 need max sum of rewards \\mathcal{R}_s^a \\mathcal{R}_s^a To get max sum of rewards \\mathcal{R}_s^a \\mathcal{R}_s^a we will rely on the Bellman Equations. 3","title":"Optimal Policy"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#bellman-equation","text":"Essentially, the Bellman Equation breaks down our value functions into two parts Immediate reward Discounted future value function State-value function can be broken into: \\begin{aligned} \\mathcal{V}_{\\pi}(s) &= \\mathbb{E}[\\mathcal{G}_t \\vert \\mathcal{S}_t = s] \\\\ &= \\mathbb{E} [\\mathcal{R}_{t+1} + \\gamma \\mathcal{R}_{t+2} + \\gamma^2 \\mathcal{R}_{t+3} + \\dots \\vert \\mathcal{S}_t = s] \\\\ &= \\mathbb{E} [\\mathcal{R}_{t+1} + \\gamma (\\mathcal{R}_{t+2} + \\gamma \\mathcal{R}_{t+3} + \\dots) \\vert \\mathcal{S}_t = s] \\\\ &= \\mathbb{E} [\\mathcal{R}_{t+1} + \\gamma \\mathcal{G}_{t+1} \\vert \\mathcal{S}_t = s] \\\\ &= \\mathbb{E} [\\mathcal{R}_{t+1} + \\gamma \\mathcal{V}_{\\pi}(\\mathcal{s}_{t+1}) \\vert \\mathcal{S}_t = s] \\end{aligned} \\begin{aligned} \\mathcal{V}_{\\pi}(s) &= \\mathbb{E}[\\mathcal{G}_t \\vert \\mathcal{S}_t = s] \\\\ &= \\mathbb{E} [\\mathcal{R}_{t+1} + \\gamma \\mathcal{R}_{t+2} + \\gamma^2 \\mathcal{R}_{t+3} + \\dots \\vert \\mathcal{S}_t = s] \\\\ &= \\mathbb{E} [\\mathcal{R}_{t+1} + \\gamma (\\mathcal{R}_{t+2} + \\gamma \\mathcal{R}_{t+3} + \\dots) \\vert \\mathcal{S}_t = s] \\\\ &= \\mathbb{E} [\\mathcal{R}_{t+1} + \\gamma \\mathcal{G}_{t+1} \\vert \\mathcal{S}_t = s] \\\\ &= \\mathbb{E} [\\mathcal{R}_{t+1} + \\gamma \\mathcal{V}_{\\pi}(\\mathcal{s}_{t+1}) \\vert \\mathcal{S}_t = s] \\end{aligned} Action-value function can be broken into: \\mathcal{Q}_{\\pi}(s, a) = \\mathbb{E} [\\mathcal{R}_{t+1} + \\gamma \\mathcal{Q}_{\\pi}(\\mathcal{s}_{t+1}, \\mathcal{a}_{t+1}) \\vert \\mathcal{S}_t = s, \\mathcal{A} = a] \\mathcal{Q}_{\\pi}(s, a) = \\mathbb{E} [\\mathcal{R}_{t+1} + \\gamma \\mathcal{Q}_{\\pi}(\\mathcal{s}_{t+1}, \\mathcal{a}_{t+1}) \\vert \\mathcal{S}_t = s, \\mathcal{A} = a]","title":"Bellman Equation"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#key-recap-on-value-functions","text":"\\mathcal{V}_{\\pi}(s) = \\mathbb{E}_{\\pi}[\\mathcal{G}_t \\vert \\mathcal{S}_t = s] \\mathcal{V}_{\\pi}(s) = \\mathbb{E}_{\\pi}[\\mathcal{G}_t \\vert \\mathcal{S}_t = s] State-value function: tells us how good to be in that state \\mathcal{Q}_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[\\mathcal{G}_t \\vert \\mathcal{S}_t = s, \\mathcal{A}_t = a] \\mathcal{Q}_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[\\mathcal{G}_t \\vert \\mathcal{S}_t = s, \\mathcal{A}_t = a] Action-value function: tells us how good to take actions given state","title":"Key Recap on Value Functions"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#bellman-expectation-equations","text":"Now we can move from Bellman Equations into Bellman Expectation Equations Basic: State-value function \\mathcal{V}_{\\pi}(s) \\mathcal{V}_{\\pi}(s) Current state \\mathcal{S} \\mathcal{S} Multiple possible actions determined by stochastic policy \\pi(a | s) \\pi(a | s) Each possible action is associated with a action-value function \\mathcal{Q}_{\\pi}(s, a) \\mathcal{Q}_{\\pi}(s, a) returning a value of that particular action Multiplying the possible actions with the action-value function and summing them gives us an indication of how good it is to be in that state Final equation: \\mathcal{V}_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a | s) \\mathcal{Q}(s, a) \\mathcal{V}_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a | s) \\mathcal{Q}(s, a) Loose intuitive interpretation : state-value = sum(policy determining actions * respective action-values) Basic: Action-value function \\mathcal{Q}_{\\pi}(s, a) \\mathcal{Q}_{\\pi}(s, a) With a list of possible multiple actions, there is a list of possible subsequent states s' s' associated with: state value function \\mathcal{V}_{\\pi}(s') \\mathcal{V}_{\\pi}(s') transition probability function \\mathcal{P}_{ss'}^a \\mathcal{P}_{ss'}^a determining where the agent could land in based on the action reward \\mathcal{R}_s^a \\mathcal{R}_s^a for taking the action Summing the reward and the transition probability function associated with the state-value function gives us an indication of how good it is to take the actions given our state Final equation: \\mathcal{Q}_{\\pi}(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a {V}_{\\pi}(s') \\mathcal{Q}_{\\pi}(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a {V}_{\\pi}(s') Loose intuitive interpretation : action-value = reward + sum(transition outcomes determining states * respective state-values) Expanded functions (substitution) Substituting action-value function into the state-value function \\mathcal{V}_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a | s) (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a {V}_{\\pi}(s')) \\mathcal{V}_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a | s) (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a {V}_{\\pi}(s')) Substituting state-value function into action-value function \\mathcal{Q}_{\\pi}(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\sum_{a' \\in \\mathcal{A}} \\pi(a' | s') \\mathcal{Q}(s', a') \\mathcal{Q}_{\\pi}(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\sum_{a' \\in \\mathcal{A}} \\pi(a' | s') \\mathcal{Q}(s', a')","title":"Bellman Expectation Equations"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#bellman-optimality-equations","text":"Remember optimal policy \\pi_* \\pi_* \u2192 optimal state-value and action-value functions \u2192 argmax of value functions \\pi_{*} = \\arg\\max_{\\pi} \\mathcal{V}_{\\pi}(s) = \\arg\\max_{\\pi} \\mathcal{Q}_{\\pi}(s, a) \\pi_{*} = \\arg\\max_{\\pi} \\mathcal{V}_{\\pi}(s) = \\arg\\max_{\\pi} \\mathcal{Q}_{\\pi}(s, a) Finally with Bellman Expectation Equations derived from Bellman Equations, we can derive the equations for the argmax of our value functions Optimal state-value function \\mathcal{V}_*(s) = \\arg\\max_{\\pi} \\mathcal{V}_{\\pi}(s) \\mathcal{V}_*(s) = \\arg\\max_{\\pi} \\mathcal{V}_{\\pi}(s) Given \\mathcal{V}_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a | s) (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a {V}_{\\pi}(s')) \\mathcal{V}_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a | s) (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a {V}_{\\pi}(s')) We have \\mathcal{V}_*(s) = \\max_{a \\in \\mathcal{A}} (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a {V}_{*}(s'))) \\mathcal{V}_*(s) = \\max_{a \\in \\mathcal{A}} (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a {V}_{*}(s'))) Optimal action-value function \\mathcal{Q}_*(s) = \\arg\\max_{\\pi} \\mathcal{Q}_{\\pi}(s) \\mathcal{Q}_*(s) = \\arg\\max_{\\pi} \\mathcal{Q}_{\\pi}(s) Given \\mathcal{Q}_{\\pi}(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\sum_{a' \\in \\mathcal{A}} \\pi(a' | s') \\mathcal{Q}(s', a') \\mathcal{Q}_{\\pi}(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\sum_{a' \\in \\mathcal{A}} \\pi(a' | s') \\mathcal{Q}(s', a') We have \\mathcal{Q}_{*}(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a max_{a' \\in \\mathcal{A}} \\mathcal{Q}_{*}(s', a') \\mathcal{Q}_{*}(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a max_{a' \\in \\mathcal{A}} \\mathcal{Q}_{*}(s', a')","title":"Bellman Optimality Equations"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#optimal-action-value-and-state-value-functions","text":"If the entire environment is known, such that we know our reward function and transition probability function, then we can solve for the optimal action-value and state-value functions via Dynamic Programming like Policy evaluation, policy improvement, and policy iteration However, typically we don't know the environment entirely then there is not closed form solution in getting optimal action-value and state-value functions. Hence, we need other iterative approaches like Monte-Carlo methods Temporal difference learning (model-free and learns with episodes) On-policy TD: SARSA Off-policy TD: Q-Learning and Deep Q-Learning (DQN) Policy gradient REINFORCE Actor-Critic A2C/A3C ACKTR PPO DPG DDPG (DQN + DPG) Closed form solution If there is a closed form solution, then the variables' values can be obtained with a finite number of mathematical operations (for example add, subtract, divide, and multiply). For example, solving 2x = 8 - 6x 2x = 8 - 6x would yield 8x = 8 8x = 8 by adding 6x 6x on both sides of the equation and finally yielding the value of x=1 x=1 by dividing both sides of the equation by 8 8 . These finite 2 steps of mathematical operations allowed us to solve for the value of x as the equation has a closed-form solution. However, many cases in deep learning and reinforcement learning there are no closed-form solutions which requires all the iterative methods mentioned above. Bellman, R. A Markovian Decision Process. Journal of Mathematics and Mechanics. 1957. \u21a9 Matthew J. Hausknecht and Peter Stone. Deep Recurrent Q-Learning for Partially Observable MDPs . 2015. \u21a9 R Bellman. On the Theory of Dynamic Programming. Proceedings of the National Academy of Sciences. 1952. \u21a9","title":"Optimal Action-value and State-value functions"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/","text":"Run Jupyter Notebook You can run the code for this section in this jupyter notebook link . Fronze Lake is a simple game where you are on a frozen lake and you need to retrieve an item on the frozen lake where some parts are frozen and some parts are holes (if you walk into them you die) Actions: \\mathcal{A} = \\{0, 1, 2, 3\\} \\mathcal{A} = \\{0, 1, 2, 3\\} LEFT: 0 DOWN = 1 RIGHT = 2 UP = 3 Whole lake is a 4 x 4 grid world, \\mathcal{S} = \\{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15\\} \\mathcal{S} = \\{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15\\} On each grid, there are 4 possibilities S: starting point, safe (code = 'SFFF') F: frozen surface, safe (code = 'FHFH') H: hole, fall to your doom (code = 'FFFH') G: goal, where the frisbee is located ('HFFG') Goal of Frozen Lake \u00b6 The key here is we want to get to G without falling into the hole H in the shortest amount of time Why Dynamic Programming? \u00b6 In this game, we know our transition probability function and reward function, essentially the whole environment, allowing us to turn this game into a simple planning problem via dynamic programming through 4 simple functions: (1) policy evaluation (2) policy improvement (3) policy iteration or (4) value iteration Before we explore how to solve this game, let's first understand how the game works in detail. Deterministic Policy Environment \u00b6 Make OpenAI Gym Environment for Frozen Lake # Import gym, installable via `pip install gym` import gym # Environment environment Slippery (stochastic policy, move left probability = 1/3) comes by default! # If we want deterministic policy, we need to create new environment # Make environment No Slippery (deterministic policy, move left = left) gym . envs . register ( id = 'FrozenLakeNotSlippery-v0' , entry_point = 'gym.envs.toy_text:FrozenLakeEnv' , kwargs = { 'map_name' : '4x4' , 'is_slippery' : False }, max_episode_steps = 100 , reward_threshold = 0.78 , # optimum = .8196 ) # You can only register once # To delete any new environment # del gym.envs.registry.env_specs['FrozenLakeNotSlippery-v0'] # Make the environment based on deterministic policy env = gym . make ( 'FrozenLakeNotSlippery-v0' ) Observation space # State space print ( env . observation_space ) Discrete(16) State space S_n = env . observation_space . n print ( S_n ) 16 Sampling state space # We should expect to see 15 possible grids from 0 to 15 when # we uniformly randomly sample from our observation space for i in range ( 10 ): print ( env . observation_space . sample ()) 11 12 7 13 7 11 14 14 4 12 Action space # Action space print ( env . action_space ) A_n = env . action_space . n print ( A_n ) Discrete(4) 4 Random sampling of actions # We should expect to see 4 actions when # we uniformly randomly sample: # 1. LEFT: 0 # 2. DOWN = 1 # 3. RIGHT = 2 # 4. UP = 3 for i in range ( 10 ): print ( env . action_space . sample ()) 0 3 1 3 3 2 3 2 2 2 Making Steps \u00b6 Initial state # This sets the initial state at S, our starting point # We can render the environment to see where we are on the 4x4 frozenlake gridworld env . reset () env . render () [S]FFF FHFH FFFH HFFG Go left # Go left (action=0), nothing should happen, and we should stay at the starting point, because there's no grid on the left env . reset () action = 0 ( observation , reward , done , prob ) = env . step ( action ) env . render () # Observation = 1: move to grid number 1 (unchanged) # Prob = 1: deterministic policy, if we choose to go left, we'll go left print ( observation , reward , done , prob ) (Left) [S]FFF FHFH FFFH HFFG 0 0.0 False {'prob': 1.0} Go down # Go down (action = 1), we should be safe as we step on frozen grid env . reset () action = 1 ( observation , reward , done , prob ) = env . step ( action ) env . render () # Observation = 4: move to grid number 4 # Prob = 1: deterministic policy, if we choose to go down we'll go down print ( observation , reward , done , prob ) (Down) SFFF [F]HFH FFFH HFFG 4 0.0 False {'prob': 1.0} Go right # Go right (action = 2), we should be safe as we step on frozen grid env . reset () action = 2 ( observation , reward , done , prob ) = env . step ( action ) env . render () # Observation = 1: move to grid number 1 # Prob = 1: deterministic policy, if we choose to go right we'll go right print ( observation , reward , done , prob ) (Right) S[F]FF FHFH FFFH HFFG 1 0.0 False {'prob': 1.0} Go right twice # Go right twice (action = 2), we should be safe as we step on 2 frozen grids env . reset () action = 2 ( observation , reward , done , prob ) = env . step ( action ) env . render () ( observation , reward , done , prob ) = env . step ( action ) env . render () # Observation = 2: move to the right twice from grid 0 to grid 2 # Prob = 1: deterministic policy, if we choose to go right twice we'll go right twice print ( observation , reward , done , prob ) (Right) S[F]FF FHFH FFFH HFFG (Right) SF[F]F FHFH FFFH HFFG 2 0.0 False {'prob': 1.0} Dying: drop in hole grid 12, H \u00b6 Go down thrice # Go down thrice (action = 1), we will die as we step onto the grid with a hole env . reset () action = 1 ( observation , reward , done , prob ) = env . step ( action ) env . render () ( observation , reward , done , prob ) = env . step ( action ) env . render () ( observation , reward , done , prob ) = env . step ( action ) env . render () # Observation = 2: move to the right twice from grid 0 to grid 2 # Prob = 1: deterministic policy, if we choose to go right twice we'll go right twice # Done = True because the game ends when we die (go onto hole grid (H) or finish the game (G)) print ( observation , reward , done , prob ) (Down) SFFF [F]HFH FFFH HFFG (Down) SFFF FHFH [F]FFH HFFG (Down) SFFF FHFH FFFH [H]FFG 12 0.0 True {'prob': 1.0} Winning: get to grid 15, G \u00b6 Go right twice, go down thrice, go right once # Go right twice (action = 2), go down thrice (action = 1), go right once (action = 2) env . reset () # Right Twice action = 2 ( observation , reward , done , prob ) = env . step ( action ) env . render () ( observation , reward , done , prob ) = env . step ( action ) env . render () # Down Thrice action = 1 ( observation , reward , done , prob ) = env . step ( action ) env . render () ( observation , reward , done , prob ) = env . step ( action ) env . render () ( observation , reward , done , prob ) = env . step ( action ) env . render () # Right Once action = 2 ( observation , reward , done , prob ) = env . step ( action ) env . render () # Observation = 2: move to the right twice from grid 0 to grid 2 # Prob = 1: deterministic policy, if we choose to go right twice we'll go right twice # Done = True because the game ends when we die (go onto hole grid (H) or finish the game (G)) print ( observation , reward , done , prob ) (Right) S[F]FF FHFH FFFH HFFG (Right) SF[F]F FHFH FFFH HFFG (Down) SFFF FH[F]H FFFH HFFG (Down) SFFF FHFH FF[F]H HFFG (Down) SFFF FHFH FFFH HF[F]mG (Right) SFFF FHFH FFFH HFF[G] 15 1.0 True {'prob': 1.0} Non-deterministic Policy Environment \u00b6 Go right? # Make the environment based on non-deterministic policy env = gym . make ( 'FrozenLake-v0' ) # Go right once (action = 2), we should go to the right but we did not! env . seed ( 8 ) env . reset () action = 2 ( observation , reward , done , prob ) = env . step ( action ) env . render () # Observation = 0: move to the right once from grid 0 to grid 1 # Prob = 1/3: non-deterministic policy, if we choose to go right, there's only a 1/3 probability we would go to the right and with this environment seed we did not print ( observation , reward , done , prob ) (Right) [S]FFF FHFH FFFH HFFG 0 0.0 False {'prob': 0.3333333333333333} Go right 10 times? # Try to go to the right 10 times, let's see how many times it goes to the right, by right we won't die because we would end up at the extreme right of grid 3 # See how it can go down/left/up/nothing instead of just right? # Intuitively when we are moving on a frozen lake, some times when we want to walk one direction we may end up in another direction as it's slippery # Setting seed here of the environment so you can reproduce my results, otherwise stochastic policy will yield different results for each run env . seed ( 8 ) env . reset () for i in range ( 10 ): action = 2 ( observation , reward , done , prob ) = env . step ( action ) env . render () ( Right ) [ S ] FFF FHFH FFFH HFFG ( Right ) S [ F ] FF FHFH FFFH HFFG ( Right ) SF [ F ] F FHFH FFFH HFFG ( Right ) SFFF FH [ F ] H FFFH HFFG ( Right ) SFFF FHF [ H ] FFFH HFFG ( Right ) SFFF FHF [ 4H ] FFFH HFFG ( Right ) SFFF FHF [ H ] FFFH HFFG ( Right ) SFFF FHF [ H ] FFFH HFFG ( Right ) SFFF FHF [ H ] FFFH HFFG ( Right ) SFFF FHF [ H ] FFFH HFFG Custom Frozen Lake Non-deterministic Policy Environment \u00b6 Because original code from OpenAI only allows us to run env.step(action) , this is challenging if we want to do some visualization of our state-value and action-value (q-value) functions for learning Hence, we'll be copying the whole code from OpenAI Frozen Lake implementation and adding just one line to make sure we can get P via self.P = P This code is not important, you can just copy it import sys from contextlib import closing import numpy as np from six import StringIO , b from gym import utils from gym.envs.toy_text import discrete LEFT = 0 DOWN = 1 RIGHT = 2 UP = 3 MAPS = { \"4x4\" : [ \"SFFF\" , \"FHFH\" , \"FFFH\" , \"HFFG\" ], \"8x8\" : [ \"SFFFFFFF\" , \"FFFFFFFF\" , \"FFFHFFFF\" , \"FFFFFHFF\" , \"FFFHFFFF\" , \"FHHFFFHF\" , \"FHFFHFHF\" , \"FFFHFFFG\" ], } # Generates a random valid map (one that has a path from start to goal) # @params size, size of each side of the grid # @prams p, probability that a tile is frozen def generate_random_map ( size = 8 , p = 0.8 ): valid = False #BFS to check that it's a valid path def is_valid ( arr , r = 0 , c = 0 ): if arr [ r ][ c ] == 'G' : return True tmp = arr [ r ][ c ] arr [ r ][ c ] = \"#\" if r + 1 < size and arr [ r + 1 ][ c ] not in '#H' : if is_valid ( arr , r + 1 , c ) == True : arr [ r ][ c ] = tmp return True if c + 1 < size and arr [ r ][ c + 1 ] not in '#H' : if is_valid ( arr , r , c + 1 ) == True : arr [ r ][ c ] = tmp return True if r - 1 >= 0 and arr [ r - 1 ][ c ] not in '#H' : if is_valid ( arr , r - 1 , c ) == True : arr [ r ][ c ] = tmp return True if c - 1 >= 0 and arr [ r ][ c - 1 ] not in '#H' : if is_valid ( arr , r , c - 1 ) == True : arr [ r ][ c ] = tmp return True arr [ r ][ c ] = tmp return False while not valid : p = min ( 1 , p ) res = np . random . choice ([ 'F' , 'H' ], ( size , size ), p = [ p , 1 - p ]) res [ 0 ][ 0 ] = 'S' res [ - 1 ][ - 1 ] = 'G' valid = is_valid ( res ) return [ \"\" . join ( x ) for x in res ] class FrozenLakeEnv ( discrete . DiscreteEnv ): \"\"\" Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend. The surface is described using a grid like the following SFFF FHFH FFFH HFFG S : starting point, safe F : frozen surface, safe H : hole, fall to your doom G : goal, where the frisbee is located The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise. \"\"\" metadata = { 'render.modes' : [ 'human' , 'ansi' ]} def __init__ ( self , desc = None , map_name = \"4x4\" , is_slippery = True ): if desc is None and map_name is None : desc = generate_random_map () elif desc is None : desc = MAPS [ map_name ] self . desc = desc = np . asarray ( desc , dtype = 'c' ) self . nrow , self . ncol = nrow , ncol = desc . shape self . reward_range = ( 0 , 1 ) nA = 4 nS = nrow * ncol isd = np . array ( desc == b 'S' ) . astype ( 'float64' ) . ravel () isd /= isd . sum () P = { s : { a : [] for a in range ( nA )} for s in range ( nS )} def to_s ( row , col ): return row * ncol + col def inc ( row , col , a ): if a == 0 : # left col = max ( col - 1 , 0 ) elif a == 1 : # down row = min ( row + 1 , nrow - 1 ) elif a == 2 : # right col = min ( col + 1 , ncol - 1 ) elif a == 3 : # up row = max ( row - 1 , 0 ) return ( row , col ) for row in range ( nrow ): for col in range ( ncol ): s = to_s ( row , col ) for a in range ( 4 ): li = P [ s ][ a ] letter = desc [ row , col ] if letter in b 'GH' : li . append (( 1.0 , s , 0 , True )) else : if is_slippery : for b in [( a - 1 ) % 4 , a , ( a + 1 ) % 4 ]: newrow , newcol = inc ( row , col , b ) newstate = to_s ( newrow , newcol ) newletter = desc [ newrow , newcol ] done = bytes ( newletter ) in b 'GH' rew = float ( newletter == b 'G' ) li . append (( 1.0 / 3.0 , newstate , rew , done )) else : newrow , newcol = inc ( row , col , a ) newstate = to_s ( newrow , newcol ) newletter = desc [ newrow , newcol ] done = bytes ( newletter ) in b 'GH' rew = float ( newletter == b 'G' ) li . append (( 1.0 , newstate , rew , done )) # New change because environment only allows step without # specific state for learning environment! self . P = P super ( FrozenLakeEnv , self ) . __init__ ( nS , nA , P , isd ) def render ( self , mode = 'human' ): outfile = StringIO () if mode == 'ansi' else sys . stdout row , col = self . s // self . ncol , self . s % self . ncol desc = self . desc . tolist () desc = [[ c . decode ( 'utf-8' ) for c in line ] for line in desc ] desc [ row ][ col ] = utils . colorize ( desc [ row ][ col ], \"red\" , highlight = True ) if self . lastaction is not None : outfile . write ( \" ( {} ) \\n \" . format ([ \"Left\" , \"Down\" , \"Right\" , \"Up\" ][ self . lastaction ])) else : outfile . write ( \" \\n \" ) outfile . write ( \" \\n \" . join ( '' . join ( line ) for line in desc ) + \" \\n \" ) if mode != 'human' : with closing ( outfile ): return outfile . getvalue () Policy Evaluation \u00b6 Transition Probability Function \u00b6 \\mathcal{P}_{ss'}^a = \\mathcal{P}(s' \\vert s, a) = \\mathbb{P} [S_{t+1} = s' \\vert S_t = s, A_t = a] \\mathcal{P}_{ss'}^a = \\mathcal{P}(s' \\vert s, a) = \\mathbb{P} [S_{t+1} = s' \\vert S_t = s, A_t = a] Deterministic Environment \u00b6 There's no probability distribution, if you decide to go left you'll go left Hence in this example, given current_state = 8 and action = 0 which is left, we will end up with probability = 1 in new_state = 9 # Deterministic env = FrozenLakeEnv ( is_slippery = False ) current_state = 10 # State from S_n=16 State space action = 0 # Left action from A_n=4 Action space [( probability , new_state , reward , done )] = env . P [ current_state ][ action ] print ( 'Probability {} , New State {} ' . format ( probability , new_state )) Probability 1.0, New State 9 Stochastic Environment \u00b6 Given S_t = 10, A_t = 0 S_t = 10, A_t = 0 in a stochastic environment, the transition probability functions indicate you can end up in grid 6, 9, 14 each with \u2153 probability: \\mathbb{P} [S_{t+1} = 6 \\vert S_t = 10, A_t = 0] = \\frac{1}{3} \\mathbb{P} [S_{t+1} = 6 \\vert S_t = 10, A_t = 0] = \\frac{1}{3} \\mathbb{P} [S_{t+1} = 9 \\vert S_t = 10, A_t = 0] = \\frac{1}{3} \\mathbb{P} [S_{t+1} = 9 \\vert S_t = 10, A_t = 0] = \\frac{1}{3} \\mathbb{P} [S_{t+1} = 14 \\vert S_t = 10, A_t = 0] = \\frac{1}{3} \\mathbb{P} [S_{t+1} = 14 \\vert S_t = 10, A_t = 0] = \\frac{1}{3} # Stochastic env = FrozenLakeEnv ( is_slippery = True ) current_state = 10 # State from S_n=16 State space action = 0 # Left action from A_n=4 Action space env . P [ current_state ][ action ] [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 14, 0.0, False)] Random Policy Function \u00b6 Random Policy function # Random policy generation def generate_random_policy ( S_n , A_n ): # return np.random.randint(A_n, size=(S_n, A_n)) return np . ones ([ S_n , A_n ]) / A_n # Given the total number of states S_n = 16 # For each state out of 16 states, we can take 4 actions # Since this is a stochastic environment, we'll initialize a policy to have equal probabilities 0.25 of doing each action each state policy = generate_random_policy ( S_n , A_n ) print ( policy . shape ) (16, 4) Policy plot import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline plt . figure ( figsize = ( 5 , 16 )) sns . heatmap ( policy , cmap = \"YlGnBu\" , annot = True , cbar = False ); Policy Evaluation Function comprising State-value Function \u00b6 How: \\mathcal{V}_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a | s) \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\big[\\mathcal{R}_s^a + \\gamma {V}_{\\pi}(s')\\big] \\mathcal{V}_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a | s) \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\big[\\mathcal{R}_s^a + \\gamma {V}_{\\pi}(s')\\big] Simple code equation: Values of state given policy = sum ( action probability * transition probability * [reward + discount * value of new state] ) Aim: getting state-values import numpy as np def policy_evaluation ( env , policy , gamma = 1. , theta = 1e-8 ): r \"\"\"Policy evaluation function. Loop until state values stable, delta < theta. Returns V comprising values of states under given policy. Args: env (gym.env): OpenAI environment class instantiated and assigned to an object. policy (np.array): policy array to evaluate gamma (float): discount rate for rewards theta (float): tiny positive number, anything below it indicates value function convergence \"\"\" # 1. Create state-value array (16,) V = np . zeros ( S_n ) while True : delta = 0 # 2. Loop through states for s in range ( S_n ): Vs = 0 # 2.1 Loop through actions for the unique state # Given each state, we've 4 actions associated with different probabilities # 0.25 x 4 in this case, so we'll be looping 4 times (4 action probabilities) at each state for a , action_prob in enumerate ( policy [ s ]): # 2.1.1 Loop through to get transition probabilities, next state, rewards and whether the game ended for prob , next_state , reward , done in env . P [ s ][ a ]: # State-value function to get our values of states given policy Vs += action_prob * prob * ( reward + gamma * V [ next_state ]) # This simple equation allows us to stop this loop when we've converged # How do we know? The new value of the state is smaller than a tiny positive value we set # State value change is tiny compared to what we have so we just stop! delta = max ( delta , np . abs ( V [ s ] - Vs )) # 2.2 Update our state value for that state V [ s ] = Vs # 3. Stop policy evaluation if our state values changes are smaller than our tiny positive number if delta < theta : break return V # Generate random policy with equal probabilities of each action given any state rand_policy = generate_random_policy ( S_n , A_n ) # Evaluate the policy to get state values V = policy_evaluation ( env , rand_policy ) # Plot heatmap plt . figure ( figsize = ( 8 , 8 )) sns . heatmap ( V . reshape ( 4 , 4 ), cmap = \"YlGnBu\" , annot = True , cbar = False ); # This is our environment # Notice how the state values near the goal have higher values? # Those with \"H\" = hole, where you die if you step, have 0 values indicating those are bad areas to be in env . render () [ S ] FFF FHFH FFFH HFFG Policy Improvement \u00b6 Action-value (Q-value) function from State-value function \u00b6 How: \\mathcal{Q}_{\\pi}(s, a) = \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\big[ \\mathcal{R}_s^a + \\gamma \\mathcal{V}_{\\pi}(s') \\big] \\mathcal{Q}_{\\pi}(s, a) = \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\big[ \\mathcal{R}_s^a + \\gamma \\mathcal{V}_{\\pi}(s') \\big] Code equation Values of action = sum ( transition probability * [reward + discount * value of next state] ) Aim: getting q-values (action-values) def q_value ( env , V , s , gamma = 1 ): r \"\"\"Q-value (action-value) function from state-value function Returns Q values, values of actions. Args: env (gym.env): OpenAI environment class instantiated and assigned to an object. V (np.array): array of state-values obtained from policy evaluation function. s (integer): integer representing current state in the gridworld gamma (float): discount rate for rewards. \"\"\" # 1. Create q-value array for one state # We have 4 actions, so let's create an array with the size of 4 q = np . zeros ( A_n ) # 2. Loop through each action for a in range ( A_n ): # 2.1 For each action, we've our transition probabilities, next state, rewards and whether the game ended for prob , next_state , reward , done in env . P [ s ][ a ]: # 2.1.1 Get our action-values from state-values q [ a ] += prob * ( reward + gamma * V [ next_state ]) # Return action values return q # For every state, we've 4 actions, hence we've 16 x 4 q values Q = np . zeros ([ S_n , A_n ]) # Loop through each state out of 16 # For each state, we will get the 4 q-values associated with the 4 actions for s in range ( env . nS ): Q [ s ] = q_value ( env , V , s ) plt . figure ( figsize = ( 5 , 16 )) sns . heatmap ( Q , cmap = \"YlGnBu\" , annot = True , cbar = False ); # Notice how 13/14, those in the last row of the gridworld just before reaching the goal of finishing the game, their action values are large? env . render () [ S ] FFF FHFH FFFH HFFG Policy Improvement Function \u00b6 How: maximizing q-values per state by choosing actions with highest q-values Aim: get improved policy def policy_improvement ( env , V , gamma = 1. ): r \"\"\"Function to improve the policy by utilizing state values and action (q) values. Args: env (gym.env): OpenAI environment class instantiated and assigned to an objects V (np.array): array of state-values obtained from policy evaluation function gamma (float): discount of rewards \"\"\" # 1. Blank policy policy = np . zeros ([ env . nS , env . nA ]) / env . nA # 2. For each state in 16 states for s in range ( env . nS ): # 2.1 Get q values: q.shape returns (4,) q = q_value ( env , V , s , gamma ) # 2.2 Find best action based on max q-value # np.argwhere(q==np.max(q)) gives the position of largest q value # given array([0.00852356, 0.01163091, 0.0108613 , 0.01550788]), this would return array([[3]]) of shape (1, 1) # .flatten() reduces the shape to (1,) where we've array([3]) best_a = np . argwhere ( q == np . max ( q )) . flatten () # 2.3 One-hot encode best action and store into policy array's row for that state # In our case where the best action is array([3]), this would return # array([0., 0., 0., 1.]) where position 3 is the best action # Now we can store the best action into our policy policy [ s ] = np . sum ([ np . eye ( env . nA )[ i ] for i in best_a ], axis = 0 ) / len ( best_a ) return policy new_policy = policy_improvement ( env , V ) plt . figure ( figsize = ( 5 , 16 )) sns . heatmap ( new_policy , cmap = \"YlGnBu\" , annot = True , cbar = False ); # Compared to this equiprobable policy, the one above is making some improvements by maximizing q-values per state plt . figure ( figsize = ( 5 , 16 )) sns . heatmap ( rand_policy , cmap = \"YlGnBu\" , annot = True , cbar = False ); Policy Iteration Function \u00b6 How: loop through policy evaluation (get state-values) and policy improvement functions (use state-values to calculate q-values to improve policy) until optimal policy obtained Aim: improve policy until convergence Convergence: difference of state values between old and new policies is very small (less than theta, a very small positive number) import copy def policy_iteration ( env , gamma = 1 , theta = 1e-8 ): # 1. Create equiprobable policy where every state has 4 actions with equal probabilities as a starting policy policy = np . ones ([ env . nS , env . nA ]) / env . nA # 2. Loop through policy_evaluation and policy_improvement functions while True : # 2.1 Get state-values V = policy_evaluation ( env , policy , gamma , theta ) # 2.2 Get new policy by getting q-values and maximizing q-values per state to get best action per state new_policy = policy_improvement ( env , V ) # 2.3 Stop if the value function estimates for successive policies has converged if np . max ( abs ( policy_evaluation ( env , policy ) - policy_evaluation ( env , new_policy ))) < theta * 1e2 : break ; # 2.4 Replace policy with new policy policy = copy . copy ( new_policy ) return policy , V # obtain the optimal policy and optimal state-value function policy_pi , V_pi = policy_iteration ( env ) # Optimal policy (pi) # LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3 plt . figure ( figsize = ( 5 , 16 )) sns . heatmap ( policy_pi , cmap = \"YlGnBu\" , annot = True , cbar = False , square = True ); # State values plt . figure ( figsize = ( 8 , 8 )) sns . heatmap ( V_pi . reshape ( 4 , 4 ), cmap = \"YlGnBu\" , annot = True , cbar = False , square = True ); # State values without policy improvement, just evaluation plt . figure ( figsize = ( 8 , 8 )) sns . heatmap ( V . reshape ( 4 , 4 ), cmap = \"YlGnBu\" , annot = True , cbar = False ); Value iteration \u00b6 Alternative to policy iteration How: loop through to find optimal value function then get one-off policy Aim: improve value function until convergence Convergence: until difference in new and old state values are small (smaller than theta, small positive number) def value_iteration ( env , gamma = 1 , theta = 1e-8 ): # 1. Create state values of shape (16,) V = np . zeros ( env . nS ) # 2. Loop through q-value function until convergence while True : delta = 0 # 2.1 Loop through each state for s in range ( env . nS ): # 2.2 Archive old state value v = V [ s ] # 2.3 New state value = max of q-value V [ s ] = max ( q_value ( env , V , s , gamma )) delta = max ( delta , abs ( V [ s ] - v )) # 2.2 If state value changes small, converged if delta < theta : break # 3. Extract one-off policy with optimal state values policy = policy_improvement ( env , V , gamma ) return policy , V policy_vi , V_vi = value_iteration ( env ) # Optimal policy # LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3 plt . figure ( figsize = ( 5 , 16 )) sns . heatmap ( policy_vi , cmap = \"YlGnBu\" , annot = True , cbar = False , square = True ); # State values plt . figure ( figsize = ( 8 , 8 )) sns . heatmap ( V_vi . reshape ( 4 , 4 ), cmap = \"YlGnBu\" , annot = True , cbar = False , square = True );","title":"Dynamic Programming"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#goal-of-frozen-lake","text":"The key here is we want to get to G without falling into the hole H in the shortest amount of time","title":"Goal of Frozen Lake"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#why-dynamic-programming","text":"In this game, we know our transition probability function and reward function, essentially the whole environment, allowing us to turn this game into a simple planning problem via dynamic programming through 4 simple functions: (1) policy evaluation (2) policy improvement (3) policy iteration or (4) value iteration Before we explore how to solve this game, let's first understand how the game works in detail.","title":"Why Dynamic Programming?"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#deterministic-policy-environment","text":"Make OpenAI Gym Environment for Frozen Lake # Import gym, installable via `pip install gym` import gym # Environment environment Slippery (stochastic policy, move left probability = 1/3) comes by default! # If we want deterministic policy, we need to create new environment # Make environment No Slippery (deterministic policy, move left = left) gym . envs . register ( id = 'FrozenLakeNotSlippery-v0' , entry_point = 'gym.envs.toy_text:FrozenLakeEnv' , kwargs = { 'map_name' : '4x4' , 'is_slippery' : False }, max_episode_steps = 100 , reward_threshold = 0.78 , # optimum = .8196 ) # You can only register once # To delete any new environment # del gym.envs.registry.env_specs['FrozenLakeNotSlippery-v0'] # Make the environment based on deterministic policy env = gym . make ( 'FrozenLakeNotSlippery-v0' ) Observation space # State space print ( env . observation_space ) Discrete(16) State space S_n = env . observation_space . n print ( S_n ) 16 Sampling state space # We should expect to see 15 possible grids from 0 to 15 when # we uniformly randomly sample from our observation space for i in range ( 10 ): print ( env . observation_space . sample ()) 11 12 7 13 7 11 14 14 4 12 Action space # Action space print ( env . action_space ) A_n = env . action_space . n print ( A_n ) Discrete(4) 4 Random sampling of actions # We should expect to see 4 actions when # we uniformly randomly sample: # 1. LEFT: 0 # 2. DOWN = 1 # 3. RIGHT = 2 # 4. UP = 3 for i in range ( 10 ): print ( env . action_space . sample ()) 0 3 1 3 3 2 3 2 2 2","title":"Deterministic Policy Environment"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#making-steps","text":"Initial state # This sets the initial state at S, our starting point # We can render the environment to see where we are on the 4x4 frozenlake gridworld env . reset () env . render () [S]FFF FHFH FFFH HFFG Go left # Go left (action=0), nothing should happen, and we should stay at the starting point, because there's no grid on the left env . reset () action = 0 ( observation , reward , done , prob ) = env . step ( action ) env . render () # Observation = 1: move to grid number 1 (unchanged) # Prob = 1: deterministic policy, if we choose to go left, we'll go left print ( observation , reward , done , prob ) (Left) [S]FFF FHFH FFFH HFFG 0 0.0 False {'prob': 1.0} Go down # Go down (action = 1), we should be safe as we step on frozen grid env . reset () action = 1 ( observation , reward , done , prob ) = env . step ( action ) env . render () # Observation = 4: move to grid number 4 # Prob = 1: deterministic policy, if we choose to go down we'll go down print ( observation , reward , done , prob ) (Down) SFFF [F]HFH FFFH HFFG 4 0.0 False {'prob': 1.0} Go right # Go right (action = 2), we should be safe as we step on frozen grid env . reset () action = 2 ( observation , reward , done , prob ) = env . step ( action ) env . render () # Observation = 1: move to grid number 1 # Prob = 1: deterministic policy, if we choose to go right we'll go right print ( observation , reward , done , prob ) (Right) S[F]FF FHFH FFFH HFFG 1 0.0 False {'prob': 1.0} Go right twice # Go right twice (action = 2), we should be safe as we step on 2 frozen grids env . reset () action = 2 ( observation , reward , done , prob ) = env . step ( action ) env . render () ( observation , reward , done , prob ) = env . step ( action ) env . render () # Observation = 2: move to the right twice from grid 0 to grid 2 # Prob = 1: deterministic policy, if we choose to go right twice we'll go right twice print ( observation , reward , done , prob ) (Right) S[F]FF FHFH FFFH HFFG (Right) SF[F]F FHFH FFFH HFFG 2 0.0 False {'prob': 1.0}","title":"Making Steps"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#dying-drop-in-hole-grid-12-h","text":"Go down thrice # Go down thrice (action = 1), we will die as we step onto the grid with a hole env . reset () action = 1 ( observation , reward , done , prob ) = env . step ( action ) env . render () ( observation , reward , done , prob ) = env . step ( action ) env . render () ( observation , reward , done , prob ) = env . step ( action ) env . render () # Observation = 2: move to the right twice from grid 0 to grid 2 # Prob = 1: deterministic policy, if we choose to go right twice we'll go right twice # Done = True because the game ends when we die (go onto hole grid (H) or finish the game (G)) print ( observation , reward , done , prob ) (Down) SFFF [F]HFH FFFH HFFG (Down) SFFF FHFH [F]FFH HFFG (Down) SFFF FHFH FFFH [H]FFG 12 0.0 True {'prob': 1.0}","title":"Dying: drop in hole grid 12,  H"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#winning-get-to-grid-15-g","text":"Go right twice, go down thrice, go right once # Go right twice (action = 2), go down thrice (action = 1), go right once (action = 2) env . reset () # Right Twice action = 2 ( observation , reward , done , prob ) = env . step ( action ) env . render () ( observation , reward , done , prob ) = env . step ( action ) env . render () # Down Thrice action = 1 ( observation , reward , done , prob ) = env . step ( action ) env . render () ( observation , reward , done , prob ) = env . step ( action ) env . render () ( observation , reward , done , prob ) = env . step ( action ) env . render () # Right Once action = 2 ( observation , reward , done , prob ) = env . step ( action ) env . render () # Observation = 2: move to the right twice from grid 0 to grid 2 # Prob = 1: deterministic policy, if we choose to go right twice we'll go right twice # Done = True because the game ends when we die (go onto hole grid (H) or finish the game (G)) print ( observation , reward , done , prob ) (Right) S[F]FF FHFH FFFH HFFG (Right) SF[F]F FHFH FFFH HFFG (Down) SFFF FH[F]H FFFH HFFG (Down) SFFF FHFH FF[F]H HFFG (Down) SFFF FHFH FFFH HF[F]mG (Right) SFFF FHFH FFFH HFF[G] 15 1.0 True {'prob': 1.0}","title":"Winning: get to grid 15, G"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#non-deterministic-policy-environment","text":"Go right? # Make the environment based on non-deterministic policy env = gym . make ( 'FrozenLake-v0' ) # Go right once (action = 2), we should go to the right but we did not! env . seed ( 8 ) env . reset () action = 2 ( observation , reward , done , prob ) = env . step ( action ) env . render () # Observation = 0: move to the right once from grid 0 to grid 1 # Prob = 1/3: non-deterministic policy, if we choose to go right, there's only a 1/3 probability we would go to the right and with this environment seed we did not print ( observation , reward , done , prob ) (Right) [S]FFF FHFH FFFH HFFG 0 0.0 False {'prob': 0.3333333333333333} Go right 10 times? # Try to go to the right 10 times, let's see how many times it goes to the right, by right we won't die because we would end up at the extreme right of grid 3 # See how it can go down/left/up/nothing instead of just right? # Intuitively when we are moving on a frozen lake, some times when we want to walk one direction we may end up in another direction as it's slippery # Setting seed here of the environment so you can reproduce my results, otherwise stochastic policy will yield different results for each run env . seed ( 8 ) env . reset () for i in range ( 10 ): action = 2 ( observation , reward , done , prob ) = env . step ( action ) env . render () ( Right ) [ S ] FFF FHFH FFFH HFFG ( Right ) S [ F ] FF FHFH FFFH HFFG ( Right ) SF [ F ] F FHFH FFFH HFFG ( Right ) SFFF FH [ F ] H FFFH HFFG ( Right ) SFFF FHF [ H ] FFFH HFFG ( Right ) SFFF FHF [ 4H ] FFFH HFFG ( Right ) SFFF FHF [ H ] FFFH HFFG ( Right ) SFFF FHF [ H ] FFFH HFFG ( Right ) SFFF FHF [ H ] FFFH HFFG ( Right ) SFFF FHF [ H ] FFFH HFFG","title":"Non-deterministic Policy Environment"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#custom-frozen-lake-non-deterministic-policy-environment","text":"Because original code from OpenAI only allows us to run env.step(action) , this is challenging if we want to do some visualization of our state-value and action-value (q-value) functions for learning Hence, we'll be copying the whole code from OpenAI Frozen Lake implementation and adding just one line to make sure we can get P via self.P = P This code is not important, you can just copy it import sys from contextlib import closing import numpy as np from six import StringIO , b from gym import utils from gym.envs.toy_text import discrete LEFT = 0 DOWN = 1 RIGHT = 2 UP = 3 MAPS = { \"4x4\" : [ \"SFFF\" , \"FHFH\" , \"FFFH\" , \"HFFG\" ], \"8x8\" : [ \"SFFFFFFF\" , \"FFFFFFFF\" , \"FFFHFFFF\" , \"FFFFFHFF\" , \"FFFHFFFF\" , \"FHHFFFHF\" , \"FHFFHFHF\" , \"FFFHFFFG\" ], } # Generates a random valid map (one that has a path from start to goal) # @params size, size of each side of the grid # @prams p, probability that a tile is frozen def generate_random_map ( size = 8 , p = 0.8 ): valid = False #BFS to check that it's a valid path def is_valid ( arr , r = 0 , c = 0 ): if arr [ r ][ c ] == 'G' : return True tmp = arr [ r ][ c ] arr [ r ][ c ] = \"#\" if r + 1 < size and arr [ r + 1 ][ c ] not in '#H' : if is_valid ( arr , r + 1 , c ) == True : arr [ r ][ c ] = tmp return True if c + 1 < size and arr [ r ][ c + 1 ] not in '#H' : if is_valid ( arr , r , c + 1 ) == True : arr [ r ][ c ] = tmp return True if r - 1 >= 0 and arr [ r - 1 ][ c ] not in '#H' : if is_valid ( arr , r - 1 , c ) == True : arr [ r ][ c ] = tmp return True if c - 1 >= 0 and arr [ r ][ c - 1 ] not in '#H' : if is_valid ( arr , r , c - 1 ) == True : arr [ r ][ c ] = tmp return True arr [ r ][ c ] = tmp return False while not valid : p = min ( 1 , p ) res = np . random . choice ([ 'F' , 'H' ], ( size , size ), p = [ p , 1 - p ]) res [ 0 ][ 0 ] = 'S' res [ - 1 ][ - 1 ] = 'G' valid = is_valid ( res ) return [ \"\" . join ( x ) for x in res ] class FrozenLakeEnv ( discrete . DiscreteEnv ): \"\"\" Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend. The surface is described using a grid like the following SFFF FHFH FFFH HFFG S : starting point, safe F : frozen surface, safe H : hole, fall to your doom G : goal, where the frisbee is located The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise. \"\"\" metadata = { 'render.modes' : [ 'human' , 'ansi' ]} def __init__ ( self , desc = None , map_name = \"4x4\" , is_slippery = True ): if desc is None and map_name is None : desc = generate_random_map () elif desc is None : desc = MAPS [ map_name ] self . desc = desc = np . asarray ( desc , dtype = 'c' ) self . nrow , self . ncol = nrow , ncol = desc . shape self . reward_range = ( 0 , 1 ) nA = 4 nS = nrow * ncol isd = np . array ( desc == b 'S' ) . astype ( 'float64' ) . ravel () isd /= isd . sum () P = { s : { a : [] for a in range ( nA )} for s in range ( nS )} def to_s ( row , col ): return row * ncol + col def inc ( row , col , a ): if a == 0 : # left col = max ( col - 1 , 0 ) elif a == 1 : # down row = min ( row + 1 , nrow - 1 ) elif a == 2 : # right col = min ( col + 1 , ncol - 1 ) elif a == 3 : # up row = max ( row - 1 , 0 ) return ( row , col ) for row in range ( nrow ): for col in range ( ncol ): s = to_s ( row , col ) for a in range ( 4 ): li = P [ s ][ a ] letter = desc [ row , col ] if letter in b 'GH' : li . append (( 1.0 , s , 0 , True )) else : if is_slippery : for b in [( a - 1 ) % 4 , a , ( a + 1 ) % 4 ]: newrow , newcol = inc ( row , col , b ) newstate = to_s ( newrow , newcol ) newletter = desc [ newrow , newcol ] done = bytes ( newletter ) in b 'GH' rew = float ( newletter == b 'G' ) li . append (( 1.0 / 3.0 , newstate , rew , done )) else : newrow , newcol = inc ( row , col , a ) newstate = to_s ( newrow , newcol ) newletter = desc [ newrow , newcol ] done = bytes ( newletter ) in b 'GH' rew = float ( newletter == b 'G' ) li . append (( 1.0 , newstate , rew , done )) # New change because environment only allows step without # specific state for learning environment! self . P = P super ( FrozenLakeEnv , self ) . __init__ ( nS , nA , P , isd ) def render ( self , mode = 'human' ): outfile = StringIO () if mode == 'ansi' else sys . stdout row , col = self . s // self . ncol , self . s % self . ncol desc = self . desc . tolist () desc = [[ c . decode ( 'utf-8' ) for c in line ] for line in desc ] desc [ row ][ col ] = utils . colorize ( desc [ row ][ col ], \"red\" , highlight = True ) if self . lastaction is not None : outfile . write ( \" ( {} ) \\n \" . format ([ \"Left\" , \"Down\" , \"Right\" , \"Up\" ][ self . lastaction ])) else : outfile . write ( \" \\n \" ) outfile . write ( \" \\n \" . join ( '' . join ( line ) for line in desc ) + \" \\n \" ) if mode != 'human' : with closing ( outfile ): return outfile . getvalue ()","title":"Custom Frozen Lake Non-deterministic Policy Environment"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#policy-evaluation","text":"","title":"Policy Evaluation"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#transition-probability-function","text":"\\mathcal{P}_{ss'}^a = \\mathcal{P}(s' \\vert s, a) = \\mathbb{P} [S_{t+1} = s' \\vert S_t = s, A_t = a] \\mathcal{P}_{ss'}^a = \\mathcal{P}(s' \\vert s, a) = \\mathbb{P} [S_{t+1} = s' \\vert S_t = s, A_t = a]","title":"Transition Probability Function"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#deterministic-environment","text":"There's no probability distribution, if you decide to go left you'll go left Hence in this example, given current_state = 8 and action = 0 which is left, we will end up with probability = 1 in new_state = 9 # Deterministic env = FrozenLakeEnv ( is_slippery = False ) current_state = 10 # State from S_n=16 State space action = 0 # Left action from A_n=4 Action space [( probability , new_state , reward , done )] = env . P [ current_state ][ action ] print ( 'Probability {} , New State {} ' . format ( probability , new_state )) Probability 1.0, New State 9","title":"Deterministic Environment"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#stochastic-environment","text":"Given S_t = 10, A_t = 0 S_t = 10, A_t = 0 in a stochastic environment, the transition probability functions indicate you can end up in grid 6, 9, 14 each with \u2153 probability: \\mathbb{P} [S_{t+1} = 6 \\vert S_t = 10, A_t = 0] = \\frac{1}{3} \\mathbb{P} [S_{t+1} = 6 \\vert S_t = 10, A_t = 0] = \\frac{1}{3} \\mathbb{P} [S_{t+1} = 9 \\vert S_t = 10, A_t = 0] = \\frac{1}{3} \\mathbb{P} [S_{t+1} = 9 \\vert S_t = 10, A_t = 0] = \\frac{1}{3} \\mathbb{P} [S_{t+1} = 14 \\vert S_t = 10, A_t = 0] = \\frac{1}{3} \\mathbb{P} [S_{t+1} = 14 \\vert S_t = 10, A_t = 0] = \\frac{1}{3} # Stochastic env = FrozenLakeEnv ( is_slippery = True ) current_state = 10 # State from S_n=16 State space action = 0 # Left action from A_n=4 Action space env . P [ current_state ][ action ] [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 14, 0.0, False)]","title":"Stochastic Environment"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#random-policy-function","text":"Random Policy function # Random policy generation def generate_random_policy ( S_n , A_n ): # return np.random.randint(A_n, size=(S_n, A_n)) return np . ones ([ S_n , A_n ]) / A_n # Given the total number of states S_n = 16 # For each state out of 16 states, we can take 4 actions # Since this is a stochastic environment, we'll initialize a policy to have equal probabilities 0.25 of doing each action each state policy = generate_random_policy ( S_n , A_n ) print ( policy . shape ) (16, 4) Policy plot import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline plt . figure ( figsize = ( 5 , 16 )) sns . heatmap ( policy , cmap = \"YlGnBu\" , annot = True , cbar = False );","title":"Random Policy Function"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#policy-evaluation-function-comprising-state-value-function","text":"How: \\mathcal{V}_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a | s) \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\big[\\mathcal{R}_s^a + \\gamma {V}_{\\pi}(s')\\big] \\mathcal{V}_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a | s) \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\big[\\mathcal{R}_s^a + \\gamma {V}_{\\pi}(s')\\big] Simple code equation: Values of state given policy = sum ( action probability * transition probability * [reward + discount * value of new state] ) Aim: getting state-values import numpy as np def policy_evaluation ( env , policy , gamma = 1. , theta = 1e-8 ): r \"\"\"Policy evaluation function. Loop until state values stable, delta < theta. Returns V comprising values of states under given policy. Args: env (gym.env): OpenAI environment class instantiated and assigned to an object. policy (np.array): policy array to evaluate gamma (float): discount rate for rewards theta (float): tiny positive number, anything below it indicates value function convergence \"\"\" # 1. Create state-value array (16,) V = np . zeros ( S_n ) while True : delta = 0 # 2. Loop through states for s in range ( S_n ): Vs = 0 # 2.1 Loop through actions for the unique state # Given each state, we've 4 actions associated with different probabilities # 0.25 x 4 in this case, so we'll be looping 4 times (4 action probabilities) at each state for a , action_prob in enumerate ( policy [ s ]): # 2.1.1 Loop through to get transition probabilities, next state, rewards and whether the game ended for prob , next_state , reward , done in env . P [ s ][ a ]: # State-value function to get our values of states given policy Vs += action_prob * prob * ( reward + gamma * V [ next_state ]) # This simple equation allows us to stop this loop when we've converged # How do we know? The new value of the state is smaller than a tiny positive value we set # State value change is tiny compared to what we have so we just stop! delta = max ( delta , np . abs ( V [ s ] - Vs )) # 2.2 Update our state value for that state V [ s ] = Vs # 3. Stop policy evaluation if our state values changes are smaller than our tiny positive number if delta < theta : break return V # Generate random policy with equal probabilities of each action given any state rand_policy = generate_random_policy ( S_n , A_n ) # Evaluate the policy to get state values V = policy_evaluation ( env , rand_policy ) # Plot heatmap plt . figure ( figsize = ( 8 , 8 )) sns . heatmap ( V . reshape ( 4 , 4 ), cmap = \"YlGnBu\" , annot = True , cbar = False ); # This is our environment # Notice how the state values near the goal have higher values? # Those with \"H\" = hole, where you die if you step, have 0 values indicating those are bad areas to be in env . render () [ S ] FFF FHFH FFFH HFFG","title":"Policy Evaluation Function comprising State-value Function"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#policy-improvement","text":"","title":"Policy Improvement"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#action-value-q-value-function-from-state-value-function","text":"How: \\mathcal{Q}_{\\pi}(s, a) = \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\big[ \\mathcal{R}_s^a + \\gamma \\mathcal{V}_{\\pi}(s') \\big] \\mathcal{Q}_{\\pi}(s, a) = \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\big[ \\mathcal{R}_s^a + \\gamma \\mathcal{V}_{\\pi}(s') \\big] Code equation Values of action = sum ( transition probability * [reward + discount * value of next state] ) Aim: getting q-values (action-values) def q_value ( env , V , s , gamma = 1 ): r \"\"\"Q-value (action-value) function from state-value function Returns Q values, values of actions. Args: env (gym.env): OpenAI environment class instantiated and assigned to an object. V (np.array): array of state-values obtained from policy evaluation function. s (integer): integer representing current state in the gridworld gamma (float): discount rate for rewards. \"\"\" # 1. Create q-value array for one state # We have 4 actions, so let's create an array with the size of 4 q = np . zeros ( A_n ) # 2. Loop through each action for a in range ( A_n ): # 2.1 For each action, we've our transition probabilities, next state, rewards and whether the game ended for prob , next_state , reward , done in env . P [ s ][ a ]: # 2.1.1 Get our action-values from state-values q [ a ] += prob * ( reward + gamma * V [ next_state ]) # Return action values return q # For every state, we've 4 actions, hence we've 16 x 4 q values Q = np . zeros ([ S_n , A_n ]) # Loop through each state out of 16 # For each state, we will get the 4 q-values associated with the 4 actions for s in range ( env . nS ): Q [ s ] = q_value ( env , V , s ) plt . figure ( figsize = ( 5 , 16 )) sns . heatmap ( Q , cmap = \"YlGnBu\" , annot = True , cbar = False ); # Notice how 13/14, those in the last row of the gridworld just before reaching the goal of finishing the game, their action values are large? env . render () [ S ] FFF FHFH FFFH HFFG","title":"Action-value (Q-value) function from State-value function"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#policy-improvement-function","text":"How: maximizing q-values per state by choosing actions with highest q-values Aim: get improved policy def policy_improvement ( env , V , gamma = 1. ): r \"\"\"Function to improve the policy by utilizing state values and action (q) values. Args: env (gym.env): OpenAI environment class instantiated and assigned to an objects V (np.array): array of state-values obtained from policy evaluation function gamma (float): discount of rewards \"\"\" # 1. Blank policy policy = np . zeros ([ env . nS , env . nA ]) / env . nA # 2. For each state in 16 states for s in range ( env . nS ): # 2.1 Get q values: q.shape returns (4,) q = q_value ( env , V , s , gamma ) # 2.2 Find best action based on max q-value # np.argwhere(q==np.max(q)) gives the position of largest q value # given array([0.00852356, 0.01163091, 0.0108613 , 0.01550788]), this would return array([[3]]) of shape (1, 1) # .flatten() reduces the shape to (1,) where we've array([3]) best_a = np . argwhere ( q == np . max ( q )) . flatten () # 2.3 One-hot encode best action and store into policy array's row for that state # In our case where the best action is array([3]), this would return # array([0., 0., 0., 1.]) where position 3 is the best action # Now we can store the best action into our policy policy [ s ] = np . sum ([ np . eye ( env . nA )[ i ] for i in best_a ], axis = 0 ) / len ( best_a ) return policy new_policy = policy_improvement ( env , V ) plt . figure ( figsize = ( 5 , 16 )) sns . heatmap ( new_policy , cmap = \"YlGnBu\" , annot = True , cbar = False ); # Compared to this equiprobable policy, the one above is making some improvements by maximizing q-values per state plt . figure ( figsize = ( 5 , 16 )) sns . heatmap ( rand_policy , cmap = \"YlGnBu\" , annot = True , cbar = False );","title":"Policy Improvement Function"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#policy-iteration-function","text":"How: loop through policy evaluation (get state-values) and policy improvement functions (use state-values to calculate q-values to improve policy) until optimal policy obtained Aim: improve policy until convergence Convergence: difference of state values between old and new policies is very small (less than theta, a very small positive number) import copy def policy_iteration ( env , gamma = 1 , theta = 1e-8 ): # 1. Create equiprobable policy where every state has 4 actions with equal probabilities as a starting policy policy = np . ones ([ env . nS , env . nA ]) / env . nA # 2. Loop through policy_evaluation and policy_improvement functions while True : # 2.1 Get state-values V = policy_evaluation ( env , policy , gamma , theta ) # 2.2 Get new policy by getting q-values and maximizing q-values per state to get best action per state new_policy = policy_improvement ( env , V ) # 2.3 Stop if the value function estimates for successive policies has converged if np . max ( abs ( policy_evaluation ( env , policy ) - policy_evaluation ( env , new_policy ))) < theta * 1e2 : break ; # 2.4 Replace policy with new policy policy = copy . copy ( new_policy ) return policy , V # obtain the optimal policy and optimal state-value function policy_pi , V_pi = policy_iteration ( env ) # Optimal policy (pi) # LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3 plt . figure ( figsize = ( 5 , 16 )) sns . heatmap ( policy_pi , cmap = \"YlGnBu\" , annot = True , cbar = False , square = True ); # State values plt . figure ( figsize = ( 8 , 8 )) sns . heatmap ( V_pi . reshape ( 4 , 4 ), cmap = \"YlGnBu\" , annot = True , cbar = False , square = True ); # State values without policy improvement, just evaluation plt . figure ( figsize = ( 8 , 8 )) sns . heatmap ( V . reshape ( 4 , 4 ), cmap = \"YlGnBu\" , annot = True , cbar = False );","title":"Policy Iteration Function"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#value-iteration","text":"Alternative to policy iteration How: loop through to find optimal value function then get one-off policy Aim: improve value function until convergence Convergence: until difference in new and old state values are small (smaller than theta, small positive number) def value_iteration ( env , gamma = 1 , theta = 1e-8 ): # 1. Create state values of shape (16,) V = np . zeros ( env . nS ) # 2. Loop through q-value function until convergence while True : delta = 0 # 2.1 Loop through each state for s in range ( env . nS ): # 2.2 Archive old state value v = V [ s ] # 2.3 New state value = max of q-value V [ s ] = max ( q_value ( env , V , s , gamma )) delta = max ( delta , abs ( V [ s ] - v )) # 2.2 If state value changes small, converged if delta < theta : break # 3. Extract one-off policy with optimal state values policy = policy_improvement ( env , V , gamma ) return policy , V policy_vi , V_vi = value_iteration ( env ) # Optimal policy # LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3 plt . figure ( figsize = ( 5 , 16 )) sns . heatmap ( policy_vi , cmap = \"YlGnBu\" , annot = True , cbar = False , square = True ); # State values plt . figure ( figsize = ( 8 , 8 )) sns . heatmap ( V_vi . reshape ( 4 , 4 ), cmap = \"YlGnBu\" , annot = True , cbar = False , square = True );","title":"Value iteration"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/rl_notes/","text":"","title":"Rl notes"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/supervised_to_rl/","text":"Supervised Learning to Reinforcement Learning \u00b6 Supervised Learning \u00b6 The tasks we've covered so far fall under the category of supervised learning Before, we have gone through 2 major tasks: classification and regression with labels Classification : we've a number of MNIST images, we take them as input and we use a neural network for a classification task where we use the ground truth (whether the digits are 0-9) to construct our cross entropy loss Classification loss function (cross entropy loss): - \\sum^K_1 L_i log(S_i) - \\sum^K_1 L_i log(S_i) K K : number of classes L_i L_i : ground truth (label/target) of i-th class S_i S_i : output of softmax for i-th class Regression : alternatively we go through a regression task of say, predicting a time-series, but we still have the ground truth we use to construct our loss function Regression loss function (mean squared error): \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i)^2 \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i)^2 \\hat{y} \\hat{y} : prediction y y : ground truth (label/target) The key emphasis here is that we have mainly gone through supervised learning tasks that requires labels. Without them, we would not be able to properly construct our loss functions for us to do 2 critical steps (1) backpropagate to get our gradients and (2) gradient descent to update our weights with our gradients Loss functions We have covered 2 basic loss functions such as cross entropy loss (classification task) and mean squared error (regression task). However there are many more loss functions for both classification and regression task that will be covered in a separate section. For example, there are alternatives to mean squared error (MSE) like mean absolute error (MAE or L1 Loss), Smooth L1 Loss (less sensitive to outliers), quantile regression loss function (allowing confidence intervals) and many more. Reinforcement learning is not supervised learning \u00b6 One difference is that there is no ground truth (label/target) There is typically no label as to what is the definitively right prediction, we have to explore to find out what's \"right\" (essentially, the best possible prediction) Instead of minimizing a loss function comprising a target and a prediction (as with supervised learning), in reinforcement learning we are typically concerned with maximizing our reward function by trying different actions and exploring what those actions yield in an environment Let's use a simple game example of driving and not colliding with puppies crossing the road Agent Driver Environment 3 lane road, puppies crossing and the agent States Left, center or right lane Actions To move from one state to another Turn left, center or right Reward Feedback on whether action is good/bad, essentially the goal of the problem Colliding with the puppy: -10 points Too close to the puppy (scares the puppy): -2 points Safe distance from the puppy: 10 points Value function Defines what is good in the long-run as compared to rewards which is immediate after an action takes the agent to another state It's somewhat the discounted sum of the rewards the agent is expected to get Policy This defines how the agent acts in its states In this case, the agent might first collide with the puppy and learn it's bad (-10 points), then try not collide as the second action and still learn it's bad to be too close (-2 points) and finally as the third action learn to steer clear of puppies (+10 points) as it yields the largest reward Gradually it'll learn to drive at a safe distance from puppies to collect points (+10 points for safe distance) To do this, the agent needs to go try different actions and learn from its mistakes (trial and error), attempting to maximize its long-term 2 Distinguishing Properties of reinforcement learning \u00b6 Essentially, 2 distinguishing properties of reinforcement learning are: 1 (1) \"Trial-and-error search\" (2) \"Delayed reward\" In the next section, we'll be covering the terms we'll dive into these key terms through the lens of Markov Decision Processes (MDPs) and Bellman Equations. Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction; 2 nd Edition . 2017. \u21a9","title":"Supervised Learning to Reinforcement Learning (RL)"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/supervised_to_rl/#supervised-learning-to-reinforcement-learning","text":"","title":"Supervised Learning to Reinforcement Learning"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/supervised_to_rl/#supervised-learning","text":"The tasks we've covered so far fall under the category of supervised learning Before, we have gone through 2 major tasks: classification and regression with labels Classification : we've a number of MNIST images, we take them as input and we use a neural network for a classification task where we use the ground truth (whether the digits are 0-9) to construct our cross entropy loss Classification loss function (cross entropy loss): - \\sum^K_1 L_i log(S_i) - \\sum^K_1 L_i log(S_i) K K : number of classes L_i L_i : ground truth (label/target) of i-th class S_i S_i : output of softmax for i-th class Regression : alternatively we go through a regression task of say, predicting a time-series, but we still have the ground truth we use to construct our loss function Regression loss function (mean squared error): \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i)^2 \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i)^2 \\hat{y} \\hat{y} : prediction y y : ground truth (label/target) The key emphasis here is that we have mainly gone through supervised learning tasks that requires labels. Without them, we would not be able to properly construct our loss functions for us to do 2 critical steps (1) backpropagate to get our gradients and (2) gradient descent to update our weights with our gradients Loss functions We have covered 2 basic loss functions such as cross entropy loss (classification task) and mean squared error (regression task). However there are many more loss functions for both classification and regression task that will be covered in a separate section. For example, there are alternatives to mean squared error (MSE) like mean absolute error (MAE or L1 Loss), Smooth L1 Loss (less sensitive to outliers), quantile regression loss function (allowing confidence intervals) and many more.","title":"Supervised Learning"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/supervised_to_rl/#reinforcement-learning-is-not-supervised-learning","text":"One difference is that there is no ground truth (label/target) There is typically no label as to what is the definitively right prediction, we have to explore to find out what's \"right\" (essentially, the best possible prediction) Instead of minimizing a loss function comprising a target and a prediction (as with supervised learning), in reinforcement learning we are typically concerned with maximizing our reward function by trying different actions and exploring what those actions yield in an environment Let's use a simple game example of driving and not colliding with puppies crossing the road Agent Driver Environment 3 lane road, puppies crossing and the agent States Left, center or right lane Actions To move from one state to another Turn left, center or right Reward Feedback on whether action is good/bad, essentially the goal of the problem Colliding with the puppy: -10 points Too close to the puppy (scares the puppy): -2 points Safe distance from the puppy: 10 points Value function Defines what is good in the long-run as compared to rewards which is immediate after an action takes the agent to another state It's somewhat the discounted sum of the rewards the agent is expected to get Policy This defines how the agent acts in its states In this case, the agent might first collide with the puppy and learn it's bad (-10 points), then try not collide as the second action and still learn it's bad to be too close (-2 points) and finally as the third action learn to steer clear of puppies (+10 points) as it yields the largest reward Gradually it'll learn to drive at a safe distance from puppies to collect points (+10 points for safe distance) To do this, the agent needs to go try different actions and learn from its mistakes (trial and error), attempting to maximize its long-term","title":"Reinforcement learning is not supervised learning"},{"location":"deep_learning/deep_reinforcement_learning_pytorch/supervised_to_rl/#2-distinguishing-properties-of-reinforcement-learning","text":"Essentially, 2 distinguishing properties of reinforcement learning are: 1 (1) \"Trial-and-error search\" (2) \"Delayed reward\" In the next section, we'll be covering the terms we'll dive into these key terms through the lens of Markov Decision Processes (MDPs) and Bellman Equations. Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction; 2 nd Edition . 2017. \u21a9","title":"2 Distinguishing Properties of reinforcement learning"},{"location":"deep_learning/fromscratch/fromscratch_cnn/","text":"CNN from Scratch \u00b6 This is an implementation of a simple CNN (one convolutional function, one non-linear function, one max pooling function, one affine function and one softargmax function) for a 10-class MNIST classification task. Data Pull \u00b6 Imports \u00b6 from sklearn.datasets import fetch_openml from sklearn.utils import check_random_state from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler import matplotlib.pyplot as plt import pandas as pd import numpy as np import torch % matplotlib inline Load Data \u00b6 # Load data from https://www.openml.org/d/554 X , y = fetch_openml ( 'mnist_784' , version = 1 , return_X_y = True ) ### Plot Single Sample # Print dataset print ( f 'X shape: { X . shape } ' ) print ( f 'y shape: { y . shape } ' ) # Print size of resized single sample single_sample = X [ 0 , :] . reshape ( 28 , 28 ) print ( f 'Single sample shape: { single_sample . shape } ' ) # Plot single sample (M x N matrix) plt . gray () plt . matshow ( single_sample ) plt . show () X shape : ( 70000 , 784 ) y shape : ( 70000 ,) Single sample shape : ( 28 , 28 ) < Figure size 432 x288 with 0 Axes > Train-Test Data Split \u00b6 # Train-test split train_samples = 60000 random_state = check_random_state ( 0 ) permutation = random_state . permutation ( X . shape [ 0 ]) X = X [ permutation ] y = y [ permutation ] X = X . reshape (( X . shape [ 0 ], - 1 )) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = train_samples , test_size = 10000 ) scaler = StandardScaler () X_train = scaler . fit_transform ( X_train ) X_test = scaler . transform ( X_test ) # Print shapes print ( f 'Training shape { X_train . shape } ' ) print ( f 'Testing shape { X_test . shape } ' ) Training shape (60000, 784) Testing shape (10000, 784) Forwardpropagation \u00b6 Convolutional Layer \u00b6 class ConvolutionalLayer : def __init__ ( self , num_kernels , kernel_shape ): # Number of kernels: 1D self . num_kernels = num_kernels # Shape of kernels: 2D self . kernel_shape = kernel_shape self . k = self . kernel_shape [ 0 ] # Kernel weights: 3D self . kernels_theta = torch . randn ( self . num_kernels , self . kernel_shape [ 0 ], self . kernel_shape [ 1 ]) def slider ( self , inp ): ''' Sliding generator that yields square areas of shape (kernel_shape, kernel_shape) sliding across our input. This assumes valid padding (no padding) and step size 1. ''' h , w = inp . shape for h_idx in range ( h - ( self . k - 1 )): for w_idx in range ( w - ( self . k - 1 )): single_slide_area = inp [ h_idx :( h_idx + self . k ), w_idx :( w_idx + self . k )] yield single_slide_area , h_idx , w_idx def forward ( self , inp ): ''' Slides kernel across image doing an element-wise MM then summing. Results in forward pass of convolutional layer of shape (output shape, output shape, number of kernels). ''' # Input: 2D of (height, width) assert single_sample . dim () == 2 , f 'Input not 2D, given { single_sample . dim () } D' # Output via Valid Padding (No Padding): 3D of (height, width, number of kernels) _ , w = inp . shape # P = 0 p = 0 # O = ((W - K + 2P) / S) + 1 = (28 - 3 + 0) + 1 = 25 o = ( w - self . k ) + 1 # Print shapes print ( 'Padding shape: \\t ' , p ) print ( 'Output shape: \\t ' , o ) # Initialize blank tensor output = torch . zeros (( o , o , self . num_kernels )) # Iterate through region for single_slide_area , h_idx , w_idx in self . slider ( inp ): if h_idx == 0 and w_idx == 0 : print ( 'Region shape: \\t ' , list ( single_slide_area . shape )) print ( 'Kernel shape: \\t ' , list ( self . kernels_theta . shape )) print ( 'Single Slide: \\t ' , list ( output [ h_idx , w_idx ] . shape )) # Sum values with each element-wise matrix multiplication across each kernel # Instead of doing another loop of each kernel, you simply just do a element-wise MM # of the single slide area with all the kernels yield, then summing the patch output [ h_idx , w_idx ] = torch . sum ( single_slide_area * self . kernels_theta , axis = ( 1 , 2 )) # Pass through non-linearity (sigmoid): 1 / 1 + exp(-output) output = 1. / ( 1. + torch . exp ( - output )) return output # Convert numpy array to torch tensor single_sample = X [ 0 , :] . reshape ( 28 , 28 ) single_sample = torch . tensor ( single_sample ) print ( '=' * 50 ) print ( f 'Input shape: \\t { list ( single_sample . shape ) } ' ) print ( '=' * 50 ) # Forward: conv conv = ConvolutionalLayer ( num_kernels = 8 , kernel_shape = [ 5 , 5 ]) output = conv . forward ( single_sample ) print ( '=' * 50 ) print ( f 'Conv (f) shape: \\t { list ( output . shape ) } ' ) print ( '=' * 50 ) ================================================== Input shape: [28, 28] ================================================== Padding shape: 0 Output shape: 24 Region shape: [5, 5] Kernel shape: [8, 5, 5] Single Slide: [8] ================================================== Conv (f) shape: [24, 24, 8] ================================================== Max Pooling Layer \u00b6 class MaxPoolLayer : # O = ((W - K) / S) + 1 def __init__ ( self , pooling_kernel_shape ): # Assume simplicity of K = S then O = W / S self . k = pooling_kernel_shape def slider ( self , inp ): ''' Sliding generator that yields areas for max pooling. ''' h , w , _ = inp . shape output_size = int ( w / self . k ) # Assume S = K for h_idx in range ( output_size ): for w_idx in range ( output_size ): single_slide_area = inp [ h_idx * self . k : h_idx * self . k + self . k , w_idx * self . k : w_idx * self . k + self . k ] yield single_slide_area , h_idx , w_idx def forward ( self , inp ): ''' Performs a forward pass of the maxpool layer using the given input. Returns a 3d numpy array with dimensions (h / 2, w / 2, num_filters). - input is a 3d numpy array with dimensions (h, w, num_filters) ''' self . last_input = inp h , w , num_kernels = inp . shape output_size = int ( w / self . k ) # Assume S = K output = torch . zeros ( output_size , output_size , num_kernels ) for single_slide_area , h_idx , w_idx in self . slider ( inp ): single_slide_area = torch . flatten ( single_slide_area , start_dim = 0 , end_dim = 1 ) output [ h_idx , w_idx ] = torch . max ( single_slide_area , dim = 0 ) . values return output print ( '=' * 50 ) print ( f 'Input shape: \\t { list ( output . shape ) } ' ) print ( '=' * 50 ) # Forward: pool pool = MaxPoolLayer ( pooling_kernel_shape = 2 ) output = pool . forward ( output ) print ( '=' * 50 ) print ( f 'Pool (f) shape: \\t { list ( output . shape ) } ' ) print ( '=' * 50 ) ================================================== Input shape: [24, 24, 8] ================================================== ================================================== Pool (f) shape: [12, 12, 8] ================================================== Affine and Soft(arg)max Layer \u00b6 class AffineAndSoftmaxLayer : def __init__ ( self , affine_weight_shape ): self . affine_weight_shape = affine_weight_shape # Weight shape: flattened input x output shape self . w = torch . zeros ( self . affine_weight_shape [ 0 ] * self . affine_weight_shape [ 1 ] * self . affine_weight_shape [ 2 ], self . affine_weight_shape [ 3 ]) self . b = torch . zeros ( self . affine_weight_shape [ 3 ]) # Initialize weight/bias via Lecun initialization of 1 / N standard deviation # Refer to DLW guide on weight initialization mathematical derivation: # https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/ print ( f 'Lecun initialization SD: { 1 / self . affine_weight_shape [ 3 ] } ' ) self . w = torch . nn . init . normal_ ( self . w , mean = 0 , std = 1 / self . affine_weight_shape [ 3 ]) self . b = torch . nn . init . normal_ ( self . b , mean = 0 , std = 1 / self . affine_weight_shape [ 3 ]) def forward ( self , inp ): ''' Performs Linear (Affine) Function & Soft(arg)max Function that returns our vector (1D) of probabilities. ''' inp = inp . reshape ( 1 , - 1 ) print ( f 'input shape: \\t { inp . shape } ' ) print ( f 'weight shape: \\t { self . w . shape } ' ) print ( f 'bias shape: \\t { self . b . shape } ' ) logits = torch . mm ( inp , self . w ) + self . b probas = torch . exp ( logits ) / torch . sum ( torch . exp ( logits )) return probas print ( '=' * 50 ) print ( f 'Input shape: \\t { list ( output . shape ) } ' ) print ( '=' * 50 ) # Forward: Affine and Softmax affinesoftmax = AffineAndSoftmaxLayer ( affine_weight_shape = ( output . shape [ 0 ], output . shape [ 1 ], output . shape [ 2 ], len ( np . unique ( y_train )))) output = affinesoftmax . forward ( output ) print ( '=' * 50 ) print ( f 'Affine & Soft(arg)max (f) shape: \\t { list ( output . shape ) } ' ) print ( '=' * 50 ) print ( f 'Probas: { pd . DataFrame ( output . numpy ()) . to_string ( index = False , header = False ) } ' ) ================================================== Input shape: [12, 12, 8] ================================================== Lecun initialization SD: 0.1 input shape: torch.Size([1, 1152]) weight shape: torch.Size([1152, 10]) bias shape: torch.Size([10]) ================================================== Affine & Soft(arg)max (f) shape: [1, 10] ================================================== Probas: 0.72574 0.001391 0.000083 0.202983 0.000503 0.037023 0.000428 0.000144 0.031293 0.000412 Dot Product, Matrix Multiplication, and Hadamard Product Hadamard product : element-wise multiplicaton of 2 matrices. Matrix Multiplication : take the first row of the first matrix and perform dot product with each of the N columns in the second matrix to form N columns in the first row of the new matrix. Repeat for remaining rows for the first matrix. # Backward & GD: linear + softmax # Backward & GD: pool # Backward & GD: conv Note on Contiguous vs Non-Contiguous \u00b6 Often in the space of passing tensors, and doing all our dot products, hadamard products, matrix multiplications, transpose, reshape operations and more, you would inevitably one day encounter into an error that says your tensor is not contiguous . This is a memory allocation problem. Certain tensor operations like transpose, view, expand, narrow etc do not change the original tensor, instead they modify the properties of the tensor. For example, transpose , demonstrated here, would change the shape (index), but both the old and modified property tensor share the same memory block with different indexes/addresses. This is why it's non-contiguous and we need to make it contiguous for some operations and typically for efficiency purpose. This is not a blanket statement, but you typically want your tensors to be contiguous as it prevents additional overhead incurred from translating addresses. Whenever there's a warning that prompts you the tensor is not contiguous, just call .contiguous() and you typically should be good to go. Contiguous 10 by 5 tensor \u00b6 contiguous_tensor = torch . arange ( 50 ) . view ( 10 , 5 ) print ( contiguous_tensor . shape ) # Pretty print quick hack via Pandas DataFrame print ( pd . DataFrame ( contiguous_tensor . numpy ()) . to_string ( index = False , header = False )) torch.Size([10, 5]) 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 Stride 5 by 1 \u00b6 Stride here shows how we need: 5 steps to move one row to the next & 1 step to move from one column to the next (contiguous) If this becomes anything other than 1 step, it becomes not contiguous print ( contiguous_tensor . stride ()) print ( contiguous_tensor . is_contiguous ()) (5, 1) True Non-contiguous Tensor via Transpose Operation \u00b6 In order to access the next \"column\" value of 5, we have to take 5 steps despite the transpose as if we would do in our original tensor Because the original tensor and this transposed tensor share the same memory block! non_contiguous_tensor = contiguous_tensor . t () print ( non_contiguous_tensor . shape ) print ( pd . DataFrame ( non_contiguous_tensor . numpy ()) . to_string ( index = False , header = False )) torch.Size([5, 10]) 0 5 10 15 20 25 30 35 40 45 1 6 11 16 21 26 31 36 41 46 2 7 12 17 22 27 32 37 42 47 3 8 13 18 23 28 33 38 43 48 4 9 14 19 24 29 34 39 44 49 Stride 1 by 5 \u00b6 print ( non_contiguous_tensor . stride ()) print ( non_contiguous_tensor . is_contiguous ()) (1, 5) False Convert to Contiguous \u00b6 # Convert to contiguous convert_contiguous = non_contiguous_tensor . contiguous () print ( convert_contiguous . is_contiguous ()) True","title":"From Scratch CNN Classification"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#cnn-from-scratch","text":"This is an implementation of a simple CNN (one convolutional function, one non-linear function, one max pooling function, one affine function and one softargmax function) for a 10-class MNIST classification task.","title":"CNN from Scratch"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#data-pull","text":"","title":"Data Pull"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#imports","text":"from sklearn.datasets import fetch_openml from sklearn.utils import check_random_state from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler import matplotlib.pyplot as plt import pandas as pd import numpy as np import torch % matplotlib inline","title":"Imports"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#load-data","text":"# Load data from https://www.openml.org/d/554 X , y = fetch_openml ( 'mnist_784' , version = 1 , return_X_y = True ) ### Plot Single Sample # Print dataset print ( f 'X shape: { X . shape } ' ) print ( f 'y shape: { y . shape } ' ) # Print size of resized single sample single_sample = X [ 0 , :] . reshape ( 28 , 28 ) print ( f 'Single sample shape: { single_sample . shape } ' ) # Plot single sample (M x N matrix) plt . gray () plt . matshow ( single_sample ) plt . show () X shape : ( 70000 , 784 ) y shape : ( 70000 ,) Single sample shape : ( 28 , 28 ) < Figure size 432 x288 with 0 Axes >","title":"Load Data"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#train-test-data-split","text":"# Train-test split train_samples = 60000 random_state = check_random_state ( 0 ) permutation = random_state . permutation ( X . shape [ 0 ]) X = X [ permutation ] y = y [ permutation ] X = X . reshape (( X . shape [ 0 ], - 1 )) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = train_samples , test_size = 10000 ) scaler = StandardScaler () X_train = scaler . fit_transform ( X_train ) X_test = scaler . transform ( X_test ) # Print shapes print ( f 'Training shape { X_train . shape } ' ) print ( f 'Testing shape { X_test . shape } ' ) Training shape (60000, 784) Testing shape (10000, 784)","title":"Train-Test Data Split"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#forwardpropagation","text":"","title":"Forwardpropagation"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#convolutional-layer","text":"class ConvolutionalLayer : def __init__ ( self , num_kernels , kernel_shape ): # Number of kernels: 1D self . num_kernels = num_kernels # Shape of kernels: 2D self . kernel_shape = kernel_shape self . k = self . kernel_shape [ 0 ] # Kernel weights: 3D self . kernels_theta = torch . randn ( self . num_kernels , self . kernel_shape [ 0 ], self . kernel_shape [ 1 ]) def slider ( self , inp ): ''' Sliding generator that yields square areas of shape (kernel_shape, kernel_shape) sliding across our input. This assumes valid padding (no padding) and step size 1. ''' h , w = inp . shape for h_idx in range ( h - ( self . k - 1 )): for w_idx in range ( w - ( self . k - 1 )): single_slide_area = inp [ h_idx :( h_idx + self . k ), w_idx :( w_idx + self . k )] yield single_slide_area , h_idx , w_idx def forward ( self , inp ): ''' Slides kernel across image doing an element-wise MM then summing. Results in forward pass of convolutional layer of shape (output shape, output shape, number of kernels). ''' # Input: 2D of (height, width) assert single_sample . dim () == 2 , f 'Input not 2D, given { single_sample . dim () } D' # Output via Valid Padding (No Padding): 3D of (height, width, number of kernels) _ , w = inp . shape # P = 0 p = 0 # O = ((W - K + 2P) / S) + 1 = (28 - 3 + 0) + 1 = 25 o = ( w - self . k ) + 1 # Print shapes print ( 'Padding shape: \\t ' , p ) print ( 'Output shape: \\t ' , o ) # Initialize blank tensor output = torch . zeros (( o , o , self . num_kernels )) # Iterate through region for single_slide_area , h_idx , w_idx in self . slider ( inp ): if h_idx == 0 and w_idx == 0 : print ( 'Region shape: \\t ' , list ( single_slide_area . shape )) print ( 'Kernel shape: \\t ' , list ( self . kernels_theta . shape )) print ( 'Single Slide: \\t ' , list ( output [ h_idx , w_idx ] . shape )) # Sum values with each element-wise matrix multiplication across each kernel # Instead of doing another loop of each kernel, you simply just do a element-wise MM # of the single slide area with all the kernels yield, then summing the patch output [ h_idx , w_idx ] = torch . sum ( single_slide_area * self . kernels_theta , axis = ( 1 , 2 )) # Pass through non-linearity (sigmoid): 1 / 1 + exp(-output) output = 1. / ( 1. + torch . exp ( - output )) return output # Convert numpy array to torch tensor single_sample = X [ 0 , :] . reshape ( 28 , 28 ) single_sample = torch . tensor ( single_sample ) print ( '=' * 50 ) print ( f 'Input shape: \\t { list ( single_sample . shape ) } ' ) print ( '=' * 50 ) # Forward: conv conv = ConvolutionalLayer ( num_kernels = 8 , kernel_shape = [ 5 , 5 ]) output = conv . forward ( single_sample ) print ( '=' * 50 ) print ( f 'Conv (f) shape: \\t { list ( output . shape ) } ' ) print ( '=' * 50 ) ================================================== Input shape: [28, 28] ================================================== Padding shape: 0 Output shape: 24 Region shape: [5, 5] Kernel shape: [8, 5, 5] Single Slide: [8] ================================================== Conv (f) shape: [24, 24, 8] ==================================================","title":"Convolutional Layer"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#max-pooling-layer","text":"class MaxPoolLayer : # O = ((W - K) / S) + 1 def __init__ ( self , pooling_kernel_shape ): # Assume simplicity of K = S then O = W / S self . k = pooling_kernel_shape def slider ( self , inp ): ''' Sliding generator that yields areas for max pooling. ''' h , w , _ = inp . shape output_size = int ( w / self . k ) # Assume S = K for h_idx in range ( output_size ): for w_idx in range ( output_size ): single_slide_area = inp [ h_idx * self . k : h_idx * self . k + self . k , w_idx * self . k : w_idx * self . k + self . k ] yield single_slide_area , h_idx , w_idx def forward ( self , inp ): ''' Performs a forward pass of the maxpool layer using the given input. Returns a 3d numpy array with dimensions (h / 2, w / 2, num_filters). - input is a 3d numpy array with dimensions (h, w, num_filters) ''' self . last_input = inp h , w , num_kernels = inp . shape output_size = int ( w / self . k ) # Assume S = K output = torch . zeros ( output_size , output_size , num_kernels ) for single_slide_area , h_idx , w_idx in self . slider ( inp ): single_slide_area = torch . flatten ( single_slide_area , start_dim = 0 , end_dim = 1 ) output [ h_idx , w_idx ] = torch . max ( single_slide_area , dim = 0 ) . values return output print ( '=' * 50 ) print ( f 'Input shape: \\t { list ( output . shape ) } ' ) print ( '=' * 50 ) # Forward: pool pool = MaxPoolLayer ( pooling_kernel_shape = 2 ) output = pool . forward ( output ) print ( '=' * 50 ) print ( f 'Pool (f) shape: \\t { list ( output . shape ) } ' ) print ( '=' * 50 ) ================================================== Input shape: [24, 24, 8] ================================================== ================================================== Pool (f) shape: [12, 12, 8] ==================================================","title":"Max Pooling Layer"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#affine-and-softargmax-layer","text":"class AffineAndSoftmaxLayer : def __init__ ( self , affine_weight_shape ): self . affine_weight_shape = affine_weight_shape # Weight shape: flattened input x output shape self . w = torch . zeros ( self . affine_weight_shape [ 0 ] * self . affine_weight_shape [ 1 ] * self . affine_weight_shape [ 2 ], self . affine_weight_shape [ 3 ]) self . b = torch . zeros ( self . affine_weight_shape [ 3 ]) # Initialize weight/bias via Lecun initialization of 1 / N standard deviation # Refer to DLW guide on weight initialization mathematical derivation: # https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/ print ( f 'Lecun initialization SD: { 1 / self . affine_weight_shape [ 3 ] } ' ) self . w = torch . nn . init . normal_ ( self . w , mean = 0 , std = 1 / self . affine_weight_shape [ 3 ]) self . b = torch . nn . init . normal_ ( self . b , mean = 0 , std = 1 / self . affine_weight_shape [ 3 ]) def forward ( self , inp ): ''' Performs Linear (Affine) Function & Soft(arg)max Function that returns our vector (1D) of probabilities. ''' inp = inp . reshape ( 1 , - 1 ) print ( f 'input shape: \\t { inp . shape } ' ) print ( f 'weight shape: \\t { self . w . shape } ' ) print ( f 'bias shape: \\t { self . b . shape } ' ) logits = torch . mm ( inp , self . w ) + self . b probas = torch . exp ( logits ) / torch . sum ( torch . exp ( logits )) return probas print ( '=' * 50 ) print ( f 'Input shape: \\t { list ( output . shape ) } ' ) print ( '=' * 50 ) # Forward: Affine and Softmax affinesoftmax = AffineAndSoftmaxLayer ( affine_weight_shape = ( output . shape [ 0 ], output . shape [ 1 ], output . shape [ 2 ], len ( np . unique ( y_train )))) output = affinesoftmax . forward ( output ) print ( '=' * 50 ) print ( f 'Affine & Soft(arg)max (f) shape: \\t { list ( output . shape ) } ' ) print ( '=' * 50 ) print ( f 'Probas: { pd . DataFrame ( output . numpy ()) . to_string ( index = False , header = False ) } ' ) ================================================== Input shape: [12, 12, 8] ================================================== Lecun initialization SD: 0.1 input shape: torch.Size([1, 1152]) weight shape: torch.Size([1152, 10]) bias shape: torch.Size([10]) ================================================== Affine & Soft(arg)max (f) shape: [1, 10] ================================================== Probas: 0.72574 0.001391 0.000083 0.202983 0.000503 0.037023 0.000428 0.000144 0.031293 0.000412 Dot Product, Matrix Multiplication, and Hadamard Product Hadamard product : element-wise multiplicaton of 2 matrices. Matrix Multiplication : take the first row of the first matrix and perform dot product with each of the N columns in the second matrix to form N columns in the first row of the new matrix. Repeat for remaining rows for the first matrix. # Backward & GD: linear + softmax # Backward & GD: pool # Backward & GD: conv","title":"Affine and Soft(arg)max Layer"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#note-on-contiguous-vs-non-contiguous","text":"Often in the space of passing tensors, and doing all our dot products, hadamard products, matrix multiplications, transpose, reshape operations and more, you would inevitably one day encounter into an error that says your tensor is not contiguous . This is a memory allocation problem. Certain tensor operations like transpose, view, expand, narrow etc do not change the original tensor, instead they modify the properties of the tensor. For example, transpose , demonstrated here, would change the shape (index), but both the old and modified property tensor share the same memory block with different indexes/addresses. This is why it's non-contiguous and we need to make it contiguous for some operations and typically for efficiency purpose. This is not a blanket statement, but you typically want your tensors to be contiguous as it prevents additional overhead incurred from translating addresses. Whenever there's a warning that prompts you the tensor is not contiguous, just call .contiguous() and you typically should be good to go.","title":"Note on Contiguous vs Non-Contiguous"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#contiguous-10-by-5-tensor","text":"contiguous_tensor = torch . arange ( 50 ) . view ( 10 , 5 ) print ( contiguous_tensor . shape ) # Pretty print quick hack via Pandas DataFrame print ( pd . DataFrame ( contiguous_tensor . numpy ()) . to_string ( index = False , header = False )) torch.Size([10, 5]) 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49","title":"Contiguous 10 by 5 tensor"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#stride-5-by-1","text":"Stride here shows how we need: 5 steps to move one row to the next & 1 step to move from one column to the next (contiguous) If this becomes anything other than 1 step, it becomes not contiguous print ( contiguous_tensor . stride ()) print ( contiguous_tensor . is_contiguous ()) (5, 1) True","title":"Stride 5 by 1"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#non-contiguous-tensor-via-transpose-operation","text":"In order to access the next \"column\" value of 5, we have to take 5 steps despite the transpose as if we would do in our original tensor Because the original tensor and this transposed tensor share the same memory block! non_contiguous_tensor = contiguous_tensor . t () print ( non_contiguous_tensor . shape ) print ( pd . DataFrame ( non_contiguous_tensor . numpy ()) . to_string ( index = False , header = False )) torch.Size([5, 10]) 0 5 10 15 20 25 30 35 40 45 1 6 11 16 21 26 31 36 41 46 2 7 12 17 22 27 32 37 42 47 3 8 13 18 23 28 33 38 43 48 4 9 14 19 24 29 34 39 44 49","title":"Non-contiguous Tensor via Transpose Operation"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#stride-1-by-5","text":"print ( non_contiguous_tensor . stride ()) print ( non_contiguous_tensor . is_contiguous ()) (1, 5) False","title":"Stride 1 by 5"},{"location":"deep_learning/fromscratch/fromscratch_cnn/#convert-to-contiguous","text":"# Convert to contiguous convert_contiguous = non_contiguous_tensor . contiguous () print ( convert_contiguous . is_contiguous ()) True","title":"Convert to Contiguous"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/","text":"Logistic Regression from Scratch \u00b6 This is an implementation of a simple logistic regression for binary class labels. We will be attempting to classify 2 flowers based on their petal width and height: setosa and versicolor. Imports \u00b6 % matplotlib inline import matplotlib.pyplot as plt import numpy as np import torch import torch.nn.functional as F from sklearn import datasets from sklearn import preprocessing from sklearn.model_selection import train_test_split from collections import Counter Preparing a custom 2-class IRIS dataset \u00b6 Load Data \u00b6 # Instantiate dataset class and assign to object iris = datasets . load_iris () # Load features and target # Take only 2 classes, and 2 features (sepal length/width) X = iris . data [: - 50 , : 2 ] # For teaching the math rather than preprocessing techniques, # we'll be using this simple scaling method. However, you must # be cautious to scale your training/testing sets subsequently. X = preprocessing . scale ( X ) y = iris . target [: - 50 ] Print Data Details \u00b6 # 50 of each iris flower print ( Counter ( y )) # Type of flower print ( list ( iris . target_names [: - 1 ])) # Shape of features print ( X . shape ) Counter({0: 50, 1: 50}) ['setosa', 'versicolor'] (100, 2) Scatterplot 2 Classes \u00b6 plt . scatter ( X [:, 0 ], X [:, 1 ], c = y ); Train/Test Split \u00b6 X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) print ( f 'X train size: { X_train . shape } ' ) print ( f 'X test size: { X_test . shape } ' ) print ( f 'y train size: { y_train . shape } ' ) print ( f 'y test size: { y_test . shape } ' ) # Distribution of both classes are roughly equal using train_test_split function print ( Counter ( y_train )) X train size: (80, 2) X test size: (20, 2) y train size: (80,) y test size: (20,) Counter({0: 41, 1: 39}) Math \u00b6 1. Forwardpropagation \u00b6 Get our logits and probabilities Affine function/transformation: z = \\theta x + b z = \\theta x + b Sigmoid/logistic function: \\hat y = \\frac{1}{1 + e^{-z}} \\hat y = \\frac{1}{1 + e^{-z}} 2. Backwardpropagation \u00b6 Calculate gradients / partial derivatives w.r.t. weights and bias Loss: L = ylog(\\hat y) + (1-y) log (1 - \\hat y) L = ylog(\\hat y) + (1-y) log (1 - \\hat y) Partial derivative of loss w.r.t weights: \\frac{\\delta L}{\\delta w} =\\frac{\\delta L}{\\delta z} \\frac{\\delta z}{\\delta w} = (\\hat y - y)(x^T) \\frac{\\delta L}{\\delta w} =\\frac{\\delta L}{\\delta z} \\frac{\\delta z}{\\delta w} = (\\hat y - y)(x^T) Partial derivative of loss w.r.t. bias: \\frac{\\delta L}{\\delta b} = \\frac{\\delta L}{\\delta z} \\frac{\\delta z}{\\delta b} = (\\hat y - y)(1) \\frac{\\delta L}{\\delta b} = \\frac{\\delta L}{\\delta z} \\frac{\\delta z}{\\delta b} = (\\hat y - y)(1) \\frac{\\delta L}{\\delta z} = \\hat y - y \\frac{\\delta L}{\\delta z} = \\hat y - y \\frac{\\delta z}{\\delta w} = x \\frac{\\delta z}{\\delta w} = x \\frac{\\delta z}{\\delta b} = 1 \\frac{\\delta z}{\\delta b} = 1 2a. Loss function clarification \u00b6 Actually, why is our loss equation L = ylog(\\hat y) + (1-y) log (1 - \\hat y) L = ylog(\\hat y) + (1-y) log (1 - \\hat y) ? We have given the intuition in the Logistic Regression tutorial on why it works. Here we will cover the derivation which essentially is merely maximizing the log likelihood of the parameters (maximizing the probability of our predicted output given our input and parameters Given: \\hat y = \\frac{1}{1 + e^{-z}} \\hat y = \\frac{1}{1 + e^{-z}} . Then: P(y=1 \\mid x;\\theta) = \\hat y P(y=1 \\mid x;\\theta) = \\hat y P(y=0 \\mid x;\\theta) = 1 - \\hat y P(y=0 \\mid x;\\theta) = 1 - \\hat y Simplified further: p(y \\mid x; \\theta) = (\\hat y)^y(1 - \\hat y)^{1-y} p(y \\mid x; \\theta) = (\\hat y)^y(1 - \\hat y)^{1-y} Given m training samples, the likelihood of the parameters is simply the product of probabilities: L(\\theta) = \\displaystyle \\prod_{i=1}^{m} p(y^i \\mid x^i; \\theta) L(\\theta) = \\displaystyle \\prod_{i=1}^{m} p(y^i \\mid x^i; \\theta) L(\\theta) = \\displaystyle \\prod_{i=1}^{m} (\\hat y^{i})^{y^i}(1 - \\hat y^{i})^{1-y^{i}} L(\\theta) = \\displaystyle \\prod_{i=1}^{m} (\\hat y^{i})^{y^i}(1 - \\hat y^{i})^{1-y^{i}} Essentially, we want to maximize the probability of our ouput given our input and parameters But it's easier to maximize the log likelihood, so we take the natural logarithm. L(\\theta) = \\displaystyle \\sum_{i=1}^{m} y^{i}log (\\hat y^{i}) + (1 - y^{i})log(1 - \\hat y^{i}) L(\\theta) = \\displaystyle \\sum_{i=1}^{m} y^{i}log (\\hat y^{i}) + (1 - y^{i})log(1 - \\hat y^{i}) Why is is easier to maximize the log likelihood? The natural logarithm is a function that monotonically increases. This allows us to find the \"max\" of the log likelihood easier compared to a non-monotonically increasing function (like a wave up and down). 3. Gradient descent: updating weights \u00b6 w = w - \\alpha (\\hat y - y)(x^T) w = w - \\alpha (\\hat y - y)(x^T) b = b - \\alpha (\\hat y - y).1 b = b - \\alpha (\\hat y - y).1 Training from Scratch \u00b6 learning_rate = 0.1 num_features = X . shape [ 1 ] weights = torch . zeros ( num_features , 1 , dtype = torch . float32 ) bias = torch . zeros ( 1 , dtype = torch . float32 ) X_train = torch . from_numpy ( X_train ) . type ( torch . float32 ) y_train = torch . from_numpy ( y_train ) . type ( torch . float32 ) for epoch in range ( num_epochs ): # 1. Forwardpropagation: # 1a. Affine Transformation: z = \\theta x + b z = torch . add ( torch . mm ( X_train , weights ), bias ) # 2a. Sigmoid/Logistic Function: y_hat = 1 / (1 + e^{-z}) y_hat = 1. / ( 1. + torch . exp ( - z )) # Backpropagation: # 1. Calculate binary cross entropy l = torch . mm ( - y_train . view ( 1 , - 1 ), torch . log ( y_hat )) - torch . mm (( 1 - y_train ) . view ( 1 , - 1 ), torch . log ( 1 - y_hat )) # 2. Calculate dl/dz dl_dz = y_train - y_hat . view ( - 1 ) # 2. Calculate partial derivative of cost w.r.t weights (gradients) # dl_dw = dl_dz dz_dw = (y_hat - y)(x^T) grad = torch . mm ( X_train . transpose ( 0 , 1 ), dl_dz . view ( - 1 , 1 )) # Gradient descent: # update our weights and bias with our gradients weights += learning_rate * grad bias += learning_rate * torch . sum ( dl_dz ) # Accuracy total = y_hat . shape [ 0 ] predicted = ( y_hat > 0.5 ) . float () . squeeze () correct = ( predicted == y_train ) . sum () acc = 100 * correct / total # Print accuracy and cost print ( f 'Epoch: { epoch } | Accuracy: { acc . item () : .4f } | Cost: { l . item () : .4f } ' ) print ( f 'Weights \\n { weights . data } ' ) print ( f 'Bias \\n { bias . data } ' ) X train size: (80, 2) X test size: (20, 2) y train size: (80,) y test size: (20,) Counter({1: 41, 0: 39}) Epoch: 0 | Accuracy: 48.0000 | Cost: 55.4518 Epoch: 1 | Accuracy: 100.0000 | Cost: 5.6060 Epoch: 2 | Accuracy: 100.0000 | Cost: 5.0319 Epoch: 3 | Accuracy: 100.0000 | Cost: 4.6001 Epoch: 4 | Accuracy: 100.0000 | Cost: 4.2595 Epoch: 5 | Accuracy: 100.0000 | Cost: 3.9819 Epoch: 6 | Accuracy: 100.0000 | Cost: 3.7498 Epoch: 7 | Accuracy: 100.0000 | Cost: 3.5521 Epoch: 8 | Accuracy: 100.0000 | Cost: 3.3810 Epoch: 9 | Accuracy: 100.0000 | Cost: 3.2310 Epoch: 10 | Accuracy: 100.0000 | Cost: 3.0981 Epoch: 11 | Accuracy: 100.0000 | Cost: 2.9794 Epoch: 12 | Accuracy: 100.0000 | Cost: 2.8724 Epoch: 13 | Accuracy: 100.0000 | Cost: 2.7754 Epoch: 14 | Accuracy: 100.0000 | Cost: 2.6869 Epoch: 15 | Accuracy: 100.0000 | Cost: 2.6057 Epoch: 16 | Accuracy: 100.0000 | Cost: 2.5308 Epoch: 17 | Accuracy: 100.0000 | Cost: 2.4616 Epoch: 18 | Accuracy: 100.0000 | Cost: 2.3973 Epoch: 19 | Accuracy: 100.0000 | Cost: 2.3374 Weights tensor([[ 4.9453], [-3.6849]]) Bias tensor([0.5570]) Inference \u00b6 # Port to tensors X_test = torch . from_numpy ( X_test ) . type ( torch . float32 ) y_test = torch . from_numpy ( y_test ) . type ( torch . float32 ) # 1. Forwardpropagation: # 1a. Affine Transformation: z = ax + b z = torch . add ( torch . mm ( X_test , weights ), bias ) # 2a. Sigmoid/Logistic Function: y_hat = 1 / (1 + e^{-z}) y_hat = 1. / ( 1. + torch . exp ( - z )) total = y_test . shape [ 0 ] predicted = ( y_hat > 0.5 ) . float () . squeeze () correct = ( predicted == y_test ) . sum () acc = 100 * correct / total # Print accuracy print ( f 'Validation Accuracy: { acc . item () : .4f } ' ) Validation Accuracy: 100.0000","title":"From Scratch Logistic Regression Classification"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#logistic-regression-from-scratch","text":"This is an implementation of a simple logistic regression for binary class labels. We will be attempting to classify 2 flowers based on their petal width and height: setosa and versicolor.","title":"Logistic Regression from Scratch"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#imports","text":"% matplotlib inline import matplotlib.pyplot as plt import numpy as np import torch import torch.nn.functional as F from sklearn import datasets from sklearn import preprocessing from sklearn.model_selection import train_test_split from collections import Counter","title":"Imports"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#preparing-a-custom-2-class-iris-dataset","text":"","title":"Preparing a custom 2-class IRIS dataset"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#load-data","text":"# Instantiate dataset class and assign to object iris = datasets . load_iris () # Load features and target # Take only 2 classes, and 2 features (sepal length/width) X = iris . data [: - 50 , : 2 ] # For teaching the math rather than preprocessing techniques, # we'll be using this simple scaling method. However, you must # be cautious to scale your training/testing sets subsequently. X = preprocessing . scale ( X ) y = iris . target [: - 50 ]","title":"Load Data"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#print-data-details","text":"# 50 of each iris flower print ( Counter ( y )) # Type of flower print ( list ( iris . target_names [: - 1 ])) # Shape of features print ( X . shape ) Counter({0: 50, 1: 50}) ['setosa', 'versicolor'] (100, 2)","title":"Print Data Details"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#scatterplot-2-classes","text":"plt . scatter ( X [:, 0 ], X [:, 1 ], c = y );","title":"Scatterplot 2 Classes"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#traintest-split","text":"X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) print ( f 'X train size: { X_train . shape } ' ) print ( f 'X test size: { X_test . shape } ' ) print ( f 'y train size: { y_train . shape } ' ) print ( f 'y test size: { y_test . shape } ' ) # Distribution of both classes are roughly equal using train_test_split function print ( Counter ( y_train )) X train size: (80, 2) X test size: (20, 2) y train size: (80,) y test size: (20,) Counter({0: 41, 1: 39})","title":"Train/Test Split"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#math","text":"","title":"Math"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#1-forwardpropagation","text":"Get our logits and probabilities Affine function/transformation: z = \\theta x + b z = \\theta x + b Sigmoid/logistic function: \\hat y = \\frac{1}{1 + e^{-z}} \\hat y = \\frac{1}{1 + e^{-z}}","title":"1. Forwardpropagation"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#2-backwardpropagation","text":"Calculate gradients / partial derivatives w.r.t. weights and bias Loss: L = ylog(\\hat y) + (1-y) log (1 - \\hat y) L = ylog(\\hat y) + (1-y) log (1 - \\hat y) Partial derivative of loss w.r.t weights: \\frac{\\delta L}{\\delta w} =\\frac{\\delta L}{\\delta z} \\frac{\\delta z}{\\delta w} = (\\hat y - y)(x^T) \\frac{\\delta L}{\\delta w} =\\frac{\\delta L}{\\delta z} \\frac{\\delta z}{\\delta w} = (\\hat y - y)(x^T) Partial derivative of loss w.r.t. bias: \\frac{\\delta L}{\\delta b} = \\frac{\\delta L}{\\delta z} \\frac{\\delta z}{\\delta b} = (\\hat y - y)(1) \\frac{\\delta L}{\\delta b} = \\frac{\\delta L}{\\delta z} \\frac{\\delta z}{\\delta b} = (\\hat y - y)(1) \\frac{\\delta L}{\\delta z} = \\hat y - y \\frac{\\delta L}{\\delta z} = \\hat y - y \\frac{\\delta z}{\\delta w} = x \\frac{\\delta z}{\\delta w} = x \\frac{\\delta z}{\\delta b} = 1 \\frac{\\delta z}{\\delta b} = 1","title":"2. Backwardpropagation"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#2a-loss-function-clarification","text":"Actually, why is our loss equation L = ylog(\\hat y) + (1-y) log (1 - \\hat y) L = ylog(\\hat y) + (1-y) log (1 - \\hat y) ? We have given the intuition in the Logistic Regression tutorial on why it works. Here we will cover the derivation which essentially is merely maximizing the log likelihood of the parameters (maximizing the probability of our predicted output given our input and parameters Given: \\hat y = \\frac{1}{1 + e^{-z}} \\hat y = \\frac{1}{1 + e^{-z}} . Then: P(y=1 \\mid x;\\theta) = \\hat y P(y=1 \\mid x;\\theta) = \\hat y P(y=0 \\mid x;\\theta) = 1 - \\hat y P(y=0 \\mid x;\\theta) = 1 - \\hat y Simplified further: p(y \\mid x; \\theta) = (\\hat y)^y(1 - \\hat y)^{1-y} p(y \\mid x; \\theta) = (\\hat y)^y(1 - \\hat y)^{1-y} Given m training samples, the likelihood of the parameters is simply the product of probabilities: L(\\theta) = \\displaystyle \\prod_{i=1}^{m} p(y^i \\mid x^i; \\theta) L(\\theta) = \\displaystyle \\prod_{i=1}^{m} p(y^i \\mid x^i; \\theta) L(\\theta) = \\displaystyle \\prod_{i=1}^{m} (\\hat y^{i})^{y^i}(1 - \\hat y^{i})^{1-y^{i}} L(\\theta) = \\displaystyle \\prod_{i=1}^{m} (\\hat y^{i})^{y^i}(1 - \\hat y^{i})^{1-y^{i}} Essentially, we want to maximize the probability of our ouput given our input and parameters But it's easier to maximize the log likelihood, so we take the natural logarithm. L(\\theta) = \\displaystyle \\sum_{i=1}^{m} y^{i}log (\\hat y^{i}) + (1 - y^{i})log(1 - \\hat y^{i}) L(\\theta) = \\displaystyle \\sum_{i=1}^{m} y^{i}log (\\hat y^{i}) + (1 - y^{i})log(1 - \\hat y^{i}) Why is is easier to maximize the log likelihood? The natural logarithm is a function that monotonically increases. This allows us to find the \"max\" of the log likelihood easier compared to a non-monotonically increasing function (like a wave up and down).","title":"2a. Loss function clarification"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#3-gradient-descent-updating-weights","text":"w = w - \\alpha (\\hat y - y)(x^T) w = w - \\alpha (\\hat y - y)(x^T) b = b - \\alpha (\\hat y - y).1 b = b - \\alpha (\\hat y - y).1","title":"3. Gradient descent: updating weights"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#training-from-scratch","text":"learning_rate = 0.1 num_features = X . shape [ 1 ] weights = torch . zeros ( num_features , 1 , dtype = torch . float32 ) bias = torch . zeros ( 1 , dtype = torch . float32 ) X_train = torch . from_numpy ( X_train ) . type ( torch . float32 ) y_train = torch . from_numpy ( y_train ) . type ( torch . float32 ) for epoch in range ( num_epochs ): # 1. Forwardpropagation: # 1a. Affine Transformation: z = \\theta x + b z = torch . add ( torch . mm ( X_train , weights ), bias ) # 2a. Sigmoid/Logistic Function: y_hat = 1 / (1 + e^{-z}) y_hat = 1. / ( 1. + torch . exp ( - z )) # Backpropagation: # 1. Calculate binary cross entropy l = torch . mm ( - y_train . view ( 1 , - 1 ), torch . log ( y_hat )) - torch . mm (( 1 - y_train ) . view ( 1 , - 1 ), torch . log ( 1 - y_hat )) # 2. Calculate dl/dz dl_dz = y_train - y_hat . view ( - 1 ) # 2. Calculate partial derivative of cost w.r.t weights (gradients) # dl_dw = dl_dz dz_dw = (y_hat - y)(x^T) grad = torch . mm ( X_train . transpose ( 0 , 1 ), dl_dz . view ( - 1 , 1 )) # Gradient descent: # update our weights and bias with our gradients weights += learning_rate * grad bias += learning_rate * torch . sum ( dl_dz ) # Accuracy total = y_hat . shape [ 0 ] predicted = ( y_hat > 0.5 ) . float () . squeeze () correct = ( predicted == y_train ) . sum () acc = 100 * correct / total # Print accuracy and cost print ( f 'Epoch: { epoch } | Accuracy: { acc . item () : .4f } | Cost: { l . item () : .4f } ' ) print ( f 'Weights \\n { weights . data } ' ) print ( f 'Bias \\n { bias . data } ' ) X train size: (80, 2) X test size: (20, 2) y train size: (80,) y test size: (20,) Counter({1: 41, 0: 39}) Epoch: 0 | Accuracy: 48.0000 | Cost: 55.4518 Epoch: 1 | Accuracy: 100.0000 | Cost: 5.6060 Epoch: 2 | Accuracy: 100.0000 | Cost: 5.0319 Epoch: 3 | Accuracy: 100.0000 | Cost: 4.6001 Epoch: 4 | Accuracy: 100.0000 | Cost: 4.2595 Epoch: 5 | Accuracy: 100.0000 | Cost: 3.9819 Epoch: 6 | Accuracy: 100.0000 | Cost: 3.7498 Epoch: 7 | Accuracy: 100.0000 | Cost: 3.5521 Epoch: 8 | Accuracy: 100.0000 | Cost: 3.3810 Epoch: 9 | Accuracy: 100.0000 | Cost: 3.2310 Epoch: 10 | Accuracy: 100.0000 | Cost: 3.0981 Epoch: 11 | Accuracy: 100.0000 | Cost: 2.9794 Epoch: 12 | Accuracy: 100.0000 | Cost: 2.8724 Epoch: 13 | Accuracy: 100.0000 | Cost: 2.7754 Epoch: 14 | Accuracy: 100.0000 | Cost: 2.6869 Epoch: 15 | Accuracy: 100.0000 | Cost: 2.6057 Epoch: 16 | Accuracy: 100.0000 | Cost: 2.5308 Epoch: 17 | Accuracy: 100.0000 | Cost: 2.4616 Epoch: 18 | Accuracy: 100.0000 | Cost: 2.3973 Epoch: 19 | Accuracy: 100.0000 | Cost: 2.3374 Weights tensor([[ 4.9453], [-3.6849]]) Bias tensor([0.5570])","title":"Training from Scratch"},{"location":"deep_learning/fromscratch/fromscratch_logistic_regression/#inference","text":"# Port to tensors X_test = torch . from_numpy ( X_test ) . type ( torch . float32 ) y_test = torch . from_numpy ( y_test ) . type ( torch . float32 ) # 1. Forwardpropagation: # 1a. Affine Transformation: z = ax + b z = torch . add ( torch . mm ( X_test , weights ), bias ) # 2a. Sigmoid/Logistic Function: y_hat = 1 / (1 + e^{-z}) y_hat = 1. / ( 1. + torch . exp ( - z )) total = y_test . shape [ 0 ] predicted = ( y_hat > 0.5 ) . float () . squeeze () correct = ( predicted == y_test ) . sum () acc = 100 * correct / total # Print accuracy print ( f 'Validation Accuracy: { acc . item () : .4f } ' ) Validation Accuracy: 100.0000","title":"Inference"},{"location":"deep_learning/practical_pytorch/pytorch_autoencoder/","text":"Autoencoders with PyTorch \u00b6 Run Jupyter Notebook You can run the code for this section in this jupyter notebook link . About Autoencoders \u00b6 Feedforward Neural Network (FNN) to Autoencoders (AEs) \u00b6 Autoencoder is a form of unsupervised learning . This is a big deviation from what we have been doing: classification and regression which are under supervised learning. There are no labels required, inputs are used as labels. This is a bit mind-boggling for some, but there're many conrete use cases as you'll soon realize. Just a quick preview of use cases we will be covering: Denoising overcomplete AEs: recreate images without the random noises originally injected. Undercomplete AEs for anomaly detection: use AEs for credit card fraud detection via anomaly detection. Variational AEs for creating synthetic faces: with a convolutional VAEs, we can make fake faces. An autoencoder's purpose is to learn an approximation of the identity function (mapping x x to \\hat x \\hat x ). Essentially we are trying to learn a function that can take our input x x and recreate it \\hat x \\hat x . Technically we can do an exact recreation of our in-sample input if we use a very wide and deep neural network. Undercomplete and Overcomplete Autoencoders \u00b6 When we highlighted some use cases, did you notice how we mentioned undercomplete and autocomplete AEs? The only difference between the two is in the encoding output's size. In the diagram above, this refers to the encoding output's size after our first affine function (yellow box) and non-linear function (pink box). Undercomplete AEs: smaller This is when our encoding output's dimension is smaller than our input's dimension. Essentially we reduced the dimension of our data (dimensionality reduction) with an undercomplete AE Overcomplete AEs: larger This is when our encoding output's dimension is larger than our input's dimension Essentially we increased the dimension of our data with an overcomplete AE Fully-connected and Convolutional Autoencoders \u00b6 Another important point is that, in our diagram we've used the example of our Feedforward Neural Networks (FNN) where we use fully-connected layers. This is called Fully-connected AE. However, we can easily swap those fully-connected layers with convolutional layers. This is called Convolutional AE. Autoencoders Series \u00b6 We'll be covering a series of autoencoders in this order Fully-connected Overcomplete Autoencoder (AEs): Denoising Images Fully-connected Undercomplete Autoencoder (AEs): Credit Card Fraud Detection Convolutional Overcomplete Variational Autoencoder (VAEs): Generate Fake Human Faces Convolutional Overcomplete Adversarial Autoencoder (AAEs): Generate Fake Human Faces Generative Adversarial Networks (GANs): Generate Better Fake Human Faces","title":"Autoencoders (AE)"},{"location":"deep_learning/practical_pytorch/pytorch_autoencoder/#autoencoders-with-pytorch","text":"Run Jupyter Notebook You can run the code for this section in this jupyter notebook link .","title":"Autoencoders with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_autoencoder/#about-autoencoders","text":"","title":"About Autoencoders"},{"location":"deep_learning/practical_pytorch/pytorch_autoencoder/#feedforward-neural-network-fnn-to-autoencoders-aes","text":"Autoencoder is a form of unsupervised learning . This is a big deviation from what we have been doing: classification and regression which are under supervised learning. There are no labels required, inputs are used as labels. This is a bit mind-boggling for some, but there're many conrete use cases as you'll soon realize. Just a quick preview of use cases we will be covering: Denoising overcomplete AEs: recreate images without the random noises originally injected. Undercomplete AEs for anomaly detection: use AEs for credit card fraud detection via anomaly detection. Variational AEs for creating synthetic faces: with a convolutional VAEs, we can make fake faces. An autoencoder's purpose is to learn an approximation of the identity function (mapping x x to \\hat x \\hat x ). Essentially we are trying to learn a function that can take our input x x and recreate it \\hat x \\hat x . Technically we can do an exact recreation of our in-sample input if we use a very wide and deep neural network.","title":"Feedforward Neural Network (FNN) to Autoencoders (AEs)"},{"location":"deep_learning/practical_pytorch/pytorch_autoencoder/#undercomplete-and-overcomplete-autoencoders","text":"When we highlighted some use cases, did you notice how we mentioned undercomplete and autocomplete AEs? The only difference between the two is in the encoding output's size. In the diagram above, this refers to the encoding output's size after our first affine function (yellow box) and non-linear function (pink box). Undercomplete AEs: smaller This is when our encoding output's dimension is smaller than our input's dimension. Essentially we reduced the dimension of our data (dimensionality reduction) with an undercomplete AE Overcomplete AEs: larger This is when our encoding output's dimension is larger than our input's dimension Essentially we increased the dimension of our data with an overcomplete AE","title":"Undercomplete and Overcomplete Autoencoders"},{"location":"deep_learning/practical_pytorch/pytorch_autoencoder/#fully-connected-and-convolutional-autoencoders","text":"Another important point is that, in our diagram we've used the example of our Feedforward Neural Networks (FNN) where we use fully-connected layers. This is called Fully-connected AE. However, we can easily swap those fully-connected layers with convolutional layers. This is called Convolutional AE.","title":"Fully-connected and Convolutional Autoencoders"},{"location":"deep_learning/practical_pytorch/pytorch_autoencoder/#autoencoders-series","text":"We'll be covering a series of autoencoders in this order Fully-connected Overcomplete Autoencoder (AEs): Denoising Images Fully-connected Undercomplete Autoencoder (AEs): Credit Card Fraud Detection Convolutional Overcomplete Variational Autoencoder (VAEs): Generate Fake Human Faces Convolutional Overcomplete Adversarial Autoencoder (AAEs): Generate Fake Human Faces Generative Adversarial Networks (GANs): Generate Better Fake Human Faces","title":"Autoencoders Series"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/","text":"Convolutional Neural Network with PyTorch \u00b6 Run Jupyter Notebook You can run the code for this section in this jupyter notebook link . About Convolutional Neural Network \u00b6 Transition From Feedforward Neural Network \u00b6 Hidden Layer Feedforward Neural Network \u00b6 Recap of FNN So let's do a recap of what we covered in the Feedforward Neural Network (FNN) section using a simple FNN with 1 hidden layer (a pair of affine function and non-linear function) [Yellow box] Pass input into an affine function \\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b} \\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b} [Pink box] Pass logits to non-linear function, for example sigmoid, tanh (hyperbolic tangent), ReLU, or LeakyReLU [Blue box] Pass output of non-linear function to another affine function [Red box] Pass output of final affine function to softmax function to get our probability distribution over K classes [Purple box] Finally we can get our loss by using our cross entropy function Basic Convolutional Neural Network (CNN) \u00b6 A basic CNN just requires 2 additional layers! Convolution and pooling layers before our feedforward neural network Fully Connected (FC) Layer A layer with an affine function & non-linear function is called a Fully Connected (FC) layer One Convolutional Layer: High Level View \u00b6 One Convolutional Layer: High Level View Summary \u00b6 As the kernel is sliding/convolving across the image \\rightarrow \\rightarrow 2 operations done per patch Element-wise multiplication Summation More kernels = = more feature map channels Can capture more information about the input Multiple Convolutional Layers: High Level View \u00b6 Pooling Layer: High Level View \u00b6 2 Common Types Max Pooling Average Pooling Multiple Pooling Layers: High Level View \u00b6 Padding \u00b6 Padding Summary \u00b6 Valid Padding (No Padding) Output size < Input Size Same Padding (Zero Padding) Output size = Input Size Dimension Calculations \u00b6 O = \\frac {W - K + 2P}{S} + 1 O = \\frac {W - K + 2P}{S} + 1 O O : output height/length W W : input height/length K K : filter size (kernel size) P P : padding P = \\frac{K - 1}{2} P = \\frac{K - 1}{2} S S : stride Example 1: Output Dimension Calculation for Valid Padding \u00b6 W = 4 W = 4 K = 3 K = 3 P = 0 P = 0 S = 1 S = 1 O = \\frac {4 - 3 + 2*0}{1} + 1 = \\frac {1}{1} + 1 = 1 + 1 = 2 O = \\frac {4 - 3 + 2*0}{1} + 1 = \\frac {1}{1} + 1 = 1 + 1 = 2 Example 2: Output Dimension Calculation for Same Padding \u00b6 W = 5 W = 5 K = 3 K = 3 P = \\frac{3 - 1}{2} = \\frac{2}{2} = 1 P = \\frac{3 - 1}{2} = \\frac{2}{2} = 1 S = 1 S = 1 O = \\frac {5 - 3 + 2*1}{1} + 1 = \\frac {4}{1} + 1 = 5 O = \\frac {5 - 3 + 2*1}{1} + 1 = \\frac {4}{1} + 1 = 5 Building a Convolutional Neural Network with PyTorch \u00b6 Model A: \u00b6 2 Convolutional Layers Same Padding (same output size) 2 Max Pooling Layers 1 Fully Connected Layer Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Step 1: Loading MNIST Train Dataset \u00b6 Images from 1 to 9 MNIST Dataset and Size of Training Dataset (Excluding Labels) import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) print ( train_dataset . train_data . size ()) torch.Size ([ 60000 , 28 , 28 ]) Size of our training dataset labels print ( train_dataset . train_labels . size ()) torch.Size ([ 60000 ]) Size of our testing dataset (excluding labels) print ( test_dataset . test_data . size ()) torch.Size ([ 10000 , 28 , 28 ]) Size of our testing dataset labels print ( test_dataset . test_labels . size ()) torch.Size ([ 10000 ]) Step 2: Make Dataset Iterable \u00b6 Load Dataset into Dataloader batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) Step 3: Create Model Class \u00b6 Output Formula for Convolution \u00b6 O = \\frac {W - K + 2P}{S} + 1 O = \\frac {W - K + 2P}{S} + 1 O O : output height/length W W : input height/length K K : filter size (kernel size) = 5 P P : same padding (non-zero) P = \\frac{K - 1}{2} = \\frac{5 - 1}{2} = 2 P = \\frac{K - 1}{2} = \\frac{5 - 1}{2} = 2 S S : stride = 1 Output Formula for Pooling \u00b6 O = \\frac {W - K}{S} + 1 O = \\frac {W - K}{S} + 1 W: input height/width K: filter size = 2 S: stride size = filter size , PyTorch defaults the stride to kernel filter size If using PyTorch default stride, this will result in the formula O = \\frac {W}{K} O = \\frac {W}{K} By default, in our tutorials, we do this for simplicity. Define our simple 2 convolutional layer CNN class CNNModel ( nn . Module ): def __init__ ( self ): super ( CNNModel , self ) . __init__ () # Convolution 1 self . cnn1 = nn . Conv2d ( in_channels = 1 , out_channels = 16 , kernel_size = 5 , stride = 1 , padding = 2 ) self . relu1 = nn . ReLU () # Max pool 1 self . maxpool1 = nn . MaxPool2d ( kernel_size = 2 ) # Convolution 2 self . cnn2 = nn . Conv2d ( in_channels = 16 , out_channels = 32 , kernel_size = 5 , stride = 1 , padding = 2 ) self . relu2 = nn . ReLU () # Max pool 2 self . maxpool2 = nn . MaxPool2d ( kernel_size = 2 ) # Fully connected 1 (readout) self . fc1 = nn . Linear ( 32 * 7 * 7 , 10 ) def forward ( self , x ): # Convolution 1 out = self . cnn1 ( x ) out = self . relu1 ( out ) # Max pool 1 out = self . maxpool1 ( out ) # Convolution 2 out = self . cnn2 ( out ) out = self . relu2 ( out ) # Max pool 2 out = self . maxpool2 ( out ) # Resize # Original size: (100, 32, 7, 7) # out.size(0): 100 # New out size: (100, 32*7*7) out = out . view ( out . size ( 0 ), - 1 ) # Linear function (readout) out = self . fc1 ( out ) return out Step 4: Instantiate Model Class \u00b6 Our model model = CNNModel () Step 5: Instantiate Loss Class \u00b6 Convolutional Neural Network: Cross Entropy Loss Feedforward Neural Network : Cross Entropy Loss Logistic Regression : Cross Entropy Loss Linear Regression : MSE Our cross entropy loss criterion = nn . CrossEntropyLoss () Step 6: Instantiate Optimizer Class \u00b6 Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : parameters' gradients Even simplier equation parameters = parameters - learning_rate * parameters_gradients At every iteration, we update our model's parameters Optimizer learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) Parameters In-Depth \u00b6 Print model's parameter print ( model . parameters ()) print ( len ( list ( model . parameters ()))) # Convolution 1: 16 Kernels print ( list ( model . parameters ())[ 0 ] . size ()) # Convolution 1 Bias: 16 Kernels print ( list ( model . parameters ())[ 1 ] . size ()) # Convolution 2: 32 Kernels with depth = 16 print ( list ( model . parameters ())[ 2 ] . size ()) # Convolution 2 Bias: 32 Kernels with depth = 16 print ( list ( model . parameters ())[ 3 ] . size ()) # Fully Connected Layer 1 print ( list ( model . parameters ())[ 4 ] . size ()) # Fully Connected Layer Bias print ( list ( model . parameters ())[ 5 ] . size ()) <generator object Module.parameters at 0x7f9864363c50> 6 torch.Size ([ 16 , 1 , 5 , 5 ]) torch.Size ([ 16 ]) torch.Size ([ 32 , 16 , 5 , 5 ]) torch.Size ([ 32 ]) torch.Size ([ 10 , 1568 ]) torch.Size ([ 10 ]) Step 7: Train Model \u00b6 Process Convert inputs to tensors with gradient accumulation abilities CNN Input: (1, 28, 28) Feedforward NN Input: (1, 28*28) Clear gradient buffets Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT Model training iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images images = images . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images images = images . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration: 500. Loss: 0.43324267864227295. Accuracy: 90 Iteration: 1000. Loss: 0.2511480152606964. Accuracy: 92 Iteration: 1500. Loss: 0.13431282341480255. Accuracy: 94 Iteration: 2000. Loss: 0.11173319816589355. Accuracy: 95 Iteration: 2500. Loss: 0.06409914791584015. Accuracy: 96 Iteration: 3000. Loss: 0.14377528429031372. Accuracy: 96 Model B: \u00b6 2 Convolutional Layers Same Padding (same output size) 2 Average Pooling Layers 1 Fully Connected Layer Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 2 Conv + 2 Average Pool + 1 FC (Zero Padding, Same Padding) import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class CNNModel ( nn . Module ): def __init__ ( self ): super ( CNNModel , self ) . __init__ () # Convolution 1 self . cnn1 = nn . Conv2d ( in_channels = 1 , out_channels = 16 , kernel_size = 5 , stride = 1 , padding = 2 ) self . relu1 = nn . ReLU () # Average pool 1 self . avgpool1 = nn . AvgPool2d ( kernel_size = 2 ) # Convolution 2 self . cnn2 = nn . Conv2d ( in_channels = 16 , out_channels = 32 , kernel_size = 5 , stride = 1 , padding = 2 ) self . relu2 = nn . ReLU () # Average pool 2 self . avgpool2 = nn . AvgPool2d ( kernel_size = 2 ) # Fully connected 1 (readout) self . fc1 = nn . Linear ( 32 * 7 * 7 , 10 ) def forward ( self , x ): # Convolution 1 out = self . cnn1 ( x ) out = self . relu1 ( out ) # Average pool 1 out = self . avgpool1 ( out ) # Convolution 2 out = self . cnn2 ( out ) out = self . relu2 ( out ) # Max pool 2 out = self . avgpool2 ( out ) # Resize # Original size: (100, 32, 7, 7) # out.size(0): 100 # New out size: (100, 32*7*7) out = out . view ( out . size ( 0 ), - 1 ) # Linear function (readout) out = self . fc1 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' model = CNNModel () ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to tensors with gradient accumulation abilities images = images . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration: 500 . Loss: 0 .6850348711013794. Accuracy: 85 Iteration: 1000 . Loss: 0 .36549052596092224. Accuracy: 88 Iteration: 1500 . Loss: 0 .31540098786354065. Accuracy: 89 Iteration: 2000 . Loss: 0 .3522164225578308. Accuracy: 90 Iteration: 2500 . Loss: 0 .2680729925632477. Accuracy: 91 Iteration: 3000 . Loss: 0 .26440390944480896. Accuracy: 92 Comparison of accuracies It seems like average pooling test accuracy is less than the max pooling accuracy! Does this mean average pooling is better? This is not definitive and depends on a lot of factors including the model's architecture, seed (that affects random weight initialization) and more. Model C: \u00b6 2 Convolutional Layers Valid Padding (smaller output size) 2 Max Pooling Layers 1 Fully Connected Layer Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 2 Conv + 2 Max Pool + 1 FC (Valid Padding, No Padding) import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class CNNModel ( nn . Module ): def __init__ ( self ): super ( CNNModel , self ) . __init__ () # Convolution 1 self . cnn1 = nn . Conv2d ( in_channels = 1 , out_channels = 16 , kernel_size = 5 , stride = 1 , padding = 0 ) self . relu1 = nn . ReLU () # Max pool 1 self . maxpool1 = nn . MaxPool2d ( kernel_size = 2 ) # Convolution 2 self . cnn2 = nn . Conv2d ( in_channels = 16 , out_channels = 32 , kernel_size = 5 , stride = 1 , padding = 0 ) self . relu2 = nn . ReLU () # Max pool 2 self . maxpool2 = nn . MaxPool2d ( kernel_size = 2 ) # Fully connected 1 (readout) self . fc1 = nn . Linear ( 32 * 4 * 4 , 10 ) def forward ( self , x ): # Convolution 1 out = self . cnn1 ( x ) out = self . relu1 ( out ) # Max pool 1 out = self . maxpool1 ( out ) # Convolution 2 out = self . cnn2 ( out ) out = self . relu2 ( out ) # Max pool 2 out = self . maxpool2 ( out ) # Resize # Original size: (100, 32, 7, 7) # out.size(0): 100 # New out size: (100, 32*7*7) out = out . view ( out . size ( 0 ), - 1 ) # Linear function (readout) out = self . fc1 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' model = CNNModel () ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to tensors with gradient accumulation abilities images = images . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration: 500 . Loss: 0 .5153220295906067. Accuracy: 88 Iteration: 1000 . Loss: 0 .28784745931625366. Accuracy: 92 Iteration: 1500 . Loss: 0 .4086027443408966. Accuracy: 94 Iteration: 2000 . Loss: 0 .09390712529420853. Accuracy: 95 Iteration: 2500 . Loss: 0 .07138358801603317. Accuracy: 95 Iteration: 3000 . Loss: 0 .05396252125501633. Accuracy: 96 Summary of Results \u00b6 Model A Model B Model C Max Pooling Average Pooling Max Pooling Same Padding Same Padding Valid Padding 97.04% 93.59% 96.5% All Models INPUT \\rightarrow \\rightarrow CONV \\rightarrow \\rightarrow POOL \\rightarrow \\rightarrow CONV \\rightarrow \\rightarrow POOL \\rightarrow \\rightarrow FC Convolution Kernel Size = 5 x 5 Convolution Kernel Stride = 1 Pooling Kernel Size = 2 x 2 General Deep Learning Notes on CNN and FNN \u00b6 3 ways to expand a convolutional neural network More convolutional layers Less aggressive downsampling Smaller kernel size for pooling (gradually downsampling) More fully connected layers Cons Need a larger dataset Curse of dimensionality Does not necessarily mean higher accuracy 3. Building a Convolutional Neural Network with PyTorch (GPU) \u00b6 Model A \u00b6 GPU: 2 things must be on GPU - model - tensors Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 2 Conv + 2 Max Pooling + 1 FC (Same Padding, Zero Padding) import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class CNNModel ( nn . Module ): def __init__ ( self ): super ( CNNModel , self ) . __init__ () # Convolution 1 self . cnn1 = nn . Conv2d ( in_channels = 1 , out_channels = 16 , kernel_size = 5 , stride = 1 , padding = 0 ) self . relu1 = nn . ReLU () # Max pool 1 self . maxpool1 = nn . MaxPool2d ( kernel_size = 2 ) # Convolution 2 self . cnn2 = nn . Conv2d ( in_channels = 16 , out_channels = 32 , kernel_size = 5 , stride = 1 , padding = 0 ) self . relu2 = nn . ReLU () # Max pool 2 self . maxpool2 = nn . MaxPool2d ( kernel_size = 2 ) # Fully connected 1 (readout) self . fc1 = nn . Linear ( 32 * 4 * 4 , 10 ) def forward ( self , x ): # Convolution 1 out = self . cnn1 ( x ) out = self . relu1 ( out ) # Max pool 1 out = self . maxpool1 ( out ) # Convolution 2 out = self . cnn2 ( out ) out = self . relu2 ( out ) # Max pool 2 out = self . maxpool2 ( out ) # Resize # Original size: (100, 32, 7, 7) # out.size(0): 100 # New out size: (100, 32*7*7) out = out . view ( out . size ( 0 ), - 1 ) # Linear function (readout) out = self . fc1 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' model = CNNModel () ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): ####################### # USE GPU FOR MODEL # ####################### images = images . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : ####################### # USE GPU FOR MODEL # ####################### images = images . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) ####################### # USE GPU FOR MODEL # ####################### # Total correct predictions if torch . cuda . is_available (): correct += ( predicted . cpu () == labels . cpu ()) . sum () else : correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration: 500 . Loss: 0 .36831170320510864. Accuracy: 88 Iteration: 1000 . Loss: 0 .31790846586227417. Accuracy: 92 Iteration: 1500 . Loss: 0 .1510857343673706. Accuracy: 94 Iteration: 2000 . Loss: 0 .08368007838726044. Accuracy: 95 Iteration: 2500 . Loss: 0 .13419771194458008. Accuracy: 96 Iteration: 3000 . Loss: 0 .16750787198543549. Accuracy: 96 More Efficient Convolutions via Toeplitz Matrices This is beyond the scope of this particular lesson. But now that we understand how convolutions work, it is critical to know that it is quite an inefficient operation if we use for-loops to perform our 2D convolutions (5 x 5 convolution kernel size for example) on our 2D images (28 x 28 MNIST image for example). A more efficient implementation is in converting our convolution kernel into a doubly block circulant/Toeplitz matrix (special case Toeplitz matrix) and our image (input) into a vector. Then, we will do just one matrix operation using our doulby block Toeplitz matrix and our input vector. There will be a whole lesson dedicated to this operation released down the road. Summary \u00b6 We've learnt to... Success Transition from Feedforward Neural Network Addition of Convolutional & Pooling Layers before Linear Layers One Convolutional Layer Basics One Pooling Layer Basics Max pooling Average pooling Padding Output Dimension Calculations and Examples O = \\frac {W - K + 2P}{S} + 1 O = \\frac {W - K + 2P}{S} + 1 Convolutional Neural Networks Model A : 2 Conv + 2 Max pool + 1 FC Same Padding Model B : 2 Conv + 2 Average pool + 1 FC Same Padding Model C : 2 Conv + 2 Max pool + 1 FC Valid Padding Model Variation in Code Modifying only step 3 Ways to Expand Model\u2019s Capacity More convolutions Gradual pooling More fully connected layers GPU Code 2 things on GPU model tensors with gradient accumulation abilities Modifying only Step 4 & Step 7 7 Step Model Building Recap Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Convolutional Neural Networks (CNN)"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#convolutional-neural-network-with-pytorch","text":"Run Jupyter Notebook You can run the code for this section in this jupyter notebook link .","title":"Convolutional Neural Network with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#about-convolutional-neural-network","text":"","title":"About Convolutional Neural Network"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#transition-from-feedforward-neural-network","text":"","title":"Transition From Feedforward Neural Network"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#hidden-layer-feedforward-neural-network","text":"Recap of FNN So let's do a recap of what we covered in the Feedforward Neural Network (FNN) section using a simple FNN with 1 hidden layer (a pair of affine function and non-linear function) [Yellow box] Pass input into an affine function \\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b} \\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b} [Pink box] Pass logits to non-linear function, for example sigmoid, tanh (hyperbolic tangent), ReLU, or LeakyReLU [Blue box] Pass output of non-linear function to another affine function [Red box] Pass output of final affine function to softmax function to get our probability distribution over K classes [Purple box] Finally we can get our loss by using our cross entropy function","title":"Hidden Layer Feedforward Neural Network"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#basic-convolutional-neural-network-cnn","text":"A basic CNN just requires 2 additional layers! Convolution and pooling layers before our feedforward neural network Fully Connected (FC) Layer A layer with an affine function & non-linear function is called a Fully Connected (FC) layer","title":"Basic Convolutional Neural Network (CNN)"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#one-convolutional-layer-high-level-view","text":"","title":"One Convolutional Layer: High Level View"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#one-convolutional-layer-high-level-view-summary","text":"As the kernel is sliding/convolving across the image \\rightarrow \\rightarrow 2 operations done per patch Element-wise multiplication Summation More kernels = = more feature map channels Can capture more information about the input","title":"One Convolutional Layer: High Level View Summary"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#multiple-convolutional-layers-high-level-view","text":"","title":"Multiple Convolutional Layers: High Level View"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#pooling-layer-high-level-view","text":"2 Common Types Max Pooling Average Pooling","title":"Pooling Layer: High Level View"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#multiple-pooling-layers-high-level-view","text":"","title":"Multiple Pooling Layers: High Level View"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#padding","text":"","title":"Padding"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#padding-summary","text":"Valid Padding (No Padding) Output size < Input Size Same Padding (Zero Padding) Output size = Input Size","title":"Padding Summary"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#dimension-calculations","text":"O = \\frac {W - K + 2P}{S} + 1 O = \\frac {W - K + 2P}{S} + 1 O O : output height/length W W : input height/length K K : filter size (kernel size) P P : padding P = \\frac{K - 1}{2} P = \\frac{K - 1}{2} S S : stride","title":"Dimension Calculations"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#example-1-output-dimension-calculation-for-valid-padding","text":"W = 4 W = 4 K = 3 K = 3 P = 0 P = 0 S = 1 S = 1 O = \\frac {4 - 3 + 2*0}{1} + 1 = \\frac {1}{1} + 1 = 1 + 1 = 2 O = \\frac {4 - 3 + 2*0}{1} + 1 = \\frac {1}{1} + 1 = 1 + 1 = 2","title":"Example 1: Output Dimension Calculation for Valid Padding"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#example-2-output-dimension-calculation-for-same-padding","text":"W = 5 W = 5 K = 3 K = 3 P = \\frac{3 - 1}{2} = \\frac{2}{2} = 1 P = \\frac{3 - 1}{2} = \\frac{2}{2} = 1 S = 1 S = 1 O = \\frac {5 - 3 + 2*1}{1} + 1 = \\frac {4}{1} + 1 = 5 O = \\frac {5 - 3 + 2*1}{1} + 1 = \\frac {4}{1} + 1 = 5","title":"Example 2: Output Dimension Calculation for Same Padding"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#building-a-convolutional-neural-network-with-pytorch","text":"","title":"Building a Convolutional Neural Network with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#model-a","text":"2 Convolutional Layers Same Padding (same output size) 2 Max Pooling Layers 1 Fully Connected Layer","title":"Model A:"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#steps","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#step-1-loading-mnist-train-dataset","text":"Images from 1 to 9 MNIST Dataset and Size of Training Dataset (Excluding Labels) import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) print ( train_dataset . train_data . size ()) torch.Size ([ 60000 , 28 , 28 ]) Size of our training dataset labels print ( train_dataset . train_labels . size ()) torch.Size ([ 60000 ]) Size of our testing dataset (excluding labels) print ( test_dataset . test_data . size ()) torch.Size ([ 10000 , 28 , 28 ]) Size of our testing dataset labels print ( test_dataset . test_labels . size ()) torch.Size ([ 10000 ])","title":"Step 1: Loading MNIST Train Dataset"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#step-2-make-dataset-iterable","text":"Load Dataset into Dataloader batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False )","title":"Step 2: Make Dataset Iterable"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#step-3-create-model-class","text":"","title":"Step 3: Create Model Class"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#output-formula-for-convolution","text":"O = \\frac {W - K + 2P}{S} + 1 O = \\frac {W - K + 2P}{S} + 1 O O : output height/length W W : input height/length K K : filter size (kernel size) = 5 P P : same padding (non-zero) P = \\frac{K - 1}{2} = \\frac{5 - 1}{2} = 2 P = \\frac{K - 1}{2} = \\frac{5 - 1}{2} = 2 S S : stride = 1","title":"Output Formula for Convolution"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#output-formula-for-pooling","text":"O = \\frac {W - K}{S} + 1 O = \\frac {W - K}{S} + 1 W: input height/width K: filter size = 2 S: stride size = filter size , PyTorch defaults the stride to kernel filter size If using PyTorch default stride, this will result in the formula O = \\frac {W}{K} O = \\frac {W}{K} By default, in our tutorials, we do this for simplicity. Define our simple 2 convolutional layer CNN class CNNModel ( nn . Module ): def __init__ ( self ): super ( CNNModel , self ) . __init__ () # Convolution 1 self . cnn1 = nn . Conv2d ( in_channels = 1 , out_channels = 16 , kernel_size = 5 , stride = 1 , padding = 2 ) self . relu1 = nn . ReLU () # Max pool 1 self . maxpool1 = nn . MaxPool2d ( kernel_size = 2 ) # Convolution 2 self . cnn2 = nn . Conv2d ( in_channels = 16 , out_channels = 32 , kernel_size = 5 , stride = 1 , padding = 2 ) self . relu2 = nn . ReLU () # Max pool 2 self . maxpool2 = nn . MaxPool2d ( kernel_size = 2 ) # Fully connected 1 (readout) self . fc1 = nn . Linear ( 32 * 7 * 7 , 10 ) def forward ( self , x ): # Convolution 1 out = self . cnn1 ( x ) out = self . relu1 ( out ) # Max pool 1 out = self . maxpool1 ( out ) # Convolution 2 out = self . cnn2 ( out ) out = self . relu2 ( out ) # Max pool 2 out = self . maxpool2 ( out ) # Resize # Original size: (100, 32, 7, 7) # out.size(0): 100 # New out size: (100, 32*7*7) out = out . view ( out . size ( 0 ), - 1 ) # Linear function (readout) out = self . fc1 ( out ) return out","title":"Output Formula for Pooling"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#step-4-instantiate-model-class","text":"Our model model = CNNModel ()","title":"Step 4: Instantiate Model Class"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#step-5-instantiate-loss-class","text":"Convolutional Neural Network: Cross Entropy Loss Feedforward Neural Network : Cross Entropy Loss Logistic Regression : Cross Entropy Loss Linear Regression : MSE Our cross entropy loss criterion = nn . CrossEntropyLoss ()","title":"Step 5: Instantiate Loss Class"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#step-6-instantiate-optimizer-class","text":"Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : parameters' gradients Even simplier equation parameters = parameters - learning_rate * parameters_gradients At every iteration, we update our model's parameters Optimizer learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate )","title":"Step 6: Instantiate Optimizer Class"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#parameters-in-depth","text":"Print model's parameter print ( model . parameters ()) print ( len ( list ( model . parameters ()))) # Convolution 1: 16 Kernels print ( list ( model . parameters ())[ 0 ] . size ()) # Convolution 1 Bias: 16 Kernels print ( list ( model . parameters ())[ 1 ] . size ()) # Convolution 2: 32 Kernels with depth = 16 print ( list ( model . parameters ())[ 2 ] . size ()) # Convolution 2 Bias: 32 Kernels with depth = 16 print ( list ( model . parameters ())[ 3 ] . size ()) # Fully Connected Layer 1 print ( list ( model . parameters ())[ 4 ] . size ()) # Fully Connected Layer Bias print ( list ( model . parameters ())[ 5 ] . size ()) <generator object Module.parameters at 0x7f9864363c50> 6 torch.Size ([ 16 , 1 , 5 , 5 ]) torch.Size ([ 16 ]) torch.Size ([ 32 , 16 , 5 , 5 ]) torch.Size ([ 32 ]) torch.Size ([ 10 , 1568 ]) torch.Size ([ 10 ])","title":"Parameters In-Depth"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#step-7-train-model","text":"Process Convert inputs to tensors with gradient accumulation abilities CNN Input: (1, 28, 28) Feedforward NN Input: (1, 28*28) Clear gradient buffets Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT Model training iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images images = images . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images images = images . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration: 500. Loss: 0.43324267864227295. Accuracy: 90 Iteration: 1000. Loss: 0.2511480152606964. Accuracy: 92 Iteration: 1500. Loss: 0.13431282341480255. Accuracy: 94 Iteration: 2000. Loss: 0.11173319816589355. Accuracy: 95 Iteration: 2500. Loss: 0.06409914791584015. Accuracy: 96 Iteration: 3000. Loss: 0.14377528429031372. Accuracy: 96","title":"Step 7: Train Model"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#model-b","text":"2 Convolutional Layers Same Padding (same output size) 2 Average Pooling Layers 1 Fully Connected Layer","title":"Model B:"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#steps_1","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 2 Conv + 2 Average Pool + 1 FC (Zero Padding, Same Padding) import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class CNNModel ( nn . Module ): def __init__ ( self ): super ( CNNModel , self ) . __init__ () # Convolution 1 self . cnn1 = nn . Conv2d ( in_channels = 1 , out_channels = 16 , kernel_size = 5 , stride = 1 , padding = 2 ) self . relu1 = nn . ReLU () # Average pool 1 self . avgpool1 = nn . AvgPool2d ( kernel_size = 2 ) # Convolution 2 self . cnn2 = nn . Conv2d ( in_channels = 16 , out_channels = 32 , kernel_size = 5 , stride = 1 , padding = 2 ) self . relu2 = nn . ReLU () # Average pool 2 self . avgpool2 = nn . AvgPool2d ( kernel_size = 2 ) # Fully connected 1 (readout) self . fc1 = nn . Linear ( 32 * 7 * 7 , 10 ) def forward ( self , x ): # Convolution 1 out = self . cnn1 ( x ) out = self . relu1 ( out ) # Average pool 1 out = self . avgpool1 ( out ) # Convolution 2 out = self . cnn2 ( out ) out = self . relu2 ( out ) # Max pool 2 out = self . avgpool2 ( out ) # Resize # Original size: (100, 32, 7, 7) # out.size(0): 100 # New out size: (100, 32*7*7) out = out . view ( out . size ( 0 ), - 1 ) # Linear function (readout) out = self . fc1 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' model = CNNModel () ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to tensors with gradient accumulation abilities images = images . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration: 500 . Loss: 0 .6850348711013794. Accuracy: 85 Iteration: 1000 . Loss: 0 .36549052596092224. Accuracy: 88 Iteration: 1500 . Loss: 0 .31540098786354065. Accuracy: 89 Iteration: 2000 . Loss: 0 .3522164225578308. Accuracy: 90 Iteration: 2500 . Loss: 0 .2680729925632477. Accuracy: 91 Iteration: 3000 . Loss: 0 .26440390944480896. Accuracy: 92 Comparison of accuracies It seems like average pooling test accuracy is less than the max pooling accuracy! Does this mean average pooling is better? This is not definitive and depends on a lot of factors including the model's architecture, seed (that affects random weight initialization) and more.","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#model-c","text":"2 Convolutional Layers Valid Padding (smaller output size) 2 Max Pooling Layers 1 Fully Connected Layer","title":"Model C:"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#steps_2","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 2 Conv + 2 Max Pool + 1 FC (Valid Padding, No Padding) import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class CNNModel ( nn . Module ): def __init__ ( self ): super ( CNNModel , self ) . __init__ () # Convolution 1 self . cnn1 = nn . Conv2d ( in_channels = 1 , out_channels = 16 , kernel_size = 5 , stride = 1 , padding = 0 ) self . relu1 = nn . ReLU () # Max pool 1 self . maxpool1 = nn . MaxPool2d ( kernel_size = 2 ) # Convolution 2 self . cnn2 = nn . Conv2d ( in_channels = 16 , out_channels = 32 , kernel_size = 5 , stride = 1 , padding = 0 ) self . relu2 = nn . ReLU () # Max pool 2 self . maxpool2 = nn . MaxPool2d ( kernel_size = 2 ) # Fully connected 1 (readout) self . fc1 = nn . Linear ( 32 * 4 * 4 , 10 ) def forward ( self , x ): # Convolution 1 out = self . cnn1 ( x ) out = self . relu1 ( out ) # Max pool 1 out = self . maxpool1 ( out ) # Convolution 2 out = self . cnn2 ( out ) out = self . relu2 ( out ) # Max pool 2 out = self . maxpool2 ( out ) # Resize # Original size: (100, 32, 7, 7) # out.size(0): 100 # New out size: (100, 32*7*7) out = out . view ( out . size ( 0 ), - 1 ) # Linear function (readout) out = self . fc1 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' model = CNNModel () ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to tensors with gradient accumulation abilities images = images . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration: 500 . Loss: 0 .5153220295906067. Accuracy: 88 Iteration: 1000 . Loss: 0 .28784745931625366. Accuracy: 92 Iteration: 1500 . Loss: 0 .4086027443408966. Accuracy: 94 Iteration: 2000 . Loss: 0 .09390712529420853. Accuracy: 95 Iteration: 2500 . Loss: 0 .07138358801603317. Accuracy: 95 Iteration: 3000 . Loss: 0 .05396252125501633. Accuracy: 96","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#summary-of-results","text":"Model A Model B Model C Max Pooling Average Pooling Max Pooling Same Padding Same Padding Valid Padding 97.04% 93.59% 96.5% All Models INPUT \\rightarrow \\rightarrow CONV \\rightarrow \\rightarrow POOL \\rightarrow \\rightarrow CONV \\rightarrow \\rightarrow POOL \\rightarrow \\rightarrow FC Convolution Kernel Size = 5 x 5 Convolution Kernel Stride = 1 Pooling Kernel Size = 2 x 2","title":"Summary of Results"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#general-deep-learning-notes-on-cnn-and-fnn","text":"3 ways to expand a convolutional neural network More convolutional layers Less aggressive downsampling Smaller kernel size for pooling (gradually downsampling) More fully connected layers Cons Need a larger dataset Curse of dimensionality Does not necessarily mean higher accuracy","title":"General Deep Learning Notes on CNN and FNN"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#3-building-a-convolutional-neural-network-with-pytorch-gpu","text":"","title":"3. Building a Convolutional Neural Network with PyTorch (GPU)"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#model-a_1","text":"GPU: 2 things must be on GPU - model - tensors","title":"Model A"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#steps_3","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 2 Conv + 2 Max Pooling + 1 FC (Same Padding, Zero Padding) import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class CNNModel ( nn . Module ): def __init__ ( self ): super ( CNNModel , self ) . __init__ () # Convolution 1 self . cnn1 = nn . Conv2d ( in_channels = 1 , out_channels = 16 , kernel_size = 5 , stride = 1 , padding = 0 ) self . relu1 = nn . ReLU () # Max pool 1 self . maxpool1 = nn . MaxPool2d ( kernel_size = 2 ) # Convolution 2 self . cnn2 = nn . Conv2d ( in_channels = 16 , out_channels = 32 , kernel_size = 5 , stride = 1 , padding = 0 ) self . relu2 = nn . ReLU () # Max pool 2 self . maxpool2 = nn . MaxPool2d ( kernel_size = 2 ) # Fully connected 1 (readout) self . fc1 = nn . Linear ( 32 * 4 * 4 , 10 ) def forward ( self , x ): # Convolution 1 out = self . cnn1 ( x ) out = self . relu1 ( out ) # Max pool 1 out = self . maxpool1 ( out ) # Convolution 2 out = self . cnn2 ( out ) out = self . relu2 ( out ) # Max pool 2 out = self . maxpool2 ( out ) # Resize # Original size: (100, 32, 7, 7) # out.size(0): 100 # New out size: (100, 32*7*7) out = out . view ( out . size ( 0 ), - 1 ) # Linear function (readout) out = self . fc1 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' model = CNNModel () ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): ####################### # USE GPU FOR MODEL # ####################### images = images . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : ####################### # USE GPU FOR MODEL # ####################### images = images . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) ####################### # USE GPU FOR MODEL # ####################### # Total correct predictions if torch . cuda . is_available (): correct += ( predicted . cpu () == labels . cpu ()) . sum () else : correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration: 500 . Loss: 0 .36831170320510864. Accuracy: 88 Iteration: 1000 . Loss: 0 .31790846586227417. Accuracy: 92 Iteration: 1500 . Loss: 0 .1510857343673706. Accuracy: 94 Iteration: 2000 . Loss: 0 .08368007838726044. Accuracy: 95 Iteration: 2500 . Loss: 0 .13419771194458008. Accuracy: 96 Iteration: 3000 . Loss: 0 .16750787198543549. Accuracy: 96 More Efficient Convolutions via Toeplitz Matrices This is beyond the scope of this particular lesson. But now that we understand how convolutions work, it is critical to know that it is quite an inefficient operation if we use for-loops to perform our 2D convolutions (5 x 5 convolution kernel size for example) on our 2D images (28 x 28 MNIST image for example). A more efficient implementation is in converting our convolution kernel into a doubly block circulant/Toeplitz matrix (special case Toeplitz matrix) and our image (input) into a vector. Then, we will do just one matrix operation using our doulby block Toeplitz matrix and our input vector. There will be a whole lesson dedicated to this operation released down the road.","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#summary","text":"We've learnt to... Success Transition from Feedforward Neural Network Addition of Convolutional & Pooling Layers before Linear Layers One Convolutional Layer Basics One Pooling Layer Basics Max pooling Average pooling Padding Output Dimension Calculations and Examples O = \\frac {W - K + 2P}{S} + 1 O = \\frac {W - K + 2P}{S} + 1 Convolutional Neural Networks Model A : 2 Conv + 2 Max pool + 1 FC Same Padding Model B : 2 Conv + 2 Average pool + 1 FC Same Padding Model C : 2 Conv + 2 Max pool + 1 FC Valid Padding Model Variation in Code Modifying only step 3 Ways to Expand Model\u2019s Capacity More convolutions Gradual pooling More fully connected layers GPU Code 2 things on GPU model tensors with gradient accumulation abilities Modifying only Step 4 & Step 7 7 Step Model Building Recap Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model","title":"Summary"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/","text":"Overcomplete Autoencoders with PyTorch \u00b6 Run Jupyter Notebook You can run the code for this section in this jupyter notebook link . Introduction \u00b6 Recap! An autoencoder's purpose is to learn an approximation of the identity function (mapping x x to \\hat x \\hat x ). Essentially we are trying to learn a function that can take our input x x and recreate it \\hat x \\hat x . Technically we can do an exact recreation of our in-sample input if we use a very wide and deep neural network. In this particular tutorial, we will be covering denoising autoencoder through overcomplete encoders. Essentially given noisy images, you can denoise and make them less noisy with this tutorial through overcomplete encoders. Fashion MNIST Dataset Exploration \u00b6 Imports \u00b6 import torch import torch.nn as nn import torch.nn.functional as F import torchvision.transforms as transforms import torchvision.datasets as dsets import matplotlib.pyplot as plt import numpy as np % matplotlib inline figsize = ( 15 , 6 ) plt . style . use ( 'fivethirtyeight' ) Name Content Examples Size Link MD5 Checksum train-images-idx3-ubyte.gz training set images 60,000 26 MBytes Download 8d4fb7e6c68d591d4c3dfef9ec88bf0d train-labels-idx1-ubyte.gz training set labels 60,000 29 KBytes Download 25c81989df183df01b3e8a0aad5dffbe t10k-images-idx3-ubyte.gz test set images 10,000 4.3 MBytes Download bef4ecab320f06d8554ea6380940ec79 t10k-labels-idx1-ubyte.gz test set labels 10,000 5.1 KBytes Download bb300cfdad3c16e7a12a480ee83cd310 Load Dataset (Step 1) \u00b6 Typically we've been leveraging on dsets.MNIST() , now we simply change to dsets.FashionMNIST ! # Fashion-MNIST data loader train_dataset = dsets . FashionMNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . FashionMNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) Downloading http : // fashion - mnist . s3 - website . eu - central - 1. amazonaws . com / train - images - idx3 - ubyte . gz Downloading http : // fashion - mnist . s3 - website . eu - central - 1. amazonaws . com / train - labels - idx1 - ubyte . gz Downloading http : // fashion - mnist . s3 - website . eu - central - 1. amazonaws . com / t10k - images - idx3 - ubyte . gz Downloading http : // fashion - mnist . s3 - website . eu - central - 1. amazonaws . com / t10k - labels - idx1 - ubyte . gz Processing ... Done ! Load Data Loader (Step 2) \u00b6 # Batch size, iterations and epochs batch_size = 100 n_iters = 5000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) Labels \u00b6 Each training and test example is assigned to one of the following labels: Label Description 0 T-shirt/top 1 Trouser 2 Pullover 3 Dress 4 Coat 5 Sandal 6 Shirt 7 Sneaker 8 Bag 9 Ankle boot Sample: Boot \u00b6 # Sample 0: boot sample_num = 0 show_img = train_dataset [ sample_num ][ 0 ] . numpy () . reshape ( 28 , 28 ) label = train_dataset [ sample_num ][ 1 ] print ( f 'Label { label } ' ) plt . imshow ( show_img , cmap = 'gray' ); Label 9 Sample: shirt \u00b6 # Sample 1: shirt sample_num = 1 show_img = train_dataset [ sample_num ][ 0 ] . numpy () . reshape ( 28 , 28 ) label = train_dataset [ sample_num ][ 1 ] print ( f 'Label { label } ' ) plt . imshow ( show_img , cmap = 'gray' ); Label 0 Sample: dress \u00b6 # Sample 3: dress sample_num = 3 show_img = train_dataset [ sample_num ][ 0 ] . numpy () . reshape ( 28 , 28 ) label = train_dataset [ sample_num ][ 1 ] print ( f 'Label { label } ' ) plt . imshow ( show_img , cmap = 'gray' ); Label 3 Maximum/minimum pixel values \u00b6 Pixel values range from 0 to 1 min_pixel_value = train_dataset [ sample_num ][ 0 ] . min () max_pixel_value = train_dataset [ sample_num ][ 0 ] . max () print ( f 'Minimum pixel value: { min_pixel_value } ' ) print ( f 'Maximum pixel value: { max_pixel_value } ' ) Minimum pixel value: 0.0 Maximum pixel value: 1.0 Overcomplete Autoencoder \u00b6 Sigmoid Function \u00b6 Sigmoid function was introduced earlier, where the function allows to bound our output from 0 to 1 inclusive given our input. This is introduced and clarified here as we would want this in our final layer of our overcomplete autoencoder as we want to bound out final output to the pixels' range of 0 and 1. # Sigmoid function has function bounded by min=0 and max=1 # So this will be what we will be using for the final layer's function x = torch . arange ( - 10. , 10. , 0.1 ) plt . figure ( figsize = figsize ); plt . plot ( x . numpy (), torch . sigmoid ( x ) . numpy ()) plt . title ( 'Sigmoid Function' ) Text(0.5, 1.0, 'Sigmoid Function') Steps \u00b6 These steps should be familiar by now! Our famous 7 steps. But I will be adding one more step here, Step 8 where we run our inference. Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Step 8: Model Inference Step 3: Create Model Class \u00b6 # Model definition class FullyConnectedAutoencoder ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super () . __init__ () # Encoder: affine function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Decoder: affine function self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Encoder: affine function out = self . fc1 ( x ) # Encoder: non-linear function out = F . leaky_relu ( out ) # Decoder: affine function out = self . fc2 ( out ) # Decoder: non-linear function out = torch . sigmoid ( out ) return out Step 4: Instantiate Model Class \u00b6 # Dimensions for overcomplete (larger latent representation) input_dim = 28 * 28 hidden_dim = int ( input_dim * 1.5 ) output_dim = input_dim # Instantiate Fully-connected Autoencoder (FC-AE) # And assign to model object model = FullyConnectedAutoencoder ( input_dim , hidden_dim , output_dim ) Step 5: Instantiate Loss Class \u00b6 # We want to minimize the per pixel reconstruction loss # So we've to use the mean squared error (MSE) loss # This is similar to our regression tasks' loss criterion = nn . MSELoss () Step 6: Instantiate Optimizer Class \u00b6 # Using basic Adam optimizer learning_rate = 1e-3 optimizer = torch . optim . Adam ( model . parameters (), lr = learning_rate ) Inspect Parameter Groups \u00b6 # Parameter inspection num_params_group = len ( list ( model . parameters ())) for group_idx in range ( num_params_group ): print ( list ( model . parameters ())[ group_idx ] . size ()) torch.Size([1176, 784]) torch.Size([1176]) torch.Size([784, 1176]) torch.Size([784]) Step 7: Train Model \u00b6 Take note there's a critical line of dropout = nn.Dropout(0.5) here which basically allows us to make noisy images from our original Fashion MNIST images. It basically drops out 50% of all pixels randomly. idx = 0 # Dropout for creating noisy images # by dropping out pixel with a 50% probability dropout = nn . Dropout ( 0.5 ) for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Noisy images noisy_images = dropout ( torch . ones ( images . shape )) * images # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output outputs = model ( noisy_images ) # Calculate Loss: MSE Loss based on pixel-to-pixel comparison loss = criterion ( outputs , images ) # Getting gradients w.r.t. parameters via backpropagation loss . backward () # Updating parameters via gradient descent optimizer . step () idx += 1 if idx % 500 == 0 : # Calculate MSE Test Loss total_test_loss = 0 total_samples = 0 # Iterate through test dataset for images , labels in test_loader : # Noisy images noisy_images = dropout ( torch . ones ( images . shape )) * images # Forward pass only to get logits/output outputs = model ( noisy_images . view ( - 1 , 28 * 28 )) # Test loss test_loss = criterion ( outputs , images . view ( - 1 , 28 * 28 )) # Total number of labels total_samples += labels . size ( 0 ) # Total test loss total_test_loss += test_loss mean_test_loss = total_test_loss / total_samples # Print Loss print ( f 'Iteration: { idx } . Average Test Loss: { mean_test_loss . item () } .' ) Iteration : 500 . Average Test Loss : 0.0001664187147980556 . Iteration : 1000 . Average Test Loss : 0.00014121478307060897 . Iteration : 1500 . Average Test Loss : 0.0001341506722383201 . Iteration : 2000 . Average Test Loss : 0.0001252180663868785 . Iteration : 2500 . Average Test Loss : 0.00012206179235363379 . Iteration : 3000 . Average Test Loss : 0.00011766648094635457 . Iteration : 3500 . Average Test Loss : 0.00011584569438127801 . Iteration : 4000 . Average Test Loss : 0.00011396891932236031 . Iteration : 4500 . Average Test Loss : 0.00011224475019844249 . Model Inference \u00b6 Raw Sample 1 \u00b6 # Test sample: Raw sample_num = 10 raw_img = test_dataset [ sample_num ][ 0 ] show_img = raw_img . numpy () . reshape ( 28 , 28 ) label = test_dataset [ sample_num ][ 1 ] print ( f 'Label { label } ' ) plt . imshow ( show_img , cmap = 'gray' ); Label 4 Test Sample 1: Noisy to Denoised \u00b6 # Test sample: Noisy sample_num = 10 raw_img = test_dataset [ sample_num ][ 0 ] noisy_image = dropout ( torch . ones ( raw_img . shape )) * raw_img show_img = noisy_image . numpy () . reshape ( 28 , 28 ) label = test_dataset [ sample_num ][ 1 ] print ( f 'Label { label } ' ) plt . imshow ( show_img , cmap = 'gray' ); Label 4 Summary \u00b6 Introduction Recap of Autoencoders Introduction of denoising autoencoders Dataset Exploration Fashion MNIST 10 classes Similar to MNIST but fashion images instead of digits 8 Step Model Building Recap Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Step 8: Model Inference Raw Sample 1 Noisy to Denoised Sample 1","title":"Fully-connected Overcomplete Autoencoder (AE)"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#overcomplete-autoencoders-with-pytorch","text":"Run Jupyter Notebook You can run the code for this section in this jupyter notebook link .","title":"Overcomplete Autoencoders with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#introduction","text":"Recap! An autoencoder's purpose is to learn an approximation of the identity function (mapping x x to \\hat x \\hat x ). Essentially we are trying to learn a function that can take our input x x and recreate it \\hat x \\hat x . Technically we can do an exact recreation of our in-sample input if we use a very wide and deep neural network. In this particular tutorial, we will be covering denoising autoencoder through overcomplete encoders. Essentially given noisy images, you can denoise and make them less noisy with this tutorial through overcomplete encoders.","title":"Introduction"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#fashion-mnist-dataset-exploration","text":"","title":"Fashion MNIST Dataset Exploration"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#imports","text":"import torch import torch.nn as nn import torch.nn.functional as F import torchvision.transforms as transforms import torchvision.datasets as dsets import matplotlib.pyplot as plt import numpy as np % matplotlib inline figsize = ( 15 , 6 ) plt . style . use ( 'fivethirtyeight' ) Name Content Examples Size Link MD5 Checksum train-images-idx3-ubyte.gz training set images 60,000 26 MBytes Download 8d4fb7e6c68d591d4c3dfef9ec88bf0d train-labels-idx1-ubyte.gz training set labels 60,000 29 KBytes Download 25c81989df183df01b3e8a0aad5dffbe t10k-images-idx3-ubyte.gz test set images 10,000 4.3 MBytes Download bef4ecab320f06d8554ea6380940ec79 t10k-labels-idx1-ubyte.gz test set labels 10,000 5.1 KBytes Download bb300cfdad3c16e7a12a480ee83cd310","title":"Imports"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#load-dataset-step-1","text":"Typically we've been leveraging on dsets.MNIST() , now we simply change to dsets.FashionMNIST ! # Fashion-MNIST data loader train_dataset = dsets . FashionMNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . FashionMNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) Downloading http : // fashion - mnist . s3 - website . eu - central - 1. amazonaws . com / train - images - idx3 - ubyte . gz Downloading http : // fashion - mnist . s3 - website . eu - central - 1. amazonaws . com / train - labels - idx1 - ubyte . gz Downloading http : // fashion - mnist . s3 - website . eu - central - 1. amazonaws . com / t10k - images - idx3 - ubyte . gz Downloading http : // fashion - mnist . s3 - website . eu - central - 1. amazonaws . com / t10k - labels - idx1 - ubyte . gz Processing ... Done !","title":"Load Dataset (Step 1)"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#load-data-loader-step-2","text":"# Batch size, iterations and epochs batch_size = 100 n_iters = 5000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False )","title":"Load Data Loader (Step 2)"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#labels","text":"Each training and test example is assigned to one of the following labels: Label Description 0 T-shirt/top 1 Trouser 2 Pullover 3 Dress 4 Coat 5 Sandal 6 Shirt 7 Sneaker 8 Bag 9 Ankle boot","title":"Labels"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#sample-boot","text":"# Sample 0: boot sample_num = 0 show_img = train_dataset [ sample_num ][ 0 ] . numpy () . reshape ( 28 , 28 ) label = train_dataset [ sample_num ][ 1 ] print ( f 'Label { label } ' ) plt . imshow ( show_img , cmap = 'gray' ); Label 9","title":"Sample: Boot"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#sample-shirt","text":"# Sample 1: shirt sample_num = 1 show_img = train_dataset [ sample_num ][ 0 ] . numpy () . reshape ( 28 , 28 ) label = train_dataset [ sample_num ][ 1 ] print ( f 'Label { label } ' ) plt . imshow ( show_img , cmap = 'gray' ); Label 0","title":"Sample: shirt"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#sample-dress","text":"# Sample 3: dress sample_num = 3 show_img = train_dataset [ sample_num ][ 0 ] . numpy () . reshape ( 28 , 28 ) label = train_dataset [ sample_num ][ 1 ] print ( f 'Label { label } ' ) plt . imshow ( show_img , cmap = 'gray' ); Label 3","title":"Sample: dress"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#maximumminimum-pixel-values","text":"Pixel values range from 0 to 1 min_pixel_value = train_dataset [ sample_num ][ 0 ] . min () max_pixel_value = train_dataset [ sample_num ][ 0 ] . max () print ( f 'Minimum pixel value: { min_pixel_value } ' ) print ( f 'Maximum pixel value: { max_pixel_value } ' ) Minimum pixel value: 0.0 Maximum pixel value: 1.0","title":"Maximum/minimum pixel values"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#overcomplete-autoencoder","text":"","title":"Overcomplete Autoencoder"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#sigmoid-function","text":"Sigmoid function was introduced earlier, where the function allows to bound our output from 0 to 1 inclusive given our input. This is introduced and clarified here as we would want this in our final layer of our overcomplete autoencoder as we want to bound out final output to the pixels' range of 0 and 1. # Sigmoid function has function bounded by min=0 and max=1 # So this will be what we will be using for the final layer's function x = torch . arange ( - 10. , 10. , 0.1 ) plt . figure ( figsize = figsize ); plt . plot ( x . numpy (), torch . sigmoid ( x ) . numpy ()) plt . title ( 'Sigmoid Function' ) Text(0.5, 1.0, 'Sigmoid Function')","title":"Sigmoid Function"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#steps","text":"These steps should be familiar by now! Our famous 7 steps. But I will be adding one more step here, Step 8 where we run our inference. Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Step 8: Model Inference","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#step-3-create-model-class","text":"# Model definition class FullyConnectedAutoencoder ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super () . __init__ () # Encoder: affine function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Decoder: affine function self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Encoder: affine function out = self . fc1 ( x ) # Encoder: non-linear function out = F . leaky_relu ( out ) # Decoder: affine function out = self . fc2 ( out ) # Decoder: non-linear function out = torch . sigmoid ( out ) return out","title":"Step 3: Create Model Class"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#step-4-instantiate-model-class","text":"# Dimensions for overcomplete (larger latent representation) input_dim = 28 * 28 hidden_dim = int ( input_dim * 1.5 ) output_dim = input_dim # Instantiate Fully-connected Autoencoder (FC-AE) # And assign to model object model = FullyConnectedAutoencoder ( input_dim , hidden_dim , output_dim )","title":"Step 4: Instantiate Model Class"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#step-5-instantiate-loss-class","text":"# We want to minimize the per pixel reconstruction loss # So we've to use the mean squared error (MSE) loss # This is similar to our regression tasks' loss criterion = nn . MSELoss ()","title":"Step 5: Instantiate Loss Class"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#step-6-instantiate-optimizer-class","text":"# Using basic Adam optimizer learning_rate = 1e-3 optimizer = torch . optim . Adam ( model . parameters (), lr = learning_rate )","title":"Step 6: Instantiate Optimizer Class"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#inspect-parameter-groups","text":"# Parameter inspection num_params_group = len ( list ( model . parameters ())) for group_idx in range ( num_params_group ): print ( list ( model . parameters ())[ group_idx ] . size ()) torch.Size([1176, 784]) torch.Size([1176]) torch.Size([784, 1176]) torch.Size([784])","title":"Inspect Parameter Groups"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#step-7-train-model","text":"Take note there's a critical line of dropout = nn.Dropout(0.5) here which basically allows us to make noisy images from our original Fashion MNIST images. It basically drops out 50% of all pixels randomly. idx = 0 # Dropout for creating noisy images # by dropping out pixel with a 50% probability dropout = nn . Dropout ( 0.5 ) for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Noisy images noisy_images = dropout ( torch . ones ( images . shape )) * images # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output outputs = model ( noisy_images ) # Calculate Loss: MSE Loss based on pixel-to-pixel comparison loss = criterion ( outputs , images ) # Getting gradients w.r.t. parameters via backpropagation loss . backward () # Updating parameters via gradient descent optimizer . step () idx += 1 if idx % 500 == 0 : # Calculate MSE Test Loss total_test_loss = 0 total_samples = 0 # Iterate through test dataset for images , labels in test_loader : # Noisy images noisy_images = dropout ( torch . ones ( images . shape )) * images # Forward pass only to get logits/output outputs = model ( noisy_images . view ( - 1 , 28 * 28 )) # Test loss test_loss = criterion ( outputs , images . view ( - 1 , 28 * 28 )) # Total number of labels total_samples += labels . size ( 0 ) # Total test loss total_test_loss += test_loss mean_test_loss = total_test_loss / total_samples # Print Loss print ( f 'Iteration: { idx } . Average Test Loss: { mean_test_loss . item () } .' ) Iteration : 500 . Average Test Loss : 0.0001664187147980556 . Iteration : 1000 . Average Test Loss : 0.00014121478307060897 . Iteration : 1500 . Average Test Loss : 0.0001341506722383201 . Iteration : 2000 . Average Test Loss : 0.0001252180663868785 . Iteration : 2500 . Average Test Loss : 0.00012206179235363379 . Iteration : 3000 . Average Test Loss : 0.00011766648094635457 . Iteration : 3500 . Average Test Loss : 0.00011584569438127801 . Iteration : 4000 . Average Test Loss : 0.00011396891932236031 . Iteration : 4500 . Average Test Loss : 0.00011224475019844249 .","title":"Step 7: Train Model"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#model-inference","text":"","title":"Model Inference"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#raw-sample-1","text":"# Test sample: Raw sample_num = 10 raw_img = test_dataset [ sample_num ][ 0 ] show_img = raw_img . numpy () . reshape ( 28 , 28 ) label = test_dataset [ sample_num ][ 1 ] print ( f 'Label { label } ' ) plt . imshow ( show_img , cmap = 'gray' ); Label 4","title":"Raw Sample 1"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#test-sample-1-noisy-to-denoised","text":"# Test sample: Noisy sample_num = 10 raw_img = test_dataset [ sample_num ][ 0 ] noisy_image = dropout ( torch . ones ( raw_img . shape )) * raw_img show_img = noisy_image . numpy () . reshape ( 28 , 28 ) label = test_dataset [ sample_num ][ 1 ] print ( f 'Label { label } ' ) plt . imshow ( show_img , cmap = 'gray' ); Label 4","title":"Test Sample 1: Noisy to Denoised"},{"location":"deep_learning/practical_pytorch/pytorch_fc_overcomplete_ae/#summary","text":"Introduction Recap of Autoencoders Introduction of denoising autoencoders Dataset Exploration Fashion MNIST 10 classes Similar to MNIST but fashion images instead of digits 8 Step Model Building Recap Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Step 8: Model Inference Raw Sample 1 Noisy to Denoised Sample 1","title":"Summary"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/","text":"Feedforward Neural Network with PyTorch \u00b6 Run Jupyter Notebook You can run the code for this section in this jupyter notebook link . About Feedforward Neural Network \u00b6 Logistic Regression Transition to Neural Networks \u00b6 Logistic Regression Review \u00b6 Define logistic regression model Import our relevant torch modules. import torch import torch.nn as nn Define our model class. class LogisticRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LogisticRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out Instantiate the logistic regression model. input_dim = 28 * 28 output_dim = 10 model = LogisticRegressionModel ( input_dim , output_dim ) When we inspect the model, we would have an input size of 784 (derived from 28 x 28) and output size of 10 (which is the number of classes we are classifying from 0 to 9). print ( model ) LogisticRegressionModel ( ( linear ): Linear ( in_features = 784 , out_features = 10 , bias = True ) ) Logistic Regression Problems \u00b6 Can represent linear functions well y = 2x + 3 y = 2x + 3 y = x_1 + x_2 y = x_1 + x_2 y = x_1 + 3x_2 + 4x_3 y = x_1 + 3x_2 + 4x_3 Cannot represent non-linear functions y = 4x_1 + 2x_2^2 +3x_3^3 y = 4x_1 + 2x_2^2 +3x_3^3 y = x_1x_2 y = x_1x_2 Introducing a Non-linear Function \u00b6 Non-linear Function In-Depth \u00b6 Function: takes a number & perform mathematical operation Common Types of Non-linearity ReLUs (Rectified Linear Units) Sigmoid Tanh Sigmoid (Logistic) \u00b6 \\sigma(x) = \\frac{1}{1 + e^{-x}} \\sigma(x) = \\frac{1}{1 + e^{-x}} Input number \\rightarrow \\rightarrow [0, 1] Large negative number \\rightarrow \\rightarrow 0 Large positive number \\rightarrow \\rightarrow 1 Cons: Activation saturates at 0 or 1 with gradients \\approx \\approx 0 No signal to update weights \\rightarrow \\rightarrow cannot learn Solution: Have to carefully initialize weights to prevent this Outputs not centered around 0 If output always positive \\rightarrow \\rightarrow gradients always positive or negative \\rightarrow \\rightarrow bad for gradient updates Tanh \u00b6 \\tanh(x) = 2 \\sigma(2x) -1 \\tanh(x) = 2 \\sigma(2x) -1 A scaled sigmoid function Input number \\rightarrow \\rightarrow [-1, 1] Cons: Activation saturates at 0 or 1 with gradients \\approx \\approx 0 No signal to update weights \\rightarrow \\rightarrow cannot learn Solution : Have to carefully initialize weights to prevent this ReLUs \u00b6 f(x) = \\max(0, x) f(x) = \\max(0, x) Pros: Accelerates convergence \\rightarrow \\rightarrow train faster Less computationally expensive operation compared to Sigmoid/Tanh exponentials Cons: Many ReLU units \"die\" \\rightarrow \\rightarrow gradients = 0 forever Solution : careful learning rate choice Building a Feedforward Neural Network with PyTorch \u00b6 Model A: 1 Hidden Layer Feedforward Neural Network (Sigmoid Activation) \u00b6 Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Step 1: Loading MNIST Train Dataset \u00b6 Images from 1 to 9 Similar to what we did in logistic regression, we will be using the same MNIST dataset where we load our training and testing datasets. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) Step 2: Make Dataset Iterable \u00b6 Batch sizes and iterations Because we have 60000 training samples (images), we need to split them up to small groups (batches) and pass these batches of samples to our feedforward neural network subsesquently. There are a few reasons why we split them into batches. Passing your whole dataset as a single batch would: (1) require a lot of RAM/VRAM on your CPU/GPU and this might result in Out-of-Memory (OOM) errors. (2) cause unstable training if you just use all the errors accumulated in 60,000 images to update the model rather than gradually update the model. In layman terms, imagine you accumulated errors for a student taking an exam with 60,000 questions and punish the student all at the same time. It is much harder for the student to learn compared to letting the student learn it made mistakes and did well in smaller batches of questions like mini-tests! If we have 60,000 images and we want a batch size of 100, then we would have 600 iterations where each iteration involves passing 600 images to the model and getting their respective predictions. 60000 / 100 600.0 Epochs An epoch means that you have successfully passed the whole training set, 60,000 images, to the model. Continuing our example above, an epoch consists of 600 iterations. If we want to go through the whole dataset 5 times (5 epochs) for the model to learn, then we need 3000 iterations (600 x 5). 600 * 5 3000.0 Bringing batch size, iterations and epochs together As we have gone through above, we want to have 5 epochs, where each epoch would have 600 iterations and each iteration has a batch size of 100. Because we want 5 epochs, we need a total of 3000 iterations. batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) Step 3: Create Model Class \u00b6 Creating our feedforward neural network Compared to logistic regression with only a single linear layer, we know for an FNN we need an additional linear layer and non-linear layer. This translates to just 4 more lines of code! class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . sigmoid = nn . Sigmoid () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function # LINEAR out = self . fc1 ( x ) # Non-linearity # NON-LINEAR out = self . sigmoid ( out ) # Linear function (readout) # LINEAR out = self . fc2 ( out ) return out Step 4: Instantiate Model Class \u00b6 Input dimension: 784 Size of image 28 \\times 28 = 784 28 \\times 28 = 784 Output dimension: 10 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 Hidden dimension: 100 Can be any number Similar term Number of neurons Number of non-linear activation functions Instantiating our model class Our input size is determined by the size of the image (numbers ranging from 0 to 9) which has a width of 28 pixels and a height of 28 pixels. Hence the size of our input is 784 (28 x 28). Our output size is what we are trying to predict. When we pass an image to our model, it will try to predict if it's 0, 1, 2, 3, 4, 5, 6, 7, 8, or 9. That is a total of 10 classes, hence we have an output size of 10. Now the tricky part is in determining our hidden layer size, that is the size of our first linear layer prior to the non-linear layer. This can be any number, a larger number implies a bigger model with more parameters. Intuitively we think a bigger model equates to a better model, but a bigger model requires more training samples to learn and converge to a good model (also called curse of dimensionality). Hence, it is wise to pick the model size for the problem at hand. Because it is a simple problem of recognizing digits, we typically would not need a big model to achieve state-of-the-art results. On the flipside, too small of a hidden size would mean there would be insufficient model capacity to predict competently. In layman terms, too small of a capacity implies a smaller brain capacity so no matter how many training samples you give it, it has a maximum capacity in terms of its predictive power. input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) Step 5: Instantiate Loss Class \u00b6 Feedforward Neural Network: Cross Entropy Loss Logistic Regression : Cross Entropy Loss Linear Regression : MSE Loss class This is exactly the same as what we did in logistic regression. Because we are going through a classification problem, cross entropy function is required to compute the loss between our softmax outputs and our binary labels. criterion = nn . CrossEntropyLoss () Step 6: Instantiate Optimizer Class \u00b6 Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our tensors with gradient accumulation capabilities) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : parameters' gradients Even simplier equation parameters = parameters - learning_rate * parameters_gradients At every iteration, we update our model's parameters Optimizer class Learning rate determines how fast the algorithm learns. Too small and the algorithm learns too slowly, too large and the algorithm learns too fast resulting in instabilities. Intuitively, we would think a larger learning rate would be better because we learn faster. But that's not true. Imagine we pass 10 images to a human to learn how to recognize whether the image is a hot dog or not, and it got half right and half wrong. A well defined learning rate (neither too small or large) is equivalent to rewarding the human with a sweet for getting the first half right, and punishing the other half the human got wrong with a smack on the palm. A large learning rate would be equivalent to feeding a thousand sweets to the human and smacking a thousand times on the human's palm. This would lead in a very unstable learning environment. Similarly, we will observe that the algorithm's convergence path will be extremely unstable if you use a large learning rate without reducing it subsequently. We are using an optimization algorithm called Stochastic Gradient Descent (SGD) which is essentially what we covered above on calculating the parameters' gradients multiplied by the learning rate then using it to update our parameters gradually. There's an in-depth analysis of various optimization algorithms on top of SGD in another section. learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) Parameters In-Depth \u00b6 Linear layers' parameters In a simple linear layer it's Y = AX + B Y = AX + B , and our parameters are A A and bias B B . Hence, each linear layer would have 2 groups of parameters A A and B B . It is critical to take note that our non-linear layers have no parameters to update. They are merely mathematical functions performed on Y Y , the output of our linear layers. This would return a Python generator object, so you need to call list on the generator object to access anything meaningful. print ( model . parameters ()) Here we call list on the generator object and getting the length of the list. This would return 4 because we've 2 linear layers, and each layer has 2 groups of parameters A A and b b . print ( len ( list ( model . parameters ()))) Our first linear layer parameters, A_1 A_1 , would be of size 100 x 784. This is because we've an input size of 784 (28 x 28) and a hidden size of 100. # FC 1 Parameters print ( list ( model . parameters ())[ 0 ] . size ()) Our first linear layer bias parameters, B_1 B_1 , would be of size 100 which is our hidden size. # FC 1 Bias Parameters print ( list ( model . parameters ())[ 1 ] . size ()) Our second linear layer is our readout layer, where the parameters A_2 A_2 would be of size 10 x 100. This is because our output size is 10 and hidden size is 100. # FC 2 Parameters print ( list ( model . parameters ())[ 2 ] . size ()) Likewise our readout layer's bias B_1 B_1 would just be 10, the size of our output. # FC 2 Bias Parameters print ( list ( model . parameters ())[ 3 ] . size ()) The diagram below shows the interaction amongst our input X X and our linear layers' parameters A_1 A_1 , B_1 B_1 , A_2 A_2 , and B_2 B_2 to reach to the final size of 10 x 1. If you're still unfamiliar with matrix product, go ahead and review the previous quick lesson where we covered it in logistic regression . < generator object Module . parameters at 0x7f1d530fa678 > 4 torch . Size ([ 100 , 784 ]) torch . Size ([ 100 ]) torch . Size ([ 10 , 100 ]) torch . Size ([ 10 ]) Step 7: Train Model \u00b6 Process Convert inputs to tensors with gradient accumulation capabilities Clear gradient buffers Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT 7-step training process iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.6457265615463257 . Accuracy : 85 Iteration : 1000. Loss : 0.39627206325531006 . Accuracy : 89 Iteration : 1500. Loss : 0.2831554412841797 . Accuracy : 90 Iteration : 2000. Loss : 0.4409525394439697 . Accuracy : 91 Iteration : 2500. Loss : 0.2397005707025528 . Accuracy : 91 Iteration : 3000. Loss : 0.3160165846347809 . Accuracy : 91 Model B: 1 Hidden Layer Feedforward Neural Network (Tanh Activation) \u00b6 Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 1-layer FNN with Tanh Activation The only difference here compared to previously is that we are using Tanh activation instead of Sigmoid activation. This affects step 3. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . tanh = nn . Tanh () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . tanh ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.4128190577030182 . Accuracy : 91 Iteration : 1000. Loss : 0.14497484266757965 . Accuracy : 92 Iteration : 1500. Loss : 0.272532194852829 . Accuracy : 93 Iteration : 2000. Loss : 0.2758277952671051 . Accuracy : 94 Iteration : 2500. Loss : 0.1603182554244995 . Accuracy : 94 Iteration : 3000. Loss : 0.08848697692155838 . Accuracy : 95 Model C: 1 Hidden Layer Feedforward Neural Network (ReLU Activation) \u00b6 Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 1-layer FNN with ReLU Activation The only difference again is in using ReLU activation and it affects step 3. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.3179700970649719 . Accuracy : 91 Iteration : 1000. Loss : 0.17288273572921753 . Accuracy : 93 Iteration : 1500. Loss : 0.16829034686088562 . Accuracy : 94 Iteration : 2000. Loss : 0.25494423508644104 . Accuracy : 94 Iteration : 2500. Loss : 0.16818439960479736 . Accuracy : 95 Iteration : 3000. Loss : 0.11110792309045792 . Accuracy : 95 Model D: 2 Hidden Layer Feedforward Neural Network (ReLU Activation) \u00b6 Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 2-layer FNN with ReLU Activation This is a bigger difference that increases your model's capacity by adding another linear layer and non-linear layer which affects step 3. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function 1: 784 --> 100 self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity 1 self . relu1 = nn . ReLU () # Linear function 2: 100 --> 100 self . fc2 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 2 self . relu2 = nn . ReLU () # Linear function 3 (readout): 100 --> 10 self . fc3 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function 1 out = self . fc1 ( x ) # Non-linearity 1 out = self . relu1 ( out ) # Linear function 2 out = self . fc2 ( out ) # Non-linearity 2 out = self . relu2 ( out ) # Linear function 3 (readout) out = self . fc3 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () labels = labels # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.2995373010635376 . Accuracy : 91 Iteration : 1000. Loss : 0.3924565613269806 . Accuracy : 93 Iteration : 1500. Loss : 0.1283276081085205 . Accuracy : 94 Iteration : 2000. Loss : 0.10905527323484421 . Accuracy : 95 Iteration : 2500. Loss : 0.11943754553794861 . Accuracy : 96 Iteration : 3000. Loss : 0.15632082521915436 . Accuracy : 96 Model E: 3 Hidden Layer Feedforward Neural Network (ReLU Activation) \u00b6 Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 3-layer FNN with ReLU Activation Let's add one more layer! Bigger model capacity. But will it be better? Remember what we talked about on curse of dimensionality? import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function 1: 784 --> 100 self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity 1 self . relu1 = nn . ReLU () # Linear function 2: 100 --> 100 self . fc2 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 2 self . relu2 = nn . ReLU () # Linear function 3: 100 --> 100 self . fc3 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 3 self . relu3 = nn . ReLU () # Linear function 4 (readout): 100 --> 10 self . fc4 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function 1 out = self . fc1 ( x ) # Non-linearity 1 out = self . relu1 ( out ) # Linear function 2 out = self . fc2 ( out ) # Non-linearity 2 out = self . relu2 ( out ) # Linear function 2 out = self . fc3 ( out ) # Non-linearity 2 out = self . relu3 ( out ) # Linear function 4 (readout) out = self . fc4 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.33234935998916626 . Accuracy : 89 Iteration : 1000. Loss : 0.3098006248474121 . Accuracy : 94 Iteration : 1500. Loss : 0.12461677193641663 . Accuracy : 95 Iteration : 2000. Loss : 0.14346086978912354 . Accuracy : 96 Iteration : 2500. Loss : 0.03763459622859955 . Accuracy : 96 Iteration : 3000. Loss : 0.1397182047367096 . Accuracy : 97 General Comments on FNNs \u00b6 2 ways to expand a neural network More non-linear activation units (neurons) More hidden layers Cons Need a larger dataset Curse of dimensionality Does not necessarily mean higher accuracy 3. Building a Feedforward Neural Network with PyTorch (GPU) \u00b6 GPU: 2 things must be on GPU - model - tensors Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 3-layer FNN with ReLU Activation on GPU Only step 4 and 7 of the CPU code will be affected and it's a simple change. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function 1: 784 --> 100 self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity 1 self . relu1 = nn . ReLU () # Linear function 2: 100 --> 100 self . fc2 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 2 self . relu2 = nn . ReLU () # Linear function 3: 100 --> 100 self . fc3 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 3 self . relu3 = nn . ReLU () # Linear function 4 (readout): 100 --> 10 self . fc4 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function 1 out = self . fc1 ( x ) # Non-linearity 1 out = self . relu1 ( out ) # Linear function 2 out = self . fc2 ( out ) # Non-linearity 2 out = self . relu2 ( out ) # Linear function 2 out = self . fc3 ( out ) # Non-linearity 2 out = self . relu3 ( out ) # Linear function 4 (readout) out = self . fc4 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () . to ( device ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) ####################### # USE GPU FOR MODEL # ####################### # Total correct predictions if torch . cuda . is_available (): correct += ( predicted . cpu () == labels . cpu ()) . sum () else : correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.3877025246620178 . Accuracy : 90 Iteration : 1000. Loss : 0.1337055265903473 . Accuracy : 93 Iteration : 1500. Loss : 0.2038637101650238 . Accuracy : 95 Iteration : 2000. Loss : 0.17892278730869293 . Accuracy : 95 Iteration : 2500. Loss : 0.14455552399158478 . Accuracy : 96 Iteration : 3000. Loss : 0.024540524929761887 . Accuracy : 96 Alternative Term of Neural Network The alternative term is Universal Function Approximator . This is because ultimately we are trying to find a function that maps our input, X X , to our output, y y . Summary \u00b6 We've learnt to... Success Logistic Regression Problems for Non-Linear Functions Representation Cannot represent non-linear functions $ y = 4x_1 + 2x_2^2 +3x_3^3 $ $ y = x_1x_2$ Introduced Non-Linearity to Logistic Regression to form a Neural Network Types of Non-Linearity Sigmoid Tanh ReLU Feedforward Neural Network Models Model A: 1 hidden layer ( sigmoid activation) Model B: 1 hidden layer ( tanh activation) Model C: 1 hidden layer ( ReLU activation) Model D: 2 hidden layers (ReLU activation) Model E: 3 hidden layers (ReLU activation) Models Variation in Code Modifying only step 3 Ways to Expand Model\u2019s Capacity More non-linear activation units ( neurons ) More hidden layers Cons of Expanding Capacity Need more data Does not necessarily mean higher accuracy GPU Code 2 things on GPU model tensors with gradient accumulation capabilities Modifying only Step 4 & Step 7 7 Step Model Building Recap Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Feedforward Neural Networks (FNN)"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#feedforward-neural-network-with-pytorch","text":"Run Jupyter Notebook You can run the code for this section in this jupyter notebook link .","title":"Feedforward Neural Network with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#about-feedforward-neural-network","text":"","title":"About Feedforward Neural Network"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#logistic-regression-transition-to-neural-networks","text":"","title":"Logistic Regression Transition to Neural Networks"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#logistic-regression-review","text":"Define logistic regression model Import our relevant torch modules. import torch import torch.nn as nn Define our model class. class LogisticRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LogisticRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out Instantiate the logistic regression model. input_dim = 28 * 28 output_dim = 10 model = LogisticRegressionModel ( input_dim , output_dim ) When we inspect the model, we would have an input size of 784 (derived from 28 x 28) and output size of 10 (which is the number of classes we are classifying from 0 to 9). print ( model ) LogisticRegressionModel ( ( linear ): Linear ( in_features = 784 , out_features = 10 , bias = True ) )","title":"Logistic Regression Review"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#logistic-regression-problems","text":"Can represent linear functions well y = 2x + 3 y = 2x + 3 y = x_1 + x_2 y = x_1 + x_2 y = x_1 + 3x_2 + 4x_3 y = x_1 + 3x_2 + 4x_3 Cannot represent non-linear functions y = 4x_1 + 2x_2^2 +3x_3^3 y = 4x_1 + 2x_2^2 +3x_3^3 y = x_1x_2 y = x_1x_2","title":"Logistic Regression Problems"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#introducing-a-non-linear-function","text":"","title":"Introducing a Non-linear Function"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#non-linear-function-in-depth","text":"Function: takes a number & perform mathematical operation Common Types of Non-linearity ReLUs (Rectified Linear Units) Sigmoid Tanh","title":"Non-linear Function In-Depth"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#sigmoid-logistic","text":"\\sigma(x) = \\frac{1}{1 + e^{-x}} \\sigma(x) = \\frac{1}{1 + e^{-x}} Input number \\rightarrow \\rightarrow [0, 1] Large negative number \\rightarrow \\rightarrow 0 Large positive number \\rightarrow \\rightarrow 1 Cons: Activation saturates at 0 or 1 with gradients \\approx \\approx 0 No signal to update weights \\rightarrow \\rightarrow cannot learn Solution: Have to carefully initialize weights to prevent this Outputs not centered around 0 If output always positive \\rightarrow \\rightarrow gradients always positive or negative \\rightarrow \\rightarrow bad for gradient updates","title":"Sigmoid (Logistic)"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#tanh","text":"\\tanh(x) = 2 \\sigma(2x) -1 \\tanh(x) = 2 \\sigma(2x) -1 A scaled sigmoid function Input number \\rightarrow \\rightarrow [-1, 1] Cons: Activation saturates at 0 or 1 with gradients \\approx \\approx 0 No signal to update weights \\rightarrow \\rightarrow cannot learn Solution : Have to carefully initialize weights to prevent this","title":"Tanh"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#relus","text":"f(x) = \\max(0, x) f(x) = \\max(0, x) Pros: Accelerates convergence \\rightarrow \\rightarrow train faster Less computationally expensive operation compared to Sigmoid/Tanh exponentials Cons: Many ReLU units \"die\" \\rightarrow \\rightarrow gradients = 0 forever Solution : careful learning rate choice","title":"ReLUs"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#building-a-feedforward-neural-network-with-pytorch","text":"","title":"Building a Feedforward Neural Network with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#model-a-1-hidden-layer-feedforward-neural-network-sigmoid-activation","text":"","title":"Model A: 1 Hidden Layer Feedforward Neural Network (Sigmoid Activation)"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#steps","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-1-loading-mnist-train-dataset","text":"Images from 1 to 9 Similar to what we did in logistic regression, we will be using the same MNIST dataset where we load our training and testing datasets. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ())","title":"Step 1: Loading MNIST Train Dataset"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-2-make-dataset-iterable","text":"Batch sizes and iterations Because we have 60000 training samples (images), we need to split them up to small groups (batches) and pass these batches of samples to our feedforward neural network subsesquently. There are a few reasons why we split them into batches. Passing your whole dataset as a single batch would: (1) require a lot of RAM/VRAM on your CPU/GPU and this might result in Out-of-Memory (OOM) errors. (2) cause unstable training if you just use all the errors accumulated in 60,000 images to update the model rather than gradually update the model. In layman terms, imagine you accumulated errors for a student taking an exam with 60,000 questions and punish the student all at the same time. It is much harder for the student to learn compared to letting the student learn it made mistakes and did well in smaller batches of questions like mini-tests! If we have 60,000 images and we want a batch size of 100, then we would have 600 iterations where each iteration involves passing 600 images to the model and getting their respective predictions. 60000 / 100 600.0 Epochs An epoch means that you have successfully passed the whole training set, 60,000 images, to the model. Continuing our example above, an epoch consists of 600 iterations. If we want to go through the whole dataset 5 times (5 epochs) for the model to learn, then we need 3000 iterations (600 x 5). 600 * 5 3000.0 Bringing batch size, iterations and epochs together As we have gone through above, we want to have 5 epochs, where each epoch would have 600 iterations and each iteration has a batch size of 100. Because we want 5 epochs, we need a total of 3000 iterations. batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False )","title":"Step 2: Make Dataset Iterable"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-3-create-model-class","text":"Creating our feedforward neural network Compared to logistic regression with only a single linear layer, we know for an FNN we need an additional linear layer and non-linear layer. This translates to just 4 more lines of code! class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . sigmoid = nn . Sigmoid () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function # LINEAR out = self . fc1 ( x ) # Non-linearity # NON-LINEAR out = self . sigmoid ( out ) # Linear function (readout) # LINEAR out = self . fc2 ( out ) return out","title":"Step 3: Create Model Class"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-4-instantiate-model-class","text":"Input dimension: 784 Size of image 28 \\times 28 = 784 28 \\times 28 = 784 Output dimension: 10 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 Hidden dimension: 100 Can be any number Similar term Number of neurons Number of non-linear activation functions Instantiating our model class Our input size is determined by the size of the image (numbers ranging from 0 to 9) which has a width of 28 pixels and a height of 28 pixels. Hence the size of our input is 784 (28 x 28). Our output size is what we are trying to predict. When we pass an image to our model, it will try to predict if it's 0, 1, 2, 3, 4, 5, 6, 7, 8, or 9. That is a total of 10 classes, hence we have an output size of 10. Now the tricky part is in determining our hidden layer size, that is the size of our first linear layer prior to the non-linear layer. This can be any number, a larger number implies a bigger model with more parameters. Intuitively we think a bigger model equates to a better model, but a bigger model requires more training samples to learn and converge to a good model (also called curse of dimensionality). Hence, it is wise to pick the model size for the problem at hand. Because it is a simple problem of recognizing digits, we typically would not need a big model to achieve state-of-the-art results. On the flipside, too small of a hidden size would mean there would be insufficient model capacity to predict competently. In layman terms, too small of a capacity implies a smaller brain capacity so no matter how many training samples you give it, it has a maximum capacity in terms of its predictive power. input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim )","title":"Step 4: Instantiate Model Class"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-5-instantiate-loss-class","text":"Feedforward Neural Network: Cross Entropy Loss Logistic Regression : Cross Entropy Loss Linear Regression : MSE Loss class This is exactly the same as what we did in logistic regression. Because we are going through a classification problem, cross entropy function is required to compute the loss between our softmax outputs and our binary labels. criterion = nn . CrossEntropyLoss ()","title":"Step 5: Instantiate Loss Class"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-6-instantiate-optimizer-class","text":"Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our tensors with gradient accumulation capabilities) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : parameters' gradients Even simplier equation parameters = parameters - learning_rate * parameters_gradients At every iteration, we update our model's parameters Optimizer class Learning rate determines how fast the algorithm learns. Too small and the algorithm learns too slowly, too large and the algorithm learns too fast resulting in instabilities. Intuitively, we would think a larger learning rate would be better because we learn faster. But that's not true. Imagine we pass 10 images to a human to learn how to recognize whether the image is a hot dog or not, and it got half right and half wrong. A well defined learning rate (neither too small or large) is equivalent to rewarding the human with a sweet for getting the first half right, and punishing the other half the human got wrong with a smack on the palm. A large learning rate would be equivalent to feeding a thousand sweets to the human and smacking a thousand times on the human's palm. This would lead in a very unstable learning environment. Similarly, we will observe that the algorithm's convergence path will be extremely unstable if you use a large learning rate without reducing it subsequently. We are using an optimization algorithm called Stochastic Gradient Descent (SGD) which is essentially what we covered above on calculating the parameters' gradients multiplied by the learning rate then using it to update our parameters gradually. There's an in-depth analysis of various optimization algorithms on top of SGD in another section. learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate )","title":"Step 6: Instantiate Optimizer Class"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#parameters-in-depth","text":"Linear layers' parameters In a simple linear layer it's Y = AX + B Y = AX + B , and our parameters are A A and bias B B . Hence, each linear layer would have 2 groups of parameters A A and B B . It is critical to take note that our non-linear layers have no parameters to update. They are merely mathematical functions performed on Y Y , the output of our linear layers. This would return a Python generator object, so you need to call list on the generator object to access anything meaningful. print ( model . parameters ()) Here we call list on the generator object and getting the length of the list. This would return 4 because we've 2 linear layers, and each layer has 2 groups of parameters A A and b b . print ( len ( list ( model . parameters ()))) Our first linear layer parameters, A_1 A_1 , would be of size 100 x 784. This is because we've an input size of 784 (28 x 28) and a hidden size of 100. # FC 1 Parameters print ( list ( model . parameters ())[ 0 ] . size ()) Our first linear layer bias parameters, B_1 B_1 , would be of size 100 which is our hidden size. # FC 1 Bias Parameters print ( list ( model . parameters ())[ 1 ] . size ()) Our second linear layer is our readout layer, where the parameters A_2 A_2 would be of size 10 x 100. This is because our output size is 10 and hidden size is 100. # FC 2 Parameters print ( list ( model . parameters ())[ 2 ] . size ()) Likewise our readout layer's bias B_1 B_1 would just be 10, the size of our output. # FC 2 Bias Parameters print ( list ( model . parameters ())[ 3 ] . size ()) The diagram below shows the interaction amongst our input X X and our linear layers' parameters A_1 A_1 , B_1 B_1 , A_2 A_2 , and B_2 B_2 to reach to the final size of 10 x 1. If you're still unfamiliar with matrix product, go ahead and review the previous quick lesson where we covered it in logistic regression . < generator object Module . parameters at 0x7f1d530fa678 > 4 torch . Size ([ 100 , 784 ]) torch . Size ([ 100 ]) torch . Size ([ 10 , 100 ]) torch . Size ([ 10 ])","title":"Parameters In-Depth"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-7-train-model","text":"Process Convert inputs to tensors with gradient accumulation capabilities Clear gradient buffers Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT 7-step training process iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.6457265615463257 . Accuracy : 85 Iteration : 1000. Loss : 0.39627206325531006 . Accuracy : 89 Iteration : 1500. Loss : 0.2831554412841797 . Accuracy : 90 Iteration : 2000. Loss : 0.4409525394439697 . Accuracy : 91 Iteration : 2500. Loss : 0.2397005707025528 . Accuracy : 91 Iteration : 3000. Loss : 0.3160165846347809 . Accuracy : 91","title":"Step 7: Train Model"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#model-b-1-hidden-layer-feedforward-neural-network-tanh-activation","text":"","title":"Model B: 1 Hidden Layer Feedforward Neural Network (Tanh Activation)"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#steps_1","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 1-layer FNN with Tanh Activation The only difference here compared to previously is that we are using Tanh activation instead of Sigmoid activation. This affects step 3. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . tanh = nn . Tanh () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . tanh ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.4128190577030182 . Accuracy : 91 Iteration : 1000. Loss : 0.14497484266757965 . Accuracy : 92 Iteration : 1500. Loss : 0.272532194852829 . Accuracy : 93 Iteration : 2000. Loss : 0.2758277952671051 . Accuracy : 94 Iteration : 2500. Loss : 0.1603182554244995 . Accuracy : 94 Iteration : 3000. Loss : 0.08848697692155838 . Accuracy : 95","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#model-c-1-hidden-layer-feedforward-neural-network-relu-activation","text":"","title":"Model C: 1 Hidden Layer Feedforward Neural Network (ReLU Activation)"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#steps_2","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 1-layer FNN with ReLU Activation The only difference again is in using ReLU activation and it affects step 3. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.3179700970649719 . Accuracy : 91 Iteration : 1000. Loss : 0.17288273572921753 . Accuracy : 93 Iteration : 1500. Loss : 0.16829034686088562 . Accuracy : 94 Iteration : 2000. Loss : 0.25494423508644104 . Accuracy : 94 Iteration : 2500. Loss : 0.16818439960479736 . Accuracy : 95 Iteration : 3000. Loss : 0.11110792309045792 . Accuracy : 95","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#model-d-2-hidden-layer-feedforward-neural-network-relu-activation","text":"","title":"Model D: 2 Hidden Layer Feedforward Neural Network (ReLU Activation)"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#steps_3","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 2-layer FNN with ReLU Activation This is a bigger difference that increases your model's capacity by adding another linear layer and non-linear layer which affects step 3. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function 1: 784 --> 100 self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity 1 self . relu1 = nn . ReLU () # Linear function 2: 100 --> 100 self . fc2 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 2 self . relu2 = nn . ReLU () # Linear function 3 (readout): 100 --> 10 self . fc3 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function 1 out = self . fc1 ( x ) # Non-linearity 1 out = self . relu1 ( out ) # Linear function 2 out = self . fc2 ( out ) # Non-linearity 2 out = self . relu2 ( out ) # Linear function 3 (readout) out = self . fc3 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () labels = labels # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.2995373010635376 . Accuracy : 91 Iteration : 1000. Loss : 0.3924565613269806 . Accuracy : 93 Iteration : 1500. Loss : 0.1283276081085205 . Accuracy : 94 Iteration : 2000. Loss : 0.10905527323484421 . Accuracy : 95 Iteration : 2500. Loss : 0.11943754553794861 . Accuracy : 96 Iteration : 3000. Loss : 0.15632082521915436 . Accuracy : 96","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#model-e-3-hidden-layer-feedforward-neural-network-relu-activation","text":"","title":"Model E: 3 Hidden Layer Feedforward Neural Network (ReLU Activation)"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#steps_4","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 3-layer FNN with ReLU Activation Let's add one more layer! Bigger model capacity. But will it be better? Remember what we talked about on curse of dimensionality? import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function 1: 784 --> 100 self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity 1 self . relu1 = nn . ReLU () # Linear function 2: 100 --> 100 self . fc2 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 2 self . relu2 = nn . ReLU () # Linear function 3: 100 --> 100 self . fc3 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 3 self . relu3 = nn . ReLU () # Linear function 4 (readout): 100 --> 10 self . fc4 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function 1 out = self . fc1 ( x ) # Non-linearity 1 out = self . relu1 ( out ) # Linear function 2 out = self . fc2 ( out ) # Non-linearity 2 out = self . relu2 ( out ) # Linear function 2 out = self . fc3 ( out ) # Non-linearity 2 out = self . relu3 ( out ) # Linear function 4 (readout) out = self . fc4 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.33234935998916626 . Accuracy : 89 Iteration : 1000. Loss : 0.3098006248474121 . Accuracy : 94 Iteration : 1500. Loss : 0.12461677193641663 . Accuracy : 95 Iteration : 2000. Loss : 0.14346086978912354 . Accuracy : 96 Iteration : 2500. Loss : 0.03763459622859955 . Accuracy : 96 Iteration : 3000. Loss : 0.1397182047367096 . Accuracy : 97","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#general-comments-on-fnns","text":"2 ways to expand a neural network More non-linear activation units (neurons) More hidden layers Cons Need a larger dataset Curse of dimensionality Does not necessarily mean higher accuracy","title":"General Comments on FNNs"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#3-building-a-feedforward-neural-network-with-pytorch-gpu","text":"GPU: 2 things must be on GPU - model - tensors","title":"3. Building a Feedforward Neural Network with PyTorch (GPU)"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#steps_5","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 3-layer FNN with ReLU Activation on GPU Only step 4 and 7 of the CPU code will be affected and it's a simple change. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function 1: 784 --> 100 self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity 1 self . relu1 = nn . ReLU () # Linear function 2: 100 --> 100 self . fc2 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 2 self . relu2 = nn . ReLU () # Linear function 3: 100 --> 100 self . fc3 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 3 self . relu3 = nn . ReLU () # Linear function 4 (readout): 100 --> 10 self . fc4 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function 1 out = self . fc1 ( x ) # Non-linearity 1 out = self . relu1 ( out ) # Linear function 2 out = self . fc2 ( out ) # Non-linearity 2 out = self . relu2 ( out ) # Linear function 2 out = self . fc3 ( out ) # Non-linearity 2 out = self . relu3 ( out ) # Linear function 4 (readout) out = self . fc4 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () . to ( device ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) ####################### # USE GPU FOR MODEL # ####################### # Total correct predictions if torch . cuda . is_available (): correct += ( predicted . cpu () == labels . cpu ()) . sum () else : correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.3877025246620178 . Accuracy : 90 Iteration : 1000. Loss : 0.1337055265903473 . Accuracy : 93 Iteration : 1500. Loss : 0.2038637101650238 . Accuracy : 95 Iteration : 2000. Loss : 0.17892278730869293 . Accuracy : 95 Iteration : 2500. Loss : 0.14455552399158478 . Accuracy : 96 Iteration : 3000. Loss : 0.024540524929761887 . Accuracy : 96 Alternative Term of Neural Network The alternative term is Universal Function Approximator . This is because ultimately we are trying to find a function that maps our input, X X , to our output, y y .","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#summary","text":"We've learnt to... Success Logistic Regression Problems for Non-Linear Functions Representation Cannot represent non-linear functions $ y = 4x_1 + 2x_2^2 +3x_3^3 $ $ y = x_1x_2$ Introduced Non-Linearity to Logistic Regression to form a Neural Network Types of Non-Linearity Sigmoid Tanh ReLU Feedforward Neural Network Models Model A: 1 hidden layer ( sigmoid activation) Model B: 1 hidden layer ( tanh activation) Model C: 1 hidden layer ( ReLU activation) Model D: 2 hidden layers (ReLU activation) Model E: 3 hidden layers (ReLU activation) Models Variation in Code Modifying only step 3 Ways to Expand Model\u2019s Capacity More non-linear activation units ( neurons ) More hidden layers Cons of Expanding Capacity Need more data Does not necessarily mean higher accuracy GPU Code 2 things on GPU model tensors with gradient accumulation capabilities Modifying only Step 4 & Step 7 7 Step Model Building Recap Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model","title":"Summary"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/practical_pytorch/pytorch_gradients/","text":"Gradients with PyTorch \u00b6 Run Jupyter Notebook You can run the code for this section in this jupyter notebook link . Tensors with Gradients \u00b6 Creating Tensors with Gradients \u00b6 Allows accumulation of gradients Method 1: Create tensor with gradients It is very similar to creating a tensor, all you need to do is to add an additional argument. import torch a = torch . ones (( 2 , 2 ), requires_grad = True ) a tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Check if tensor requires gradients This should return True otherwise you've not done it right. a . requires_grad True Method 2: Create tensor with gradients This allows you to create a tensor as usual then an additional line to allow it to accumulate gradients. # Normal way of creating gradients a = torch . ones (( 2 , 2 )) # Requires gradient a . requires_grad_ () # Check if requires gradient a . requires_grad True A tensor without gradients just for comparison If you do not do either of the methods above, you'll realize you will get False for checking for gradients. # Not a variable no_gradient = torch . ones ( 2 , 2 ) no_gradient . requires_grad False Tensor with gradients addition operation # Behaves similarly to tensors b = torch . ones (( 2 , 2 ), requires_grad = True ) print ( a + b ) print ( torch . add ( a , b )) tensor ([[ 2. , 2. ], [ 2. , 2. ]]) tensor ([[ 2. , 2. ], [ 2. , 2. ]]) Tensor with gradients multiplication operation As usual, the operations we learnt previously for tensors apply for tensors with gradients. Feel free to try divisions, mean or standard deviation! print ( a * b ) print ( torch . mul ( a , b )) tensor ([[ 1. , 1. ], [ 1. , 1. ]]) tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Manually and Automatically Calculating Gradients \u00b6 What exactly is requires_grad ? - Allows calculation of gradients w.r.t. the tensor that all allows gradients accumulation y_i = 5(x_i+1)^2 y_i = 5(x_i+1)^2 Create tensor of size 2x1 filled with 1's that requires gradient x = torch . ones ( 2 , requires_grad = True ) x tensor ([ 1. , 1. ]) Simple linear equation with x tensor created y_i\\bigr\\rvert_{x_i=1} = 5(1 + 1)^2 = 5(2)^2 = 5(4) = 20 y_i\\bigr\\rvert_{x_i=1} = 5(1 + 1)^2 = 5(2)^2 = 5(4) = 20 We should get a value of 20 by replicating this simple equation y = 5 * ( x + 1 ) ** 2 y tensor ([ 20. , 20. ]) Simple equation with y tensor Backward should be called only on a scalar (i.e. 1-element tensor) or with gradient w.r.t. the variable Let's reduce y to a scalar then... o = \\frac{1}{2}\\sum_i y_i o = \\frac{1}{2}\\sum_i y_i As you can see above, we've a tensor filled with 20's, so average them would return 20 o = ( 1 / 2 ) * torch . sum ( y ) o tensor ( 20. ) Calculating first derivative Recap y equation : y_i = 5(x_i+1)^2 y_i = 5(x_i+1)^2 Recap o equation : o = \\frac{1}{2}\\sum_i y_i o = \\frac{1}{2}\\sum_i y_i Substitute y into o equation : o = \\frac{1}{2} \\sum_i 5(x_i+1)^2 o = \\frac{1}{2} \\sum_i 5(x_i+1)^2 \\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i+1)] \\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i+1)] \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10}{2}(2) = 10 \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10}{2}(2) = 10 We should expect to get 10, and it's so simple to do this with PyTorch with the following line... Get first derivative: o . backward () Print out first derivative: x . grad tensor ([ 10. , 10. ]) If x requires gradient and you create new objects with it, you get all gradients print ( x . requires_grad ) print ( y . requires_grad ) print ( o . requires_grad ) True True True Summary \u00b6 We've learnt to... Success Tensor with Gradients Wraps a tensor for gradient accumulation Gradients Define original equation Substitute equation with x values Reduce to scalar output, o through mean Calculate gradients with o.backward() Then access gradients of the x tensor with requires_grad through x.grad Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Gradients"},{"location":"deep_learning/practical_pytorch/pytorch_gradients/#gradients-with-pytorch","text":"Run Jupyter Notebook You can run the code for this section in this jupyter notebook link .","title":"Gradients with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_gradients/#tensors-with-gradients","text":"","title":"Tensors with Gradients"},{"location":"deep_learning/practical_pytorch/pytorch_gradients/#creating-tensors-with-gradients","text":"Allows accumulation of gradients Method 1: Create tensor with gradients It is very similar to creating a tensor, all you need to do is to add an additional argument. import torch a = torch . ones (( 2 , 2 ), requires_grad = True ) a tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Check if tensor requires gradients This should return True otherwise you've not done it right. a . requires_grad True Method 2: Create tensor with gradients This allows you to create a tensor as usual then an additional line to allow it to accumulate gradients. # Normal way of creating gradients a = torch . ones (( 2 , 2 )) # Requires gradient a . requires_grad_ () # Check if requires gradient a . requires_grad True A tensor without gradients just for comparison If you do not do either of the methods above, you'll realize you will get False for checking for gradients. # Not a variable no_gradient = torch . ones ( 2 , 2 ) no_gradient . requires_grad False Tensor with gradients addition operation # Behaves similarly to tensors b = torch . ones (( 2 , 2 ), requires_grad = True ) print ( a + b ) print ( torch . add ( a , b )) tensor ([[ 2. , 2. ], [ 2. , 2. ]]) tensor ([[ 2. , 2. ], [ 2. , 2. ]]) Tensor with gradients multiplication operation As usual, the operations we learnt previously for tensors apply for tensors with gradients. Feel free to try divisions, mean or standard deviation! print ( a * b ) print ( torch . mul ( a , b )) tensor ([[ 1. , 1. ], [ 1. , 1. ]]) tensor ([[ 1. , 1. ], [ 1. , 1. ]])","title":"Creating Tensors with Gradients"},{"location":"deep_learning/practical_pytorch/pytorch_gradients/#manually-and-automatically-calculating-gradients","text":"What exactly is requires_grad ? - Allows calculation of gradients w.r.t. the tensor that all allows gradients accumulation y_i = 5(x_i+1)^2 y_i = 5(x_i+1)^2 Create tensor of size 2x1 filled with 1's that requires gradient x = torch . ones ( 2 , requires_grad = True ) x tensor ([ 1. , 1. ]) Simple linear equation with x tensor created y_i\\bigr\\rvert_{x_i=1} = 5(1 + 1)^2 = 5(2)^2 = 5(4) = 20 y_i\\bigr\\rvert_{x_i=1} = 5(1 + 1)^2 = 5(2)^2 = 5(4) = 20 We should get a value of 20 by replicating this simple equation y = 5 * ( x + 1 ) ** 2 y tensor ([ 20. , 20. ]) Simple equation with y tensor Backward should be called only on a scalar (i.e. 1-element tensor) or with gradient w.r.t. the variable Let's reduce y to a scalar then... o = \\frac{1}{2}\\sum_i y_i o = \\frac{1}{2}\\sum_i y_i As you can see above, we've a tensor filled with 20's, so average them would return 20 o = ( 1 / 2 ) * torch . sum ( y ) o tensor ( 20. ) Calculating first derivative Recap y equation : y_i = 5(x_i+1)^2 y_i = 5(x_i+1)^2 Recap o equation : o = \\frac{1}{2}\\sum_i y_i o = \\frac{1}{2}\\sum_i y_i Substitute y into o equation : o = \\frac{1}{2} \\sum_i 5(x_i+1)^2 o = \\frac{1}{2} \\sum_i 5(x_i+1)^2 \\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i+1)] \\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i+1)] \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10}{2}(2) = 10 \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10}{2}(2) = 10 We should expect to get 10, and it's so simple to do this with PyTorch with the following line... Get first derivative: o . backward () Print out first derivative: x . grad tensor ([ 10. , 10. ]) If x requires gradient and you create new objects with it, you get all gradients print ( x . requires_grad ) print ( y . requires_grad ) print ( o . requires_grad ) True True True","title":"Manually and Automatically Calculating Gradients"},{"location":"deep_learning/practical_pytorch/pytorch_gradients/#summary","text":"We've learnt to... Success Tensor with Gradients Wraps a tensor for gradient accumulation Gradients Define original equation Substitute equation with x values Reduce to scalar output, o through mean Calculate gradients with o.backward() Then access gradients of the x tensor with requires_grad through x.grad","title":"Summary"},{"location":"deep_learning/practical_pytorch/pytorch_gradients/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/","text":"Linear Regression with PyTorch \u00b6 Run Jupyter Notebook You can run the code for this section in this jupyter notebook link . About Linear Regression \u00b6 Simple Linear Regression Basics \u00b6 Allows us to understand relationship between two continuous variables Example x: independent variable weight y: dependent variable height y = \\alpha x + \\beta y = \\alpha x + \\beta Example of simple linear regression \u00b6 Create plot for simple linear regression Take note that this code is not important at all. It simply creates random data points and does a simple best-fit line to best approximate the underlying function if one even exists. import numpy as np import matplotlib.pyplot as plt % matplotlib inline # Creates 50 random x and y numbers np . random . seed ( 1 ) n = 50 x = np . random . randn ( n ) y = x * np . random . randn ( n ) # Makes the dots colorful colors = np . random . rand ( n ) # Plots best-fit line via polyfit plt . plot ( np . unique ( x ), np . poly1d ( np . polyfit ( x , y , 1 ))( np . unique ( x ))) # Plots the random x and y data points we created # Interestingly, alpha makes it more aesthetically pleasing plt . scatter ( x , y , c = colors , alpha = 0.5 ) plt . show () Aim of Linear Regression \u00b6 Minimize the distance between the points and the line ( y = \\alpha x + \\beta y = \\alpha x + \\beta ) Adjusting Coefficient: \\alpha \\alpha Bias/intercept: \\beta \\beta Building a Linear Regression Model with PyTorch \u00b6 Example \u00b6 Coefficient: \\alpha = 2 \\alpha = 2 Bias/intercept: \\beta = 1 \\beta = 1 Equation: y = 2x + 1 y = 2x + 1 Building a Toy Dataset \u00b6 Create a list of values from 0 to 11 x_values = [ i for i in range ( 11 )] x_values [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] Convert list of numbers to numpy array # Convert to numpy x_train = np . array ( x_values , dtype = np . float32 ) x_train . shape ( 11 ,) Convert to 2-dimensional array If you don't this you will get an error stating you need 2D. Simply just reshape accordingly if you ever face such errors down the road. # IMPORTANT: 2D required x_train = x_train . reshape ( - 1 , 1 ) x_train . shape ( 11 , 1 ) Create list of y values We want y values for every x value we have above. y = 2x + 1 y = 2x + 1 y_values = [ 2 * i + 1 for i in x_values ] y_values [ 1 , 3 , 5 , 7 , 9 , 11 , 13 , 15 , 17 , 19 , 21 ] Alternative to create list of y values If you're weak in list iterators, this might be an easier alternative. # In case you're weak in list iterators... y_values = [] for i in x_values : result = 2 * i + 1 y_values . append ( result ) y_values [ 1 , 3 , 5 , 7 , 9 , 11 , 13 , 15 , 17 , 19 , 21 ] Convert to numpy array You will slowly get a hang on how when you deal with PyTorch tensors, you just keep on making sure your raw data is in numpy form to make sure everything's good. y_train = np . array ( y_values , dtype = np . float32 ) y_train . shape ( 11 ,) Reshape y numpy array to 2-dimension # IMPORTANT: 2D required y_train = y_train . reshape ( - 1 , 1 ) y_train . shape ( 11 , 1 ) Building Model \u00b6 Critical Imports import torch import torch.nn as nn Create Model Linear model True Equation: y = 2x + 1 y = 2x + 1 Forward Example Input x = 1 x = 1 Output \\hat y = ? \\hat y = ? # Create class class LinearRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LinearRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out Instantiate Model Class input: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] desired output: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21] input_dim = 1 output_dim = 1 model = LinearRegressionModel ( input_dim , output_dim ) Instantiate Loss Class MSE Loss: Mean Squared Error MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i)^2 MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i)^2 \\hat y \\hat y : prediction y y : true value criterion = nn . MSELoss () Instantiate Optimizer Class Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our variables) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : parameters' gradients Even simplier equation parameters = parameters - learning_rate * parameters_gradients parameters: \\alpha \\alpha and \\beta \\beta in y = \\alpha x + \\beta y = \\alpha x + \\beta desired parameters: \\alpha = 2 \\alpha = 2 and \\beta = 1 \\beta = 1 in y = 2x + 1 y = 2x + 1 learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) Train Model 1 epoch: going through the whole x_train data once 100 epochs: 100x mapping x_train = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] Process Convert inputs/labels to tensors with gradients Clear gradient buffets Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT epochs = 100 for epoch in range ( epochs ): epoch += 1 # Convert numpy array to torch Variable inputs = torch . from_numpy ( x_train ) . requires_grad_ () labels = torch . from_numpy ( y_train ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward to get output outputs = model ( inputs ) # Calculate Loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () print ( 'epoch {} , loss {} ' . format ( epoch , loss . item ())) epoch 1 , loss 140.58143615722656 epoch 2 , loss 11.467253684997559 epoch 3 , loss 0.9358152747154236 epoch 4 , loss 0.07679400593042374 epoch 5 , loss 0.0067212567664682865 epoch 6 , loss 0.0010006226366385818 epoch 7 , loss 0.0005289533291943371 epoch 8 , loss 0.0004854927829001099 epoch 9 , loss 0.00047700389404781163 epoch 10 , loss 0.0004714332753792405 epoch 11 , loss 0.00046614606981165707 epoch 12 , loss 0.0004609318566508591 epoch 13 , loss 0.0004557870561257005 epoch 14 , loss 0.00045069155748933554 epoch 15 , loss 0.00044567222357727587 epoch 16 , loss 0.00044068993884138763 epoch 17 , loss 0.00043576463940553367 epoch 18 , loss 0.00043090470717288554 epoch 19 , loss 0.00042609183583408594 epoch 20 , loss 0.0004213254142086953 epoch 21 , loss 0.0004166301223449409 epoch 22 , loss 0.0004119801160413772 epoch 23 , loss 0.00040738462121225893 epoch 24 , loss 0.0004028224211651832 epoch 25 , loss 0.0003983367350883782 epoch 26 , loss 0.0003938761365134269 epoch 27 , loss 0.000389480876037851 epoch 28 , loss 0.00038514015614055097 epoch 29 , loss 0.000380824290914461 epoch 30 , loss 0.00037657516077160835 epoch 31 , loss 0.000372376263840124 epoch 32 , loss 0.0003682126116473228 epoch 33 , loss 0.0003640959912445396 epoch 34 , loss 0.00036003670538775623 epoch 35 , loss 0.00035601368290372193 epoch 36 , loss 0.00035203873994760215 epoch 37 , loss 0.00034810820943675935 epoch 38 , loss 0.000344215368386358 epoch 39 , loss 0.0003403784066904336 epoch 40 , loss 0.00033658024040050805 epoch 41 , loss 0.0003328165039420128 epoch 42 , loss 0.0003291067841928452 epoch 43 , loss 0.0003254293987993151 epoch 44 , loss 0.0003217888588551432 epoch 45 , loss 0.0003182037326041609 epoch 46 , loss 0.0003146533854305744 epoch 47 , loss 0.00031113551813177764 epoch 48 , loss 0.0003076607536058873 epoch 49 , loss 0.00030422292184084654 epoch 50 , loss 0.00030083119054324925 epoch 51 , loss 0.00029746422660537064 epoch 52 , loss 0.0002941471466328949 epoch 53 , loss 0.00029085995629429817 epoch 54 , loss 0.0002876132493838668 epoch 55 , loss 0.00028440452297218144 epoch 56 , loss 0.00028122696676291525 epoch 57 , loss 0.00027808290906250477 epoch 58 , loss 0.00027497278642840683 epoch 59 , loss 0.00027190230321139097 epoch 60 , loss 0.00026887087733484805 epoch 61 , loss 0.0002658693410921842 epoch 62 , loss 0.0002629039518069476 epoch 63 , loss 0.00025996880140155554 epoch 64 , loss 0.0002570618235040456 epoch 65 , loss 0.00025419273879379034 epoch 66 , loss 0.00025135406758636236 epoch 67 , loss 0.0002485490695107728 epoch 68 , loss 0.0002457649679854512 epoch 69 , loss 0.0002430236927466467 epoch 70 , loss 0.00024031475186347961 epoch 71 , loss 0.00023762597993481904 epoch 72 , loss 0.00023497406800743192 epoch 73 , loss 0.0002323519001947716 epoch 74 , loss 0.00022976362379267812 epoch 75 , loss 0.0002271933335578069 epoch 76 , loss 0.00022465786605607718 epoch 77 , loss 0.00022214400814846158 epoch 78 , loss 0.00021966728672850877 epoch 79 , loss 0.0002172116219298914 epoch 80 , loss 0.00021478648704942316 epoch 81 , loss 0.00021239375928416848 epoch 82 , loss 0.0002100227284245193 epoch 83 , loss 0.00020767028036061674 epoch 84 , loss 0.00020534756185952574 epoch 85 , loss 0.00020305956422816962 epoch 86 , loss 0.0002007894654525444 epoch 87 , loss 0.00019854879064951092 epoch 88 , loss 0.00019633043848443776 epoch 89 , loss 0.00019413618429098278 epoch 90 , loss 0.00019197272195015103 epoch 91 , loss 0.0001898303598864004 epoch 92 , loss 0.00018771187751553953 epoch 93 , loss 0.00018561164324637502 epoch 94 , loss 0.00018354636267758906 epoch 95 , loss 0.00018149390234611928 epoch 96 , loss 0.0001794644631445408 epoch 97 , loss 0.00017746571393217891 epoch 98 , loss 0.00017548113828524947 epoch 99 , loss 0.00017352371651213616 epoch 100 , loss 0.00017157981346827 Looking at predicted values # Purely inference predicted = model ( torch . from_numpy ( x_train ) . requires_grad_ ()) . data . numpy () predicted array ([[ 0.9756333 ], [ 2.9791424 ], [ 4.982651 ], [ 6.9861603 ], [ 8.98967 ], [ 10.993179 ], [ 12.996688 ], [ 15.000196 ], [ 17.003706 ], [ 19.007215 ], [ 21.010725 ]], dtype = float32 ) Looking at training values These are the true values, you can see how it's able to predict similar values. # y = 2x + 1 y_train array ([[ 1. ], [ 3. ], [ 5. ], [ 7. ], [ 9. ], [ 11. ], [ 13. ], [ 15. ], [ 17. ], [ 19. ], [ 21. ]], dtype = float32 ) Plot of predicted and actual values # Clear figure plt . clf () # Get predictions predicted = model ( torch . from_numpy ( x_train ) . requires_grad_ ()) . data . numpy () # Plot true data plt . plot ( x_train , y_train , 'go' , label = 'True data' , alpha = 0.5 ) # Plot predictions plt . plot ( x_train , predicted , '--' , label = 'Predictions' , alpha = 0.5 ) # Legend and plot plt . legend ( loc = 'best' ) plt . show () Save Model save_model = False if save_model is True : # Saves only parameters # alpha & beta torch . save ( model . state_dict (), 'awesome_model.pkl' ) Load Model load_model = False if load_model is True : model . load_state_dict ( torch . load ( 'awesome_model.pkl' )) Building a Linear Regression Model with PyTorch (GPU) \u00b6 CPU Summary import torch import torch.nn as nn ''' STEP 1: CREATE MODEL CLASS ''' class LinearRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LinearRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out ''' STEP 2: INSTANTIATE MODEL CLASS ''' input_dim = 1 output_dim = 1 model = LinearRegressionModel ( input_dim , output_dim ) ''' STEP 3: INSTANTIATE LOSS CLASS ''' criterion = nn . MSELoss () ''' STEP 4: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 5: TRAIN THE MODEL ''' epochs = 100 for epoch in range ( epochs ): epoch += 1 # Convert numpy array to torch Variable inputs = torch . from_numpy ( x_train ) . requires_grad_ () labels = torch . from_numpy ( y_train ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward to get output outputs = model ( inputs ) # Calculate Loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () GPU Summary Just remember always 2 things must be on GPU model tensors import torch import torch.nn as nn import numpy as np ''' STEP 1: CREATE MODEL CLASS ''' class LinearRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LinearRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out ''' STEP 2: INSTANTIATE MODEL CLASS ''' input_dim = 1 output_dim = 1 model = LinearRegressionModel ( input_dim , output_dim ) ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 3: INSTANTIATE LOSS CLASS ''' criterion = nn . MSELoss () ''' STEP 4: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 5: TRAIN THE MODEL ''' epochs = 100 for epoch in range ( epochs ): epoch += 1 # Convert numpy array to torch Variable ####################### # USE GPU FOR MODEL # ####################### inputs = torch . from_numpy ( x_train ) . to ( device ) labels = torch . from_numpy ( y_train ) . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward to get output outputs = model ( inputs ) # Calculate Loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () # Logging print ( 'epoch {} , loss {} ' . format ( epoch , loss . item ())) epoch 1 , loss 336.0314025878906 epoch 2 , loss 27.67657470703125 epoch 3 , loss 2.5220539569854736 epoch 4 , loss 0.46732547879219055 epoch 5 , loss 0.2968060076236725 epoch 6 , loss 0.2800087630748749 epoch 7 , loss 0.27578213810920715 epoch 8 , loss 0.2726128399372101 epoch 9 , loss 0.269561231136322 epoch 10 , loss 0.2665504515171051 epoch 11 , loss 0.2635740041732788 epoch 12 , loss 0.26063060760498047 epoch 13 , loss 0.2577202618122101 epoch 14 , loss 0.2548423111438751 epoch 15 , loss 0.25199657678604126 epoch 16 , loss 0.24918246269226074 epoch 17 , loss 0.24639996886253357 epoch 18 , loss 0.24364829063415527 epoch 19 , loss 0.24092751741409302 epoch 20 , loss 0.2382371574640274 epoch 21 , loss 0.23557686805725098 epoch 22 , loss 0.2329462170600891 epoch 23 , loss 0.2303449958562851 epoch 24 , loss 0.22777271270751953 epoch 25 , loss 0.2252292037010193 epoch 26 , loss 0.22271405160427094 epoch 27 , loss 0.22022713720798492 epoch 28 , loss 0.21776780486106873 epoch 29 , loss 0.21533599495887756 epoch 30 , loss 0.21293145418167114 epoch 31 , loss 0.21055366098880768 epoch 32 , loss 0.20820240676403046 epoch 33 , loss 0.2058774083852768 epoch 34 , loss 0.20357847213745117 epoch 35 , loss 0.20130516588687897 epoch 36 , loss 0.1990572065114975 epoch 37 , loss 0.19683438539505005 epoch 38 , loss 0.19463638961315155 epoch 39 , loss 0.19246290624141693 epoch 40 , loss 0.1903136670589447 epoch 41 , loss 0.1881885528564453 epoch 42 , loss 0.18608702719211578 epoch 43 , loss 0.18400898575782776 epoch 44 , loss 0.18195408582687378 epoch 45 , loss 0.17992223799228668 epoch 46 , loss 0.17791320383548737 epoch 47 , loss 0.17592646181583405 epoch 48 , loss 0.17396186292171478 epoch 49 , loss 0.17201924324035645 epoch 50 , loss 0.17009828984737396 epoch 51 , loss 0.16819894313812256 epoch 52 , loss 0.16632060706615448 epoch 53 , loss 0.16446338593959808 epoch 54 , loss 0.16262666881084442 epoch 55 , loss 0.16081078350543976 epoch 56 , loss 0.15901507437229156 epoch 57 , loss 0.15723931789398193 epoch 58 , loss 0.15548335015773773 epoch 59 , loss 0.15374726057052612 epoch 60 , loss 0.1520303338766098 epoch 61 , loss 0.15033268928527832 epoch 62 , loss 0.14865389466285706 epoch 63 , loss 0.14699392020702362 epoch 64 , loss 0.14535246789455414 epoch 65 , loss 0.14372935891151428 epoch 66 , loss 0.14212435483932495 epoch 67 , loss 0.14053721725940704 epoch 68 , loss 0.13896773755550385 epoch 69 , loss 0.1374160647392273 epoch 70 , loss 0.1358814686536789 epoch 71 , loss 0.13436420261859894 epoch 72 , loss 0.13286370038986206 epoch 73 , loss 0.1313801407814026 epoch 74 , loss 0.12991292774677277 epoch 75 , loss 0.12846232950687408 epoch 76 , loss 0.1270277351140976 epoch 77 , loss 0.12560924887657166 epoch 78 , loss 0.12420656532049179 epoch 79 , loss 0.12281957268714905 epoch 80 , loss 0.1214480847120285 epoch 81 , loss 0.12009195983409882 epoch 82 , loss 0.1187509223818779 epoch 83 , loss 0.11742479354143143 epoch 84 , loss 0.11611353605985641 epoch 85 , loss 0.11481687426567078 epoch 86 , loss 0.11353478580713272 epoch 87 , loss 0.11226697266101837 epoch 88 , loss 0.11101329326629639 epoch 89 , loss 0.10977360606193542 epoch 90 , loss 0.10854770988225937 epoch 91 , loss 0.10733554512262344 epoch 92 , loss 0.10613703727722168 epoch 93 , loss 0.10495180636644363 epoch 94 , loss 0.10377981513738632 epoch 95 , loss 0.10262089222669601 epoch 96 , loss 0.10147502273321152 epoch 97 , loss 0.1003417894244194 epoch 98 , loss 0.09922132641077042 epoch 99 , loss 0.0981132984161377 epoch 100 , loss 0.09701769798994064 Summary \u00b6 We've learnt to... Success Simple linear regression basics y = Ax + B y = Ax + B y = 2x + 1 y = 2x + 1 Example of simple linear regression Aim of linear regression Minimizing distance between the points and the line Calculate \"distance\" through MSE Calculate gradients Update parameters with parameters = parameters - learning_rate * gradients Slowly update parameters A A and B B model the linear relationship between y y and x x of the form y = 2x + 1 y = 2x + 1 Built a linear regression model in CPU and GPU Step 1: Create Model Class Step 2: Instantiate Model Class Step 3: Instantiate Loss Class Step 4: Instantiate Optimizer Class Step 5: Train Model Important things to be on GPU model tensors with gradients How to bring to GPU ? model_name.to(device) variable_name.to(device) Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Linear Regression"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#linear-regression-with-pytorch","text":"Run Jupyter Notebook You can run the code for this section in this jupyter notebook link .","title":"Linear Regression with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#about-linear-regression","text":"","title":"About Linear Regression"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#simple-linear-regression-basics","text":"Allows us to understand relationship between two continuous variables Example x: independent variable weight y: dependent variable height y = \\alpha x + \\beta y = \\alpha x + \\beta","title":"Simple Linear Regression Basics"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#example-of-simple-linear-regression","text":"Create plot for simple linear regression Take note that this code is not important at all. It simply creates random data points and does a simple best-fit line to best approximate the underlying function if one even exists. import numpy as np import matplotlib.pyplot as plt % matplotlib inline # Creates 50 random x and y numbers np . random . seed ( 1 ) n = 50 x = np . random . randn ( n ) y = x * np . random . randn ( n ) # Makes the dots colorful colors = np . random . rand ( n ) # Plots best-fit line via polyfit plt . plot ( np . unique ( x ), np . poly1d ( np . polyfit ( x , y , 1 ))( np . unique ( x ))) # Plots the random x and y data points we created # Interestingly, alpha makes it more aesthetically pleasing plt . scatter ( x , y , c = colors , alpha = 0.5 ) plt . show ()","title":"Example of simple linear regression"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#aim-of-linear-regression","text":"Minimize the distance between the points and the line ( y = \\alpha x + \\beta y = \\alpha x + \\beta ) Adjusting Coefficient: \\alpha \\alpha Bias/intercept: \\beta \\beta","title":"Aim of Linear Regression"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#building-a-linear-regression-model-with-pytorch","text":"","title":"Building a Linear Regression Model with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#example","text":"Coefficient: \\alpha = 2 \\alpha = 2 Bias/intercept: \\beta = 1 \\beta = 1 Equation: y = 2x + 1 y = 2x + 1","title":"Example"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#building-a-toy-dataset","text":"Create a list of values from 0 to 11 x_values = [ i for i in range ( 11 )] x_values [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] Convert list of numbers to numpy array # Convert to numpy x_train = np . array ( x_values , dtype = np . float32 ) x_train . shape ( 11 ,) Convert to 2-dimensional array If you don't this you will get an error stating you need 2D. Simply just reshape accordingly if you ever face such errors down the road. # IMPORTANT: 2D required x_train = x_train . reshape ( - 1 , 1 ) x_train . shape ( 11 , 1 ) Create list of y values We want y values for every x value we have above. y = 2x + 1 y = 2x + 1 y_values = [ 2 * i + 1 for i in x_values ] y_values [ 1 , 3 , 5 , 7 , 9 , 11 , 13 , 15 , 17 , 19 , 21 ] Alternative to create list of y values If you're weak in list iterators, this might be an easier alternative. # In case you're weak in list iterators... y_values = [] for i in x_values : result = 2 * i + 1 y_values . append ( result ) y_values [ 1 , 3 , 5 , 7 , 9 , 11 , 13 , 15 , 17 , 19 , 21 ] Convert to numpy array You will slowly get a hang on how when you deal with PyTorch tensors, you just keep on making sure your raw data is in numpy form to make sure everything's good. y_train = np . array ( y_values , dtype = np . float32 ) y_train . shape ( 11 ,) Reshape y numpy array to 2-dimension # IMPORTANT: 2D required y_train = y_train . reshape ( - 1 , 1 ) y_train . shape ( 11 , 1 )","title":"Building a Toy Dataset"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#building-model","text":"Critical Imports import torch import torch.nn as nn Create Model Linear model True Equation: y = 2x + 1 y = 2x + 1 Forward Example Input x = 1 x = 1 Output \\hat y = ? \\hat y = ? # Create class class LinearRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LinearRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out Instantiate Model Class input: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] desired output: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21] input_dim = 1 output_dim = 1 model = LinearRegressionModel ( input_dim , output_dim ) Instantiate Loss Class MSE Loss: Mean Squared Error MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i)^2 MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i)^2 \\hat y \\hat y : prediction y y : true value criterion = nn . MSELoss () Instantiate Optimizer Class Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our variables) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : parameters' gradients Even simplier equation parameters = parameters - learning_rate * parameters_gradients parameters: \\alpha \\alpha and \\beta \\beta in y = \\alpha x + \\beta y = \\alpha x + \\beta desired parameters: \\alpha = 2 \\alpha = 2 and \\beta = 1 \\beta = 1 in y = 2x + 1 y = 2x + 1 learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) Train Model 1 epoch: going through the whole x_train data once 100 epochs: 100x mapping x_train = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] Process Convert inputs/labels to tensors with gradients Clear gradient buffets Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT epochs = 100 for epoch in range ( epochs ): epoch += 1 # Convert numpy array to torch Variable inputs = torch . from_numpy ( x_train ) . requires_grad_ () labels = torch . from_numpy ( y_train ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward to get output outputs = model ( inputs ) # Calculate Loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () print ( 'epoch {} , loss {} ' . format ( epoch , loss . item ())) epoch 1 , loss 140.58143615722656 epoch 2 , loss 11.467253684997559 epoch 3 , loss 0.9358152747154236 epoch 4 , loss 0.07679400593042374 epoch 5 , loss 0.0067212567664682865 epoch 6 , loss 0.0010006226366385818 epoch 7 , loss 0.0005289533291943371 epoch 8 , loss 0.0004854927829001099 epoch 9 , loss 0.00047700389404781163 epoch 10 , loss 0.0004714332753792405 epoch 11 , loss 0.00046614606981165707 epoch 12 , loss 0.0004609318566508591 epoch 13 , loss 0.0004557870561257005 epoch 14 , loss 0.00045069155748933554 epoch 15 , loss 0.00044567222357727587 epoch 16 , loss 0.00044068993884138763 epoch 17 , loss 0.00043576463940553367 epoch 18 , loss 0.00043090470717288554 epoch 19 , loss 0.00042609183583408594 epoch 20 , loss 0.0004213254142086953 epoch 21 , loss 0.0004166301223449409 epoch 22 , loss 0.0004119801160413772 epoch 23 , loss 0.00040738462121225893 epoch 24 , loss 0.0004028224211651832 epoch 25 , loss 0.0003983367350883782 epoch 26 , loss 0.0003938761365134269 epoch 27 , loss 0.000389480876037851 epoch 28 , loss 0.00038514015614055097 epoch 29 , loss 0.000380824290914461 epoch 30 , loss 0.00037657516077160835 epoch 31 , loss 0.000372376263840124 epoch 32 , loss 0.0003682126116473228 epoch 33 , loss 0.0003640959912445396 epoch 34 , loss 0.00036003670538775623 epoch 35 , loss 0.00035601368290372193 epoch 36 , loss 0.00035203873994760215 epoch 37 , loss 0.00034810820943675935 epoch 38 , loss 0.000344215368386358 epoch 39 , loss 0.0003403784066904336 epoch 40 , loss 0.00033658024040050805 epoch 41 , loss 0.0003328165039420128 epoch 42 , loss 0.0003291067841928452 epoch 43 , loss 0.0003254293987993151 epoch 44 , loss 0.0003217888588551432 epoch 45 , loss 0.0003182037326041609 epoch 46 , loss 0.0003146533854305744 epoch 47 , loss 0.00031113551813177764 epoch 48 , loss 0.0003076607536058873 epoch 49 , loss 0.00030422292184084654 epoch 50 , loss 0.00030083119054324925 epoch 51 , loss 0.00029746422660537064 epoch 52 , loss 0.0002941471466328949 epoch 53 , loss 0.00029085995629429817 epoch 54 , loss 0.0002876132493838668 epoch 55 , loss 0.00028440452297218144 epoch 56 , loss 0.00028122696676291525 epoch 57 , loss 0.00027808290906250477 epoch 58 , loss 0.00027497278642840683 epoch 59 , loss 0.00027190230321139097 epoch 60 , loss 0.00026887087733484805 epoch 61 , loss 0.0002658693410921842 epoch 62 , loss 0.0002629039518069476 epoch 63 , loss 0.00025996880140155554 epoch 64 , loss 0.0002570618235040456 epoch 65 , loss 0.00025419273879379034 epoch 66 , loss 0.00025135406758636236 epoch 67 , loss 0.0002485490695107728 epoch 68 , loss 0.0002457649679854512 epoch 69 , loss 0.0002430236927466467 epoch 70 , loss 0.00024031475186347961 epoch 71 , loss 0.00023762597993481904 epoch 72 , loss 0.00023497406800743192 epoch 73 , loss 0.0002323519001947716 epoch 74 , loss 0.00022976362379267812 epoch 75 , loss 0.0002271933335578069 epoch 76 , loss 0.00022465786605607718 epoch 77 , loss 0.00022214400814846158 epoch 78 , loss 0.00021966728672850877 epoch 79 , loss 0.0002172116219298914 epoch 80 , loss 0.00021478648704942316 epoch 81 , loss 0.00021239375928416848 epoch 82 , loss 0.0002100227284245193 epoch 83 , loss 0.00020767028036061674 epoch 84 , loss 0.00020534756185952574 epoch 85 , loss 0.00020305956422816962 epoch 86 , loss 0.0002007894654525444 epoch 87 , loss 0.00019854879064951092 epoch 88 , loss 0.00019633043848443776 epoch 89 , loss 0.00019413618429098278 epoch 90 , loss 0.00019197272195015103 epoch 91 , loss 0.0001898303598864004 epoch 92 , loss 0.00018771187751553953 epoch 93 , loss 0.00018561164324637502 epoch 94 , loss 0.00018354636267758906 epoch 95 , loss 0.00018149390234611928 epoch 96 , loss 0.0001794644631445408 epoch 97 , loss 0.00017746571393217891 epoch 98 , loss 0.00017548113828524947 epoch 99 , loss 0.00017352371651213616 epoch 100 , loss 0.00017157981346827 Looking at predicted values # Purely inference predicted = model ( torch . from_numpy ( x_train ) . requires_grad_ ()) . data . numpy () predicted array ([[ 0.9756333 ], [ 2.9791424 ], [ 4.982651 ], [ 6.9861603 ], [ 8.98967 ], [ 10.993179 ], [ 12.996688 ], [ 15.000196 ], [ 17.003706 ], [ 19.007215 ], [ 21.010725 ]], dtype = float32 ) Looking at training values These are the true values, you can see how it's able to predict similar values. # y = 2x + 1 y_train array ([[ 1. ], [ 3. ], [ 5. ], [ 7. ], [ 9. ], [ 11. ], [ 13. ], [ 15. ], [ 17. ], [ 19. ], [ 21. ]], dtype = float32 ) Plot of predicted and actual values # Clear figure plt . clf () # Get predictions predicted = model ( torch . from_numpy ( x_train ) . requires_grad_ ()) . data . numpy () # Plot true data plt . plot ( x_train , y_train , 'go' , label = 'True data' , alpha = 0.5 ) # Plot predictions plt . plot ( x_train , predicted , '--' , label = 'Predictions' , alpha = 0.5 ) # Legend and plot plt . legend ( loc = 'best' ) plt . show () Save Model save_model = False if save_model is True : # Saves only parameters # alpha & beta torch . save ( model . state_dict (), 'awesome_model.pkl' ) Load Model load_model = False if load_model is True : model . load_state_dict ( torch . load ( 'awesome_model.pkl' ))","title":"Building Model"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#building-a-linear-regression-model-with-pytorch-gpu","text":"CPU Summary import torch import torch.nn as nn ''' STEP 1: CREATE MODEL CLASS ''' class LinearRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LinearRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out ''' STEP 2: INSTANTIATE MODEL CLASS ''' input_dim = 1 output_dim = 1 model = LinearRegressionModel ( input_dim , output_dim ) ''' STEP 3: INSTANTIATE LOSS CLASS ''' criterion = nn . MSELoss () ''' STEP 4: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 5: TRAIN THE MODEL ''' epochs = 100 for epoch in range ( epochs ): epoch += 1 # Convert numpy array to torch Variable inputs = torch . from_numpy ( x_train ) . requires_grad_ () labels = torch . from_numpy ( y_train ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward to get output outputs = model ( inputs ) # Calculate Loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () GPU Summary Just remember always 2 things must be on GPU model tensors import torch import torch.nn as nn import numpy as np ''' STEP 1: CREATE MODEL CLASS ''' class LinearRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LinearRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out ''' STEP 2: INSTANTIATE MODEL CLASS ''' input_dim = 1 output_dim = 1 model = LinearRegressionModel ( input_dim , output_dim ) ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 3: INSTANTIATE LOSS CLASS ''' criterion = nn . MSELoss () ''' STEP 4: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 5: TRAIN THE MODEL ''' epochs = 100 for epoch in range ( epochs ): epoch += 1 # Convert numpy array to torch Variable ####################### # USE GPU FOR MODEL # ####################### inputs = torch . from_numpy ( x_train ) . to ( device ) labels = torch . from_numpy ( y_train ) . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward to get output outputs = model ( inputs ) # Calculate Loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () # Logging print ( 'epoch {} , loss {} ' . format ( epoch , loss . item ())) epoch 1 , loss 336.0314025878906 epoch 2 , loss 27.67657470703125 epoch 3 , loss 2.5220539569854736 epoch 4 , loss 0.46732547879219055 epoch 5 , loss 0.2968060076236725 epoch 6 , loss 0.2800087630748749 epoch 7 , loss 0.27578213810920715 epoch 8 , loss 0.2726128399372101 epoch 9 , loss 0.269561231136322 epoch 10 , loss 0.2665504515171051 epoch 11 , loss 0.2635740041732788 epoch 12 , loss 0.26063060760498047 epoch 13 , loss 0.2577202618122101 epoch 14 , loss 0.2548423111438751 epoch 15 , loss 0.25199657678604126 epoch 16 , loss 0.24918246269226074 epoch 17 , loss 0.24639996886253357 epoch 18 , loss 0.24364829063415527 epoch 19 , loss 0.24092751741409302 epoch 20 , loss 0.2382371574640274 epoch 21 , loss 0.23557686805725098 epoch 22 , loss 0.2329462170600891 epoch 23 , loss 0.2303449958562851 epoch 24 , loss 0.22777271270751953 epoch 25 , loss 0.2252292037010193 epoch 26 , loss 0.22271405160427094 epoch 27 , loss 0.22022713720798492 epoch 28 , loss 0.21776780486106873 epoch 29 , loss 0.21533599495887756 epoch 30 , loss 0.21293145418167114 epoch 31 , loss 0.21055366098880768 epoch 32 , loss 0.20820240676403046 epoch 33 , loss 0.2058774083852768 epoch 34 , loss 0.20357847213745117 epoch 35 , loss 0.20130516588687897 epoch 36 , loss 0.1990572065114975 epoch 37 , loss 0.19683438539505005 epoch 38 , loss 0.19463638961315155 epoch 39 , loss 0.19246290624141693 epoch 40 , loss 0.1903136670589447 epoch 41 , loss 0.1881885528564453 epoch 42 , loss 0.18608702719211578 epoch 43 , loss 0.18400898575782776 epoch 44 , loss 0.18195408582687378 epoch 45 , loss 0.17992223799228668 epoch 46 , loss 0.17791320383548737 epoch 47 , loss 0.17592646181583405 epoch 48 , loss 0.17396186292171478 epoch 49 , loss 0.17201924324035645 epoch 50 , loss 0.17009828984737396 epoch 51 , loss 0.16819894313812256 epoch 52 , loss 0.16632060706615448 epoch 53 , loss 0.16446338593959808 epoch 54 , loss 0.16262666881084442 epoch 55 , loss 0.16081078350543976 epoch 56 , loss 0.15901507437229156 epoch 57 , loss 0.15723931789398193 epoch 58 , loss 0.15548335015773773 epoch 59 , loss 0.15374726057052612 epoch 60 , loss 0.1520303338766098 epoch 61 , loss 0.15033268928527832 epoch 62 , loss 0.14865389466285706 epoch 63 , loss 0.14699392020702362 epoch 64 , loss 0.14535246789455414 epoch 65 , loss 0.14372935891151428 epoch 66 , loss 0.14212435483932495 epoch 67 , loss 0.14053721725940704 epoch 68 , loss 0.13896773755550385 epoch 69 , loss 0.1374160647392273 epoch 70 , loss 0.1358814686536789 epoch 71 , loss 0.13436420261859894 epoch 72 , loss 0.13286370038986206 epoch 73 , loss 0.1313801407814026 epoch 74 , loss 0.12991292774677277 epoch 75 , loss 0.12846232950687408 epoch 76 , loss 0.1270277351140976 epoch 77 , loss 0.12560924887657166 epoch 78 , loss 0.12420656532049179 epoch 79 , loss 0.12281957268714905 epoch 80 , loss 0.1214480847120285 epoch 81 , loss 0.12009195983409882 epoch 82 , loss 0.1187509223818779 epoch 83 , loss 0.11742479354143143 epoch 84 , loss 0.11611353605985641 epoch 85 , loss 0.11481687426567078 epoch 86 , loss 0.11353478580713272 epoch 87 , loss 0.11226697266101837 epoch 88 , loss 0.11101329326629639 epoch 89 , loss 0.10977360606193542 epoch 90 , loss 0.10854770988225937 epoch 91 , loss 0.10733554512262344 epoch 92 , loss 0.10613703727722168 epoch 93 , loss 0.10495180636644363 epoch 94 , loss 0.10377981513738632 epoch 95 , loss 0.10262089222669601 epoch 96 , loss 0.10147502273321152 epoch 97 , loss 0.1003417894244194 epoch 98 , loss 0.09922132641077042 epoch 99 , loss 0.0981132984161377 epoch 100 , loss 0.09701769798994064","title":"Building a Linear Regression Model with PyTorch (GPU)"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#summary","text":"We've learnt to... Success Simple linear regression basics y = Ax + B y = Ax + B y = 2x + 1 y = 2x + 1 Example of simple linear regression Aim of linear regression Minimizing distance between the points and the line Calculate \"distance\" through MSE Calculate gradients Update parameters with parameters = parameters - learning_rate * gradients Slowly update parameters A A and B B model the linear relationship between y y and x x of the form y = 2x + 1 y = 2x + 1 Built a linear regression model in CPU and GPU Step 1: Create Model Class Step 2: Instantiate Model Class Step 3: Instantiate Loss Class Step 4: Instantiate Optimizer Class Step 5: Train Model Important things to be on GPU model tensors with gradients How to bring to GPU ? model_name.to(device) variable_name.to(device)","title":"Summary"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/","text":"Logistic Regression with PyTorch \u00b6 Run Jupyter Notebook You can run the code for this section in this jupyter notebook link . About Logistic Regression \u00b6 Logistic Regression Basics \u00b6 Classification algorithm \u00b6 Example: Spam vs No Spam Input: Bunch of words Output: Probability spam or not Basic Comparison \u00b6 Linear regression Output: numeric value given inputs Logistic regression : Output: probability [0, 1] given input belonging to a class Input/Output Comparison \u00b6 Linear regression: Multiplication Input: [1] Output: 2 Input: [2] Output: 4 Trying to model the relationship y = 2x Logistic regression: Spam Input: \"Sign up to get 1 million dollars by tonight\" Output: p = 0.8 Input: \"This is a receipt for your recent purchase with Amazon\" Output: p = 0.3 p: probability it is spam Problems of Linear Regression \u00b6 Example Fever Input : temperature Output : fever or no fever Remember Linear regression : minimize error between points and line Linear Regression Problem 1: Fever value can go negative (below 0) and positive (above 1) If you simply tried to do a simple linear regression on this fever problem, you would realize an apparent error. Fever can go beyond 1 and below 0 which does not make sense in this context. import numpy as np import matplotlib.pyplot as plt % matplotlib inline x = [ 1 , 5 , 10 , 10 , 25 , 50 , 70 , 75 , 100 ,] y = [ 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 ] colors = np . random . rand ( len ( x )) plt . plot ( np . unique ( x ), np . poly1d ( np . polyfit ( x , y , 1 ))( np . unique ( x ))) plt . ylabel ( \"Fever\" ) plt . xlabel ( \"Temperature\" ) plt . scatter ( x , y , c = colors , alpha = 0.5 ) plt . show () Linear Regression Problem 2: Fever points are not predicted with the presence of outliers Previously at least some points could be properly predicted. However, with the presence of outliers, everything goes wonky for simple linear regression, having no predictive capacity at all. import numpy as np import matplotlib.pyplot as plt x = [ 1 , 5 , 10 , 10 , 25 , 50 , 70 , 75 , 300 ] y = [ 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 ] colors = np . random . rand ( len ( x )) plt . plot ( np . unique ( x ), np . poly1d ( np . polyfit ( x , y , 1 ))( np . unique ( x ))) plt . ylabel ( \"Fever\" ) plt . xlabel ( \"Temperature\" ) plt . scatter ( x , y , c = colors , alpha = 0.5 ) plt . show () Logistic Regression In-Depth \u00b6 Predicting Probability \u00b6 Linear regression doesn't work Instead of predicting direct values: predict probability Logistic Function g() \u00b6 \"Two-class logistic regression\" \\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b} \\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b} Where \\boldsymbol{y} \\boldsymbol{y} is a vector comprising the 2-class prediction y_0 y_0 and y_1 y_1 Where the labels are y_0 = 0 y_0 = 0 and y_1 = 1 y_1 = 1 Also, it's bolded because it's a vector, not a matrix. g(y_1) = \\frac {1} {1 + e^{-y_1}} g(y_1) = \\frac {1} {1 + e^{-y_1}} g(y_1) g(y_1) = Estimated probability that y = 1 y = 1 g(y_0) = 1 - g(y_1) g(y_0) = 1 - g(y_1) g(y_0) g(y_0) = Estimated probability that y = 0 y = 0 For our illustration above, we have 4 classes, so we have to use softmax function explained below Softmax Function g() \u00b6 \"Multi-class logistic regression\" Generalization of logistic function, where you can derive back to the logistic function if you've a 2 class classification problem Here, we will use a 4 class example (K = 4) as shown above to be very clear in how it relates back to that simple examaple. \\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b} \\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b} Where \\boldsymbol{y} \\boldsymbol{y} is a vector comprising the 4-class prediction y_0, y_1, y_2, y_3 y_0, y_1, y_2, y_3 Where the 4 labels (K = 4) are y_0 = 0, y_1 = 1, y_2 = 2, y_3 = 3 y_0 = 0, y_1 = 1, y_2 = 2, y_3 = 3 g(y_i) = \\frac {e^{y_i} } {\\sum^K_i e^{y_i}} g(y_i) = \\frac {e^{y_i} } {\\sum^K_i e^{y_i}} where K = 4 because we have 4 classes To put numbers to this equation in relation to the illustration above where we've y_0 = 1.3, y_1 = 1.2, y = 4.5, y = 4.8 y_0 = 1.3, y_1 = 1.2, y = 4.5, y = 4.8 g(y_0) = \\frac {e^{1.3}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.017 g(y_0) = \\frac {e^{1.3}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.017 g(y_1) = \\frac {e^{1.2}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.015 g(y_1) = \\frac {e^{1.2}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.015 g(y_2) = \\frac {e^{4.5}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.412 g(y_2) = \\frac {e^{4.5}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.412 g(y_3) = \\frac {e^{4.8}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.556 g(y_3) = \\frac {e^{4.8}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.556 g(y_0) + g(y_1) + g(y_2) + g(y_3) = 1.0 g(y_0) + g(y_1) + g(y_2) + g(y_3) = 1.0 All softmax outputs have to sum to one as they represent a probability distribution over K classes. Take note how these numbers are not exactly as in the illustration in the softmax box but the concept is important (intentionally made so). y_0 y_0 and y_1 y_1 are approximately similar in values and they return similar probabilities. Similarly, y_2 y_2 and y_3 y_3 are approximately similar in values and they return similar probabilities. Softmax versus Soft(arg)max Do you know many researchers and anyone in deep learning in general use the term softmax when it should be soft(arg)max. This is because soft(arg)max returns the probability distribution over K classes, a vector. However, softmax only returns the max! This means you will be getting a scalar value versus a probability distribution. According to my friend, Alfredo Canziani (postdoc in NYU under Yann Lecun), it was actually a mistake made in the original paper previously but it was too late because the term softmax was adopted. Full credits to him for this tip. Cross Entropy Function D() for 2 Class \u00b6 Take note that here, S S is our softmax outputs and L L are our labels D(S, L) = -(L log S + (1-L)log(1-S)) D(S, L) = -(L log S + (1-L)log(1-S)) If L = 0 (label) D(S, 0) = - log(1-S) D(S, 0) = - log(1-S) - log(1-S) - log(1-S) : less positive if S \\longrightarrow 0 S \\longrightarrow 0 - log(1-S) - log(1-S) : more positive if S \\longrightarrow 1 S \\longrightarrow 1 (BIGGER LOSS) If L = 1 (label) D(S, 1) = - log S D(S, 1) = - log S -log(S) -log(S) : less positive if S \\longrightarrow 1 S \\longrightarrow 1 -log(S) -log(S) : more positive if S \\longrightarrow 0 S \\longrightarrow 0 (BIGGER LOSS) Numerical example of bigger or small loss You get a small error of 1e-5 if your label = 0 and your S is closer to 0 (very correct prediction). import math print ( - math . log ( 1 - 0.00001 )) You get a large error of 11.51 if your label is 0 and S is near to 1 (very wrong prediction). print ( - math . log ( 1 - 0.99999 )) You get a small error of -1e-5 if your label is 1 and S is near 1 (very correct prediction). print ( - math . log ( 0.99999 )) You get a big error of -11.51 if your label is 1 and S is near 0 (very wrong prediction). print ( - math . log ( 0.00001 )) 1.0000050000287824e-05 11.51292546497478 1.0000050000287824e-05 11.512925464970229 Cross Entropy Function D() for More Than 2 Class \u00b6 For the case where we have more than 2 class, we need a more generalized function D(S, L) = - \\sum^K_1 L_i log(S_i) D(S, L) = - \\sum^K_1 L_i log(S_i) K K : number of classes L_i L_i : label of i-th class, 1 if that's the class else 0 S_i S_i : output of softmax for i-th class Cross Entropy Loss over N samples \u00b6 Goal: Minimizing Cross Entropy Loss, L Loss = \\frac {1}{N} \\sum_j^N D_j Loss = \\frac {1}{N} \\sum_j^N D_j D_j D_j : j-th sample of cross entropy function D(S, L) D(S, L) N N : number of samples Loss Loss : average cross entropy loss over N samples Building a Logistic Regression Model with PyTorch \u00b6 Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Step 1a: Loading MNIST Train Dataset \u00b6 Images from 1 to 9 Inspect length of training dataset You can easily load MNIST dataset with PyTorch. Here we inspect the training set, where our algorithms will learn from, and you will discover it is made up of 60,000 images. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) len ( train_dataset ) 60000 Inspecting a single image So this is how a single image is represented in numbers. It's actually a 28 pixel x 28 pixel image which is why you would end up with this 28x28 matrix of numbers. train_dataset [ 0 ] ( tensor ([[[ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0118 , 0.0706 , 0.0706 , 0.0706 , 0.4941 , 0.5333 , 0.6863 , 0.1020 , 0.6510 , 1.0000 , 0.9686 , 0.4980 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1176 , 0.1412 , 0.3686 , 0.6039 , 0.6667 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.8824 , 0.6745 , 0.9922 , 0.9490 , 0.7647 , 0.2510 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1922 , 0.9333 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9843 , 0.3647 , 0.3216 , 0.3216 , 0.2196 , 0.1529 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0706 , 0.8588 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.7765 , 0.7137 , 0.9686 , 0.9451 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.3137 , 0.6118 , 0.4196 , 0.9922 , 0.9922 , 0.8039 , 0.0431 , 0.0000 , 0.1686 , 0.6039 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0549 , 0.0039 , 0.6039 , 0.9922 , 0.3529 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.5451 , 0.9922 , 0.7451 , 0.0078 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0431 , 0.7451 , 0.9922 , 0.2745 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1373 , 0.9451 , 0.8824 , 0.6275 , 0.4235 , 0.0039 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.3176 , 0.9412 , 0.9922 , 0.9922 , 0.4667 , 0.0980 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1765 , 0.7294 , 0.9922 , 0.9922 , 0.5882 , 0.1059 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0627 , 0.3647 , 0.9882 , 0.9922 , 0.7333 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.9765 , 0.9922 , 0.9765 , 0.2510 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1804 , 0.5098 , 0.7176 , 0.9922 , 0.9922 , 0.8118 , 0.0078 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1529 , 0.5804 , 0.8980 , 0.9922 , 0.9922 , 0.9922 , 0.9804 , 0.7137 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0941 , 0.4471 , 0.8667 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.7882 , 0.3059 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0902 , 0.2588 , 0.8353 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.7765 , 0.3176 , 0.0078 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0706 , 0.6706 , 0.8588 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.7647 , 0.3137 , 0.0353 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.2157 , 0.6745 , 0.8863 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9569 , 0.5216 , 0.0431 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.5333 , 0.9922 , 0.9922 , 0.9922 , 0.8314 , 0.5294 , 0.5176 , 0.0627 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ]]]), tensor ( 5 )) Inspecting a single data point in the training dataset When you load MNIST dataset, each data point is actually a tuple containing the image matrix and the label. type ( train_dataset [ 0 ]) tuple Inspecting training dataset first element of tuple This means to access the image, you need to access the first element in the tuple. # Input Matrix train_dataset [ 0 ][ 0 ] . size () # A 28x28 sized image of a digit torch . Size ([ 1 , 28 , 28 ]) Inspecting training dataset second element of tuple The second element actually represents the image's label. Meaning if the second element says 5, it means the 28x28 matrix of numbers represent a digit 5. # Label train_dataset [ 0 ][ 1 ] tensor ( 5 ) Displaying MNIST \u00b6 Verifying shape of MNIST image As mentioned, a single MNIST image is of the shape 28 pixel x 28 pixel. import matplotlib.pyplot as plt % matplotlib inline import numpy as np train_dataset [ 0 ][ 0 ] . numpy () . shape ( 1 , 28 , 28 ) Plot image of MNIST image show_img = train_dataset [ 0 ][ 0 ] . numpy () . reshape ( 28 , 28 ) plt . imshow ( show_img , cmap = 'gray' ) Second element of tuple shows label As you would expect, the label is 5. # Label train_dataset [ 0 ][ 1 ] tensor ( 5 ) Plot second image of MNIST image show_img = train_dataset [ 1 ][ 0 ] . numpy () . reshape ( 28 , 28 ) plt . imshow ( show_img , cmap = 'gray' ) Second element of tuple shows label We should see 0 here as the label. # Label train_dataset [ 1 ][ 1 ] tensor ( 0 ) Step 1b: Loading MNIST Test Dataset \u00b6 Show our algorithm works beyond the data we have trained on. Out-of-sample Load test dataset Compared to the 60k images in the training set, the testing set where the model will not be trained on has 10k images to check for its out-of-sample performance. test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) len ( test_dataset ) 10000 Test dataset elements Exactly like the training set, the testing set has 10k tuples containing the 28x28 matrices and their respective labels. type ( test_dataset [ 0 ]) tuple Test dataset first element in tuple This contains the image matrix, similar to the training set. # Image matrix test_dataset [ 0 ][ 0 ] . size () torch . Size ([ 1 , 28 , 28 ]) Plot image sample from test dataset show_img = test_dataset [ 0 ][ 0 ] . numpy () . reshape ( 28 , 28 ) plt . imshow ( show_img , cmap = 'gray' ) Test dataset second element in tuple # Label test_dataset [ 0 ][ 1 ] tensor ( 7 ) Step 2: Make Dataset Iterable \u00b6 Aim: make the dataset iterable totaldata : 60000 minibatch : 100 Number of examples in 1 iteration iterations : 3000 1 iteration: one mini-batch forward & backward pass epochs 1 epoch: running through the whole dataset once epochs = iterations \\div \\frac{totaldata}{minibatch} = 3000 \\div \\frac{60000}{100} = 5 epochs = iterations \\div \\frac{totaldata}{minibatch} = 3000 \\div \\frac{60000}{100} = 5 Recap training dataset Remember training dataset has 60k images and testing dataset has 10k images. len ( train_dataset ) 60000 Defining epochs When the model goes through the whole 60k images once, learning how to classify 0-9, it's consider 1 epoch. However, there's a concept of batch size where it means the model would look at 100 images before updating the model's weights, thereby learning. When the model updates its weights (parameters) after looking at all the images, this is considered 1 iteration. batch_size = 100 We arbitrarily set 3000 iterations here which means the model would update 3000 times. n_iters = 3000 One epoch consists of 60,000 / 100 = 600 iterations. Because we would like to go through 3000 iterations, this implies we would have 3000 / 600 = 5 epochs as each epoch has 600 iterations. num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) num_epochs 5 Create Iterable Object: Training Dataset train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) Check Iterability import collections isinstance ( train_loader , collections . Iterable ) True Create Iterable Object: Testing Dataset # Iterable object test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) Check iterability of testing dataset isinstance ( test_loader , collections . Iterable ) True Iterate through dataset This is just a simplified example of what we're doing above where we're creating an iterable object lst to loop through so we can access all the images img_1 and img_2 . Above, the equivalent of lst is train_loader and test_loader . img_1 = np . ones (( 28 , 28 )) img_2 = np . ones (( 28 , 28 )) lst = [ img_1 , img_2 ] # Need to iterate # Think of numbers as the images for i in lst : print ( i . shape ) ( 28 , 28 ) ( 28 , 28 ) Step 3: Building Model \u00b6 Create model class # Same as linear regression! class LogisticRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LogisticRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out Step 4: Instantiate Model Class \u00b6 Input dimension: Size of image 28 \\times 28 = 784 28 \\times 28 = 784 Output dimension: 10 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 Check size of dataset This should be 28x28. # Size of images train_dataset [ 0 ][ 0 ] . size () torch . Size ([ 1 , 28 , 28 ]) Instantiate model class based on input and out dimensions As we're trying to classify digits 0-9 a total of 10 classes, our output dimension is 10. And we're feeding the model with 28x28 images, hence our input dimension is 28x28. input_dim = 28 * 28 output_dim = 10 model = LogisticRegressionModel ( input_dim , output_dim ) Step 5: Instantiate Loss Class \u00b6 Logistic Regression : Cross Entropy Loss Linear Regression: MSE Create Cross Entry Loss Class Unlike linear regression, we do not use MSE here, we need Cross Entry Loss to calculate our loss before we backpropagate and update our parameters. criterion = nn . CrossEntropyLoss () What happens in nn.CrossEntropyLoss()? It does 2 things at the same time. 1. Computes softmax (logistic/softmax function) 2. Computes cross entropy Step 6: Instantiate Optimizer Class \u00b6 Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our variables) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : parameters' gradients Even simplier equation parameters = parameters - learning_rate * parameters_gradients At every iteration, we update our model's parameters Create optimizer Similar to what we've covered above, this calculates the parameters' gradients and update them subsequently. learning_rate = 0.001 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) Parameters In-Depth You'll realize we have 2 sets of parameters, 10x784 which is A and 10x1 which is b in the y = AX + b y = AX + b equation where X is our input of size 784. We'll go into details subsequently how these parameters interact with our input to produce our 10x1 output. # Type of parameter object print ( model . parameters ()) # Length of parameters print ( len ( list ( model . parameters ()))) # FC 1 Parameters print ( list ( model . parameters ())[ 0 ] . size ()) # FC 1 Bias Parameters print ( list ( model . parameters ())[ 1 ] . size ()) < generator object Module . parameters at 0x7ff7c884f830 > 2 torch . Size ([ 10 , 784 ]) torch . Size ([ 10 ]) Quick Matrix Product Review Example 1: matrix product A: (100, 10) A: (100, 10) B: (10, 1) B: (10, 1) A \\cdot B = (100, 10) \\cdot (10, 1) = (100, 1) A \\cdot B = (100, 10) \\cdot (10, 1) = (100, 1) Example 2: matrix product A: (50, 5) A: (50, 5) B: (5, 2) B: (5, 2) A \\cdot B = (50, 5) \\cdot (5, 2) = (50, 2) A \\cdot B = (50, 5) \\cdot (5, 2) = (50, 2) Example 3: element-wise addition A: (10, 1) A: (10, 1) B: (10, 1) B: (10, 1) A + B = (10, 1) A + B = (10, 1) Step 7: Train Model \u00b6 7 step process for training models Process Convert inputs/labels to tensors with gradients Clear gradient buffets Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () labels = labels # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 1.8513233661651611 . Accuracy : 70 Iteration : 1000. Loss : 1.5732524394989014 . Accuracy : 77 Iteration : 1500. Loss : 1.3840199708938599 . Accuracy : 79 Iteration : 2000. Loss : 1.1711134910583496 . Accuracy : 81 Iteration : 2500. Loss : 1.1094708442687988 . Accuracy : 82 Iteration : 3000. Loss : 1.002761721611023 . Accuracy : 82 Break Down Accuracy Calculation \u00b6 Printing outputs of our model As we've trained our model, we can extract the accuracy calculation portion to understand what's happening without re-training the model. This would print out the output of the model's predictions on your notebook. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) if iter_test == 1 : print ( 'OUTPUTS' ) print ( outputs ) _ , predicted = torch . max ( outputs . data , 1 ) OUTPUTS tensor ([[ - 0.4181 , - 1.0784 , - 0.4840 , - 0.0985 , - 0.2394 , - 0.1801 , - 1.1639 , 2.9352 , - 0.1552 , 0.8852 ], [ 0.5117 , - 0.1099 , 1.5295 , 0.8863 , - 1.8813 , 0.5967 , 1.3632 , - 1.8977 , 0.4183 , - 1.4990 ], [ - 1.0126 , 2.4112 , 0.2373 , 0.0857 , - 0.7007 , - 0.2015 , - 0.3428 , - 0.2548 , 0.1659 , - 0.4703 ], [ 2.8072 , - 2.2973 , - 0.0984 , - 0.4313 , - 0.9619 , 0.8670 , 1.2201 , 0.3752 , - 0.2873 , - 0.3272 ], [ - 0.0343 , - 2.0043 , 0.5081 , - 0.6452 , 1.8647 , - 0.6924 , 0.1435 , 0.4330 , 0.2958 , 1.0339 ], [ - 1.5392 , 2.9070 , 0.2297 , 0.3139 , - 0.6863 , - 0.2734 , - 0.8377 , - 0.1238 , 0.3285 , - 0.3004 ], [ - 1.2037 , - 1.3739 , - 0.5947 , 0.3530 , 1.4205 , 0.0593 , - 0.7307 , 0.6642 , 0.3937 , 0.8004 ], [ - 1.4439 , - 0.3284 , - 0.7652 , - 0.0952 , 0.9323 , 0.3006 , 0.0238 , - 0.0810 , 0.0612 , 1.3295 ], [ 0.5409 , - 0.5266 , 0.9914 , - 1.2369 , 0.6583 , 0.0992 , 0.8525 , - 1.0562 , 0.2013 , 0.0462 ], [ - 0.6548 , - 0.7253 , - 0.9825 , - 1.1663 , 0.9076 , - 0.0694 , - 0.3708 , 1.8270 , 0.2457 , 1.5921 ], [ 3.2147 , - 1.7689 , 0.8531 , 1.2320 , - 0.8126 , 1.1251 , - 0.2776 , - 1.4244 , 0.5930 , - 1.6183 ], [ 0.7470 , - 0.5545 , 1.0251 , 0.0529 , 0.4384 , - 0.5934 , 0.7666 , - 1.0084 , 0.5313 , - 0.3465 ], [ - 0.7916 , - 1.7064 , - 0.7805 , - 1.1588 , 1.3284 , - 0.1708 , - 0.2092 , 0.9495 , 0.1033 , 2.0208 ], [ 3.0602 , - 2.3578 , - 0.2576 , - 0.2198 , - 0.2372 , 0.9765 , - 0.1514 , - 0.5380 , 0.7970 , 0.1374 ], [ - 1.2613 , 2.8594 , - 0.0874 , 0.1974 , - 1.2018 , - 0.0064 , - 0.0923 , - 0.2142 , 0.2575 , - 0.3218 ], [ 0.4348 , - 0.7216 , 0.0021 , 1.2864 , - 0.5062 , 0.7761 , - 0.3236 , - 0.5667 , 0.5431 , - 0.7781 ], [ - 0.2157 , - 2.0200 , 0.1829 , - 0.6882 , 1.3815 , - 0.7609 , - 0.0902 , 0.8647 , 0.3679 , 1.8843 ], [ 0.0950 , - 1.5009 , - 0.6347 , 0.3662 , - 0.4679 , - 0.0359 , - 0.7671 , 2.7155 , - 0.3991 , 0.5737 ], [ - 0.7005 , - 0.5366 , - 0.0434 , 1.1289 , - 0.5873 , 0.2555 , 0.8187 , - 0.6557 , 0.1241 , - 0.4297 ], [ - 1.0635 , - 1.5991 , - 0.4677 , - 0.1231 , 2.0445 , 0.1128 , - 0.1825 , 0.1075 , 0.0348 , 1.4317 ], [ - 1.0319 , - 0.1595 , - 1.3415 , 0.1095 , 0.5339 , 0.1973 , - 1.3272 , 1.5765 , 0.4784 , 1.4176 ], [ - 0.4928 , - 1.5653 , - 0.0672 , 0.3325 , 0.5359 , 0.5368 , 2.1542 , - 1.4276 , 0.3605 , 0.0587 ], [ - 0.4761 , 0.2958 , 0.6597 , - 0.2658 , 1.1279 , - 1.0676 , 1.2506 , - 0.2059 , - 0.1489 , 0.1051 ], [ - 0.0764 , - 0.9274 , - 0.6838 , 0.3464 , - 0.2656 , 1.4099 , 0.4486 , - 0.9527 , 0.5682 , 0.0156 ], [ - 0.6900 , - 0.9611 , 0.1395 , - 0.0079 , 1.5424 , - 0.3208 , - 0.2682 , 0.3586 , - 0.2771 , 1.0389 ], [ 4.3606 , - 2.8621 , 0.6310 , - 0.9657 , - 0.2486 , 1.2009 , 1.1873 , - 0.8255 , - 0.2103 , - 1.2172 ], [ - 0.1000 , - 1.4268 , - 0.4627 , - 0.1041 , 0.2959 , - 0.1392 , - 0.6855 , 1.8622 , - 0.2580 , 1.1347 ], [ - 0.3625 , - 2.1323 , - 0.2224 , - 0.8754 , 2.4684 , 0.0295 , 0.1161 , - 0.2660 , 0.3037 , 1.4570 ], [ 2.8688 , - 2.4517 , 0.1782 , 1.1149 , - 1.0898 , 1.1062 , - 0.0681 , - 0.5697 , 0.8888 , - 0.6965 ], [ - 1.0429 , 1.4446 , - 0.3349 , 0.1254 , - 0.5017 , 0.2286 , 0.2328 , - 0.3290 , 0.3949 , - 0.2586 ], [ - 0.8476 , - 0.0004 , - 1.1003 , 2.2806 , - 1.2226 , 0.9251 , - 0.3165 , 0.4957 , 0.0690 , 0.0232 ], [ - 0.9108 , 1.1355 , - 0.2715 , 0.2233 , - 0.3681 , 0.1442 , - 0.0001 , - 0.0174 , 0.1454 , 0.2286 ], [ - 1.0663 , - 0.8466 , - 0.7147 , 2.5685 , - 0.2090 , 1.2993 , - 0.3057 , - 0.8314 , 0.7046 , - 0.0176 ], [ 1.7013 , - 1.8051 , 0.7541 , - 1.5248 , 0.8972 , 0.1518 , 1.4876 , - 0.8454 , - 0.2022 , - 0.2829 ], [ - 0.8179 , - 0.1239 , 0.8630 , - 0.2137 , - 0.2275 , - 0.5411 , - 1.3448 , 1.7354 , 0.7751 , 0.6234 ], [ 0.6515 , - 1.0431 , 2.7165 , 0.1873 , - 1.0623 , 0.1286 , 0.3597 , - 0.2739 , 0.3871 , - 1.6699 ], [ - 0.2828 , - 1.4663 , 0.1182 , - 0.0896 , - 0.3640 , - 0.5129 , - 0.4905 , 2.2914 , - 0.2227 , 0.9463 ], [ - 1.2596 , 2.0468 , - 0.4405 , - 0.0411 , - 0.8073 , 0.0490 , - 0.0604 , - 0.1206 , 0.3504 , - 0.1059 ], [ 0.6089 , 0.5885 , 0.7898 , 1.1318 , - 1.9008 , 0.5875 , 0.4227 , - 1.1815 , 0.5652 , - 1.3590 ], [ - 1.4551 , 2.9537 , - 0.2805 , 0.2372 , - 1.4180 , 0.0297 , - 0.1515 , - 0.6111 , 0.6140 , - 0.3354 ], [ - 0.7182 , 1.6778 , 0.0553 , 0.0461 , - 0.5446 , - 0.0338 , - 0.0215 , - 0.0881 , 0.1506 , - 0.2107 ], [ - 0.8027 , - 0.7854 , - 0.1275 , - 0.3177 , - 0.1600 , - 0.1964 , - 0.6084 , 2.1285 , - 0.1815 , 1.1911 ], [ - 2.0656 , - 0.4959 , - 0.1154 , - 0.1363 , 2.2426 , - 0.7441 , - 0.8413 , 0.4675 , 0.3269 , 1.7279 ], [ - 0.3004 , 1.0166 , 1.1175 , - 0.0618 , - 0.0937 , - 0.4221 , 0.1943 , - 1.1020 , 0.3670 , - 0.4683 ], [ - 1.0720 , 0.2252 , 0.0175 , 1.3644 , - 0.7409 , 0.4655 , 0.5439 , 0.0380 , 0.1279 , - 0.2302 ], [ 0.2409 , - 1.2622 , - 0.6336 , 1.8240 , - 0.5951 , 1.3408 , 0.2130 , - 1.3789 , 0.8363 , - 0.2101 ], [ - 1.3849 , 0.3773 , - 0.0585 , 0.6896 , - 0.0998 , 0.2804 , 0.0696 , - 0.2529 , 0.3143 , 0.3409 ], [ - 0.9103 , - 0.1578 , 1.6673 , - 0.4817 , 0.4088 , - 0.5484 , 0.6103 , - 0.2287 , - 0.0665 , 0.0055 ], [ - 1.1692 , - 2.8531 , - 1.2499 , - 0.0257 , 2.8580 , 0.2616 , - 0.7122 , - 0.0551 , 0.8112 , 2.3233 ], [ - 0.2790 , - 1.9494 , 0.6096 , - 0.5653 , 2.2792 , - 1.0687 , 0.1634 , 0.3122 , 0.1053 , 1.0884 ], [ 0.1267 , - 1.2297 , - 0.1315 , 0.2428 , - 0.5436 , 0.4123 , 2.3060 , - 0.9278 , - 0.1528 , - 0.4224 ], [ - 0.0235 , - 0.9137 , - 0.1457 , 1.6858 , - 0.7552 , 0.7293 , 0.2510 , - 0.3955 , - 0.2187 , - 0.1505 ], [ 0.5643 , - 1.2783 , - 1.4149 , 0.0304 , 0.8375 , 1.5018 , 0.0338 , - 0.3875 , - 0.0117 , 0.5751 ], [ 0.2926 , - 0.7486 , - 0.3238 , 1.0384 , 0.0308 , 0.6792 , - 0.0170 , - 0.5797 , 0.2819 , - 0.3510 ], [ 0.1219 , - 0.5862 , 1.5817 , - 0.1297 , 0.4730 , - 0.9171 , 0.7886 , - 0.7022 , - 0.0501 , - 0.2812 ], [ 1.7587 , - 2.4511 , - 0.7369 , 0.4082 , - 0.6426 , 1.1784 , 0.6052 , - 0.7178 , 1.6161 , - 0.2220 ], [ - 0.1267 , - 2.6719 , 0.0505 , - 0.4972 , 2.9027 , - 0.1461 , 0.2807 , - 0.2921 , 0.2231 , 1.1327 ], [ - 0.9892 , 2.4401 , 0.1274 , 0.2838 , - 0.7535 , - 0.1684 , - 0.6493 , - 0.1908 , 0.2290 , - 0.2150 ], [ - 0.2071 , - 2.1351 , - 0.9191 , - 0.9309 , 1.7747 , - 0.3046 , 0.0183 , 1.0136 , - 0.1016 , 2.1288 ], [ - 0.0103 , 0.3280 , - 0.6974 , - 0.2504 , 0.3187 , 0.4390 , - 0.1879 , 0.3954 , 0.2332 , - 0.1971 ], [ - 0.2280 , - 1.6754 , - 0.7438 , 0.5078 , 0.2544 , - 0.1020 , - 0.2503 , 2.0799 , - 0.5033 , 0.5890 ], [ 0.3972 , - 0.9369 , 1.2696 , - 1.6713 , - 0.4159 , - 0.0221 , 0.6489 , - 0.4777 , 1.2497 , 0.3931 ], [ - 0.7566 , - 0.8230 , - 0.0785 , - 0.3083 , 0.7821 , 0.1880 , 0.1037 , - 0.0956 , 0.4219 , 1.0798 ], [ - 1.0328 , - 0.1700 , 1.3806 , 0.5445 , - 0.2624 , - 0.0780 , - 0.3595 , - 0.6253 , 0.4309 , 0.1813 ], [ - 1.0360 , - 0.4704 , 0.1948 , - 0.7066 , 0.6600 , - 0.4633 , - 0.3602 , 1.7494 , 0.1522 , 0.6086 ], [ - 1.2032 , - 0.7903 , - 0.5754 , 0.4722 , 0.6068 , 0.5752 , 0.2151 , - 0.2495 , 0.3420 , 0.9278 ], [ 0.2247 , - 0.1361 , 0.9374 , - 0.1543 , 0.4921 , - 0.6553 , 0.5885 , 0.2617 , - 0.2216 , - 0.3736 ], [ - 0.2867 , - 1.4486 , 0.6658 , - 0.8755 , 2.3195 , - 0.7627 , - 0.2132 , 0.2488 , 0.3484 , 1.0860 ], [ - 1.4031 , - 0.4518 , - 0.3181 , 2.8268 , - 0.5371 , 1.0154 , - 0.9247 , - 0.7385 , 1.1031 , 0.0422 ], [ 2.8604 , - 1.5413 , 0.6241 , - 0.8017 , - 1.4104 , 0.6314 , 0.4614 , - 0.0218 , - 0.3411 , - 0.2609 ], [ 0.2113 , - 1.2348 , - 0.8535 , - 0.1041 , - 0.2703 , - 0.1294 , - 0.7057 , 2.7552 , - 0.4429 , 0.4517 ], [ 4.5191 , - 2.7407 , 1.1091 , 0.3975 , - 0.9456 , 1.2277 , 0.3616 , - 1.6564 , 0.5063 , - 1.4274 ], [ 1.4615 , - 1.0765 , 1.8388 , 1.5006 , - 1.2351 , 0.2781 , 0.2830 , - 0.8491 , 0.2222 , - 1.7779 ], [ - 1.2160 , 0.8502 , 0.2413 , - 0.0798 , - 0.7880 , - 0.4286 , - 0.8060 , 0.7194 , 1.2663 , 0.6412 ], [ - 1.3318 , 2.3388 , - 0.4003 , - 0.1094 , - 1.0285 , 0.1021 , - 0.0388 , - 0.0497 , 0.5137 , - 0.2507 ], [ - 1.7853 , 0.5884 , - 0.6108 , - 0.5557 , 0.8696 , - 0.6226 , - 0.7983 , 1.7169 , - 0.0145 , 0.8231 ], [ - 0.1739 , 0.1562 , - 0.2933 , 2.3195 , - 0.9480 , 1.2019 , - 0.4834 , - 1.0567 , 0.5685 , - 0.6841 ], [ - 0.7920 , - 0.3339 , 0.7452 , - 0.6529 , - 0.3307 , - 0.6092 , - 0.0950 , 1.7311 , - 0.3481 , 0.3801 ], [ - 1.7810 , 1.0676 , - 0.7611 , 0.3658 , - 0.0431 , - 0.1012 , - 0.6048 , 0.3089 , 0.9998 , 0.7164 ], [ - 0.5856 , - 0.5261 , - 0.4859 , - 1.0551 , - 0.1838 , - 0.2144 , - 1.2599 , 3.3891 , 0.4691 , 0.7566 ], [ - 0.4984 , - 1.7770 , - 1.1998 , - 0.1075 , 1.0882 , 0.4539 , - 0.5651 , 1.4381 , - 0.5678 , 1.7479 ], [ 0.2938 , - 1.8536 , 0.4259 , - 0.5429 , 0.0066 , 0.4120 , 2.3793 , - 0.3666 , - 0.2604 , 0.0382 ], [ - 0.4080 , - 0.9851 , 4.0264 , 0.1099 , - 0.1766 , - 1.1557 , 0.6419 , - 0.8147 , 0.7535 , - 1.1452 ], [ - 0.4636 , - 1.7323 , - 0.6433 , - 0.0274 , 0.7227 , - 0.1799 , - 0.9336 , 2.1881 , - 0.2073 , 1.6522 ], [ - 0.9617 , - 0.0348 , - 0.3980 , - 0.4738 , 0.7790 , 0.4671 , - 0.6115 , - 0.7067 , 1.3036 , 0.4923 ], [ - 1.0151 , - 2.5385 , - 0.6072 , 0.2902 , 3.1570 , 0.1062 , - 0.2169 , - 0.4491 , 0.6326 , 1.6829 ], [ - 1.8852 , 0.6066 , - 0.2840 , - 0.4475 , - 0.1147 , - 0.7858 , - 1.1805 , 3.0723 , 0.3960 , 0.9720 ], [ 0.0344 , - 1.4878 , - 0.9675 , 1.9649 , - 0.3146 , 1.2183 , 0.6730 , - 0.3650 , 0.0646 , - 0.0898 ], [ - 0.2118 , - 2.0350 , 0.9917 , - 0.8993 , 1.2334 , - 0.6723 , 2.5847 , - 0.0454 , - 0.4149 , 0.3927 ], [ - 1.7365 , 3.0447 , 0.5115 , 0.0786 , - 0.7544 , - 0.2158 , - 0.4876 , - 0.2891 , 0.5089 , - 0.6719 ], [ 0.3652 , - 0.5457 , - 0.1167 , 2.9056 , - 1.1622 , 0.8192 , - 1.3245 , - 0.6414 , 0.8097 , - 0.4958 ], [ - 0.8755 , - 0.6983 , 0.2208 , - 0.6463 , 0.5276 , 0.1145 , 2.7229 , - 1.0316 , 0.1905 , 0.2090 ], [ - 0.9702 , 0.1265 , - 0.0007 , - 0.5106 , 0.4970 , - 0.0804 , 0.0017 , 0.0607 , 0.6164 , 0.4490 ], [ - 0.8271 , - 0.6822 , - 0.7434 , 2.6457 , - 1.6143 , 1.1486 , - 1.0705 , 0.5611 , 0.6422 , 0.1250 ], [ - 1.9979 , 1.8175 , - 0.1658 , - 0.0343 , - 0.6292 , 0.1774 , 0.3150 , - 0.4633 , 0.9266 , 0.0252 ], [ - 0.9039 , - 0.6030 , - 0.2173 , - 1.1768 , 2.3198 , - 0.5072 , 0.3418 , - 0.1551 , 0.1282 , 1.4250 ], [ - 0.9891 , 0.5212 , - 0.4518 , 0.3267 , - 0.0759 , 0.3826 , - 0.0341 , 0.0382 , 0.2451 , 0.3658 ], [ - 2.1217 , 1.5102 , - 0.7828 , 0.3554 , - 0.4192 , - 0.0772 , 0.0578 , 0.8070 , 0.1701 , 0.5880 ], [ 1.0665 , - 1.3826 , 0.6243 , - 0.8096 , - 0.4227 , 0.5925 , 1.8112 , - 0.9946 , 0.2010 , - 0.7731 ], [ - 1.1263 , - 1.7484 , 0.0041 , - 0.5439 , 1.7242 , - 0.9475 , - 0.3835 , 0.8452 , 0.3077 , 2.2689 ]]) Printing output size This produces a 100x10 matrix because each iteration has a batch size of 100 and each prediction across the 10 classes, with the largest number indicating the likely number it is predicting. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) if iter_test == 1 : print ( 'OUTPUTS' ) print ( outputs . size ()) _ , predicted = torch . max ( outputs . data , 1 ) OUTPUTS torch . Size ([ 100 , 10 ]) Printing one output This would be a 1x10 matrix where the largest number is what the model thinks the image is. Here we can see that in the tensor, position 7 has the largest number, indicating the model thinks the image is 7. number 0: -0.4181 number 1: -1.0784 ... number 7: 2.9352 iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) if iter_test == 1 : print ( 'OUTPUTS' ) print ( outputs [ 0 , :]) _ , predicted = torch . max ( outputs . data , 1 ) OUTPUTS tensor([-0.4181, -1.0784, -0.4840, -0.0985, -0.2394, -0.1801, -1.1639, 2.9352, -0.1552, 0.8852]) Printing prediction output Because our output is of size 100 (our batch size), our prediction size would also of the size 100. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) if iter_test == 1 : print ( 'PREDICTION' ) print ( predicted . size ()) PREDICTION torch . Size ([ 100 ]) Print prediction value We are printing our prediction which as verified above, should be digit 7. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) if iter_test == 1 : print ( 'PREDICTION' ) print ( predicted [ 0 ]) PREDICTION tensor ( 7 ) Print prediction, label and label size We are trying to show what we are predicting and the actual values. In this case, we're predicting the right value 7! iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) if iter_test == 1 : print ( 'PREDICTION' ) print ( predicted [ 0 ]) print ( 'LABEL SIZE' ) print ( labels . size ()) print ( 'LABEL FOR IMAGE 0' ) print ( labels [ 0 ]) PREDICTION tensor ( 7 ) LABEL SIZE torch . Size ([ 100 ]) LABEL FOR IMAGE 0 tensor ( 7 ) Print second prediction and ground truth Again, the prediction is correct. Naturally, as our model is quite competent in this simple task. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) if iter_test == 1 : print ( 'PREDICTION' ) print ( predicted [ 1 ]) print ( 'LABEL SIZE' ) print ( labels . size ()) print ( 'LABEL FOR IMAGE 1' ) print ( labels [ 1 ]) PREDICTION tensor ( 2 ) LABEL SIZE torch . Size ([ 100 ]) LABEL FOR IMAGE 1 tensor ( 2 ) Print accuracy Now we know what each object represents, we can understand how we arrived at our accuracy numbers. One last thing to note is that correct.item() has this syntax is because correct is a PyTorch tensor and to get the value to compute with total which is an integer, we need to do this. correct = 0 total = 0 iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * ( correct . item () / total ) print ( accuracy ) 82.94 Explanation of Python's .sum() function Python's .sum() function allows you to do a comparison between two matrices and sum the ones that return True or in our case, those predictions that match actual labels (correct predictions). # Explaining .sum() python built-in function # correct += (predicted == labels).sum() import numpy as np a = np . ones (( 10 )) print ( a ) b = np . ones (( 10 )) print ( b ) print ( a == b ) print (( a == b ) . sum ()) # matrix a [ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. ] # matrix b [ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. ] # boolean array [ True True True True True True True True True True ] # number of elementswhere a matches b 10 Saving Model \u00b6 Saving PyTorch model This is how you save your model. Feel free to just change save_model = True to save your model save_model = False if save_model is True : # Saves only parameters torch . save ( model . state_dict (), 'awesome_model.pkl' ) Building a Logistic Regression Model with PyTorch (GPU) \u00b6 CPU version The usual 7-step process, getting repetitive by now which we like. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class LogisticRegressionModel ( nn . Module ): def __init__ ( self , input_size , num_classes ): super ( LogisticRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 output_dim = 10 model = LogisticRegressionModel ( input_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.001 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () labels = labels # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # 100 x 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value # 100 x 1 _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct . item () / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 1.876196026802063 . Accuracy : 64.44 Iteration : 1000. Loss : 1.5153584480285645 . Accuracy : 75.68 Iteration : 1500. Loss : 1.3521136045455933 . Accuracy : 78.98 Iteration : 2000. Loss : 1.2136967182159424 . Accuracy : 80.95 Iteration : 2500. Loss : 1.0934826135635376 . Accuracy : 81.97 Iteration : 3000. Loss : 1.024120569229126 . Accuracy : 82.49 GPU version 2 things must be on GPU - model - tensors Remember step 4 and 7 will be affected and this will be the same for all model building moving forward. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class LogisticRegressionModel ( nn . Module ): def __init__ ( self , input_size , num_classes ): super ( LogisticRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 output_dim = 10 model = LogisticRegressionModel ( input_dim , output_dim ) ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.001 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , 28 * 28 ) . to ( device ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) ####################### # USE GPU FOR MODEL # ####################### # Total correct predictions if torch . cuda . is_available (): correct += ( predicted . cpu () == labels . cpu ()) . sum () else : correct += ( predicted == labels ) . sum () accuracy = 100 * correct . item () / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 1.8571407794952393 . Accuracy : 68.99 Iteration : 1000. Loss : 1.5415704250335693 . Accuracy : 75.86 Iteration : 1500. Loss : 1.2755383253097534 . Accuracy : 78.92 Iteration : 2000. Loss : 1.2468739748001099 . Accuracy : 80.72 Iteration : 2500. Loss : 1.0708973407745361 . Accuracy : 81.73 Iteration : 3000. Loss : 1.0359245538711548 . Accuracy : 82.74 Summary \u00b6 We've learnt to... Success Logistic regression basics Problems of linear regression In-depth Logistic Regression Get logits Get softmax Get cross-entropy loss Aim : reduce cross-entropy loss Built a logistic regression model in CPU and GPU Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Important things to be on GPU model tensors with gradients Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Logistic Regression"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#logistic-regression-with-pytorch","text":"Run Jupyter Notebook You can run the code for this section in this jupyter notebook link .","title":"Logistic Regression with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#about-logistic-regression","text":"","title":"About Logistic Regression"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#logistic-regression-basics","text":"","title":"Logistic Regression Basics"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#classification-algorithm","text":"Example: Spam vs No Spam Input: Bunch of words Output: Probability spam or not","title":"Classification algorithm"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#basic-comparison","text":"Linear regression Output: numeric value given inputs Logistic regression : Output: probability [0, 1] given input belonging to a class","title":"Basic Comparison"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#inputoutput-comparison","text":"Linear regression: Multiplication Input: [1] Output: 2 Input: [2] Output: 4 Trying to model the relationship y = 2x Logistic regression: Spam Input: \"Sign up to get 1 million dollars by tonight\" Output: p = 0.8 Input: \"This is a receipt for your recent purchase with Amazon\" Output: p = 0.3 p: probability it is spam","title":"Input/Output Comparison"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#problems-of-linear-regression","text":"Example Fever Input : temperature Output : fever or no fever Remember Linear regression : minimize error between points and line Linear Regression Problem 1: Fever value can go negative (below 0) and positive (above 1) If you simply tried to do a simple linear regression on this fever problem, you would realize an apparent error. Fever can go beyond 1 and below 0 which does not make sense in this context. import numpy as np import matplotlib.pyplot as plt % matplotlib inline x = [ 1 , 5 , 10 , 10 , 25 , 50 , 70 , 75 , 100 ,] y = [ 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 ] colors = np . random . rand ( len ( x )) plt . plot ( np . unique ( x ), np . poly1d ( np . polyfit ( x , y , 1 ))( np . unique ( x ))) plt . ylabel ( \"Fever\" ) plt . xlabel ( \"Temperature\" ) plt . scatter ( x , y , c = colors , alpha = 0.5 ) plt . show () Linear Regression Problem 2: Fever points are not predicted with the presence of outliers Previously at least some points could be properly predicted. However, with the presence of outliers, everything goes wonky for simple linear regression, having no predictive capacity at all. import numpy as np import matplotlib.pyplot as plt x = [ 1 , 5 , 10 , 10 , 25 , 50 , 70 , 75 , 300 ] y = [ 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 ] colors = np . random . rand ( len ( x )) plt . plot ( np . unique ( x ), np . poly1d ( np . polyfit ( x , y , 1 ))( np . unique ( x ))) plt . ylabel ( \"Fever\" ) plt . xlabel ( \"Temperature\" ) plt . scatter ( x , y , c = colors , alpha = 0.5 ) plt . show ()","title":"Problems of Linear Regression"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#logistic-regression-in-depth","text":"","title":"Logistic Regression In-Depth"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#predicting-probability","text":"Linear regression doesn't work Instead of predicting direct values: predict probability","title":"Predicting Probability"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#logistic-function-g","text":"\"Two-class logistic regression\" \\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b} \\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b} Where \\boldsymbol{y} \\boldsymbol{y} is a vector comprising the 2-class prediction y_0 y_0 and y_1 y_1 Where the labels are y_0 = 0 y_0 = 0 and y_1 = 1 y_1 = 1 Also, it's bolded because it's a vector, not a matrix. g(y_1) = \\frac {1} {1 + e^{-y_1}} g(y_1) = \\frac {1} {1 + e^{-y_1}} g(y_1) g(y_1) = Estimated probability that y = 1 y = 1 g(y_0) = 1 - g(y_1) g(y_0) = 1 - g(y_1) g(y_0) g(y_0) = Estimated probability that y = 0 y = 0 For our illustration above, we have 4 classes, so we have to use softmax function explained below","title":"Logistic Function g()"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#softmax-function-g","text":"\"Multi-class logistic regression\" Generalization of logistic function, where you can derive back to the logistic function if you've a 2 class classification problem Here, we will use a 4 class example (K = 4) as shown above to be very clear in how it relates back to that simple examaple. \\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b} \\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b} Where \\boldsymbol{y} \\boldsymbol{y} is a vector comprising the 4-class prediction y_0, y_1, y_2, y_3 y_0, y_1, y_2, y_3 Where the 4 labels (K = 4) are y_0 = 0, y_1 = 1, y_2 = 2, y_3 = 3 y_0 = 0, y_1 = 1, y_2 = 2, y_3 = 3 g(y_i) = \\frac {e^{y_i} } {\\sum^K_i e^{y_i}} g(y_i) = \\frac {e^{y_i} } {\\sum^K_i e^{y_i}} where K = 4 because we have 4 classes To put numbers to this equation in relation to the illustration above where we've y_0 = 1.3, y_1 = 1.2, y = 4.5, y = 4.8 y_0 = 1.3, y_1 = 1.2, y = 4.5, y = 4.8 g(y_0) = \\frac {e^{1.3}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.017 g(y_0) = \\frac {e^{1.3}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.017 g(y_1) = \\frac {e^{1.2}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.015 g(y_1) = \\frac {e^{1.2}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.015 g(y_2) = \\frac {e^{4.5}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.412 g(y_2) = \\frac {e^{4.5}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.412 g(y_3) = \\frac {e^{4.8}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.556 g(y_3) = \\frac {e^{4.8}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.556 g(y_0) + g(y_1) + g(y_2) + g(y_3) = 1.0 g(y_0) + g(y_1) + g(y_2) + g(y_3) = 1.0 All softmax outputs have to sum to one as they represent a probability distribution over K classes. Take note how these numbers are not exactly as in the illustration in the softmax box but the concept is important (intentionally made so). y_0 y_0 and y_1 y_1 are approximately similar in values and they return similar probabilities. Similarly, y_2 y_2 and y_3 y_3 are approximately similar in values and they return similar probabilities. Softmax versus Soft(arg)max Do you know many researchers and anyone in deep learning in general use the term softmax when it should be soft(arg)max. This is because soft(arg)max returns the probability distribution over K classes, a vector. However, softmax only returns the max! This means you will be getting a scalar value versus a probability distribution. According to my friend, Alfredo Canziani (postdoc in NYU under Yann Lecun), it was actually a mistake made in the original paper previously but it was too late because the term softmax was adopted. Full credits to him for this tip.","title":"Softmax Function g()"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#cross-entropy-function-d-for-2-class","text":"Take note that here, S S is our softmax outputs and L L are our labels D(S, L) = -(L log S + (1-L)log(1-S)) D(S, L) = -(L log S + (1-L)log(1-S)) If L = 0 (label) D(S, 0) = - log(1-S) D(S, 0) = - log(1-S) - log(1-S) - log(1-S) : less positive if S \\longrightarrow 0 S \\longrightarrow 0 - log(1-S) - log(1-S) : more positive if S \\longrightarrow 1 S \\longrightarrow 1 (BIGGER LOSS) If L = 1 (label) D(S, 1) = - log S D(S, 1) = - log S -log(S) -log(S) : less positive if S \\longrightarrow 1 S \\longrightarrow 1 -log(S) -log(S) : more positive if S \\longrightarrow 0 S \\longrightarrow 0 (BIGGER LOSS) Numerical example of bigger or small loss You get a small error of 1e-5 if your label = 0 and your S is closer to 0 (very correct prediction). import math print ( - math . log ( 1 - 0.00001 )) You get a large error of 11.51 if your label is 0 and S is near to 1 (very wrong prediction). print ( - math . log ( 1 - 0.99999 )) You get a small error of -1e-5 if your label is 1 and S is near 1 (very correct prediction). print ( - math . log ( 0.99999 )) You get a big error of -11.51 if your label is 1 and S is near 0 (very wrong prediction). print ( - math . log ( 0.00001 )) 1.0000050000287824e-05 11.51292546497478 1.0000050000287824e-05 11.512925464970229","title":"Cross Entropy Function D() for 2 Class"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#cross-entropy-function-d-for-more-than-2-class","text":"For the case where we have more than 2 class, we need a more generalized function D(S, L) = - \\sum^K_1 L_i log(S_i) D(S, L) = - \\sum^K_1 L_i log(S_i) K K : number of classes L_i L_i : label of i-th class, 1 if that's the class else 0 S_i S_i : output of softmax for i-th class","title":"Cross Entropy Function D() for More Than 2 Class"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#cross-entropy-loss-over-n-samples","text":"Goal: Minimizing Cross Entropy Loss, L Loss = \\frac {1}{N} \\sum_j^N D_j Loss = \\frac {1}{N} \\sum_j^N D_j D_j D_j : j-th sample of cross entropy function D(S, L) D(S, L) N N : number of samples Loss Loss : average cross entropy loss over N samples","title":"Cross Entropy Loss over N samples"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#building-a-logistic-regression-model-with-pytorch","text":"","title":"Building a Logistic Regression Model with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#steps","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-1a-loading-mnist-train-dataset","text":"Images from 1 to 9 Inspect length of training dataset You can easily load MNIST dataset with PyTorch. Here we inspect the training set, where our algorithms will learn from, and you will discover it is made up of 60,000 images. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) len ( train_dataset ) 60000 Inspecting a single image So this is how a single image is represented in numbers. It's actually a 28 pixel x 28 pixel image which is why you would end up with this 28x28 matrix of numbers. train_dataset [ 0 ] ( tensor ([[[ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0118 , 0.0706 , 0.0706 , 0.0706 , 0.4941 , 0.5333 , 0.6863 , 0.1020 , 0.6510 , 1.0000 , 0.9686 , 0.4980 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1176 , 0.1412 , 0.3686 , 0.6039 , 0.6667 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.8824 , 0.6745 , 0.9922 , 0.9490 , 0.7647 , 0.2510 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1922 , 0.9333 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9843 , 0.3647 , 0.3216 , 0.3216 , 0.2196 , 0.1529 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0706 , 0.8588 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.7765 , 0.7137 , 0.9686 , 0.9451 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.3137 , 0.6118 , 0.4196 , 0.9922 , 0.9922 , 0.8039 , 0.0431 , 0.0000 , 0.1686 , 0.6039 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0549 , 0.0039 , 0.6039 , 0.9922 , 0.3529 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.5451 , 0.9922 , 0.7451 , 0.0078 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0431 , 0.7451 , 0.9922 , 0.2745 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1373 , 0.9451 , 0.8824 , 0.6275 , 0.4235 , 0.0039 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.3176 , 0.9412 , 0.9922 , 0.9922 , 0.4667 , 0.0980 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1765 , 0.7294 , 0.9922 , 0.9922 , 0.5882 , 0.1059 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0627 , 0.3647 , 0.9882 , 0.9922 , 0.7333 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.9765 , 0.9922 , 0.9765 , 0.2510 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1804 , 0.5098 , 0.7176 , 0.9922 , 0.9922 , 0.8118 , 0.0078 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1529 , 0.5804 , 0.8980 , 0.9922 , 0.9922 , 0.9922 , 0.9804 , 0.7137 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0941 , 0.4471 , 0.8667 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.7882 , 0.3059 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0902 , 0.2588 , 0.8353 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.7765 , 0.3176 , 0.0078 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0706 , 0.6706 , 0.8588 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.7647 , 0.3137 , 0.0353 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.2157 , 0.6745 , 0.8863 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9569 , 0.5216 , 0.0431 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.5333 , 0.9922 , 0.9922 , 0.9922 , 0.8314 , 0.5294 , 0.5176 , 0.0627 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ]]]), tensor ( 5 )) Inspecting a single data point in the training dataset When you load MNIST dataset, each data point is actually a tuple containing the image matrix and the label. type ( train_dataset [ 0 ]) tuple Inspecting training dataset first element of tuple This means to access the image, you need to access the first element in the tuple. # Input Matrix train_dataset [ 0 ][ 0 ] . size () # A 28x28 sized image of a digit torch . Size ([ 1 , 28 , 28 ]) Inspecting training dataset second element of tuple The second element actually represents the image's label. Meaning if the second element says 5, it means the 28x28 matrix of numbers represent a digit 5. # Label train_dataset [ 0 ][ 1 ] tensor ( 5 )","title":"Step 1a: Loading MNIST Train Dataset"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#displaying-mnist","text":"Verifying shape of MNIST image As mentioned, a single MNIST image is of the shape 28 pixel x 28 pixel. import matplotlib.pyplot as plt % matplotlib inline import numpy as np train_dataset [ 0 ][ 0 ] . numpy () . shape ( 1 , 28 , 28 ) Plot image of MNIST image show_img = train_dataset [ 0 ][ 0 ] . numpy () . reshape ( 28 , 28 ) plt . imshow ( show_img , cmap = 'gray' ) Second element of tuple shows label As you would expect, the label is 5. # Label train_dataset [ 0 ][ 1 ] tensor ( 5 ) Plot second image of MNIST image show_img = train_dataset [ 1 ][ 0 ] . numpy () . reshape ( 28 , 28 ) plt . imshow ( show_img , cmap = 'gray' ) Second element of tuple shows label We should see 0 here as the label. # Label train_dataset [ 1 ][ 1 ] tensor ( 0 )","title":"Displaying MNIST"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-1b-loading-mnist-test-dataset","text":"Show our algorithm works beyond the data we have trained on. Out-of-sample Load test dataset Compared to the 60k images in the training set, the testing set where the model will not be trained on has 10k images to check for its out-of-sample performance. test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) len ( test_dataset ) 10000 Test dataset elements Exactly like the training set, the testing set has 10k tuples containing the 28x28 matrices and their respective labels. type ( test_dataset [ 0 ]) tuple Test dataset first element in tuple This contains the image matrix, similar to the training set. # Image matrix test_dataset [ 0 ][ 0 ] . size () torch . Size ([ 1 , 28 , 28 ]) Plot image sample from test dataset show_img = test_dataset [ 0 ][ 0 ] . numpy () . reshape ( 28 , 28 ) plt . imshow ( show_img , cmap = 'gray' ) Test dataset second element in tuple # Label test_dataset [ 0 ][ 1 ] tensor ( 7 )","title":"Step 1b: Loading MNIST Test Dataset"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-2-make-dataset-iterable","text":"Aim: make the dataset iterable totaldata : 60000 minibatch : 100 Number of examples in 1 iteration iterations : 3000 1 iteration: one mini-batch forward & backward pass epochs 1 epoch: running through the whole dataset once epochs = iterations \\div \\frac{totaldata}{minibatch} = 3000 \\div \\frac{60000}{100} = 5 epochs = iterations \\div \\frac{totaldata}{minibatch} = 3000 \\div \\frac{60000}{100} = 5 Recap training dataset Remember training dataset has 60k images and testing dataset has 10k images. len ( train_dataset ) 60000 Defining epochs When the model goes through the whole 60k images once, learning how to classify 0-9, it's consider 1 epoch. However, there's a concept of batch size where it means the model would look at 100 images before updating the model's weights, thereby learning. When the model updates its weights (parameters) after looking at all the images, this is considered 1 iteration. batch_size = 100 We arbitrarily set 3000 iterations here which means the model would update 3000 times. n_iters = 3000 One epoch consists of 60,000 / 100 = 600 iterations. Because we would like to go through 3000 iterations, this implies we would have 3000 / 600 = 5 epochs as each epoch has 600 iterations. num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) num_epochs 5 Create Iterable Object: Training Dataset train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) Check Iterability import collections isinstance ( train_loader , collections . Iterable ) True Create Iterable Object: Testing Dataset # Iterable object test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) Check iterability of testing dataset isinstance ( test_loader , collections . Iterable ) True Iterate through dataset This is just a simplified example of what we're doing above where we're creating an iterable object lst to loop through so we can access all the images img_1 and img_2 . Above, the equivalent of lst is train_loader and test_loader . img_1 = np . ones (( 28 , 28 )) img_2 = np . ones (( 28 , 28 )) lst = [ img_1 , img_2 ] # Need to iterate # Think of numbers as the images for i in lst : print ( i . shape ) ( 28 , 28 ) ( 28 , 28 )","title":"Step 2: Make Dataset Iterable"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-3-building-model","text":"Create model class # Same as linear regression! class LogisticRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LogisticRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out","title":"Step 3: Building Model"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-4-instantiate-model-class","text":"Input dimension: Size of image 28 \\times 28 = 784 28 \\times 28 = 784 Output dimension: 10 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 Check size of dataset This should be 28x28. # Size of images train_dataset [ 0 ][ 0 ] . size () torch . Size ([ 1 , 28 , 28 ]) Instantiate model class based on input and out dimensions As we're trying to classify digits 0-9 a total of 10 classes, our output dimension is 10. And we're feeding the model with 28x28 images, hence our input dimension is 28x28. input_dim = 28 * 28 output_dim = 10 model = LogisticRegressionModel ( input_dim , output_dim )","title":"Step 4: Instantiate Model Class"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-5-instantiate-loss-class","text":"Logistic Regression : Cross Entropy Loss Linear Regression: MSE Create Cross Entry Loss Class Unlike linear regression, we do not use MSE here, we need Cross Entry Loss to calculate our loss before we backpropagate and update our parameters. criterion = nn . CrossEntropyLoss () What happens in nn.CrossEntropyLoss()? It does 2 things at the same time. 1. Computes softmax (logistic/softmax function) 2. Computes cross entropy","title":"Step 5: Instantiate Loss Class"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-6-instantiate-optimizer-class","text":"Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our variables) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : parameters' gradients Even simplier equation parameters = parameters - learning_rate * parameters_gradients At every iteration, we update our model's parameters Create optimizer Similar to what we've covered above, this calculates the parameters' gradients and update them subsequently. learning_rate = 0.001 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) Parameters In-Depth You'll realize we have 2 sets of parameters, 10x784 which is A and 10x1 which is b in the y = AX + b y = AX + b equation where X is our input of size 784. We'll go into details subsequently how these parameters interact with our input to produce our 10x1 output. # Type of parameter object print ( model . parameters ()) # Length of parameters print ( len ( list ( model . parameters ()))) # FC 1 Parameters print ( list ( model . parameters ())[ 0 ] . size ()) # FC 1 Bias Parameters print ( list ( model . parameters ())[ 1 ] . size ()) < generator object Module . parameters at 0x7ff7c884f830 > 2 torch . Size ([ 10 , 784 ]) torch . Size ([ 10 ]) Quick Matrix Product Review Example 1: matrix product A: (100, 10) A: (100, 10) B: (10, 1) B: (10, 1) A \\cdot B = (100, 10) \\cdot (10, 1) = (100, 1) A \\cdot B = (100, 10) \\cdot (10, 1) = (100, 1) Example 2: matrix product A: (50, 5) A: (50, 5) B: (5, 2) B: (5, 2) A \\cdot B = (50, 5) \\cdot (5, 2) = (50, 2) A \\cdot B = (50, 5) \\cdot (5, 2) = (50, 2) Example 3: element-wise addition A: (10, 1) A: (10, 1) B: (10, 1) B: (10, 1) A + B = (10, 1) A + B = (10, 1)","title":"Step 6: Instantiate Optimizer Class"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-7-train-model","text":"7 step process for training models Process Convert inputs/labels to tensors with gradients Clear gradient buffets Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () labels = labels # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 1.8513233661651611 . Accuracy : 70 Iteration : 1000. Loss : 1.5732524394989014 . Accuracy : 77 Iteration : 1500. Loss : 1.3840199708938599 . Accuracy : 79 Iteration : 2000. Loss : 1.1711134910583496 . Accuracy : 81 Iteration : 2500. Loss : 1.1094708442687988 . Accuracy : 82 Iteration : 3000. Loss : 1.002761721611023 . Accuracy : 82","title":"Step 7: Train Model"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#break-down-accuracy-calculation","text":"Printing outputs of our model As we've trained our model, we can extract the accuracy calculation portion to understand what's happening without re-training the model. This would print out the output of the model's predictions on your notebook. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) if iter_test == 1 : print ( 'OUTPUTS' ) print ( outputs ) _ , predicted = torch . max ( outputs . data , 1 ) OUTPUTS tensor ([[ - 0.4181 , - 1.0784 , - 0.4840 , - 0.0985 , - 0.2394 , - 0.1801 , - 1.1639 , 2.9352 , - 0.1552 , 0.8852 ], [ 0.5117 , - 0.1099 , 1.5295 , 0.8863 , - 1.8813 , 0.5967 , 1.3632 , - 1.8977 , 0.4183 , - 1.4990 ], [ - 1.0126 , 2.4112 , 0.2373 , 0.0857 , - 0.7007 , - 0.2015 , - 0.3428 , - 0.2548 , 0.1659 , - 0.4703 ], [ 2.8072 , - 2.2973 , - 0.0984 , - 0.4313 , - 0.9619 , 0.8670 , 1.2201 , 0.3752 , - 0.2873 , - 0.3272 ], [ - 0.0343 , - 2.0043 , 0.5081 , - 0.6452 , 1.8647 , - 0.6924 , 0.1435 , 0.4330 , 0.2958 , 1.0339 ], [ - 1.5392 , 2.9070 , 0.2297 , 0.3139 , - 0.6863 , - 0.2734 , - 0.8377 , - 0.1238 , 0.3285 , - 0.3004 ], [ - 1.2037 , - 1.3739 , - 0.5947 , 0.3530 , 1.4205 , 0.0593 , - 0.7307 , 0.6642 , 0.3937 , 0.8004 ], [ - 1.4439 , - 0.3284 , - 0.7652 , - 0.0952 , 0.9323 , 0.3006 , 0.0238 , - 0.0810 , 0.0612 , 1.3295 ], [ 0.5409 , - 0.5266 , 0.9914 , - 1.2369 , 0.6583 , 0.0992 , 0.8525 , - 1.0562 , 0.2013 , 0.0462 ], [ - 0.6548 , - 0.7253 , - 0.9825 , - 1.1663 , 0.9076 , - 0.0694 , - 0.3708 , 1.8270 , 0.2457 , 1.5921 ], [ 3.2147 , - 1.7689 , 0.8531 , 1.2320 , - 0.8126 , 1.1251 , - 0.2776 , - 1.4244 , 0.5930 , - 1.6183 ], [ 0.7470 , - 0.5545 , 1.0251 , 0.0529 , 0.4384 , - 0.5934 , 0.7666 , - 1.0084 , 0.5313 , - 0.3465 ], [ - 0.7916 , - 1.7064 , - 0.7805 , - 1.1588 , 1.3284 , - 0.1708 , - 0.2092 , 0.9495 , 0.1033 , 2.0208 ], [ 3.0602 , - 2.3578 , - 0.2576 , - 0.2198 , - 0.2372 , 0.9765 , - 0.1514 , - 0.5380 , 0.7970 , 0.1374 ], [ - 1.2613 , 2.8594 , - 0.0874 , 0.1974 , - 1.2018 , - 0.0064 , - 0.0923 , - 0.2142 , 0.2575 , - 0.3218 ], [ 0.4348 , - 0.7216 , 0.0021 , 1.2864 , - 0.5062 , 0.7761 , - 0.3236 , - 0.5667 , 0.5431 , - 0.7781 ], [ - 0.2157 , - 2.0200 , 0.1829 , - 0.6882 , 1.3815 , - 0.7609 , - 0.0902 , 0.8647 , 0.3679 , 1.8843 ], [ 0.0950 , - 1.5009 , - 0.6347 , 0.3662 , - 0.4679 , - 0.0359 , - 0.7671 , 2.7155 , - 0.3991 , 0.5737 ], [ - 0.7005 , - 0.5366 , - 0.0434 , 1.1289 , - 0.5873 , 0.2555 , 0.8187 , - 0.6557 , 0.1241 , - 0.4297 ], [ - 1.0635 , - 1.5991 , - 0.4677 , - 0.1231 , 2.0445 , 0.1128 , - 0.1825 , 0.1075 , 0.0348 , 1.4317 ], [ - 1.0319 , - 0.1595 , - 1.3415 , 0.1095 , 0.5339 , 0.1973 , - 1.3272 , 1.5765 , 0.4784 , 1.4176 ], [ - 0.4928 , - 1.5653 , - 0.0672 , 0.3325 , 0.5359 , 0.5368 , 2.1542 , - 1.4276 , 0.3605 , 0.0587 ], [ - 0.4761 , 0.2958 , 0.6597 , - 0.2658 , 1.1279 , - 1.0676 , 1.2506 , - 0.2059 , - 0.1489 , 0.1051 ], [ - 0.0764 , - 0.9274 , - 0.6838 , 0.3464 , - 0.2656 , 1.4099 , 0.4486 , - 0.9527 , 0.5682 , 0.0156 ], [ - 0.6900 , - 0.9611 , 0.1395 , - 0.0079 , 1.5424 , - 0.3208 , - 0.2682 , 0.3586 , - 0.2771 , 1.0389 ], [ 4.3606 , - 2.8621 , 0.6310 , - 0.9657 , - 0.2486 , 1.2009 , 1.1873 , - 0.8255 , - 0.2103 , - 1.2172 ], [ - 0.1000 , - 1.4268 , - 0.4627 , - 0.1041 , 0.2959 , - 0.1392 , - 0.6855 , 1.8622 , - 0.2580 , 1.1347 ], [ - 0.3625 , - 2.1323 , - 0.2224 , - 0.8754 , 2.4684 , 0.0295 , 0.1161 , - 0.2660 , 0.3037 , 1.4570 ], [ 2.8688 , - 2.4517 , 0.1782 , 1.1149 , - 1.0898 , 1.1062 , - 0.0681 , - 0.5697 , 0.8888 , - 0.6965 ], [ - 1.0429 , 1.4446 , - 0.3349 , 0.1254 , - 0.5017 , 0.2286 , 0.2328 , - 0.3290 , 0.3949 , - 0.2586 ], [ - 0.8476 , - 0.0004 , - 1.1003 , 2.2806 , - 1.2226 , 0.9251 , - 0.3165 , 0.4957 , 0.0690 , 0.0232 ], [ - 0.9108 , 1.1355 , - 0.2715 , 0.2233 , - 0.3681 , 0.1442 , - 0.0001 , - 0.0174 , 0.1454 , 0.2286 ], [ - 1.0663 , - 0.8466 , - 0.7147 , 2.5685 , - 0.2090 , 1.2993 , - 0.3057 , - 0.8314 , 0.7046 , - 0.0176 ], [ 1.7013 , - 1.8051 , 0.7541 , - 1.5248 , 0.8972 , 0.1518 , 1.4876 , - 0.8454 , - 0.2022 , - 0.2829 ], [ - 0.8179 , - 0.1239 , 0.8630 , - 0.2137 , - 0.2275 , - 0.5411 , - 1.3448 , 1.7354 , 0.7751 , 0.6234 ], [ 0.6515 , - 1.0431 , 2.7165 , 0.1873 , - 1.0623 , 0.1286 , 0.3597 , - 0.2739 , 0.3871 , - 1.6699 ], [ - 0.2828 , - 1.4663 , 0.1182 , - 0.0896 , - 0.3640 , - 0.5129 , - 0.4905 , 2.2914 , - 0.2227 , 0.9463 ], [ - 1.2596 , 2.0468 , - 0.4405 , - 0.0411 , - 0.8073 , 0.0490 , - 0.0604 , - 0.1206 , 0.3504 , - 0.1059 ], [ 0.6089 , 0.5885 , 0.7898 , 1.1318 , - 1.9008 , 0.5875 , 0.4227 , - 1.1815 , 0.5652 , - 1.3590 ], [ - 1.4551 , 2.9537 , - 0.2805 , 0.2372 , - 1.4180 , 0.0297 , - 0.1515 , - 0.6111 , 0.6140 , - 0.3354 ], [ - 0.7182 , 1.6778 , 0.0553 , 0.0461 , - 0.5446 , - 0.0338 , - 0.0215 , - 0.0881 , 0.1506 , - 0.2107 ], [ - 0.8027 , - 0.7854 , - 0.1275 , - 0.3177 , - 0.1600 , - 0.1964 , - 0.6084 , 2.1285 , - 0.1815 , 1.1911 ], [ - 2.0656 , - 0.4959 , - 0.1154 , - 0.1363 , 2.2426 , - 0.7441 , - 0.8413 , 0.4675 , 0.3269 , 1.7279 ], [ - 0.3004 , 1.0166 , 1.1175 , - 0.0618 , - 0.0937 , - 0.4221 , 0.1943 , - 1.1020 , 0.3670 , - 0.4683 ], [ - 1.0720 , 0.2252 , 0.0175 , 1.3644 , - 0.7409 , 0.4655 , 0.5439 , 0.0380 , 0.1279 , - 0.2302 ], [ 0.2409 , - 1.2622 , - 0.6336 , 1.8240 , - 0.5951 , 1.3408 , 0.2130 , - 1.3789 , 0.8363 , - 0.2101 ], [ - 1.3849 , 0.3773 , - 0.0585 , 0.6896 , - 0.0998 , 0.2804 , 0.0696 , - 0.2529 , 0.3143 , 0.3409 ], [ - 0.9103 , - 0.1578 , 1.6673 , - 0.4817 , 0.4088 , - 0.5484 , 0.6103 , - 0.2287 , - 0.0665 , 0.0055 ], [ - 1.1692 , - 2.8531 , - 1.2499 , - 0.0257 , 2.8580 , 0.2616 , - 0.7122 , - 0.0551 , 0.8112 , 2.3233 ], [ - 0.2790 , - 1.9494 , 0.6096 , - 0.5653 , 2.2792 , - 1.0687 , 0.1634 , 0.3122 , 0.1053 , 1.0884 ], [ 0.1267 , - 1.2297 , - 0.1315 , 0.2428 , - 0.5436 , 0.4123 , 2.3060 , - 0.9278 , - 0.1528 , - 0.4224 ], [ - 0.0235 , - 0.9137 , - 0.1457 , 1.6858 , - 0.7552 , 0.7293 , 0.2510 , - 0.3955 , - 0.2187 , - 0.1505 ], [ 0.5643 , - 1.2783 , - 1.4149 , 0.0304 , 0.8375 , 1.5018 , 0.0338 , - 0.3875 , - 0.0117 , 0.5751 ], [ 0.2926 , - 0.7486 , - 0.3238 , 1.0384 , 0.0308 , 0.6792 , - 0.0170 , - 0.5797 , 0.2819 , - 0.3510 ], [ 0.1219 , - 0.5862 , 1.5817 , - 0.1297 , 0.4730 , - 0.9171 , 0.7886 , - 0.7022 , - 0.0501 , - 0.2812 ], [ 1.7587 , - 2.4511 , - 0.7369 , 0.4082 , - 0.6426 , 1.1784 , 0.6052 , - 0.7178 , 1.6161 , - 0.2220 ], [ - 0.1267 , - 2.6719 , 0.0505 , - 0.4972 , 2.9027 , - 0.1461 , 0.2807 , - 0.2921 , 0.2231 , 1.1327 ], [ - 0.9892 , 2.4401 , 0.1274 , 0.2838 , - 0.7535 , - 0.1684 , - 0.6493 , - 0.1908 , 0.2290 , - 0.2150 ], [ - 0.2071 , - 2.1351 , - 0.9191 , - 0.9309 , 1.7747 , - 0.3046 , 0.0183 , 1.0136 , - 0.1016 , 2.1288 ], [ - 0.0103 , 0.3280 , - 0.6974 , - 0.2504 , 0.3187 , 0.4390 , - 0.1879 , 0.3954 , 0.2332 , - 0.1971 ], [ - 0.2280 , - 1.6754 , - 0.7438 , 0.5078 , 0.2544 , - 0.1020 , - 0.2503 , 2.0799 , - 0.5033 , 0.5890 ], [ 0.3972 , - 0.9369 , 1.2696 , - 1.6713 , - 0.4159 , - 0.0221 , 0.6489 , - 0.4777 , 1.2497 , 0.3931 ], [ - 0.7566 , - 0.8230 , - 0.0785 , - 0.3083 , 0.7821 , 0.1880 , 0.1037 , - 0.0956 , 0.4219 , 1.0798 ], [ - 1.0328 , - 0.1700 , 1.3806 , 0.5445 , - 0.2624 , - 0.0780 , - 0.3595 , - 0.6253 , 0.4309 , 0.1813 ], [ - 1.0360 , - 0.4704 , 0.1948 , - 0.7066 , 0.6600 , - 0.4633 , - 0.3602 , 1.7494 , 0.1522 , 0.6086 ], [ - 1.2032 , - 0.7903 , - 0.5754 , 0.4722 , 0.6068 , 0.5752 , 0.2151 , - 0.2495 , 0.3420 , 0.9278 ], [ 0.2247 , - 0.1361 , 0.9374 , - 0.1543 , 0.4921 , - 0.6553 , 0.5885 , 0.2617 , - 0.2216 , - 0.3736 ], [ - 0.2867 , - 1.4486 , 0.6658 , - 0.8755 , 2.3195 , - 0.7627 , - 0.2132 , 0.2488 , 0.3484 , 1.0860 ], [ - 1.4031 , - 0.4518 , - 0.3181 , 2.8268 , - 0.5371 , 1.0154 , - 0.9247 , - 0.7385 , 1.1031 , 0.0422 ], [ 2.8604 , - 1.5413 , 0.6241 , - 0.8017 , - 1.4104 , 0.6314 , 0.4614 , - 0.0218 , - 0.3411 , - 0.2609 ], [ 0.2113 , - 1.2348 , - 0.8535 , - 0.1041 , - 0.2703 , - 0.1294 , - 0.7057 , 2.7552 , - 0.4429 , 0.4517 ], [ 4.5191 , - 2.7407 , 1.1091 , 0.3975 , - 0.9456 , 1.2277 , 0.3616 , - 1.6564 , 0.5063 , - 1.4274 ], [ 1.4615 , - 1.0765 , 1.8388 , 1.5006 , - 1.2351 , 0.2781 , 0.2830 , - 0.8491 , 0.2222 , - 1.7779 ], [ - 1.2160 , 0.8502 , 0.2413 , - 0.0798 , - 0.7880 , - 0.4286 , - 0.8060 , 0.7194 , 1.2663 , 0.6412 ], [ - 1.3318 , 2.3388 , - 0.4003 , - 0.1094 , - 1.0285 , 0.1021 , - 0.0388 , - 0.0497 , 0.5137 , - 0.2507 ], [ - 1.7853 , 0.5884 , - 0.6108 , - 0.5557 , 0.8696 , - 0.6226 , - 0.7983 , 1.7169 , - 0.0145 , 0.8231 ], [ - 0.1739 , 0.1562 , - 0.2933 , 2.3195 , - 0.9480 , 1.2019 , - 0.4834 , - 1.0567 , 0.5685 , - 0.6841 ], [ - 0.7920 , - 0.3339 , 0.7452 , - 0.6529 , - 0.3307 , - 0.6092 , - 0.0950 , 1.7311 , - 0.3481 , 0.3801 ], [ - 1.7810 , 1.0676 , - 0.7611 , 0.3658 , - 0.0431 , - 0.1012 , - 0.6048 , 0.3089 , 0.9998 , 0.7164 ], [ - 0.5856 , - 0.5261 , - 0.4859 , - 1.0551 , - 0.1838 , - 0.2144 , - 1.2599 , 3.3891 , 0.4691 , 0.7566 ], [ - 0.4984 , - 1.7770 , - 1.1998 , - 0.1075 , 1.0882 , 0.4539 , - 0.5651 , 1.4381 , - 0.5678 , 1.7479 ], [ 0.2938 , - 1.8536 , 0.4259 , - 0.5429 , 0.0066 , 0.4120 , 2.3793 , - 0.3666 , - 0.2604 , 0.0382 ], [ - 0.4080 , - 0.9851 , 4.0264 , 0.1099 , - 0.1766 , - 1.1557 , 0.6419 , - 0.8147 , 0.7535 , - 1.1452 ], [ - 0.4636 , - 1.7323 , - 0.6433 , - 0.0274 , 0.7227 , - 0.1799 , - 0.9336 , 2.1881 , - 0.2073 , 1.6522 ], [ - 0.9617 , - 0.0348 , - 0.3980 , - 0.4738 , 0.7790 , 0.4671 , - 0.6115 , - 0.7067 , 1.3036 , 0.4923 ], [ - 1.0151 , - 2.5385 , - 0.6072 , 0.2902 , 3.1570 , 0.1062 , - 0.2169 , - 0.4491 , 0.6326 , 1.6829 ], [ - 1.8852 , 0.6066 , - 0.2840 , - 0.4475 , - 0.1147 , - 0.7858 , - 1.1805 , 3.0723 , 0.3960 , 0.9720 ], [ 0.0344 , - 1.4878 , - 0.9675 , 1.9649 , - 0.3146 , 1.2183 , 0.6730 , - 0.3650 , 0.0646 , - 0.0898 ], [ - 0.2118 , - 2.0350 , 0.9917 , - 0.8993 , 1.2334 , - 0.6723 , 2.5847 , - 0.0454 , - 0.4149 , 0.3927 ], [ - 1.7365 , 3.0447 , 0.5115 , 0.0786 , - 0.7544 , - 0.2158 , - 0.4876 , - 0.2891 , 0.5089 , - 0.6719 ], [ 0.3652 , - 0.5457 , - 0.1167 , 2.9056 , - 1.1622 , 0.8192 , - 1.3245 , - 0.6414 , 0.8097 , - 0.4958 ], [ - 0.8755 , - 0.6983 , 0.2208 , - 0.6463 , 0.5276 , 0.1145 , 2.7229 , - 1.0316 , 0.1905 , 0.2090 ], [ - 0.9702 , 0.1265 , - 0.0007 , - 0.5106 , 0.4970 , - 0.0804 , 0.0017 , 0.0607 , 0.6164 , 0.4490 ], [ - 0.8271 , - 0.6822 , - 0.7434 , 2.6457 , - 1.6143 , 1.1486 , - 1.0705 , 0.5611 , 0.6422 , 0.1250 ], [ - 1.9979 , 1.8175 , - 0.1658 , - 0.0343 , - 0.6292 , 0.1774 , 0.3150 , - 0.4633 , 0.9266 , 0.0252 ], [ - 0.9039 , - 0.6030 , - 0.2173 , - 1.1768 , 2.3198 , - 0.5072 , 0.3418 , - 0.1551 , 0.1282 , 1.4250 ], [ - 0.9891 , 0.5212 , - 0.4518 , 0.3267 , - 0.0759 , 0.3826 , - 0.0341 , 0.0382 , 0.2451 , 0.3658 ], [ - 2.1217 , 1.5102 , - 0.7828 , 0.3554 , - 0.4192 , - 0.0772 , 0.0578 , 0.8070 , 0.1701 , 0.5880 ], [ 1.0665 , - 1.3826 , 0.6243 , - 0.8096 , - 0.4227 , 0.5925 , 1.8112 , - 0.9946 , 0.2010 , - 0.7731 ], [ - 1.1263 , - 1.7484 , 0.0041 , - 0.5439 , 1.7242 , - 0.9475 , - 0.3835 , 0.8452 , 0.3077 , 2.2689 ]]) Printing output size This produces a 100x10 matrix because each iteration has a batch size of 100 and each prediction across the 10 classes, with the largest number indicating the likely number it is predicting. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) if iter_test == 1 : print ( 'OUTPUTS' ) print ( outputs . size ()) _ , predicted = torch . max ( outputs . data , 1 ) OUTPUTS torch . Size ([ 100 , 10 ]) Printing one output This would be a 1x10 matrix where the largest number is what the model thinks the image is. Here we can see that in the tensor, position 7 has the largest number, indicating the model thinks the image is 7. number 0: -0.4181 number 1: -1.0784 ... number 7: 2.9352 iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) if iter_test == 1 : print ( 'OUTPUTS' ) print ( outputs [ 0 , :]) _ , predicted = torch . max ( outputs . data , 1 ) OUTPUTS tensor([-0.4181, -1.0784, -0.4840, -0.0985, -0.2394, -0.1801, -1.1639, 2.9352, -0.1552, 0.8852]) Printing prediction output Because our output is of size 100 (our batch size), our prediction size would also of the size 100. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) if iter_test == 1 : print ( 'PREDICTION' ) print ( predicted . size ()) PREDICTION torch . Size ([ 100 ]) Print prediction value We are printing our prediction which as verified above, should be digit 7. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) if iter_test == 1 : print ( 'PREDICTION' ) print ( predicted [ 0 ]) PREDICTION tensor ( 7 ) Print prediction, label and label size We are trying to show what we are predicting and the actual values. In this case, we're predicting the right value 7! iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) if iter_test == 1 : print ( 'PREDICTION' ) print ( predicted [ 0 ]) print ( 'LABEL SIZE' ) print ( labels . size ()) print ( 'LABEL FOR IMAGE 0' ) print ( labels [ 0 ]) PREDICTION tensor ( 7 ) LABEL SIZE torch . Size ([ 100 ]) LABEL FOR IMAGE 0 tensor ( 7 ) Print second prediction and ground truth Again, the prediction is correct. Naturally, as our model is quite competent in this simple task. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) if iter_test == 1 : print ( 'PREDICTION' ) print ( predicted [ 1 ]) print ( 'LABEL SIZE' ) print ( labels . size ()) print ( 'LABEL FOR IMAGE 1' ) print ( labels [ 1 ]) PREDICTION tensor ( 2 ) LABEL SIZE torch . Size ([ 100 ]) LABEL FOR IMAGE 1 tensor ( 2 ) Print accuracy Now we know what each object represents, we can understand how we arrived at our accuracy numbers. One last thing to note is that correct.item() has this syntax is because correct is a PyTorch tensor and to get the value to compute with total which is an integer, we need to do this. correct = 0 total = 0 iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * ( correct . item () / total ) print ( accuracy ) 82.94 Explanation of Python's .sum() function Python's .sum() function allows you to do a comparison between two matrices and sum the ones that return True or in our case, those predictions that match actual labels (correct predictions). # Explaining .sum() python built-in function # correct += (predicted == labels).sum() import numpy as np a = np . ones (( 10 )) print ( a ) b = np . ones (( 10 )) print ( b ) print ( a == b ) print (( a == b ) . sum ()) # matrix a [ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. ] # matrix b [ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. ] # boolean array [ True True True True True True True True True True ] # number of elementswhere a matches b 10","title":"Break Down Accuracy Calculation"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#saving-model","text":"Saving PyTorch model This is how you save your model. Feel free to just change save_model = True to save your model save_model = False if save_model is True : # Saves only parameters torch . save ( model . state_dict (), 'awesome_model.pkl' )","title":"Saving Model"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#building-a-logistic-regression-model-with-pytorch-gpu","text":"CPU version The usual 7-step process, getting repetitive by now which we like. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class LogisticRegressionModel ( nn . Module ): def __init__ ( self , input_size , num_classes ): super ( LogisticRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 output_dim = 10 model = LogisticRegressionModel ( input_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.001 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () labels = labels # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # 100 x 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value # 100 x 1 _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct . item () / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 1.876196026802063 . Accuracy : 64.44 Iteration : 1000. Loss : 1.5153584480285645 . Accuracy : 75.68 Iteration : 1500. Loss : 1.3521136045455933 . Accuracy : 78.98 Iteration : 2000. Loss : 1.2136967182159424 . Accuracy : 80.95 Iteration : 2500. Loss : 1.0934826135635376 . Accuracy : 81.97 Iteration : 3000. Loss : 1.024120569229126 . Accuracy : 82.49 GPU version 2 things must be on GPU - model - tensors Remember step 4 and 7 will be affected and this will be the same for all model building moving forward. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class LogisticRegressionModel ( nn . Module ): def __init__ ( self , input_size , num_classes ): super ( LogisticRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 output_dim = 10 model = LogisticRegressionModel ( input_dim , output_dim ) ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.001 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , 28 * 28 ) . to ( device ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) ####################### # USE GPU FOR MODEL # ####################### # Total correct predictions if torch . cuda . is_available (): correct += ( predicted . cpu () == labels . cpu ()) . sum () else : correct += ( predicted == labels ) . sum () accuracy = 100 * correct . item () / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 1.8571407794952393 . Accuracy : 68.99 Iteration : 1000. Loss : 1.5415704250335693 . Accuracy : 75.86 Iteration : 1500. Loss : 1.2755383253097534 . Accuracy : 78.92 Iteration : 2000. Loss : 1.2468739748001099 . Accuracy : 80.72 Iteration : 2500. Loss : 1.0708973407745361 . Accuracy : 81.73 Iteration : 3000. Loss : 1.0359245538711548 . Accuracy : 82.74","title":"Building a Logistic Regression Model with PyTorch (GPU)"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#summary","text":"We've learnt to... Success Logistic regression basics Problems of linear regression In-depth Logistic Regression Get logits Get softmax Get cross-entropy loss Aim : reduce cross-entropy loss Built a logistic regression model in CPU and GPU Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Important things to be on GPU model tensors with gradients","title":"Summary"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/","text":"Long Short-Term Memory (LSTM) network with PyTorch \u00b6 Run Jupyter Notebook You can run the code for this section in this jupyter notebook link . About LSTMs: Special RNN \u00b6 Capable of learning long-term dependencies LSTM = RNN on super juice RNN Transition to LSTM \u00b6 Building an LSTM with PyTorch \u00b6 Model A: 1 Hidden Layer \u00b6 Unroll 28 time steps Each step input size: 28 x 1 Total per unroll: 28 x 28 Feedforward Neural Network input size: 28 x 28 1 Hidden layer Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Step 1: Loading MNIST Train Dataset \u00b6 Images from 1 to 9 The usual loading of our MNIST dataset As usual, we've 60k training images and 10k testing images. Subsequently, we'll have 3 groups: training, validation and testing for a more robust evaluation of algorithms. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) print ( train_dataset . train_data . size ()) print ( train_dataset . train_labels . size ()) print ( test_dataset . test_data . size ()) ``` python print ( test_dataset . test_labels . size ()) torch . Size ([ 60000 , 28 , 28 ]) torch . Size ([ 60000 ]) torch . Size ([ 10000 , 28 , 28 ]) torch . Size ([ 10000 ]) Step 2: Make Dataset Iterable \u00b6 Creating an iterable object for our dataset batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) Step 3: Create Model Class \u00b6 Creating an LSTM model class It is very similar to RNN in terms of the shape of our input of batch_dim x seq_dim x feature_dim . The only change is that we have our cell state on top of our hidden state. PyTorch's LSTM module handles all the other weights for our other gates. class LSTMModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( LSTMModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your LSTM # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . lstm = nn . LSTM ( input_dim , hidden_dim , layer_dim , batch_first = True ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # Initialize cell state c0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # 28 time steps # We need to detach as we are doing truncated backpropagation through time (BPTT) # If we don't, we'll backprop all the way to the start even after going through another batch out , ( hn , cn ) = self . lstm ( x , ( h0 . detach (), c0 . detach ())) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out Step 4: Instantiate Model Class \u00b6 28 time steps Each time step: input dimension = 28 1 hidden layer MNIST 1-9 digits \\rightarrow \\rightarrow output dimension = 10 Instantiate our LSTM model input_dim = 28 hidden_dim = 100 layer_dim = 1 output_dim = 10 model = LSTMModel ( input_dim , hidden_dim , layer_dim , output_dim ) Step 5: Instantiate Loss Class \u00b6 Long Short-Term Memory Neural Network: Cross Entropy Loss Recurrent Neural Network : Cross Entropy Loss Convolutional Neural Network : Cross Entropy Loss Feedforward Neural Network : Cross Entropy Loss Logistic Regression : Cross Entropy Loss Linear Regression : MSE Cross Entry Loss Function Because we are doing a classification problem we'll be using a Cross Entropy function. If we were to do a regression problem, then we would typically use a MSE function. criterion = nn . CrossEntropyLoss () Step 6: Instantiate Optimizer Class \u00b6 Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our variables) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : parameters' gradients Even simplier equation parameters = parameters - learning_rate * parameters_gradients At every iteration, we update our model's parameters Mini-batch Stochastic Gradient Descent learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) Parameters In-Depth \u00b6 1 Layer LSTM Groups of Parameters We will have 6 groups of parameters here comprising weights and biases from: - Input to Hidden Layer Affine Function - Hidden Layer to Output Affine Function - Hidden Layer to Hidden Layer Affine Function Notice how this is exactly the same number of groups of parameters as our RNN? But the sizes of these groups will be larger for an LSTM due to its gates. len ( list ( model . parameters ())) 6 In-depth Parameters Analysis Comparing to RNN's parameters , we've the same number of groups but for LSTM we've 4x the number of parameters! for i in range ( len ( list ( model . parameters ()))): print ( list ( model . parameters ())[ i ] . size ()) torch . Size ([ 400 , 28 ]) torch . Size ([ 400 , 100 ]) torch . Size ([ 400 ]) torch . Size ([ 400 ]) torch . Size ([ 10 , 100 ]) torch . Size ([ 10 ]) Parameters Breakdown \u00b6 This is the breakdown of the parameters associated with the respective affine functions Input \\rightarrow \\rightarrow Gates [400, 28] \\rightarrow w_1, w_3, w_5, w_7 [400, 28] \\rightarrow w_1, w_3, w_5, w_7 [400] \\rightarrow b_1, b_3, b_5, b_7 [400] \\rightarrow b_1, b_3, b_5, b_7 Hidden State \\rightarrow \\rightarrow Gates [400,100] \\rightarrow w_2, w_4, w_6, w_8 [400,100] \\rightarrow w_2, w_4, w_6, w_8 [400] \\rightarrow b_2, b_4, b_6, b_8 [400] \\rightarrow b_2, b_4, b_6, b_8 Hidden State \\rightarrow \\rightarrow Output [10, 100] \\rightarrow w_9 [10, 100] \\rightarrow w_9 [10] \\rightarrow b_9 [10] \\rightarrow b_9 Step 7: Train Model \u00b6 Process Convert inputs/labels to variables LSTM Input: (1, 28) RNN Input: (1, 28) CNN Input: (1, 28, 28) FNN Input: (1, 28*28) Clear gradient buffets Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT Training 1 Hidden Layer LSTM # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as a torch tensor with gradient accumulation abilities images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Resize images images = images . view ( - 1 , seq_dim , input_dim ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.8390830755233765 . Accuracy : 72 Iteration : 1000. Loss : 0.46470555663108826 . Accuracy : 85 Iteration : 1500. Loss : 0.31465113162994385 . Accuracy : 91 Iteration : 2000. Loss : 0.19143860042095184 . Accuracy : 94 Iteration : 2500. Loss : 0.16134005784988403 . Accuracy : 95 Iteration : 3000. Loss : 0.255976140499115 . Accuracy : 95 Model B: 2 Hidden Layer \u00b6 Unroll 28 time steps Each step input size: 28 x 1 Total per unroll: 28 x 28 Feedforward Neural Network inpt size: 28 x 28 2 Hidden layer Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Train 2 Hidden Layer LSTM import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class LSTMModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( LSTMModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your LSTM # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . lstm = nn . LSTM ( input_dim , hidden_dim , layer_dim , batch_first = True ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # Initialize cell state c0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # One time step # We need to detach as we are doing truncated backpropagation through time (BPTT) # If we don't, we'll backprop all the way to the start even after going through another batch out , ( hn , cn ) = self . lstm ( x , ( h0 . detach (), c0 . detach ())) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 hidden_dim = 100 layer_dim = 2 # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER output_dim = 10 model = LSTMModel ( input_dim , hidden_dim , layer_dim , output_dim ) # JUST PRINTING MODEL & PARAMETERS print ( model ) print ( len ( list ( model . parameters ()))) for i in range ( len ( list ( model . parameters ()))): print ( list ( model . parameters ())[ i ] . size ()) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as torch tensor with gradient accumulation abilities images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Resize image images = images . view ( - 1 , seq_dim , input_dim ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) LSTMModel ( ( lstm ): LSTM ( 28 , 100 , num_layers = 2 , batch_first = True ) ( fc ): Linear ( in_features = 100 , out_features = 10 , bias = True ) ) 10 torch . Size ([ 400 , 28 ]) torch . Size ([ 400 , 100 ]) torch . Size ([ 400 ]) torch . Size ([ 400 ]) torch . Size ([ 400 , 100 ]) torch . Size ([ 400 , 100 ]) torch . Size ([ 400 ]) torch . Size ([ 400 ]) torch . Size ([ 10 , 100 ]) torch . Size ([ 10 ]) Iteration : 500. Loss : 2.3074915409088135 . Accuracy : 11 Iteration : 1000. Loss : 1.8854578733444214 . Accuracy : 35 Iteration : 1500. Loss : 0.5317062139511108 . Accuracy : 80 Iteration : 2000. Loss : 0.15290376543998718 . Accuracy : 92 Iteration : 2500. Loss : 0.19500978291034698 . Accuracy : 93 Iteration : 3000. Loss : 0.10683634132146835 . Accuracy : 95 Parameters Breakdown (Layer 1) \u00b6 Input \\rightarrow \\rightarrow Gates [400, 28] [400, 28] [400] [400] Hidden State \\rightarrow \\rightarrow Gates [400,100] [400,100] [400] [400] Parameters Breakdown (Layer 2) \u00b6 Input \\rightarrow \\rightarrow Gates [400, 100] [400, 100] [400] [400] Hidden State \\rightarrow \\rightarrow Gates [400,100] [400,100] [400] [400] Parameters Breakdown (Readout Layer) \u00b6 Hidden State \\rightarrow \\rightarrow Output [10, 100] [10, 100] [10] [10] Model C: 3 Hidden Layer \u00b6 Unroll 28 time steps Each step input size: 28 x 1 Total per unroll: 28 x 28 Feedforward Neural Network inpt size: 28 x 28 3 Hidden layer Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 3 Hidden Layer LSTM import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class LSTMModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( LSTMModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your LSTM # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . lstm = nn . LSTM ( input_dim , hidden_dim , layer_dim , batch_first = True ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # Initialize cell state c0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # One time step # We need to detach as we are doing truncated backpropagation through time (BPTT) # If we don't, we'll backprop all the way to the start even after going through another batch out , ( hn , cn ) = self . lstm ( x , ( h0 . detach (), c0 . detach ())) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 hidden_dim = 100 layer_dim = 3 # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER output_dim = 10 model = LSTMModel ( input_dim , hidden_dim , layer_dim , output_dim ) # JUST PRINTING MODEL & PARAMETERS print ( model ) print ( len ( list ( model . parameters ()))) for i in range ( len ( list ( model . parameters ()))): print ( list ( model . parameters ())[ i ] . size ()) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) LSTMModel( (lstm): LSTM(28, 100, num_layers=3, batch_first=True) (fc): Linear(in_features=100, out_features=10, bias=True) ) 14 torch.Size([400, 28]) torch.Size([400, 100]) torch.Size([400]) torch.Size([400]) torch.Size([400, 100]) torch.Size([400, 100]) torch.Size([400]) torch.Size([400]) torch.Size([400, 100]) torch.Size([400, 100]) torch.Size([400]) torch.Size([400]) torch.Size([10, 100]) torch.Size([10]) Iteration: 500. Loss: 2.2927396297454834. Accuracy: 11 Iteration: 1000. Loss: 2.29740309715271. Accuracy: 11 Iteration: 1500. Loss: 2.1950502395629883. Accuracy: 20 Iteration: 2000. Loss: 1.0738657712936401. Accuracy: 59 Iteration: 2500. Loss: 0.5988132357597351. Accuracy: 79 Iteration: 3000. Loss: 0.4107239246368408. Accuracy: 88 Parameters Breakdown (Layer 1) \u00b6 Input \\rightarrow \\rightarrow Gates [400, 28] [400] Hidden State \\rightarrow \\rightarrow Gates [400,100] [400] Parameters Breakdown (Layer 2) \u00b6 Input \\rightarrow \\rightarrow Gates [400, 100] [400] Hidden State \\rightarrow \\rightarrow Gates [400,100] [400] Parameters Breakdown (Layer 3) \u00b6 Input \\rightarrow \\rightarrow Gates [400, 100] [400] Hidden State \\rightarrow \\rightarrow Gates [400,100] [400] Parameters Breakdown (Readout Layer) \u00b6 Hidden State \\rightarrow \\rightarrow Output [10, 100] [10] Comparison with RNN \u00b6 Model A RNN Model B RNN Model C RNN ReLU ReLU Tanh 1 Hidden Layer 2 Hidden Layers 3 Hidden Layers 100 Hidden Units 100 Hidden Units 100 Hidden Units 92.48% 95.09% 95.54% Model A LSTM Model B LSTM Model C LSTM 1 Hidden Layer 2 Hidden Layers 3 Hidden Layers 100 Hidden Units 100 Hidden Units 100 Hidden Units 96.05% 95.24% 91.22% Deep Learning Notes \u00b6 2 ways to expand a recurrent neural network More hidden units (o, i, f, g) gates More hidden layers Cons Need a larger dataset Curse of dimensionality Does not necessarily mean higher accuracy 3. Building a Recurrent Neural Network with PyTorch (GPU) \u00b6 Model A: 3 Hidden Layers \u00b6 GPU: 2 things must be on GPU - model - tensors Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 3 Hidden Layer LSTM on GPU import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class LSTMModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( LSTMModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your LSTM # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . lstm = nn . LSTM ( input_dim , hidden_dim , layer_dim , batch_first = True ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros ####################### # USE GPU FOR MODEL # ####################### h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () . to ( device ) # Initialize cell state c0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () . to ( device ) # One time step out , ( hn , cn ) = self . lstm ( x , ( h0 . detach (), c0 . detach ())) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 hidden_dim = 100 layer_dim = 3 # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER output_dim = 10 model = LSTMModel ( input_dim , hidden_dim , layer_dim , output_dim ) ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , seq_dim , input_dim ) . to ( device ) labels = labels . to ( device ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions ####################### # USE GPU FOR MODEL # ####################### if torch . cuda . is_available (): correct += ( predicted . cpu () == labels . cpu ()) . sum () else : correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 2.3068575859069824 . Accuracy : 11 Iteration : 1000. Loss : 2.291989803314209 . Accuracy : 14 Iteration : 1500. Loss : 1.909593105316162 . Accuracy : 28 Iteration : 2000. Loss : 0.7345633506774902 . Accuracy : 71 Iteration : 2500. Loss : 0.45030108094215393 . Accuracy : 86 Iteration : 3000. Loss : 0.2627193331718445 . Accuracy : 89 Summary \u00b6 We've learnt to... Success RNN transition to LSTM LSTM Models in PyTorch Model A: 1 Hidden Layer LSTM Model B: 2 Hidden Layer LSTM Model C: 3 Hidden Layer LSTM Models Variation in Code Modifying only step 4 Ways to Expand Model\u2019s Capacity More hidden units More hidden layers Cons of Expanding Capacity Need more data Does not necessarily mean higher accuracy GPU Code 2 things on GPU model tensors Modifying only Step 3, 4 and 7 7 Step Model Building Recap Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Long Short Term Memory Neural Networks (LSTM)"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#long-short-term-memory-lstm-network-with-pytorch","text":"Run Jupyter Notebook You can run the code for this section in this jupyter notebook link .","title":"Long Short-Term Memory (LSTM) network with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#about-lstms-special-rnn","text":"Capable of learning long-term dependencies LSTM = RNN on super juice","title":"About LSTMs: Special RNN"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#rnn-transition-to-lstm","text":"","title":"RNN Transition to LSTM"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#building-an-lstm-with-pytorch","text":"","title":"Building an LSTM with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#model-a-1-hidden-layer","text":"Unroll 28 time steps Each step input size: 28 x 1 Total per unroll: 28 x 28 Feedforward Neural Network input size: 28 x 28 1 Hidden layer","title":"Model A: 1 Hidden Layer"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#steps","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#step-1-loading-mnist-train-dataset","text":"Images from 1 to 9 The usual loading of our MNIST dataset As usual, we've 60k training images and 10k testing images. Subsequently, we'll have 3 groups: training, validation and testing for a more robust evaluation of algorithms. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) print ( train_dataset . train_data . size ()) print ( train_dataset . train_labels . size ()) print ( test_dataset . test_data . size ()) ``` python print ( test_dataset . test_labels . size ()) torch . Size ([ 60000 , 28 , 28 ]) torch . Size ([ 60000 ]) torch . Size ([ 10000 , 28 , 28 ]) torch . Size ([ 10000 ])","title":"Step 1: Loading MNIST Train Dataset"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#step-2-make-dataset-iterable","text":"Creating an iterable object for our dataset batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False )","title":"Step 2: Make Dataset Iterable"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#step-3-create-model-class","text":"Creating an LSTM model class It is very similar to RNN in terms of the shape of our input of batch_dim x seq_dim x feature_dim . The only change is that we have our cell state on top of our hidden state. PyTorch's LSTM module handles all the other weights for our other gates. class LSTMModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( LSTMModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your LSTM # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . lstm = nn . LSTM ( input_dim , hidden_dim , layer_dim , batch_first = True ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # Initialize cell state c0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # 28 time steps # We need to detach as we are doing truncated backpropagation through time (BPTT) # If we don't, we'll backprop all the way to the start even after going through another batch out , ( hn , cn ) = self . lstm ( x , ( h0 . detach (), c0 . detach ())) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out","title":"Step 3: Create Model Class"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#step-4-instantiate-model-class","text":"28 time steps Each time step: input dimension = 28 1 hidden layer MNIST 1-9 digits \\rightarrow \\rightarrow output dimension = 10 Instantiate our LSTM model input_dim = 28 hidden_dim = 100 layer_dim = 1 output_dim = 10 model = LSTMModel ( input_dim , hidden_dim , layer_dim , output_dim )","title":"Step 4: Instantiate Model Class"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#step-5-instantiate-loss-class","text":"Long Short-Term Memory Neural Network: Cross Entropy Loss Recurrent Neural Network : Cross Entropy Loss Convolutional Neural Network : Cross Entropy Loss Feedforward Neural Network : Cross Entropy Loss Logistic Regression : Cross Entropy Loss Linear Regression : MSE Cross Entry Loss Function Because we are doing a classification problem we'll be using a Cross Entropy function. If we were to do a regression problem, then we would typically use a MSE function. criterion = nn . CrossEntropyLoss ()","title":"Step 5: Instantiate Loss Class"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#step-6-instantiate-optimizer-class","text":"Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our variables) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : parameters' gradients Even simplier equation parameters = parameters - learning_rate * parameters_gradients At every iteration, we update our model's parameters Mini-batch Stochastic Gradient Descent learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate )","title":"Step 6: Instantiate Optimizer Class"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-in-depth","text":"1 Layer LSTM Groups of Parameters We will have 6 groups of parameters here comprising weights and biases from: - Input to Hidden Layer Affine Function - Hidden Layer to Output Affine Function - Hidden Layer to Hidden Layer Affine Function Notice how this is exactly the same number of groups of parameters as our RNN? But the sizes of these groups will be larger for an LSTM due to its gates. len ( list ( model . parameters ())) 6 In-depth Parameters Analysis Comparing to RNN's parameters , we've the same number of groups but for LSTM we've 4x the number of parameters! for i in range ( len ( list ( model . parameters ()))): print ( list ( model . parameters ())[ i ] . size ()) torch . Size ([ 400 , 28 ]) torch . Size ([ 400 , 100 ]) torch . Size ([ 400 ]) torch . Size ([ 400 ]) torch . Size ([ 10 , 100 ]) torch . Size ([ 10 ])","title":"Parameters In-Depth"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-breakdown","text":"This is the breakdown of the parameters associated with the respective affine functions Input \\rightarrow \\rightarrow Gates [400, 28] \\rightarrow w_1, w_3, w_5, w_7 [400, 28] \\rightarrow w_1, w_3, w_5, w_7 [400] \\rightarrow b_1, b_3, b_5, b_7 [400] \\rightarrow b_1, b_3, b_5, b_7 Hidden State \\rightarrow \\rightarrow Gates [400,100] \\rightarrow w_2, w_4, w_6, w_8 [400,100] \\rightarrow w_2, w_4, w_6, w_8 [400] \\rightarrow b_2, b_4, b_6, b_8 [400] \\rightarrow b_2, b_4, b_6, b_8 Hidden State \\rightarrow \\rightarrow Output [10, 100] \\rightarrow w_9 [10, 100] \\rightarrow w_9 [10] \\rightarrow b_9 [10] \\rightarrow b_9","title":"Parameters Breakdown"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#step-7-train-model","text":"Process Convert inputs/labels to variables LSTM Input: (1, 28) RNN Input: (1, 28) CNN Input: (1, 28, 28) FNN Input: (1, 28*28) Clear gradient buffets Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT Training 1 Hidden Layer LSTM # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as a torch tensor with gradient accumulation abilities images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Resize images images = images . view ( - 1 , seq_dim , input_dim ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.8390830755233765 . Accuracy : 72 Iteration : 1000. Loss : 0.46470555663108826 . Accuracy : 85 Iteration : 1500. Loss : 0.31465113162994385 . Accuracy : 91 Iteration : 2000. Loss : 0.19143860042095184 . Accuracy : 94 Iteration : 2500. Loss : 0.16134005784988403 . Accuracy : 95 Iteration : 3000. Loss : 0.255976140499115 . Accuracy : 95","title":"Step 7: Train Model"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#model-b-2-hidden-layer","text":"Unroll 28 time steps Each step input size: 28 x 1 Total per unroll: 28 x 28 Feedforward Neural Network inpt size: 28 x 28 2 Hidden layer","title":"Model B: 2 Hidden Layer"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#steps_1","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Train 2 Hidden Layer LSTM import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class LSTMModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( LSTMModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your LSTM # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . lstm = nn . LSTM ( input_dim , hidden_dim , layer_dim , batch_first = True ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # Initialize cell state c0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # One time step # We need to detach as we are doing truncated backpropagation through time (BPTT) # If we don't, we'll backprop all the way to the start even after going through another batch out , ( hn , cn ) = self . lstm ( x , ( h0 . detach (), c0 . detach ())) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 hidden_dim = 100 layer_dim = 2 # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER output_dim = 10 model = LSTMModel ( input_dim , hidden_dim , layer_dim , output_dim ) # JUST PRINTING MODEL & PARAMETERS print ( model ) print ( len ( list ( model . parameters ()))) for i in range ( len ( list ( model . parameters ()))): print ( list ( model . parameters ())[ i ] . size ()) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as torch tensor with gradient accumulation abilities images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Resize image images = images . view ( - 1 , seq_dim , input_dim ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) LSTMModel ( ( lstm ): LSTM ( 28 , 100 , num_layers = 2 , batch_first = True ) ( fc ): Linear ( in_features = 100 , out_features = 10 , bias = True ) ) 10 torch . Size ([ 400 , 28 ]) torch . Size ([ 400 , 100 ]) torch . Size ([ 400 ]) torch . Size ([ 400 ]) torch . Size ([ 400 , 100 ]) torch . Size ([ 400 , 100 ]) torch . Size ([ 400 ]) torch . Size ([ 400 ]) torch . Size ([ 10 , 100 ]) torch . Size ([ 10 ]) Iteration : 500. Loss : 2.3074915409088135 . Accuracy : 11 Iteration : 1000. Loss : 1.8854578733444214 . Accuracy : 35 Iteration : 1500. Loss : 0.5317062139511108 . Accuracy : 80 Iteration : 2000. Loss : 0.15290376543998718 . Accuracy : 92 Iteration : 2500. Loss : 0.19500978291034698 . Accuracy : 93 Iteration : 3000. Loss : 0.10683634132146835 . Accuracy : 95","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-breakdown-layer-1","text":"Input \\rightarrow \\rightarrow Gates [400, 28] [400, 28] [400] [400] Hidden State \\rightarrow \\rightarrow Gates [400,100] [400,100] [400] [400]","title":"Parameters Breakdown (Layer 1)"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-breakdown-layer-2","text":"Input \\rightarrow \\rightarrow Gates [400, 100] [400, 100] [400] [400] Hidden State \\rightarrow \\rightarrow Gates [400,100] [400,100] [400] [400]","title":"Parameters Breakdown (Layer 2)"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-breakdown-readout-layer","text":"Hidden State \\rightarrow \\rightarrow Output [10, 100] [10, 100] [10] [10]","title":"Parameters Breakdown (Readout Layer)"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#model-c-3-hidden-layer","text":"Unroll 28 time steps Each step input size: 28 x 1 Total per unroll: 28 x 28 Feedforward Neural Network inpt size: 28 x 28 3 Hidden layer","title":"Model C: 3 Hidden Layer"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#steps_2","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 3 Hidden Layer LSTM import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class LSTMModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( LSTMModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your LSTM # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . lstm = nn . LSTM ( input_dim , hidden_dim , layer_dim , batch_first = True ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # Initialize cell state c0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # One time step # We need to detach as we are doing truncated backpropagation through time (BPTT) # If we don't, we'll backprop all the way to the start even after going through another batch out , ( hn , cn ) = self . lstm ( x , ( h0 . detach (), c0 . detach ())) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 hidden_dim = 100 layer_dim = 3 # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER output_dim = 10 model = LSTMModel ( input_dim , hidden_dim , layer_dim , output_dim ) # JUST PRINTING MODEL & PARAMETERS print ( model ) print ( len ( list ( model . parameters ()))) for i in range ( len ( list ( model . parameters ()))): print ( list ( model . parameters ())[ i ] . size ()) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) LSTMModel( (lstm): LSTM(28, 100, num_layers=3, batch_first=True) (fc): Linear(in_features=100, out_features=10, bias=True) ) 14 torch.Size([400, 28]) torch.Size([400, 100]) torch.Size([400]) torch.Size([400]) torch.Size([400, 100]) torch.Size([400, 100]) torch.Size([400]) torch.Size([400]) torch.Size([400, 100]) torch.Size([400, 100]) torch.Size([400]) torch.Size([400]) torch.Size([10, 100]) torch.Size([10]) Iteration: 500. Loss: 2.2927396297454834. Accuracy: 11 Iteration: 1000. Loss: 2.29740309715271. Accuracy: 11 Iteration: 1500. Loss: 2.1950502395629883. Accuracy: 20 Iteration: 2000. Loss: 1.0738657712936401. Accuracy: 59 Iteration: 2500. Loss: 0.5988132357597351. Accuracy: 79 Iteration: 3000. Loss: 0.4107239246368408. Accuracy: 88","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-breakdown-layer-1_1","text":"Input \\rightarrow \\rightarrow Gates [400, 28] [400] Hidden State \\rightarrow \\rightarrow Gates [400,100] [400]","title":"Parameters Breakdown (Layer 1)"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-breakdown-layer-2_1","text":"Input \\rightarrow \\rightarrow Gates [400, 100] [400] Hidden State \\rightarrow \\rightarrow Gates [400,100] [400]","title":"Parameters Breakdown (Layer 2)"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-breakdown-layer-3","text":"Input \\rightarrow \\rightarrow Gates [400, 100] [400] Hidden State \\rightarrow \\rightarrow Gates [400,100] [400]","title":"Parameters Breakdown (Layer 3)"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-breakdown-readout-layer_1","text":"Hidden State \\rightarrow \\rightarrow Output [10, 100] [10]","title":"Parameters Breakdown (Readout Layer)"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#comparison-with-rnn","text":"Model A RNN Model B RNN Model C RNN ReLU ReLU Tanh 1 Hidden Layer 2 Hidden Layers 3 Hidden Layers 100 Hidden Units 100 Hidden Units 100 Hidden Units 92.48% 95.09% 95.54% Model A LSTM Model B LSTM Model C LSTM 1 Hidden Layer 2 Hidden Layers 3 Hidden Layers 100 Hidden Units 100 Hidden Units 100 Hidden Units 96.05% 95.24% 91.22%","title":"Comparison with RNN"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#deep-learning-notes","text":"2 ways to expand a recurrent neural network More hidden units (o, i, f, g) gates More hidden layers Cons Need a larger dataset Curse of dimensionality Does not necessarily mean higher accuracy","title":"Deep Learning Notes"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#3-building-a-recurrent-neural-network-with-pytorch-gpu","text":"","title":"3. Building a Recurrent Neural Network with PyTorch (GPU)"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#model-a-3-hidden-layers","text":"GPU: 2 things must be on GPU - model - tensors","title":"Model A: 3 Hidden Layers"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#steps_3","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 3 Hidden Layer LSTM on GPU import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class LSTMModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( LSTMModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your LSTM # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . lstm = nn . LSTM ( input_dim , hidden_dim , layer_dim , batch_first = True ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros ####################### # USE GPU FOR MODEL # ####################### h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () . to ( device ) # Initialize cell state c0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () . to ( device ) # One time step out , ( hn , cn ) = self . lstm ( x , ( h0 . detach (), c0 . detach ())) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 hidden_dim = 100 layer_dim = 3 # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER output_dim = 10 model = LSTMModel ( input_dim , hidden_dim , layer_dim , output_dim ) ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , seq_dim , input_dim ) . to ( device ) labels = labels . to ( device ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions ####################### # USE GPU FOR MODEL # ####################### if torch . cuda . is_available (): correct += ( predicted . cpu () == labels . cpu ()) . sum () else : correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 2.3068575859069824 . Accuracy : 11 Iteration : 1000. Loss : 2.291989803314209 . Accuracy : 14 Iteration : 1500. Loss : 1.909593105316162 . Accuracy : 28 Iteration : 2000. Loss : 0.7345633506774902 . Accuracy : 71 Iteration : 2500. Loss : 0.45030108094215393 . Accuracy : 86 Iteration : 3000. Loss : 0.2627193331718445 . Accuracy : 89","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#summary","text":"We've learnt to... Success RNN transition to LSTM LSTM Models in PyTorch Model A: 1 Hidden Layer LSTM Model B: 2 Hidden Layer LSTM Model C: 3 Hidden Layer LSTM Models Variation in Code Modifying only step 4 Ways to Expand Model\u2019s Capacity More hidden units More hidden layers Cons of Expanding Capacity Need more data Does not necessarily mean higher accuracy GPU Code 2 things on GPU model tensors Modifying only Step 3, 4 and 7 7 Step Model Building Recap Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model","title":"Summary"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/","text":"Matrices with PyTorch \u00b6 Run Jupyter Notebook You can run the code for this section in this jupyter notebook link . Matrices \u00b6 Matrices Brief Introduction \u00b6 Basic definition: rectangular array of numbers. Tensors (PyTorch) Ndarrays (NumPy) 2 x 2 Matrix (R x C) 1 1 1 1 2 x 3 Matrix 1 1 1 1 1 1 Creating Matrices \u00b6 Create list # Creating a 2x2 array arr = [[ 1 , 2 ], [ 3 , 4 ]] print ( arr ) [[ 1 , 2 ], [ 3 , 4 ]] Create numpy array via list import numpy as np # Convert to NumPy np . array ( arr ) array ([[ 1 , 2 ], [ 3 , 4 ]]) Convert numpy array to PyTorch tensor import torch # Convert to PyTorch Tensor torch . Tensor ( arr ) 1 2 3 4 [ torch . FloatTensor of size 2 x2 ] Create Matrices with Default Values \u00b6 Create 2x2 numpy array of 1's np . ones (( 2 , 2 )) array ([[ 1. , 1. ], [ 1. , 1. ]]) Create 2x2 torch tensor of 1's torch . ones (( 2 , 2 )) 1 1 1 1 [torch.FloatTensor of size 2x2] Create 2x2 numpy array of random numbers np . random . rand ( 2 , 2 ) array ([[ 0.68270631 , 0.87721678 ], [ 0.07420986 , 0.79669375 ]]) Create 2x2 PyTorch tensor of random numbers torch . rand ( 2 , 2 ) 0.3900 0.8268 0.3888 0.5914 [ torch . FloatTensor of size 2 x2 ] Seeds for Reproducibility \u00b6 Why do we need seeds? We need seeds to enable reproduction of experimental results. This becomes critical later on where you can easily let people reproduce your code's output exactly as you've produced. Create seed to enable fixed numbers for random number generation # Seed np . random . seed ( 0 ) np . random . rand ( 2 , 2 ) array ([[ 0.5488135 , 0.71518937 ], [ 0.60276338 , 0.54488318 ]]) Repeat random array generation to check If you do not set the seed, you would not get the same set of numbers like here. # Seed np . random . seed ( 0 ) np . random . rand ( 2 , 2 ) array ([[ 0.5488135 , 0.71518937 ], [ 0.60276338 , 0.54488318 ]]) Create a numpy array without seed Notice how you get different numbers compared to the first 2 tries? # No seed np . random . rand ( 2 , 2 ) array ([[ 0.56804456 , 0.92559664 ], [ 0.07103606 , 0.0871293 ]]) Repeat numpy array generation without seed You get the point now, you get a totally different set of numbers. # No seed np . random . rand ( 2 , 2 ) array ([[ 0.0202184 , 0.83261985 ], [ 0.77815675 , 0.87001215 ]]) Create a PyTorch tensor with a fixed seed # Torch Seed torch . manual_seed ( 0 ) torch . rand ( 2 , 2 ) Repeat creating a PyTorch fixed seed tensor # Torch Seed torch . manual_seed ( 0 ) torch . rand ( 2 , 2 ) 0.5488 0.5928 0.7152 0.8443 [ torch . FloatTensor of size 2 x2 ] Creating a PyTorch tensor without seed Like with a numpy array of random numbers without seed, you will not get the same results as above. # Torch No Seed torch . rand ( 2 , 2 ) 0.6028 0.8579 0.5449 0.8473 [ torch . FloatTensor of size 2 x2 ] Repeat creating a PyTorch tensor without seed Notice how these are different numbers again? # Torch No Seed torch . rand ( 2 , 2 ) 0.4237 0.6236 0.6459 0.3844 [ torch . FloatTensor of size 2 x2 ] Seed for GPU is different for now... Fix a seed for GPU tensors When you conduct deep learning experiments, typically you want to use GPUs to accelerate your computations and fixing seed for tensors on GPUs is different from CPUs as we have done above. if torch . cuda . is_available (): torch . cuda . manual_seed_all ( 0 ) NumPy and Torch Bridge \u00b6 NumPy to Torch \u00b6 Create a numpy array of 1's # Numpy array np_array = np . ones (( 2 , 2 )) print ( np_array ) [[ 1. 1. ] [ 1. 1. ]] Get the type of class for the numpy array print ( type ( np_array )) < class ' numpy . ndarray '> Convert numpy array to PyTorch tensor # Convert to Torch Tensor torch_tensor = torch . from_numpy ( np_array ) print ( torch_tensor ) 1 1 1 1 [ torch . DoubleTensor of size 2 x2 ] Get type of class for PyTorch tensor Notice how it shows it's a torch DoubleTensor? There're actually tensor types and it depends on the numpy data type. print ( type ( torch_tensor )) < class ' torch . DoubleTensor '> Create PyTorch tensor from a different numpy datatype You will get an error running this code because PyTorch tensor don't support all datatype. # Data types matter: intentional error np_array_new = np . ones (( 2 , 2 ), dtype = np . int8 ) torch . from_numpy ( np_array_new ) --------------------------------------------------------------------------- RuntimeError Traceback ( most recent call last ) < ipython - input - 57 - b8b085f9b39d > in < module > () 1 # Data types matter 2 np_array_new = np . ones (( 2 , 2 ), dtype = np . int8 ) ----> 3 torch . from_numpy ( np_array_new ) RuntimeError : can 't convert a given np.ndarray to a tensor - it has an invalid type. The only supported types are: double, float, int64, int32, and uint8. What conversion support does Numpy to PyTorch tensor bridge gives? double float int64 , int32 , uint8 Create PyTorch long tensor See how a int64 numpy array gives you a PyTorch long tensor? # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . int64 ) torch . from_numpy ( np_array_new ) 1 1 1 1 [torch.LongTensor of size 2x2] Create PyTorch int tensor # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . int32 ) torch . from_numpy ( np_array_new ) 1 1 1 1 [ torch . IntTensor of size 2 x2 ] Create PyTorch byte tensor # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . uint8 ) torch . from_numpy ( np_array_new ) 1 1 1 1 [ torch . ByteTensor of size 2 x2 ] Create PyTorch Double Tensor # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . float64 ) torch . from_numpy ( np_array_new ) Alternatively you can do this too via np.double # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . double ) torch . from_numpy ( np_array_new ) 1 1 1 1 [ torch . DoubleTensor of size 2 x2 ] Create PyTorch Float Tensor # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . float32 ) torch . from_numpy ( np_array_new ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Tensor Type Bug Guide These things don't matter much now. But later when you see error messages that require these particular tensor types, refer to this guide! NumPy Array Type Torch Tensor Type int64 LongTensor int32 IntegerTensor uint8 ByteTensor float64 DoubleTensor float32 FloatTensor double DoubleTensor Torch to NumPy \u00b6 Create PyTorch tensor of 1's You would realize this defaults to a float tensor by default if you do this. torch_tensor = torch . ones ( 2 , 2 ) type ( torch_tensor ) torch . FloatTensor Convert tensor to numpy It's as simple as this. torch_to_numpy = torch_tensor . numpy () type ( torch_to_numpy ) # Wowza, we did it. numpy . ndarray Tensors on CPU vs GPU \u00b6 Move tensor to CPU and back This by default creates a tensor on CPU. You do not need to do anything. # CPU tensor_cpu = torch . ones ( 2 , 2 ) If you would like to send a tensor to your GPU, you just need to do a simple .cuda() # CPU to GPU device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) tensor_cpu . to ( device ) And if you want to move that tensor on the GPU back to the CPU, just do the following. # GPU to CPU tensor_cpu . cpu () Tensor Operations \u00b6 Resizing Tensor \u00b6 Creating a 2x2 tensor a = torch . ones ( 2 , 2 ) print ( a ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Getting size of tensor print ( a . size ()) torch . Size ([ 2 , 2 ]) Resize tensor to 4x1 a . view ( 4 ) 1 1 1 1 [ torch . FloatTensor of size 4 ] Get size of resized tensor a . view ( 4 ) . size () torch . Size ([ 4 ]) Element-wise Addition \u00b6 Creating first 2x2 tensor a = torch . ones ( 2 , 2 ) print ( a ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Creating second 2x2 tensor b = torch . ones ( 2 , 2 ) print ( b ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Element-wise addition of 2 tensors # Element-wise addition c = a + b print ( c ) 2 2 2 2 [ torch . FloatTensor of size 2 x2 ] Alternative element-wise addition of 2 tensors # Element-wise addition c = torch . add ( a , b ) print ( c ) 2 2 2 2 [ torch . FloatTensor of size 2 x2 ] In-place element-wise addition This would replace the c tensor values with the new addition. # In-place addition print ( 'Old c tensor' ) print ( c ) c . add_ ( a ) print ( '-' * 60 ) print ( 'New c tensor' ) print ( c ) Old c tensor 2 2 2 2 [ torch . FloatTensor of size 2 x2 ] ------------------------------------------------------------ New c tensor 3 3 3 3 [ torch . FloatTensor of size 2 x2 ] Element-wise Subtraction \u00b6 Check values of tensor a and b' Take note that you've created tensor a and b of sizes 2x2 filled with 1's each above. print ( a ) print ( b ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Element-wise subtraction: method 1 a - b 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise subtraction: method 2 # Not in-place print ( a . sub ( b )) print ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Element-wise subtraction: method 3 This will replace a with the final result filled with 2's # Inplace print ( a . sub_ ( b )) print ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-Wise Multiplication \u00b6 Create tensor a and b of sizes 2x2 filled with 1's and 0's a = torch . ones ( 2 , 2 ) print ( a ) b = torch . zeros ( 2 , 2 ) print ( b ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise multiplication: method 1 a * b 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise multiplication: method 2 # Not in-place print ( torch . mul ( a , b )) print ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Element-wise multiplication: method 3 # In-place print ( a . mul_ ( b )) print ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-Wise Division \u00b6 Create tensor a and b of sizes 2x2 filled with 1's and 0's a = torch . ones ( 2 , 2 ) print ( a ) b = torch . zeros ( 2 , 2 ) print ( b ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise division: method 1 b / a 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise division: method 2 torch . div ( b , a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise division: method 3 # Inplace b . div_ ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Tensor Mean \u00b6 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55 mean = 55 /10 = 5.5 mean = 55 /10 = 5.5 Create tensor of size 10 filled from 1 to 10 a = torch . Tensor ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ]) a . size () torch . Size ([ 10 ]) Get tensor mean Here we get 5.5 as we've calculated manually above. a . mean ( dim = 0 ) 5.5000 [ torch . FloatTensor of size 1 ] Get tensor mean on second dimension Here we get an error because the tensor is of size 10 and not 10x1 so there's no second dimension to calculate. a . mean ( dim = 1 ) RuntimeError Traceback ( most recent call last ) < ipython - input - 7 - 81 aec0cf1c00 > in < module > () ----> 1 a . mean ( dim = 1 ) RuntimeError : dimension out of range ( expected to be in range of [ - 1 , 0 ], but got 1 ) Create a 2x10 Tensor, of 1-10 digits each a = torch . Tensor ([[ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ], [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ]]) a . size () torch . Size ([ 2 , 10 ]) Get tensor mean on second dimension Here we won't get an error like previously because we've a tensor of size 2x10 a . mean ( dim = 1 ) 5.5000 5.5000 [ torch . FloatTensor of size 2 x1 ] Tensor Standard Deviation \u00b6 Get standard deviation of tensor a = torch . Tensor ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ]) a . std ( dim = 0 ) 3.0277 [ torch . FloatTensor of size 1 ] Summary \u00b6 We've learnt to... Success Create Matrices Create Matrices with Default Initialization Values Zeros Ones Initialize Seeds for Reproducibility on GPU and CPU Convert Matrices: NumPy to Torch and Torch to NumPy Move Tensors: CPU to GPU and GPU to CPU Run Important Tensor Operations Element-wise addition, subtraction, multiplication and division Resize Calculate mean Calculate standard deviation Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Matrices"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#matrices-with-pytorch","text":"Run Jupyter Notebook You can run the code for this section in this jupyter notebook link .","title":"Matrices with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#matrices","text":"","title":"Matrices"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#matrices-brief-introduction","text":"Basic definition: rectangular array of numbers. Tensors (PyTorch) Ndarrays (NumPy) 2 x 2 Matrix (R x C) 1 1 1 1 2 x 3 Matrix 1 1 1 1 1 1","title":"Matrices Brief Introduction"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#creating-matrices","text":"Create list # Creating a 2x2 array arr = [[ 1 , 2 ], [ 3 , 4 ]] print ( arr ) [[ 1 , 2 ], [ 3 , 4 ]] Create numpy array via list import numpy as np # Convert to NumPy np . array ( arr ) array ([[ 1 , 2 ], [ 3 , 4 ]]) Convert numpy array to PyTorch tensor import torch # Convert to PyTorch Tensor torch . Tensor ( arr ) 1 2 3 4 [ torch . FloatTensor of size 2 x2 ]","title":"Creating Matrices"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#create-matrices-with-default-values","text":"Create 2x2 numpy array of 1's np . ones (( 2 , 2 )) array ([[ 1. , 1. ], [ 1. , 1. ]]) Create 2x2 torch tensor of 1's torch . ones (( 2 , 2 )) 1 1 1 1 [torch.FloatTensor of size 2x2] Create 2x2 numpy array of random numbers np . random . rand ( 2 , 2 ) array ([[ 0.68270631 , 0.87721678 ], [ 0.07420986 , 0.79669375 ]]) Create 2x2 PyTorch tensor of random numbers torch . rand ( 2 , 2 ) 0.3900 0.8268 0.3888 0.5914 [ torch . FloatTensor of size 2 x2 ]","title":"Create Matrices with Default Values"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#seeds-for-reproducibility","text":"Why do we need seeds? We need seeds to enable reproduction of experimental results. This becomes critical later on where you can easily let people reproduce your code's output exactly as you've produced. Create seed to enable fixed numbers for random number generation # Seed np . random . seed ( 0 ) np . random . rand ( 2 , 2 ) array ([[ 0.5488135 , 0.71518937 ], [ 0.60276338 , 0.54488318 ]]) Repeat random array generation to check If you do not set the seed, you would not get the same set of numbers like here. # Seed np . random . seed ( 0 ) np . random . rand ( 2 , 2 ) array ([[ 0.5488135 , 0.71518937 ], [ 0.60276338 , 0.54488318 ]]) Create a numpy array without seed Notice how you get different numbers compared to the first 2 tries? # No seed np . random . rand ( 2 , 2 ) array ([[ 0.56804456 , 0.92559664 ], [ 0.07103606 , 0.0871293 ]]) Repeat numpy array generation without seed You get the point now, you get a totally different set of numbers. # No seed np . random . rand ( 2 , 2 ) array ([[ 0.0202184 , 0.83261985 ], [ 0.77815675 , 0.87001215 ]]) Create a PyTorch tensor with a fixed seed # Torch Seed torch . manual_seed ( 0 ) torch . rand ( 2 , 2 ) Repeat creating a PyTorch fixed seed tensor # Torch Seed torch . manual_seed ( 0 ) torch . rand ( 2 , 2 ) 0.5488 0.5928 0.7152 0.8443 [ torch . FloatTensor of size 2 x2 ] Creating a PyTorch tensor without seed Like with a numpy array of random numbers without seed, you will not get the same results as above. # Torch No Seed torch . rand ( 2 , 2 ) 0.6028 0.8579 0.5449 0.8473 [ torch . FloatTensor of size 2 x2 ] Repeat creating a PyTorch tensor without seed Notice how these are different numbers again? # Torch No Seed torch . rand ( 2 , 2 ) 0.4237 0.6236 0.6459 0.3844 [ torch . FloatTensor of size 2 x2 ] Seed for GPU is different for now... Fix a seed for GPU tensors When you conduct deep learning experiments, typically you want to use GPUs to accelerate your computations and fixing seed for tensors on GPUs is different from CPUs as we have done above. if torch . cuda . is_available (): torch . cuda . manual_seed_all ( 0 )","title":"Seeds for Reproducibility"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#numpy-and-torch-bridge","text":"","title":"NumPy and Torch Bridge"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#numpy-to-torch","text":"Create a numpy array of 1's # Numpy array np_array = np . ones (( 2 , 2 )) print ( np_array ) [[ 1. 1. ] [ 1. 1. ]] Get the type of class for the numpy array print ( type ( np_array )) < class ' numpy . ndarray '> Convert numpy array to PyTorch tensor # Convert to Torch Tensor torch_tensor = torch . from_numpy ( np_array ) print ( torch_tensor ) 1 1 1 1 [ torch . DoubleTensor of size 2 x2 ] Get type of class for PyTorch tensor Notice how it shows it's a torch DoubleTensor? There're actually tensor types and it depends on the numpy data type. print ( type ( torch_tensor )) < class ' torch . DoubleTensor '> Create PyTorch tensor from a different numpy datatype You will get an error running this code because PyTorch tensor don't support all datatype. # Data types matter: intentional error np_array_new = np . ones (( 2 , 2 ), dtype = np . int8 ) torch . from_numpy ( np_array_new ) --------------------------------------------------------------------------- RuntimeError Traceback ( most recent call last ) < ipython - input - 57 - b8b085f9b39d > in < module > () 1 # Data types matter 2 np_array_new = np . ones (( 2 , 2 ), dtype = np . int8 ) ----> 3 torch . from_numpy ( np_array_new ) RuntimeError : can 't convert a given np.ndarray to a tensor - it has an invalid type. The only supported types are: double, float, int64, int32, and uint8. What conversion support does Numpy to PyTorch tensor bridge gives? double float int64 , int32 , uint8 Create PyTorch long tensor See how a int64 numpy array gives you a PyTorch long tensor? # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . int64 ) torch . from_numpy ( np_array_new ) 1 1 1 1 [torch.LongTensor of size 2x2] Create PyTorch int tensor # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . int32 ) torch . from_numpy ( np_array_new ) 1 1 1 1 [ torch . IntTensor of size 2 x2 ] Create PyTorch byte tensor # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . uint8 ) torch . from_numpy ( np_array_new ) 1 1 1 1 [ torch . ByteTensor of size 2 x2 ] Create PyTorch Double Tensor # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . float64 ) torch . from_numpy ( np_array_new ) Alternatively you can do this too via np.double # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . double ) torch . from_numpy ( np_array_new ) 1 1 1 1 [ torch . DoubleTensor of size 2 x2 ] Create PyTorch Float Tensor # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . float32 ) torch . from_numpy ( np_array_new ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Tensor Type Bug Guide These things don't matter much now. But later when you see error messages that require these particular tensor types, refer to this guide! NumPy Array Type Torch Tensor Type int64 LongTensor int32 IntegerTensor uint8 ByteTensor float64 DoubleTensor float32 FloatTensor double DoubleTensor","title":"NumPy to Torch"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#torch-to-numpy","text":"Create PyTorch tensor of 1's You would realize this defaults to a float tensor by default if you do this. torch_tensor = torch . ones ( 2 , 2 ) type ( torch_tensor ) torch . FloatTensor Convert tensor to numpy It's as simple as this. torch_to_numpy = torch_tensor . numpy () type ( torch_to_numpy ) # Wowza, we did it. numpy . ndarray","title":"Torch to NumPy"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#tensors-on-cpu-vs-gpu","text":"Move tensor to CPU and back This by default creates a tensor on CPU. You do not need to do anything. # CPU tensor_cpu = torch . ones ( 2 , 2 ) If you would like to send a tensor to your GPU, you just need to do a simple .cuda() # CPU to GPU device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) tensor_cpu . to ( device ) And if you want to move that tensor on the GPU back to the CPU, just do the following. # GPU to CPU tensor_cpu . cpu ()","title":"Tensors on CPU vs GPU"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#tensor-operations","text":"","title":"Tensor Operations"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#resizing-tensor","text":"Creating a 2x2 tensor a = torch . ones ( 2 , 2 ) print ( a ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Getting size of tensor print ( a . size ()) torch . Size ([ 2 , 2 ]) Resize tensor to 4x1 a . view ( 4 ) 1 1 1 1 [ torch . FloatTensor of size 4 ] Get size of resized tensor a . view ( 4 ) . size () torch . Size ([ 4 ])","title":"Resizing Tensor"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#element-wise-addition","text":"Creating first 2x2 tensor a = torch . ones ( 2 , 2 ) print ( a ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Creating second 2x2 tensor b = torch . ones ( 2 , 2 ) print ( b ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Element-wise addition of 2 tensors # Element-wise addition c = a + b print ( c ) 2 2 2 2 [ torch . FloatTensor of size 2 x2 ] Alternative element-wise addition of 2 tensors # Element-wise addition c = torch . add ( a , b ) print ( c ) 2 2 2 2 [ torch . FloatTensor of size 2 x2 ] In-place element-wise addition This would replace the c tensor values with the new addition. # In-place addition print ( 'Old c tensor' ) print ( c ) c . add_ ( a ) print ( '-' * 60 ) print ( 'New c tensor' ) print ( c ) Old c tensor 2 2 2 2 [ torch . FloatTensor of size 2 x2 ] ------------------------------------------------------------ New c tensor 3 3 3 3 [ torch . FloatTensor of size 2 x2 ]","title":"Element-wise Addition"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#element-wise-subtraction","text":"Check values of tensor a and b' Take note that you've created tensor a and b of sizes 2x2 filled with 1's each above. print ( a ) print ( b ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Element-wise subtraction: method 1 a - b 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise subtraction: method 2 # Not in-place print ( a . sub ( b )) print ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Element-wise subtraction: method 3 This will replace a with the final result filled with 2's # Inplace print ( a . sub_ ( b )) print ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] 0 0 0 0 [ torch . FloatTensor of size 2 x2 ]","title":"Element-wise Subtraction"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#element-wise-multiplication","text":"Create tensor a and b of sizes 2x2 filled with 1's and 0's a = torch . ones ( 2 , 2 ) print ( a ) b = torch . zeros ( 2 , 2 ) print ( b ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise multiplication: method 1 a * b 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise multiplication: method 2 # Not in-place print ( torch . mul ( a , b )) print ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Element-wise multiplication: method 3 # In-place print ( a . mul_ ( b )) print ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] 0 0 0 0 [ torch . FloatTensor of size 2 x2 ]","title":"Element-Wise Multiplication"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#element-wise-division","text":"Create tensor a and b of sizes 2x2 filled with 1's and 0's a = torch . ones ( 2 , 2 ) print ( a ) b = torch . zeros ( 2 , 2 ) print ( b ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise division: method 1 b / a 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise division: method 2 torch . div ( b , a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise division: method 3 # Inplace b . div_ ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ]","title":"Element-Wise Division"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#tensor-mean","text":"1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55 mean = 55 /10 = 5.5 mean = 55 /10 = 5.5 Create tensor of size 10 filled from 1 to 10 a = torch . Tensor ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ]) a . size () torch . Size ([ 10 ]) Get tensor mean Here we get 5.5 as we've calculated manually above. a . mean ( dim = 0 ) 5.5000 [ torch . FloatTensor of size 1 ] Get tensor mean on second dimension Here we get an error because the tensor is of size 10 and not 10x1 so there's no second dimension to calculate. a . mean ( dim = 1 ) RuntimeError Traceback ( most recent call last ) < ipython - input - 7 - 81 aec0cf1c00 > in < module > () ----> 1 a . mean ( dim = 1 ) RuntimeError : dimension out of range ( expected to be in range of [ - 1 , 0 ], but got 1 ) Create a 2x10 Tensor, of 1-10 digits each a = torch . Tensor ([[ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ], [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ]]) a . size () torch . Size ([ 2 , 10 ]) Get tensor mean on second dimension Here we won't get an error like previously because we've a tensor of size 2x10 a . mean ( dim = 1 ) 5.5000 5.5000 [ torch . FloatTensor of size 2 x1 ]","title":"Tensor Mean"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#tensor-standard-deviation","text":"Get standard deviation of tensor a = torch . Tensor ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ]) a . std ( dim = 0 ) 3.0277 [ torch . FloatTensor of size 1 ]","title":"Tensor Standard Deviation"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#summary","text":"We've learnt to... Success Create Matrices Create Matrices with Default Initialization Values Zeros Ones Initialize Seeds for Reproducibility on GPU and CPU Convert Matrices: NumPy to Torch and Torch to NumPy Move Tensors: CPU to GPU and GPU to CPU Run Important Tensor Operations Element-wise addition, subtraction, multiplication and division Resize Calculate mean Calculate standard deviation","title":"Summary"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/","text":"Recurrent Neural Network with PyTorch \u00b6 Run Jupyter Notebook You can run the code for this section in this jupyter notebook link . About Recurrent Neural Network \u00b6 Feedforward Neural Networks Transition to 1 Layer Recurrent Neural Networks (RNN) \u00b6 RNN is essentially an FNN but with a hidden layer (non-linear output) that passes on information to the next FNN Compared to an FNN, we've one additional set of weight and bias that allows information to flow from one FNN to another FNN sequentially that allows time-dependency. The diagram below shows the only difference between an FNN and a RNN. 2 Layer RNN Breakdown \u00b6 Building a Recurrent Neural Network with PyTorch \u00b6 Model A: 1 Hidden Layer (ReLU) \u00b6 Unroll 28 time steps Each step input size: 28 x 1 Total per unroll: 28 x 28 Feedforward Neural Network input size: 28 x 28 1 Hidden layer ReLU Activation Function Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Step 1: Loading MNIST Train Dataset \u00b6 Images from 1 to 9 Looking into the MNIST Dataset import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) We would have 60k training images of size 28 x 28 pixels. print ( train_dataset . train_data . size ()) print ( train_dataset . train_labels . size ()) Here we would have 10k testing images of the same size, 28 x 28 pixels. print ( test_dataset . test_data . size ()) print ( test_dataset . test_labels . size ()) torch . Size ([ 60000 , 28 , 28 ]) torch . Size ([ 60000 ]) torch . Size ([ 10000 , 28 , 28 ]) torch . Size ([ 10000 ]) Step 2: Make Dataset Iterable \u00b6 Creating iterable objects to loop through subsequently batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) Step 3: Create Model Class \u00b6 1 Layer RNN class RNNModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( RNNModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your RNN # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, input_dim) # batch_dim = number of samples per batch self . rnn = nn . RNN ( input_dim , hidden_dim , layer_dim , batch_first = True , nonlinearity = 'relu' ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros # (layer_dim, batch_size, hidden_dim) h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # We need to detach the hidden state to prevent exploding/vanishing gradients # This is part of truncated backpropagation through time (BPTT) out , hn = self . rnn ( x , h0 . detach ()) # Index hidden state of last time step # out.size() --> 100, 28, 10 # out[:, -1, :] --> 100, 10 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out Step 4: Instantiate Model Class \u00b6 28 time steps Each time step: input dimension = 28 1 hidden layer MNIST 1-9 digits \\rightarrow \\rightarrow output dimension = 10 Instantiate model class and assign to an object input_dim = 28 hidden_dim = 100 layer_dim = 1 output_dim = 10 model = RNNModel ( input_dim , hidden_dim , layer_dim , output_dim ) Step 5: Instantiate Loss Class \u00b6 Recurrent Neural Network: Cross Entropy Loss Convolutional Neural Network : Cross Entropy Loss Feedforward Neural Network : Cross Entropy Loss Logistic Regression : Cross Entropy Loss Linear Regression : MSE Cross Entropy Loss for Classification Task criterion = nn . CrossEntropyLoss () Cross Entropy vs MSE Take note that there are cases where RNN, CNN and FNN use MSE as a loss function. We use cross entropy for classification tasks (predicting 0-9 digits in MNIST for example). And we use MSE for regression tasks (predicting temperatures in every December in San Francisco for example). Step 6: Instantiate Optimizer Class \u00b6 Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our tensors with gradient accumulation abilities) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : gradients of loss with respect to the model's parameters Even simplier equation parameters = parameters - learning_rate * parameters_gradients At every iteration, we update our model's parameters learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) Parameters In-Depth \u00b6 Input to Hidden Layer Affine Function A1, B1 Hidden Layer to Output Affine Function A2, B2 Hidden Layer to Hidden Layer Affine Function A3, B3 Total groups of parameters We should have 6 groups as shown above. len ( list ( model . parameters ())) 6 Input to Hidden Weight Remember we defined our hidden layer to have a size of 100. Because our input is a size of 28 at each time step, this gives rise to a weight matrix of 100 x 28. # Input --> Hidden (A1) list ( model . parameters ())[ 0 ] . size () torch . Size ([ 100 , 28 ]) Input to Hidden Bias # Input --> Hidden BIAS (B1) list ( model . parameters ())[ 2 ] . size () torch . Size ([ 100 ]) Hidden to Hidden # Hidden --> Hidden (A3) list ( model . parameters ())[ 1 ] . size () torch . Size ([ 100 , 100 ]) Hidden to Hidden Bias # Hidden --> Hidden BIAS(B3) list ( model . parameters ())[ 3 ] . size () torch . Size ([ 100 ]) Hidden to Output # Hidden --> Output (A2) list ( model . parameters ())[ 4 ] . size () torch . Size ([ 10 , 100 ]) Hidden to Output Bias # Hidden --> Output BIAS (B2) list ( model . parameters ())[ 5 ] . size () torch . Size ([ 10 ]) Step 7: Train Model \u00b6 Process Convert inputs/labels to tensors with gradient accumulation abilities RNN Input: (1, 28) CNN Input: (1, 28, 28) FNN Input: (1, 28*28) Clear gradient buffets Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT Same 7 step process for training models # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): model . train () # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : model . eval () # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch tensors with gradient accumulation abilities images = images . view ( - 1 , seq_dim , input_dim ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 2.301494836807251 . Accuracy : 12 Iteration : 1000. Loss : 2.2986037731170654 . Accuracy : 14 Iteration : 1500. Loss : 2.278566598892212 . Accuracy : 18 Iteration : 2000. Loss : 2.169614315032959 . Accuracy : 21 Iteration : 2500. Loss : 1.1662731170654297 . Accuracy : 51 Iteration : 3000. Loss : 0.9290509223937988 . Accuracy : 71 Model B: 2 Hidden Layer (ReLU) \u00b6 Unroll 28 time steps Each step input size: 28 x 1 Total per unroll: 28 x 28 Feedforward Neural Network inpt size: 28 x 28 2 Hidden layer ReLU Activation Function Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 2 Hidden Layer + ReLU import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class RNNModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( RNNModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your RNN # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . rnn = nn . RNN ( input_dim , hidden_dim , layer_dim , batch_first = True , nonlinearity = 'relu' ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # We need to detach the hidden state to prevent exploding/vanishing gradients # This is part of truncated backpropagation through time (BPTT) out , hn = self . rnn ( x , h0 . detach ()) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 hidden_dim = 100 layer_dim = 2 # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER output_dim = 10 model = RNNModel ( input_dim , hidden_dim , layer_dim , output_dim ) # JUST PRINTING MODEL & PARAMETERS print ( model ) print ( len ( list ( model . parameters ()))) for i in range ( len ( list ( model . parameters ()))): print ( list ( model . parameters ())[ i ] . size ()) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): model . train () # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : model . eval () # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Resize images images = images . view ( - 1 , seq_dim , input_dim ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) RNNModel ( ( rnn ): RNN ( 28 , 100 , num_layers = 2 , batch_first = True ) ( fc ): Linear ( in_features = 100 , out_features = 10 , bias = True ) ) 10 torch . Size ([ 100 , 28 ]) torch . Size ([ 100 , 100 ]) torch . Size ([ 100 ]) torch . Size ([ 100 ]) torch . Size ([ 100 , 100 ]) torch . Size ([ 100 , 100 ]) torch . Size ([ 100 ]) torch . Size ([ 100 ]) torch . Size ([ 10 , 100 ]) torch . Size ([ 10 ]) Iteration : 500. Loss : 2.3019518852233887 . Accuracy : 11 Iteration : 1000. Loss : 2.299217700958252 . Accuracy : 11 Iteration : 1500. Loss : 2.279090166091919 . Accuracy : 14 Iteration : 2000. Loss : 2.126953125 . Accuracy : 25 Iteration : 2500. Loss : 1.356347680091858 . Accuracy : 57 Iteration : 3000. Loss : 0.7377720475196838 . Accuracy : 69 10 sets of parameters First hidden Layer A_1 = [100, 28] A_1 = [100, 28] A_3 = [100, 100] A_3 = [100, 100] B_1 = [100] B_1 = [100] B_3 = [100] B_3 = [100] Second hidden layer A_2 = [100, 100] A_2 = [100, 100] A_5 = [100, 100] A_5 = [100, 100] B_2 = [100] B_2 = [100] B_5 = [100] B_5 = [100] Readout layer A_4 = [10, 100] A_4 = [10, 100] B_4 = [10] B_4 = [10] Model C: 2 Hidden Layer \u00b6 Unroll 28 time steps Each step input size: 28 x 1 Total per unroll: 28 x 28 Feedforward Neural Network inpt size: 28 x 28 2 Hidden layer Tanh Activation Function Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model !!! \"2 Hidden + ReLU\" import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class RNNModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( RNNModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your RNN # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . rnn = nn . RNN ( input_dim , hidden_dim , layer_dim , batch_first = True , nonlinearity = 'tanh' ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # One time step # We need to detach the hidden state to prevent exploding/vanishing gradients # This is part of truncated backpropagation through time (BPTT) out , hn = self . rnn ( x , h0 . detach ()) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 hidden_dim = 100 layer_dim = 2 # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER output_dim = 10 model = RNNModel ( input_dim , hidden_dim , layer_dim , output_dim ) # JUST PRINTING MODEL & PARAMETERS print ( model ) print ( len ( list ( model . parameters ()))) for i in range ( len ( list ( model . parameters ()))): print ( list ( model . parameters ())[ i ] . size ()) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Resize images images = images . view ( - 1 , seq_dim , input_dim ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) RNNModel ( ( rnn ): RNN ( 28 , 100 , num_layers = 2 , batch_first = True ) ( fc ): Linear ( in_features = 100 , out_features = 10 , bias = True ) ) 10 torch . Size ([ 100 , 28 ]) torch . Size ([ 100 , 100 ]) torch . Size ([ 100 ]) torch . Size ([ 100 ]) torch . Size ([ 100 , 100 ]) torch . Size ([ 100 , 100 ]) torch . Size ([ 100 ]) torch . Size ([ 100 ]) torch . Size ([ 10 , 100 ]) torch . Size ([ 10 ]) Iteration : 500. Loss : 0.5943437218666077 . Accuracy : 77 Iteration : 1000. Loss : 0.22048641741275787 . Accuracy : 91 Iteration : 1500. Loss : 0.18479223549365997 . Accuracy : 94 Iteration : 2000. Loss : 0.2723771929740906 . Accuracy : 91 Iteration : 2500. Loss : 0.18817797303199768 . Accuracy : 92 Iteration : 3000. Loss : 0.1685929149389267 . Accuracy : 92 Summary of Results \u00b6 Model A Model B Model C ReLU ReLU Tanh 1 Hidden Layer 2 Hidden Layers 2 Hidden Layers 100 Hidden Units 100 Hidden Units 100 Hidden Units 92.48% 95.09% 95.54% General Deep Learning Notes \u00b6 2 ways to expand a recurrent neural network More non-linear activation units (neurons) More hidden layers Cons Need a larger dataset Curse of dimensionality Does not necessarily mean higher accuracy 3. Building a Recurrent Neural Network with PyTorch (GPU) \u00b6 Model C: 2 Hidden Layer (Tanh) \u00b6 GPU: 2 things must be on GPU - model - tensors Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 2 Layer RNN + Tanh import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class RNNModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( RNNModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your RNN # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . rnn = nn . RNN ( input_dim , hidden_dim , layer_dim , batch_first = True , nonlinearity = 'tanh' ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros ####################### # USE GPU FOR MODEL # ####################### h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . to ( device ) # One time step # We need to detach the hidden state to prevent exploding/vanishing gradients # This is part of truncated backpropagation through time (BPTT) out , hn = self . rnn ( x , h0 . detach ()) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 hidden_dim = 100 layer_dim = 2 # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER output_dim = 10 model = RNNModel ( input_dim , hidden_dim , layer_dim , output_dim ) ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , seq_dim , input_dim ) . to ( device ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions ####################### # USE GPU FOR MODEL # ####################### if torch . cuda . is_available (): correct += ( predicted . cpu () == labels . cpu ()) . sum () else : correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.5983774662017822 . Accuracy : 81 Iteration : 1000. Loss : 0.2960105836391449 . Accuracy : 86 Iteration : 1500. Loss : 0.19428101181983948 . Accuracy : 93 Iteration : 2000. Loss : 0.11918395012617111 . Accuracy : 95 Iteration : 2500. Loss : 0.11246936023235321 . Accuracy : 95 Iteration : 3000. Loss : 0.15849310159683228 . Accuracy : 95 Summary \u00b6 We've learnt to... Success Feedforward Neural Networks Transition to Recurrent Neural Networks RNN Models in PyTorch Model A: 1 Hidden Layer RNN (ReLU) Model B: 2 Hidden Layer RNN (ReLU) Model C: 2 Hidden Layer RNN (Tanh) Models Variation in Code Modifying only step 4 Ways to Expand Model\u2019s Capacity More non-linear activation units ( neurons ) More hidden layers Cons of Expanding Capacity Need more data Does not necessarily mean higher accuracy GPU Code 2 things on GPU model tensors with gradient accumulation abilities Modifying only Step 3, 4 and 7 7 Step Model Building Recap Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Step 7: Train Model Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Recurrent Neural Networks (RNN)"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#recurrent-neural-network-with-pytorch","text":"Run Jupyter Notebook You can run the code for this section in this jupyter notebook link .","title":"Recurrent Neural Network with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#about-recurrent-neural-network","text":"","title":"About Recurrent Neural Network"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#feedforward-neural-networks-transition-to-1-layer-recurrent-neural-networks-rnn","text":"RNN is essentially an FNN but with a hidden layer (non-linear output) that passes on information to the next FNN Compared to an FNN, we've one additional set of weight and bias that allows information to flow from one FNN to another FNN sequentially that allows time-dependency. The diagram below shows the only difference between an FNN and a RNN.","title":"Feedforward Neural Networks Transition to 1 Layer Recurrent Neural Networks (RNN)"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#2-layer-rnn-breakdown","text":"","title":"2 Layer RNN Breakdown"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#building-a-recurrent-neural-network-with-pytorch","text":"","title":"Building a Recurrent Neural Network with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#model-a-1-hidden-layer-relu","text":"Unroll 28 time steps Each step input size: 28 x 1 Total per unroll: 28 x 28 Feedforward Neural Network input size: 28 x 28 1 Hidden layer ReLU Activation Function","title":"Model A: 1 Hidden Layer (ReLU)"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#steps","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#step-1-loading-mnist-train-dataset","text":"Images from 1 to 9 Looking into the MNIST Dataset import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) We would have 60k training images of size 28 x 28 pixels. print ( train_dataset . train_data . size ()) print ( train_dataset . train_labels . size ()) Here we would have 10k testing images of the same size, 28 x 28 pixels. print ( test_dataset . test_data . size ()) print ( test_dataset . test_labels . size ()) torch . Size ([ 60000 , 28 , 28 ]) torch . Size ([ 60000 ]) torch . Size ([ 10000 , 28 , 28 ]) torch . Size ([ 10000 ])","title":"Step 1: Loading MNIST Train Dataset"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#step-2-make-dataset-iterable","text":"Creating iterable objects to loop through subsequently batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False )","title":"Step 2: Make Dataset Iterable"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#step-3-create-model-class","text":"1 Layer RNN class RNNModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( RNNModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your RNN # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, input_dim) # batch_dim = number of samples per batch self . rnn = nn . RNN ( input_dim , hidden_dim , layer_dim , batch_first = True , nonlinearity = 'relu' ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros # (layer_dim, batch_size, hidden_dim) h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # We need to detach the hidden state to prevent exploding/vanishing gradients # This is part of truncated backpropagation through time (BPTT) out , hn = self . rnn ( x , h0 . detach ()) # Index hidden state of last time step # out.size() --> 100, 28, 10 # out[:, -1, :] --> 100, 10 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out","title":"Step 3: Create Model Class"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#step-4-instantiate-model-class","text":"28 time steps Each time step: input dimension = 28 1 hidden layer MNIST 1-9 digits \\rightarrow \\rightarrow output dimension = 10 Instantiate model class and assign to an object input_dim = 28 hidden_dim = 100 layer_dim = 1 output_dim = 10 model = RNNModel ( input_dim , hidden_dim , layer_dim , output_dim )","title":"Step 4: Instantiate Model Class"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#step-5-instantiate-loss-class","text":"Recurrent Neural Network: Cross Entropy Loss Convolutional Neural Network : Cross Entropy Loss Feedforward Neural Network : Cross Entropy Loss Logistic Regression : Cross Entropy Loss Linear Regression : MSE Cross Entropy Loss for Classification Task criterion = nn . CrossEntropyLoss () Cross Entropy vs MSE Take note that there are cases where RNN, CNN and FNN use MSE as a loss function. We use cross entropy for classification tasks (predicting 0-9 digits in MNIST for example). And we use MSE for regression tasks (predicting temperatures in every December in San Francisco for example).","title":"Step 5: Instantiate Loss Class"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#step-6-instantiate-optimizer-class","text":"Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our tensors with gradient accumulation abilities) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : gradients of loss with respect to the model's parameters Even simplier equation parameters = parameters - learning_rate * parameters_gradients At every iteration, we update our model's parameters learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate )","title":"Step 6: Instantiate Optimizer Class"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#parameters-in-depth","text":"Input to Hidden Layer Affine Function A1, B1 Hidden Layer to Output Affine Function A2, B2 Hidden Layer to Hidden Layer Affine Function A3, B3 Total groups of parameters We should have 6 groups as shown above. len ( list ( model . parameters ())) 6 Input to Hidden Weight Remember we defined our hidden layer to have a size of 100. Because our input is a size of 28 at each time step, this gives rise to a weight matrix of 100 x 28. # Input --> Hidden (A1) list ( model . parameters ())[ 0 ] . size () torch . Size ([ 100 , 28 ]) Input to Hidden Bias # Input --> Hidden BIAS (B1) list ( model . parameters ())[ 2 ] . size () torch . Size ([ 100 ]) Hidden to Hidden # Hidden --> Hidden (A3) list ( model . parameters ())[ 1 ] . size () torch . Size ([ 100 , 100 ]) Hidden to Hidden Bias # Hidden --> Hidden BIAS(B3) list ( model . parameters ())[ 3 ] . size () torch . Size ([ 100 ]) Hidden to Output # Hidden --> Output (A2) list ( model . parameters ())[ 4 ] . size () torch . Size ([ 10 , 100 ]) Hidden to Output Bias # Hidden --> Output BIAS (B2) list ( model . parameters ())[ 5 ] . size () torch . Size ([ 10 ])","title":"Parameters In-Depth"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#step-7-train-model","text":"Process Convert inputs/labels to tensors with gradient accumulation abilities RNN Input: (1, 28) CNN Input: (1, 28, 28) FNN Input: (1, 28*28) Clear gradient buffets Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT Same 7 step process for training models # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): model . train () # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : model . eval () # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch tensors with gradient accumulation abilities images = images . view ( - 1 , seq_dim , input_dim ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 2.301494836807251 . Accuracy : 12 Iteration : 1000. Loss : 2.2986037731170654 . Accuracy : 14 Iteration : 1500. Loss : 2.278566598892212 . Accuracy : 18 Iteration : 2000. Loss : 2.169614315032959 . Accuracy : 21 Iteration : 2500. Loss : 1.1662731170654297 . Accuracy : 51 Iteration : 3000. Loss : 0.9290509223937988 . Accuracy : 71","title":"Step 7: Train Model"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#model-b-2-hidden-layer-relu","text":"Unroll 28 time steps Each step input size: 28 x 1 Total per unroll: 28 x 28 Feedforward Neural Network inpt size: 28 x 28 2 Hidden layer ReLU Activation Function","title":"Model B: 2 Hidden Layer (ReLU)"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#steps_1","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 2 Hidden Layer + ReLU import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class RNNModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( RNNModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your RNN # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . rnn = nn . RNN ( input_dim , hidden_dim , layer_dim , batch_first = True , nonlinearity = 'relu' ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # We need to detach the hidden state to prevent exploding/vanishing gradients # This is part of truncated backpropagation through time (BPTT) out , hn = self . rnn ( x , h0 . detach ()) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 hidden_dim = 100 layer_dim = 2 # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER output_dim = 10 model = RNNModel ( input_dim , hidden_dim , layer_dim , output_dim ) # JUST PRINTING MODEL & PARAMETERS print ( model ) print ( len ( list ( model . parameters ()))) for i in range ( len ( list ( model . parameters ()))): print ( list ( model . parameters ())[ i ] . size ()) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): model . train () # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : model . eval () # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Resize images images = images . view ( - 1 , seq_dim , input_dim ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) RNNModel ( ( rnn ): RNN ( 28 , 100 , num_layers = 2 , batch_first = True ) ( fc ): Linear ( in_features = 100 , out_features = 10 , bias = True ) ) 10 torch . Size ([ 100 , 28 ]) torch . Size ([ 100 , 100 ]) torch . Size ([ 100 ]) torch . Size ([ 100 ]) torch . Size ([ 100 , 100 ]) torch . Size ([ 100 , 100 ]) torch . Size ([ 100 ]) torch . Size ([ 100 ]) torch . Size ([ 10 , 100 ]) torch . Size ([ 10 ]) Iteration : 500. Loss : 2.3019518852233887 . Accuracy : 11 Iteration : 1000. Loss : 2.299217700958252 . Accuracy : 11 Iteration : 1500. Loss : 2.279090166091919 . Accuracy : 14 Iteration : 2000. Loss : 2.126953125 . Accuracy : 25 Iteration : 2500. Loss : 1.356347680091858 . Accuracy : 57 Iteration : 3000. Loss : 0.7377720475196838 . Accuracy : 69 10 sets of parameters First hidden Layer A_1 = [100, 28] A_1 = [100, 28] A_3 = [100, 100] A_3 = [100, 100] B_1 = [100] B_1 = [100] B_3 = [100] B_3 = [100] Second hidden layer A_2 = [100, 100] A_2 = [100, 100] A_5 = [100, 100] A_5 = [100, 100] B_2 = [100] B_2 = [100] B_5 = [100] B_5 = [100] Readout layer A_4 = [10, 100] A_4 = [10, 100] B_4 = [10] B_4 = [10]","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#model-c-2-hidden-layer","text":"Unroll 28 time steps Each step input size: 28 x 1 Total per unroll: 28 x 28 Feedforward Neural Network inpt size: 28 x 28 2 Hidden layer Tanh Activation Function","title":"Model C: 2 Hidden Layer"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#steps_2","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model !!! \"2 Hidden + ReLU\" import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class RNNModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( RNNModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your RNN # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . rnn = nn . RNN ( input_dim , hidden_dim , layer_dim , batch_first = True , nonlinearity = 'tanh' ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # One time step # We need to detach the hidden state to prevent exploding/vanishing gradients # This is part of truncated backpropagation through time (BPTT) out , hn = self . rnn ( x , h0 . detach ()) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 hidden_dim = 100 layer_dim = 2 # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER output_dim = 10 model = RNNModel ( input_dim , hidden_dim , layer_dim , output_dim ) # JUST PRINTING MODEL & PARAMETERS print ( model ) print ( len ( list ( model . parameters ()))) for i in range ( len ( list ( model . parameters ()))): print ( list ( model . parameters ())[ i ] . size ()) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Resize images images = images . view ( - 1 , seq_dim , input_dim ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) RNNModel ( ( rnn ): RNN ( 28 , 100 , num_layers = 2 , batch_first = True ) ( fc ): Linear ( in_features = 100 , out_features = 10 , bias = True ) ) 10 torch . Size ([ 100 , 28 ]) torch . Size ([ 100 , 100 ]) torch . Size ([ 100 ]) torch . Size ([ 100 ]) torch . Size ([ 100 , 100 ]) torch . Size ([ 100 , 100 ]) torch . Size ([ 100 ]) torch . Size ([ 100 ]) torch . Size ([ 10 , 100 ]) torch . Size ([ 10 ]) Iteration : 500. Loss : 0.5943437218666077 . Accuracy : 77 Iteration : 1000. Loss : 0.22048641741275787 . Accuracy : 91 Iteration : 1500. Loss : 0.18479223549365997 . Accuracy : 94 Iteration : 2000. Loss : 0.2723771929740906 . Accuracy : 91 Iteration : 2500. Loss : 0.18817797303199768 . Accuracy : 92 Iteration : 3000. Loss : 0.1685929149389267 . Accuracy : 92","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#summary-of-results","text":"Model A Model B Model C ReLU ReLU Tanh 1 Hidden Layer 2 Hidden Layers 2 Hidden Layers 100 Hidden Units 100 Hidden Units 100 Hidden Units 92.48% 95.09% 95.54%","title":"Summary of Results"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#general-deep-learning-notes","text":"2 ways to expand a recurrent neural network More non-linear activation units (neurons) More hidden layers Cons Need a larger dataset Curse of dimensionality Does not necessarily mean higher accuracy","title":"General Deep Learning Notes"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#3-building-a-recurrent-neural-network-with-pytorch-gpu","text":"","title":"3. Building a Recurrent Neural Network with PyTorch (GPU)"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#model-c-2-hidden-layer-tanh","text":"GPU: 2 things must be on GPU - model - tensors","title":"Model C: 2 Hidden Layer (Tanh)"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#steps_3","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 2 Layer RNN + Tanh import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class RNNModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( RNNModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your RNN # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . rnn = nn . RNN ( input_dim , hidden_dim , layer_dim , batch_first = True , nonlinearity = 'tanh' ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros ####################### # USE GPU FOR MODEL # ####################### h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . to ( device ) # One time step # We need to detach the hidden state to prevent exploding/vanishing gradients # This is part of truncated backpropagation through time (BPTT) out , hn = self . rnn ( x , h0 . detach ()) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 hidden_dim = 100 layer_dim = 2 # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER output_dim = 10 model = RNNModel ( input_dim , hidden_dim , layer_dim , output_dim ) ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , seq_dim , input_dim ) . to ( device ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions ####################### # USE GPU FOR MODEL # ####################### if torch . cuda . is_available (): correct += ( predicted . cpu () == labels . cpu ()) . sum () else : correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {} . Loss: {} . Accuracy: {} ' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.5983774662017822 . Accuracy : 81 Iteration : 1000. Loss : 0.2960105836391449 . Accuracy : 86 Iteration : 1500. Loss : 0.19428101181983948 . Accuracy : 93 Iteration : 2000. Loss : 0.11918395012617111 . Accuracy : 95 Iteration : 2500. Loss : 0.11246936023235321 . Accuracy : 95 Iteration : 3000. Loss : 0.15849310159683228 . Accuracy : 95","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#summary","text":"We've learnt to... Success Feedforward Neural Networks Transition to Recurrent Neural Networks RNN Models in PyTorch Model A: 1 Hidden Layer RNN (ReLU) Model B: 2 Hidden Layer RNN (ReLU) Model C: 2 Hidden Layer RNN (Tanh) Models Variation in Code Modifying only step 4 Ways to Expand Model\u2019s Capacity More non-linear activation units ( neurons ) More hidden layers Cons of Expanding Capacity Need more data Does not necessarily mean higher accuracy GPU Code 2 things on GPU model tensors with gradient accumulation abilities Modifying only Step 3, 4 and 7 7 Step Model Building Recap Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Step 7: Train Model","title":"Summary"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/production_pytorch/speed_optimization_basics_numba/","text":"Speed Optimization Basics: Numba \u00b6 When to use Numba \u00b6 Numba works well when the code relies a lot on (1) numpy, (2) loops, and/or (2) cuda. Hence, we would like to maximize the use of numba in our code where possible where there are loops/numpy Numba CPU: nopython \u00b6 For a basic numba application, we can cecorate python function thus allowing it to run without python interpreter Essentially, it will compile the function with specific arguments once into machine code, then uses the cache subsequently With Numba: no python \u00b6 from numba import jit , prange import numpy as np # Numpy array of 10k elements input_ndarray = np . random . rand ( 10000 ) . reshape ( 10000 ) # This is the only extra line of code you need to add # which is a decorator @jit ( nopython = True ) def go_fast ( a ): trace = 0 for i in range ( a . shape [ 0 ]): trace += np . tanh ( a [ i ]) return a + trace % timeit go_fast ( input_ndarray ) 161 \u00b5s \u00b1 2.62 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each) Without numba \u00b6 This is much slower, time measured in the millisecond space rather than microsecond with @jit(nopython=True) or @njit # Without numba: notice how this is really slow def go_normal ( a ): trace = 0 for i in range ( a . shape [ 0 ]): trace += np . tanh ( a [ i ]) return a + trace % timeit go_normal ( input_ndarray ) 10.5 ms \u00b1 163 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) Numba CPU: parallel \u00b6 Here, instead of the normal range() function we would use for loops, we would need to use prange() which allows us to execute the loops in parallel on separate threads As you can see, it's slightly faster than @jit(nopython=True) @jit ( nopython = True , parallel = True ) def go_even_faster ( a ): trace = 0 for i in prange ( a . shape [ 0 ]): trace += np . tanh ( a [ i ]) return a + trace % timeit go_even_faster ( input_ndarray ) 148 \u00b5s \u00b1 71.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) Numba CPU: fastmath \u00b6 What if we relax our condition of strictly adhering to IEEE 754 . We can have faster performance (depends) I would say this is the least additional speed-up unless you really dig into areas where fastmath=True thrives @jit ( nopython = True , parallel = True , fastmath = True ) def go_super_fast ( a ): trace = 0 for i in prange ( a . shape [ 0 ]): trace += np . tanh ( a [ i ]) return a + trace % timeit go_super_fast ( input_ndarray ) 113 \u00b5s \u00b1 39.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) Summary \u00b6 When to use Numba (1) numpy array or torch tensors, (2) loops, and/or (3) cuda Numba CPU: nopython\u00b6 Numba CPU: parallel Numba CPU: fastmath","title":"Speed Optimization Basics Numba"},{"location":"deep_learning/production_pytorch/speed_optimization_basics_numba/#speed-optimization-basics-numba","text":"","title":"Speed Optimization Basics: Numba"},{"location":"deep_learning/production_pytorch/speed_optimization_basics_numba/#when-to-use-numba","text":"Numba works well when the code relies a lot on (1) numpy, (2) loops, and/or (2) cuda. Hence, we would like to maximize the use of numba in our code where possible where there are loops/numpy","title":"When to use Numba"},{"location":"deep_learning/production_pytorch/speed_optimization_basics_numba/#numba-cpu-nopython","text":"For a basic numba application, we can cecorate python function thus allowing it to run without python interpreter Essentially, it will compile the function with specific arguments once into machine code, then uses the cache subsequently","title":"Numba CPU: nopython"},{"location":"deep_learning/production_pytorch/speed_optimization_basics_numba/#with-numba-no-python","text":"from numba import jit , prange import numpy as np # Numpy array of 10k elements input_ndarray = np . random . rand ( 10000 ) . reshape ( 10000 ) # This is the only extra line of code you need to add # which is a decorator @jit ( nopython = True ) def go_fast ( a ): trace = 0 for i in range ( a . shape [ 0 ]): trace += np . tanh ( a [ i ]) return a + trace % timeit go_fast ( input_ndarray ) 161 \u00b5s \u00b1 2.62 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)","title":"With Numba: no python"},{"location":"deep_learning/production_pytorch/speed_optimization_basics_numba/#without-numba","text":"This is much slower, time measured in the millisecond space rather than microsecond with @jit(nopython=True) or @njit # Without numba: notice how this is really slow def go_normal ( a ): trace = 0 for i in range ( a . shape [ 0 ]): trace += np . tanh ( a [ i ]) return a + trace % timeit go_normal ( input_ndarray ) 10.5 ms \u00b1 163 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)","title":"Without numba"},{"location":"deep_learning/production_pytorch/speed_optimization_basics_numba/#numba-cpu-parallel","text":"Here, instead of the normal range() function we would use for loops, we would need to use prange() which allows us to execute the loops in parallel on separate threads As you can see, it's slightly faster than @jit(nopython=True) @jit ( nopython = True , parallel = True ) def go_even_faster ( a ): trace = 0 for i in prange ( a . shape [ 0 ]): trace += np . tanh ( a [ i ]) return a + trace % timeit go_even_faster ( input_ndarray ) 148 \u00b5s \u00b1 71.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)","title":"Numba CPU: parallel"},{"location":"deep_learning/production_pytorch/speed_optimization_basics_numba/#numba-cpu-fastmath","text":"What if we relax our condition of strictly adhering to IEEE 754 . We can have faster performance (depends) I would say this is the least additional speed-up unless you really dig into areas where fastmath=True thrives @jit ( nopython = True , parallel = True , fastmath = True ) def go_super_fast ( a ): trace = 0 for i in prange ( a . shape [ 0 ]): trace += np . tanh ( a [ i ]) return a + trace % timeit go_super_fast ( input_ndarray ) 113 \u00b5s \u00b1 39.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)","title":"Numba CPU: fastmath"},{"location":"deep_learning/production_pytorch/speed_optimization_basics_numba/#summary","text":"When to use Numba (1) numpy array or torch tensors, (2) loops, and/or (3) cuda Numba CPU: nopython\u00b6 Numba CPU: parallel Numba CPU: fastmath","title":"Summary"},{"location":"machine_learning/intro/","text":"Machine Learning \u00b6 We'll be covering both CPU and GPU implementations of machine learning algorithms and data wrangling libraries. Packages and Languages you will Learn to Use Python RAPIDS cuDF RAPIDS cuML Scikit-learn Pandas NumPy CuPy Work in progress This open-source portion is still a work in progress. Stay tuned while we gradually upload our tutorials and notes. Feel free to contact Ritchie Ng if you would like to contribute via our Facebook page.","title":"Introduction"},{"location":"machine_learning/intro/#machine-learning","text":"We'll be covering both CPU and GPU implementations of machine learning algorithms and data wrangling libraries. Packages and Languages you will Learn to Use Python RAPIDS cuDF RAPIDS cuML Scikit-learn Pandas NumPy CuPy Work in progress This open-source portion is still a work in progress. Stay tuned while we gradually upload our tutorials and notes. Feel free to contact Ritchie Ng if you would like to contribute via our Facebook page.","title":"Machine Learning"},{"location":"machine_learning/gpu/gpu_fractional_differencing/","text":"GFD: GPU Fractional Differencing \u00b6 Summary \u00b6 Typically we attempt to achieve some form of stationarity via a transformation on our time series through common methods including integer differencing. However, integer differencing unnecessarily removes too much memory to achieve stationarity. An alternative, fractional differencing, allows us to achieve stationarity while maintaining the maximum amount of memory compared to integer differencing. While existing CPU-based implementations are inefficient for running fractional differencing on many large-scale time series, our GPU-based implementation enables rapid fractional differencing. This tutorial walks you through our GPU implementation of fractional differencing (GFD). Also, this tutorial is a special collaboration between industry and academia led by me. Credits \u00b6 Before we start, I would like to thank and give credits for this particular post to: 1. NVIDIA (Ettikan, Chris and Nick), Amazon AWS, ensemblecap.ai, and NExT++ (NUS School of Computing) 2. Marcos Lopez de Prado for his recent push on the use of fractional differencing of which this guide is based on. 3. Hosking for his paper in 1981 on fractional differencing. Links \u00b6 Code Repository: https://github.com/ritchieng/fractional_differencing_gpu Presentation: https://www.researchgate.net/publication/335159299_GFD_GPU_Fractional_Differencing_for_Rapid_Large-scale_Stationarizing_of_Time_Series_Data_while_Minimizing_Memory_Loss Installation of Libraries (Optional) \u00b6 These steps are for easily running RAPIDS in Google Colab. You can skip this section if you've RAPIDS installed. import pynvml pynvml . nvmlInit () handle = pynvml . nvmlDeviceGetHandleByIndex ( 0 ) device_name = pynvml . nvmlDeviceGetName ( handle ) print ( f 'Currently running on { device_name } ' ) if device_name != b 'Tesla T4' : raise Exception ( \"\"\" Unfortunately this instance does not have a T4 GPU. Please make sure you've configured Colab to request a GPU instance type. Sometimes Colab allocates a Tesla K80 instead of a T4. Resetting the instance. If you get a K80 GPU, try Runtime -> Reset all runtimes -> Keep trying! \"\"\" ) else : print ( 'Woo! You got the right kind of GPU, a Tesla T4!' ) Currently running on b'Tesla T4' Woo! You got the right kind of GPU, a Tesla T4! # intall miniconda ! wget - c https : // repo . continuum . io / miniconda / Miniconda3 - 4.5 . 4 - Linux - x86_64 . sh ! chmod + x Miniconda3 - 4.5 . 4 - Linux - x86_64 . sh ! bash ./ Miniconda3 - 4.5 . 4 - Linux - x86_64 . sh - b - f - p / usr / local # install RAPIDS packages ! conda install - q - y -- prefix / usr / local - c conda - forge \\ - c rapidsai - nightly / label / cuda10 . 0 - c nvidia / label / cuda10 . 0 \\ cudf cuml # set environment vars import sys , os , shutil sys . path . append ( '/usr/local/lib/python3.6/site-packages/' ) os . environ [ 'NUMBAPRO_NVVM' ] = '/usr/local/cuda/nvvm/lib64/libnvvm.so' os . environ [ 'NUMBAPRO_LIBDEVICE' ] = '/usr/local/cuda/nvvm/libdevice/' # copy .so files to current working dir for fn in [ 'libcudf.so' , 'librmm.so' ]: shutil . copy ( '/usr/local/lib/' + fn , os . getcwd ()) -- 2019 - 08 - 19 05 : 36 : 25 -- https : // repo . continuum . io / miniconda / Miniconda3 - 4.5 . 4 - Linux - x86_64 . sh Resolving repo . continuum . io ( repo . continuum . io ) ... 104.18 . 200.79 , 104.18 . 201.79 , 2606 : 4700 :: 6812 : c84f , ... Connecting to repo . continuum . io ( repo . continuum . io ) | 104.18 . 200.79 | : 443. .. connected . HTTP request sent , awaiting response ... 416 Requested Range Not Satisfiable The file is already fully retrieved ; nothing to do . PREFIX =/ usr / local installing : python - 3.6 . 5 - hc3d631a_2 ... Python 3.6 . 5 :: Anaconda , Inc . installing : ca - certificates - 2018.03 . 07 - 0 ... installing : conda - env - 2.6 . 0 - h36134e3_1 ... installing : libgcc - ng - 7.2 . 0 - hdf63c60_3 ... installing : libstdcxx - ng - 7.2 . 0 - hdf63c60_3 ... installing : libffi - 3.2 . 1 - hd88cf55_4 ... installing : ncurses - 6.1 - hf484d3e_0 ... installing : openssl - 1.0 . 2 o - h20670df_0 ... installing : tk - 8.6 . 7 - hc745277_3 ... installing : xz - 5.2 . 4 - h14c3975_4 ... installing : yaml - 0.1 . 7 - had09818_2 ... installing : zlib - 1.2 . 11 - ha838bed_2 ... installing : libedit - 3.1 . 20170329 - h6b74fdf_2 ... installing : readline - 7.0 - ha6073c6_4 ... installing : sqlite - 3.23 . 1 - he433501_0 ... installing : asn1crypto - 0.24 . 0 - py36_0 ... installing : certifi - 2018.4 . 16 - py36_0 ... installing : chardet - 3.0 . 4 - py36h0f667ec_1 ... installing : idna - 2.6 - py36h82fb2a8_1 ... installing : pycosat - 0.6 . 3 - py36h0a5515d_0 ... installing : pycparser - 2.18 - py36hf9f622e_1 ... installing : pysocks - 1.6 . 8 - py36_0 ... installing : ruamel_yaml - 0.15 . 37 - py36h14c3975_2 ... installing : six - 1.11 . 0 - py36h372c433_1 ... installing : cffi - 1.11 . 5 - py36h9745a5d_0 ... installing : setuptools - 39.2 . 0 - py36_0 ... installing : cryptography - 2.2 . 2 - py36h14c3975_0 ... installing : wheel - 0.31 . 1 - py36_0 ... installing : pip - 10.0 . 1 - py36_0 ... installing : pyopenssl - 18.0 . 0 - py36_0 ... installing : urllib3 - 1.22 - py36hbe7ace6_0 ... installing : requests - 2.18 . 4 - py36he2e5f8d_1 ... installing : conda - 4.5 . 4 - py36_0 ... unlinking : ca - certificates - 2019.6 . 16 - hecc5488_0 unlinking : certifi - 2019.6 . 16 - py36_1 unlinking : conda - 4.7 . 11 - py36_0 unlinking : cryptography - 2.7 - py36h72c5cf5_0 unlinking : libgcc - ng - 9.1 . 0 - hdf63c60_0 unlinking : libstdcxx - ng - 9.1 . 0 - hdf63c60_0 unlinking : openssl - 1.1 . 1 c - h516909a_0 unlinking : python - 3.6 . 7 - h381d211_1004 unlinking : sqlite - 3.28 . 0 - h8b20d00_0 unlinking : tk - 8.6 . 9 - hed695b0_1002 installation finished . WARNING : You currently have a PYTHONPATH environment variable set . This may cause unexpected behavior when running the Python interpreter in Miniconda3 . For best results , please verify that your PYTHONPATH only points to directories of packages that are compatible with the Python interpreter in Miniconda3 : / usr / local Solving environment : ... working ... done ## Package Plan ## environment location : / usr / local added / updated specs : - cudf - cuml The following packages will be UPDATED : ca - certificates : 2018.03 . 07 - 0 --> 2019.6 . 16 - hecc5488_0 conda - forge certifi : 2018.4 . 16 - py36_0 --> 2019.6 . 16 - py36_1 conda - forge conda : 4.5 . 4 - py36_0 --> 4.7 . 11 - py36_0 conda - forge cryptography : 2.2 . 2 - py36h14c3975_0 --> 2.7 - py36h72c5cf5_0 conda - forge libgcc - ng : 7.2 . 0 - hdf63c60_3 --> 9.1 . 0 - hdf63c60_0 libstdcxx - ng : 7.2 . 0 - hdf63c60_3 --> 9.1 . 0 - hdf63c60_0 openssl : 1.0 . 2 o - h20670df_0 --> 1.1 . 1 c - h516909a_0 conda - forge python : 3.6 . 5 - hc3d631a_2 --> 3.6 . 7 - h381d211_1004 conda - forge sqlite : 3.23 . 1 - he433501_0 --> 3.28 . 0 - h8b20d00_0 conda - forge tk : 8.6 . 7 - hc745277_3 --> 8.6 . 9 - hed695b0_1002 conda - forge Preparing transaction : ... working ... done Verifying transaction : ... working ... done Executing transaction : ... working ... done You would need to do a Runtime > Restart and Run All on your Google Colab notebook at this step. Imports \u00b6 # Critical imports for GPU cuDF import nvstrings , nvcategory , cudf # Other imports import numpy as np import pandas as pd import time import pandas_datareader.data as web from datetime import datetime from matplotlib import pyplot as plt from numba import cuda Plotting Style \u00b6 # Display settings: just to make it slightly prettier # There'll be a separate post on beautiful plots % matplotlib inline figsize = ( 25 , 6 ) plt . style . use ( 'fivethirtyeight' ) Necessity of Fractional Differencing \u00b6 Before we get into why fractional differencing is critical, let us inspect a simple time series pulled from the Fed's database: S&P 500. Pulling, Processing & Plotting SPX Time Series \u00b6 # Read SPX data 2010-2019 from FED database: https://fred.stlouisfed.org/categories/94 asset_name = 'SPX' start = datetime ( 2010 , 1 , 1 ) end = datetime ( 2019 , 8 , 1 ) df_raw = web . DataReader ( 'SP500' , 'fred' , start , end ) # Basic clean up data: dropna (there are other more robust treatments) df_raw = df_raw . dropna () df_raw . columns = [ asset_name ] df_raw . plot ( figsize = figsize ) plt . title ( 'S&P 500 Absolute Levels' ); Integer Differencing (1 Day Returns) SPX Time Series \u00b6 Traditionally we typically might difference our time series by one day (daily returns) or more to reach some form of stationarity via tests like ADF, KPSS, PP and more. However, these forms of integer differencing causes more information than needed to be lost. # One day returns calculation through differencing by 1 differencing_factor = 1 df_daily_returns = ( df_raw - df_raw . shift ( differencing_factor )) / df_raw . shift ( differencing_factor ) * 100 df_daily_returns . plot ( figsize = figsize ) plt . title ( f ' { asset_name } Daily Returns via Differencing 1 Day' ); Integer Differencing Causes Unnecessary Memory Loss \u00b6 Because of this reason, we propose fractional differencing to achieve stationarity while minimizing memory loss. Basic Fractional Differencing \u00b6 Fractional differencing allows us to achieve stationarity while maintaining the maximum amount of memory compared to integer differencing. This was originally introduced in 1981 in his paper \u201cFractional Differencing\u201d by J. R. M. Hosking1 and subsequent work by others concentrated on fast and efficient implementations for fractional differentiation for continuous stochastic processes. Recently, fractional differencing was introduced for financial time series through the fixed window fractional differencing instead of the expanding window method by Marcos Lopez de Prado2. Fractional Differencing Weight Function and Plot \u00b6 Weights formula: w_k = -w_{k-1} \\frac{d - k + 1}{k} w_k = -w_{k-1} \\frac{d - k + 1}{k} Weight converges to zero: w_k \\rightarrow 0 w_k \\rightarrow 0 def get_weights ( d , num_k ): r \"\"\"Calculate weights ($w$) for each lag ($k$) through $w_k = -w_{k-1} \\frac{d - k + 1}{k}$. Args: d (int): differencing value. num_k (int): number of lags (typically length of timeseries) to calculate w. \"\"\" w_k = np . array ([ 1 ]) for k in range ( 1 , num_k ): w_k = np . append ( w_k , - w_k [ - 1 ] * (( d - k + 1 )) / k ) w_k = w_k . reshape ( - 1 , 1 ) return w_k def plot_weights ( range_d , num_k , num_d_interval ): r \"\"\"Plot weights ($w$) for each lag ($k$) for varying differencing value ($d$). Args: range_d (list): range of differencing values to plot. num_k (int): number of lags (typically length of timeseries) to plot. num_d_interval (int): number of d interval. \"\"\" # Get differencing values interval = np . linspace ( range_d [ 0 ], range_d [ 1 ], num_d_interval ) # Dataframe of lags (rows) x number of differencing intervals (columns) df_wk = pd . DataFrame ( np . zeros (( num_k , num_d_interval ))) # Get weights array per differencing value for i , d in enumerate ( interval ): df_wk [ i ] = get_weights ( d , num_k ) # Rename columns for legend df_wk . columns = [ round ( x , 1 ) for x in interval ] # Plot df_wk . plot ( figsize = figsize ) plt . title ( 'Lag weights ($w_k$) for various differencing values (d)' ) plt . legend ( title = 'Differencing value (d)' ) plt . ylabel ( 'Weights (w)' ) plt . xlabel ( 'Lag (k)' ) plt . show () # Return weights return df_wk df_wk = plot_weights ( range_d = [ 0 , 1 ], num_k = 7 , num_d_interval = 6 ) Fractional Differencing Weight Function Table \u00b6 # Dataframe for lag weights (w) for various differencing values (d) df_wk .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0.0 0.2 0.4 0.6 0.8 1.0 0 1.0 1.000000 1.000000 1.000000 1.000000 1.0 1 -0.0 -0.200000 -0.400000 -0.600000 -0.800000 -1.0 2 -0.0 -0.080000 -0.120000 -0.120000 -0.080000 0.0 3 -0.0 -0.048000 -0.064000 -0.056000 -0.032000 0.0 4 -0.0 -0.033600 -0.041600 -0.033600 -0.017600 0.0 5 -0.0 -0.025536 -0.029952 -0.022848 -0.011264 0.0 6 -0.0 -0.020429 -0.022963 -0.016755 -0.007885 0.0 Fractional Differencing Value Calculation Toy Example \u00b6 Assume spot have values 100, 99, 98, 97, 96, 95, 94 from k=0 to k=6 For d = 0 d = 0 (no differencing) \u00b6 differenced_value = 100 * 1 + 99 * 0 + 98 * 0 + 97 * 0 + 96 * 0 + 95 * 0 + 94 * 0 print ( f 'Differenced value: { differenced_value } ' ) df_wk . iloc [:, 0 ] Differenced value : 100 0 1 . 0 1 - 0 . 0 2 - 0 . 0 3 - 0 . 0 4 - 0 . 0 5 - 0 . 0 6 - 0 . 0 Name : 0 . 0 , dtype : float64 For d = 0.4 d = 0.4 (fractional differencing) \u00b6 differenced_value = 100 * 1 + 99 * - 0.400000 + 98 * - 0.120000 + 97 * - 0.064000 + 96 * - 0.041600 + 95 * - 0.041600 + 94 * - 0.041600 print ( f 'Differenced value: { differenced_value } ' ) df_wk . iloc [:, 1 ] Differenced value : 30 . 576000000000004 0 1 . 000000 1 - 0 . 200000 2 - 0 . 080000 3 - 0 . 048000 4 - 0 . 033600 5 - 0 . 025536 6 - 0 . 020429 Name : 0 . 2 , dtype : float64 For d = 1 d = 1 (integer differencing) \u00b6 differenced_value = 100 * 1 + 99 * - 1 + 98 * 0 + 97 * 0 + 96 * 0 + 95 * 0 + 94 * 0 print ( f 'Differenced value: { differenced_value } ' ) df_wk . iloc [:, - 1 ] Differenced value : 1 0 1 . 0 1 - 1 . 0 2 0 . 0 3 0 . 0 4 0 . 0 5 0 . 0 6 0 . 0 Name : 1 . 0 , dtype : float64 Cumulative Sum of Weights Behavior \u00b6 Higher differencing value would accelerate cumulative sum weight decay This causes: Less information taken from further lags More information loss from further lags df_wk . cumsum () . plot ( figsize = figsize ) plt . title ( 'Cumulative Sum of Lag weights ($\\sum_0^k w_k$) for various differencing values (d)' ) plt . legend ( title = 'Differencing value ($d$)' ) plt . ylabel ( 'Cumulative Sum of Weights ($\\sum_0^k w_k$)' ) plt . xlabel ( 'Lag (k)' ); Fixed Window Fractional Differencing (CPU) \u00b6 Floored Weights Function \u00b6 For computational efficiency, we want to stop the calculation of weights when the weights are too small (below a certain small predetermined small value). Image we had a time series with 1,000,000 data points, we do not want to compute the weights up till 1m lag for our latest data point! We can simply calculate the weights till they're too small which is what this function does. def get_weights_floored ( d , num_k , floor = 1e-3 ): r \"\"\"Calculate weights ($w$) for each lag ($k$) through $w_k = -w_{k-1} \\frac{d - k + 1}{k}$ provided weight above a minimum value (floor) for the weights to prevent computation of weights for the entire time series. Args: d (int): differencing value. num_k (int): number of lags (typically length of timeseries) to calculate w. floor (float): minimum value for the weights for computational efficiency. \"\"\" w_k = np . array ([ 1 ]) k = 1 while k < num_k : w_k_latest = - w_k [ - 1 ] * (( d - k + 1 )) / k if abs ( w_k_latest ) <= floor : break w_k = np . append ( w_k , w_k_latest ) k += 1 w_k = w_k . reshape ( - 1 , 1 ) return w_k # Show large time series being stopped # Notice how max lag is at 9? d = 0.5 num_k = 1000 weights = get_weights_floored ( d = d , num_k = num_k ) pd . DataFrame ( weights ) . plot ( legend = False , figsize = figsize ) # Plot plt . title ( f 'Lag weights ($w_k$) for differencing value $d= { d } $' ) plt . ylabel ( 'Weights (w)' ) plt . xlabel ( 'Lag (k)' ); Fixed Window Fractional Differencing Function (CPU) \u00b6 Here, we use the new floored weights function compared to our normal weights function Calculate our differenced values through the dot product of our transposed weights matrix and our original time series within that window Keep repeating by shifting the window by one time step until the end of the time series def frac_diff ( df , d , floor = 1e-3 ): r \"\"\"Fractionally difference time series via CPU. Args: df (pd.DataFrame): dataframe of raw time series values. d (float): differencing value from 0 to 1 where > 1 has no FD. floor (float): minimum value of weights, ignoring anything smaller. \"\"\" # Get weights window weights = get_weights_floored ( d = d , num_k = len ( df ), floor = floor ) weights_window_size = len ( weights ) # Reverse weights weights = weights [:: - 1 ] # Blank fractionally differenced series to be filled df_fd = [] # Slide window of time series, to calculated fractionally differenced values # per window for idx in range ( weights_window_size , df . shape [ 0 ]): # Dot product of weights and original values # to get fractionally differenced values date_idx = df . index [ idx ] df_fd . append ( np . dot ( weights . T , df . iloc [ idx - weights_window_size : idx ]) . item ()) # Return FD values and weights df_fd = pd . DataFrame ( df_fd ) return df_fd , weights # Start timer start = time . time () df_raw_fd , weights = frac_diff ( df_raw , d = 0.5 , floor = 5e-5 ) # End timer end = time . time () print ( f 'Time { end - start } s' ) # Plot df_raw_fd . plot ( figsize = figsize ); Time 0.5417799949645996 s import multiprocessing multiprocessing . cpu_count () 4 Existing CPU-based implementations are inefficient for running fractional differencing on many large-scale time-series. GPU-based implementations provide an avenue to adapt to this century\u2019s big data requirements. Fixed Window Fractional Differencing Function (GPU) \u00b6 This gives a slight 1.5x to 2x speed-up on an NVIDIA T4 on Google Colab for this tiny dataset For larger time series in line with real-world situations, it can easily be up to 100x to 1000x dataset (shown below). def moving_dot_product_kernel ( in_data , out , window_size , weights ): # Set the first window_size-1 rows in each chunk to np.nan due # insufficient history for i in range ( cuda . threadIdx . x , window_size - 1 , cuda . blockDim . x ): out [ i ] = np . nan # Compute dot product of preceding window_size rows for i in range ( cuda . threadIdx . x + window_size - 1 , in_data . size , cuda . blockDim . x ): rolling_dot_product = 0.0 k = 0 for j in range ( i - window_size + 1 , i + 1 ): rolling_dot_product += in_data [ j ] * weights [ k ][ 0 ] k += 1 out [ i ] = rolling_dot_product def frac_diff_gpu ( df , d , floor = 1e-3 ): r \"\"\"Fractionally difference time series via GPU. Args: df (pd.DataFrame): dataframe of raw time series values. d (float): differencing value from 0 to 1 where > 1 has no FD. floor (float): minimum value of weights, ignoring anything smaller. \"\"\" # Bring dataframe to GPU, reset index for GPU dot product kernel gdf_raw = cudf . from_pandas ( df ) . reset_index ( drop = True ) gdf_raw . columns = [ 'in_data' ] # Get weights window weights = get_weights_floored ( d = d , num_k = len ( gdf_raw ), floor = floor ) weights_window_size = len ( weights ) # Reverse weights and as contiguous weights = np . ascontiguousarray ( weights [:: - 1 ]) # Bring weights to GPU gdf_weights = cudf . DataFrame () gdf_weights [ gdf_raw . columns [ 0 ]] = weights . reshape ( - 1 ) # Length of data data_length = len ( gdf_raw ) # T4: max of 518 threads per block. # V100: max 1024 threads per block threads_per_block = 518 # Chunk size split # This has to be improved, but as a v0.1, it's sufficient to show speed-up # Up to easily 100 million data points trunk_size = data_length # Get fractionally differenced time series through GPU function gdf_raw_fd = gdf_raw . apply_chunks ( moving_dot_product_kernel , incols = [ 'in_data' ], outcols = dict ( out = np . float64 ), kwargs = dict ( window_size = weights_window_size , weights = weights ), chunks = list ( range ( 0 , data_length , trunk_size )) + [ data_length ], tpb = threads_per_block ) # Bring to CPU for normal manipulation df_raw_fd = gdf_raw_fd . to_pandas () . dropna () . iloc [: - 1 , 1 ] return df_raw_fd , weights # Start timer start = time . time () df_raw_fd_from_gpu , weights = frac_diff_gpu ( df_raw , d = 0.5 , floor = 5e-5 ) # End timer end = time . time () print ( f 'Time { end - start } s' ) # Plot df_raw_fd_from_gpu . plot ( figsize = figsize ); Time 0.27262187004089355 s Check Values \u00b6 # Compare tail values print ( 'Tail values check' ) print ( df_raw_fd_from_gpu . tail () . values ) print ( df_raw_fd . tail () . values . reshape ( - 1 ,)) Tail values check [107.00340988 133.85520208 117.92878691 109.44132697 79.32562638] [107.00340988 133.85520208 117.92878691 109.44132697 79.32562638] # Compare tail values print ( 'Head values check' ) print ( df_raw_fd_from_gpu . head () . values ) print ( df_raw_fd . head () . values . reshape ( - 1 ,)) Head values check [56.74989213 52.56766288 47.32421832 45.89772154 37.74401501] [56.74989213 52.56766288 47.32421832 45.89772154 37.74401501] num_rows_true = ( df_raw_fd . values . astype ( np . float32 ) == df_raw_fd_from_gpu . values . astype ( np . float32 )) . astype ( int ) . sum () total_rows = df_raw_fd . shape [ 0 ] print ( f 'Number of rows equal: { num_rows_true } ' ) print ( f 'Total number of rows: { total_rows } ' ) Number of rows equal: 2093 Total number of rows: 2093 Stationarity \u00b6 \"A stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time\", Robert Nau, Duke University Essentially what we are trying to do here with fractional differencing is to attempt to have a stationary time series without losing too much memory. There are many ways to check if a time series is stationary, but we will be using 1 test here (they're more, but it's not the purpose of this tutorial) Augmented Dickey\u2013Fuller (ADF) test: check for unit root ADF Test \u00b6 # Import adf/kpss from statsmodels.tsa.stattools import adfuller import warnings warnings . filterwarnings ( \"ignore\" ) # Ignore deprecations for cleaner output ADF: raw data (d=0) \u00b6 # ADF on raw data print ( 'Stationarity Test for SPX Absolute Levels (d=0)' ) print ( '-' * 50 ) result = adfuller ( df_raw [ 'SPX' ], regression = 'c' ) print ( f 't-stat \\n\\t { result [ 0 ] : .2f } ' ) print ( f 'p-value \\n\\t { result [ 1 ] : .2f } ' ) print ( f 'Critical Values' ) for key , value in result [ 4 ] . items (): print ( f ' \\t { key } : { value : .2f } ' ) # test statistic > test statistic (1%) AND p-value > 0.01 # Fail to reject the null hypothesis that there's unit root at the 1% significance level # ==> Fail to reject H0 ==> Unit root ==> Non-stationary Stationarity Test for SPX Absolute Levels (d=0) -------------------------------------------------- t-stat -0.11 p-value 0.95 Critical Values 1%: -3.43 5%: -2.86 10%: -2.57 ADF: daily return data (d=1) \u00b6 # ADF on 1 day returns (d=1) print ( 'Stationarity Test for SPX Daily Returns (d=1)' ) print ( '-' * 50 ) result = adfuller ( df_raw [ 'SPX' ] . pct_change ( 1 ) . dropna (), regression = 'c' ) print ( f 't-stat \\n\\t { result [ 0 ] : .2f } ' ) print ( f 'p-value \\n\\t { result [ 1 ] : .2f } ' ) print ( f 'Critical Values' ) for key , value in result [ 4 ] . items (): print ( f ' \\t { key } : { value : .2f } ' ) # test statistic < test statistic (1%) AND p-value < 0.01 # Reject the null hypothesis that there's unit root at the 1% significance level # ==> Reject H0 ==> No unit root ==> Stationary Stationarity Test for SPX Daily Returns (d=1) -------------------------------------------------- t-stat -11.12 p-value 0.00 Critical Values 1%: -3.43 5%: -2.86 10%: -2.57 ADF: fractionally differenced data (d=0.5) \u00b6 # ADF on fractionally differenced values print ( 'Stationarity Test (ADF) for SPX Fractionally Differenced (d=0.5)' ) print ( '-' * 50 ) df_raw_fd_from_gpu , weights = frac_diff_gpu ( df_raw , d = 0.5 , floor = 5e-5 ) result = adfuller ( df_raw_fd_from_gpu , regression = 'c' ) print ( f 't-stat \\n\\t { result [ 0 ] : .2f } ' ) print ( f 'p-value \\n\\t { result [ 1 ] : .2f } ' ) print ( f 'Critical Values' ) for key , value in result [ 4 ] . items (): print ( f ' \\t { key } : { value : .2f } ' ) # test statistic < test statistic (1%) AND p-value < 0.01 # Reject the null hypothesis that there's unit root at the 1% significance level # ==> Reject H0 ==> No unit root ==> Stationary Stationarity Test (ADF) for SPX Fractionally Differenced (d=0.5) -------------------------------------------------- t-stat -3.86 p-value 0.00 Critical Values 1%: -3.43 5%: -2.86 10%: -2.57 Large-scale Rapid Fractional Differencing \u00b6 Here we test on a 100k, 1m, 10m and 100m datapoint dataframe. # Create 100m data points large_time_series_length = int ( 1e6 ) print ( large_time_series_length ) 1000000 # Start timer start = time . time () df_raw = pd . DataFrame ( np . arange ( large_time_series_length ) * np . random . rand ( large_time_series_length )) df_raw_fd_from_gpu , weights = frac_diff_gpu ( df_raw , d = 0.5 , floor = 5e-5 ) # End timer end = time . time () print ( f 'Time { end - start } s' ) # Check print ( 'FD Shape' , df_raw_fd_from_gpu . shape [ 0 ]) print ( 'Correct Shape' , df_raw . shape [ 0 ] - weights . shape [ 0 ]) print ( 'Original Shape' , df_raw . shape [ 0 ]) Time 0.4671785831451416 s FD Shape 999682 Correct Shape 999682 Original Shape 1000000 # Start timer start = time . time () # Create 100m data points df_raw = pd . DataFrame ( np . arange ( large_time_series_length ) * np . random . rand ( large_time_series_length )) df_raw_fd , weights = frac_diff ( df_raw , d = 0.5 , floor = 5e-5 ) # End timer end = time . time () print ( f 'Time { end - start } s' ) # Check print ( 'FD Shape' , df_raw_fd_from_gpu . shape [ 0 ]) print ( 'Correct Shape' , df_raw . shape [ 0 ] - weights . shape [ 0 ]) print ( 'Original Shape' , df_raw . shape [ 0 ]) Time 128.29733324050903 s FD Shape 999682 Correct Shape 999682 Original Shape 1000000","title":"GPU/CPU Fractional Differencing"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#gfd-gpu-fractional-differencing","text":"","title":"GFD: GPU Fractional Differencing"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#summary","text":"Typically we attempt to achieve some form of stationarity via a transformation on our time series through common methods including integer differencing. However, integer differencing unnecessarily removes too much memory to achieve stationarity. An alternative, fractional differencing, allows us to achieve stationarity while maintaining the maximum amount of memory compared to integer differencing. While existing CPU-based implementations are inefficient for running fractional differencing on many large-scale time series, our GPU-based implementation enables rapid fractional differencing. This tutorial walks you through our GPU implementation of fractional differencing (GFD). Also, this tutorial is a special collaboration between industry and academia led by me.","title":"Summary"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#credits","text":"Before we start, I would like to thank and give credits for this particular post to: 1. NVIDIA (Ettikan, Chris and Nick), Amazon AWS, ensemblecap.ai, and NExT++ (NUS School of Computing) 2. Marcos Lopez de Prado for his recent push on the use of fractional differencing of which this guide is based on. 3. Hosking for his paper in 1981 on fractional differencing.","title":"Credits"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#links","text":"Code Repository: https://github.com/ritchieng/fractional_differencing_gpu Presentation: https://www.researchgate.net/publication/335159299_GFD_GPU_Fractional_Differencing_for_Rapid_Large-scale_Stationarizing_of_Time_Series_Data_while_Minimizing_Memory_Loss","title":"Links"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#installation-of-libraries-optional","text":"These steps are for easily running RAPIDS in Google Colab. You can skip this section if you've RAPIDS installed. import pynvml pynvml . nvmlInit () handle = pynvml . nvmlDeviceGetHandleByIndex ( 0 ) device_name = pynvml . nvmlDeviceGetName ( handle ) print ( f 'Currently running on { device_name } ' ) if device_name != b 'Tesla T4' : raise Exception ( \"\"\" Unfortunately this instance does not have a T4 GPU. Please make sure you've configured Colab to request a GPU instance type. Sometimes Colab allocates a Tesla K80 instead of a T4. Resetting the instance. If you get a K80 GPU, try Runtime -> Reset all runtimes -> Keep trying! \"\"\" ) else : print ( 'Woo! You got the right kind of GPU, a Tesla T4!' ) Currently running on b'Tesla T4' Woo! You got the right kind of GPU, a Tesla T4! # intall miniconda ! wget - c https : // repo . continuum . io / miniconda / Miniconda3 - 4.5 . 4 - Linux - x86_64 . sh ! chmod + x Miniconda3 - 4.5 . 4 - Linux - x86_64 . sh ! bash ./ Miniconda3 - 4.5 . 4 - Linux - x86_64 . sh - b - f - p / usr / local # install RAPIDS packages ! conda install - q - y -- prefix / usr / local - c conda - forge \\ - c rapidsai - nightly / label / cuda10 . 0 - c nvidia / label / cuda10 . 0 \\ cudf cuml # set environment vars import sys , os , shutil sys . path . append ( '/usr/local/lib/python3.6/site-packages/' ) os . environ [ 'NUMBAPRO_NVVM' ] = '/usr/local/cuda/nvvm/lib64/libnvvm.so' os . environ [ 'NUMBAPRO_LIBDEVICE' ] = '/usr/local/cuda/nvvm/libdevice/' # copy .so files to current working dir for fn in [ 'libcudf.so' , 'librmm.so' ]: shutil . copy ( '/usr/local/lib/' + fn , os . getcwd ()) -- 2019 - 08 - 19 05 : 36 : 25 -- https : // repo . continuum . io / miniconda / Miniconda3 - 4.5 . 4 - Linux - x86_64 . sh Resolving repo . continuum . io ( repo . continuum . io ) ... 104.18 . 200.79 , 104.18 . 201.79 , 2606 : 4700 :: 6812 : c84f , ... Connecting to repo . continuum . io ( repo . continuum . io ) | 104.18 . 200.79 | : 443. .. connected . HTTP request sent , awaiting response ... 416 Requested Range Not Satisfiable The file is already fully retrieved ; nothing to do . PREFIX =/ usr / local installing : python - 3.6 . 5 - hc3d631a_2 ... Python 3.6 . 5 :: Anaconda , Inc . installing : ca - certificates - 2018.03 . 07 - 0 ... installing : conda - env - 2.6 . 0 - h36134e3_1 ... installing : libgcc - ng - 7.2 . 0 - hdf63c60_3 ... installing : libstdcxx - ng - 7.2 . 0 - hdf63c60_3 ... installing : libffi - 3.2 . 1 - hd88cf55_4 ... installing : ncurses - 6.1 - hf484d3e_0 ... installing : openssl - 1.0 . 2 o - h20670df_0 ... installing : tk - 8.6 . 7 - hc745277_3 ... installing : xz - 5.2 . 4 - h14c3975_4 ... installing : yaml - 0.1 . 7 - had09818_2 ... installing : zlib - 1.2 . 11 - ha838bed_2 ... installing : libedit - 3.1 . 20170329 - h6b74fdf_2 ... installing : readline - 7.0 - ha6073c6_4 ... installing : sqlite - 3.23 . 1 - he433501_0 ... installing : asn1crypto - 0.24 . 0 - py36_0 ... installing : certifi - 2018.4 . 16 - py36_0 ... installing : chardet - 3.0 . 4 - py36h0f667ec_1 ... installing : idna - 2.6 - py36h82fb2a8_1 ... installing : pycosat - 0.6 . 3 - py36h0a5515d_0 ... installing : pycparser - 2.18 - py36hf9f622e_1 ... installing : pysocks - 1.6 . 8 - py36_0 ... installing : ruamel_yaml - 0.15 . 37 - py36h14c3975_2 ... installing : six - 1.11 . 0 - py36h372c433_1 ... installing : cffi - 1.11 . 5 - py36h9745a5d_0 ... installing : setuptools - 39.2 . 0 - py36_0 ... installing : cryptography - 2.2 . 2 - py36h14c3975_0 ... installing : wheel - 0.31 . 1 - py36_0 ... installing : pip - 10.0 . 1 - py36_0 ... installing : pyopenssl - 18.0 . 0 - py36_0 ... installing : urllib3 - 1.22 - py36hbe7ace6_0 ... installing : requests - 2.18 . 4 - py36he2e5f8d_1 ... installing : conda - 4.5 . 4 - py36_0 ... unlinking : ca - certificates - 2019.6 . 16 - hecc5488_0 unlinking : certifi - 2019.6 . 16 - py36_1 unlinking : conda - 4.7 . 11 - py36_0 unlinking : cryptography - 2.7 - py36h72c5cf5_0 unlinking : libgcc - ng - 9.1 . 0 - hdf63c60_0 unlinking : libstdcxx - ng - 9.1 . 0 - hdf63c60_0 unlinking : openssl - 1.1 . 1 c - h516909a_0 unlinking : python - 3.6 . 7 - h381d211_1004 unlinking : sqlite - 3.28 . 0 - h8b20d00_0 unlinking : tk - 8.6 . 9 - hed695b0_1002 installation finished . WARNING : You currently have a PYTHONPATH environment variable set . This may cause unexpected behavior when running the Python interpreter in Miniconda3 . For best results , please verify that your PYTHONPATH only points to directories of packages that are compatible with the Python interpreter in Miniconda3 : / usr / local Solving environment : ... working ... done ## Package Plan ## environment location : / usr / local added / updated specs : - cudf - cuml The following packages will be UPDATED : ca - certificates : 2018.03 . 07 - 0 --> 2019.6 . 16 - hecc5488_0 conda - forge certifi : 2018.4 . 16 - py36_0 --> 2019.6 . 16 - py36_1 conda - forge conda : 4.5 . 4 - py36_0 --> 4.7 . 11 - py36_0 conda - forge cryptography : 2.2 . 2 - py36h14c3975_0 --> 2.7 - py36h72c5cf5_0 conda - forge libgcc - ng : 7.2 . 0 - hdf63c60_3 --> 9.1 . 0 - hdf63c60_0 libstdcxx - ng : 7.2 . 0 - hdf63c60_3 --> 9.1 . 0 - hdf63c60_0 openssl : 1.0 . 2 o - h20670df_0 --> 1.1 . 1 c - h516909a_0 conda - forge python : 3.6 . 5 - hc3d631a_2 --> 3.6 . 7 - h381d211_1004 conda - forge sqlite : 3.23 . 1 - he433501_0 --> 3.28 . 0 - h8b20d00_0 conda - forge tk : 8.6 . 7 - hc745277_3 --> 8.6 . 9 - hed695b0_1002 conda - forge Preparing transaction : ... working ... done Verifying transaction : ... working ... done Executing transaction : ... working ... done You would need to do a Runtime > Restart and Run All on your Google Colab notebook at this step.","title":"Installation of Libraries (Optional)"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#imports","text":"# Critical imports for GPU cuDF import nvstrings , nvcategory , cudf # Other imports import numpy as np import pandas as pd import time import pandas_datareader.data as web from datetime import datetime from matplotlib import pyplot as plt from numba import cuda","title":"Imports"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#plotting-style","text":"# Display settings: just to make it slightly prettier # There'll be a separate post on beautiful plots % matplotlib inline figsize = ( 25 , 6 ) plt . style . use ( 'fivethirtyeight' )","title":"Plotting Style"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#necessity-of-fractional-differencing","text":"Before we get into why fractional differencing is critical, let us inspect a simple time series pulled from the Fed's database: S&P 500.","title":"Necessity of Fractional Differencing"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#pulling-processing-plotting-spx-time-series","text":"# Read SPX data 2010-2019 from FED database: https://fred.stlouisfed.org/categories/94 asset_name = 'SPX' start = datetime ( 2010 , 1 , 1 ) end = datetime ( 2019 , 8 , 1 ) df_raw = web . DataReader ( 'SP500' , 'fred' , start , end ) # Basic clean up data: dropna (there are other more robust treatments) df_raw = df_raw . dropna () df_raw . columns = [ asset_name ] df_raw . plot ( figsize = figsize ) plt . title ( 'S&P 500 Absolute Levels' );","title":"Pulling, Processing &amp; Plotting SPX Time Series"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#integer-differencing-1-day-returns-spx-time-series","text":"Traditionally we typically might difference our time series by one day (daily returns) or more to reach some form of stationarity via tests like ADF, KPSS, PP and more. However, these forms of integer differencing causes more information than needed to be lost. # One day returns calculation through differencing by 1 differencing_factor = 1 df_daily_returns = ( df_raw - df_raw . shift ( differencing_factor )) / df_raw . shift ( differencing_factor ) * 100 df_daily_returns . plot ( figsize = figsize ) plt . title ( f ' { asset_name } Daily Returns via Differencing 1 Day' );","title":"Integer Differencing (1 Day Returns) SPX Time Series"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#integer-differencing-causes-unnecessary-memory-loss","text":"Because of this reason, we propose fractional differencing to achieve stationarity while minimizing memory loss.","title":"Integer Differencing Causes Unnecessary Memory Loss"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#basic-fractional-differencing","text":"Fractional differencing allows us to achieve stationarity while maintaining the maximum amount of memory compared to integer differencing. This was originally introduced in 1981 in his paper \u201cFractional Differencing\u201d by J. R. M. Hosking1 and subsequent work by others concentrated on fast and efficient implementations for fractional differentiation for continuous stochastic processes. Recently, fractional differencing was introduced for financial time series through the fixed window fractional differencing instead of the expanding window method by Marcos Lopez de Prado2.","title":"Basic Fractional Differencing"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#fractional-differencing-weight-function-and-plot","text":"Weights formula: w_k = -w_{k-1} \\frac{d - k + 1}{k} w_k = -w_{k-1} \\frac{d - k + 1}{k} Weight converges to zero: w_k \\rightarrow 0 w_k \\rightarrow 0 def get_weights ( d , num_k ): r \"\"\"Calculate weights ($w$) for each lag ($k$) through $w_k = -w_{k-1} \\frac{d - k + 1}{k}$. Args: d (int): differencing value. num_k (int): number of lags (typically length of timeseries) to calculate w. \"\"\" w_k = np . array ([ 1 ]) for k in range ( 1 , num_k ): w_k = np . append ( w_k , - w_k [ - 1 ] * (( d - k + 1 )) / k ) w_k = w_k . reshape ( - 1 , 1 ) return w_k def plot_weights ( range_d , num_k , num_d_interval ): r \"\"\"Plot weights ($w$) for each lag ($k$) for varying differencing value ($d$). Args: range_d (list): range of differencing values to plot. num_k (int): number of lags (typically length of timeseries) to plot. num_d_interval (int): number of d interval. \"\"\" # Get differencing values interval = np . linspace ( range_d [ 0 ], range_d [ 1 ], num_d_interval ) # Dataframe of lags (rows) x number of differencing intervals (columns) df_wk = pd . DataFrame ( np . zeros (( num_k , num_d_interval ))) # Get weights array per differencing value for i , d in enumerate ( interval ): df_wk [ i ] = get_weights ( d , num_k ) # Rename columns for legend df_wk . columns = [ round ( x , 1 ) for x in interval ] # Plot df_wk . plot ( figsize = figsize ) plt . title ( 'Lag weights ($w_k$) for various differencing values (d)' ) plt . legend ( title = 'Differencing value (d)' ) plt . ylabel ( 'Weights (w)' ) plt . xlabel ( 'Lag (k)' ) plt . show () # Return weights return df_wk df_wk = plot_weights ( range_d = [ 0 , 1 ], num_k = 7 , num_d_interval = 6 )","title":"Fractional Differencing Weight Function and Plot"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#fractional-differencing-weight-function-table","text":"# Dataframe for lag weights (w) for various differencing values (d) df_wk .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0.0 0.2 0.4 0.6 0.8 1.0 0 1.0 1.000000 1.000000 1.000000 1.000000 1.0 1 -0.0 -0.200000 -0.400000 -0.600000 -0.800000 -1.0 2 -0.0 -0.080000 -0.120000 -0.120000 -0.080000 0.0 3 -0.0 -0.048000 -0.064000 -0.056000 -0.032000 0.0 4 -0.0 -0.033600 -0.041600 -0.033600 -0.017600 0.0 5 -0.0 -0.025536 -0.029952 -0.022848 -0.011264 0.0 6 -0.0 -0.020429 -0.022963 -0.016755 -0.007885 0.0","title":"Fractional Differencing Weight Function Table"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#fractional-differencing-value-calculation-toy-example","text":"Assume spot have values 100, 99, 98, 97, 96, 95, 94 from k=0 to k=6","title":"Fractional Differencing Value Calculation Toy Example"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#for-d-0d-0-no-differencing","text":"differenced_value = 100 * 1 + 99 * 0 + 98 * 0 + 97 * 0 + 96 * 0 + 95 * 0 + 94 * 0 print ( f 'Differenced value: { differenced_value } ' ) df_wk . iloc [:, 0 ] Differenced value : 100 0 1 . 0 1 - 0 . 0 2 - 0 . 0 3 - 0 . 0 4 - 0 . 0 5 - 0 . 0 6 - 0 . 0 Name : 0 . 0 , dtype : float64","title":"For d = 0d = 0 (no differencing)"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#for-d-04d-04-fractional-differencing","text":"differenced_value = 100 * 1 + 99 * - 0.400000 + 98 * - 0.120000 + 97 * - 0.064000 + 96 * - 0.041600 + 95 * - 0.041600 + 94 * - 0.041600 print ( f 'Differenced value: { differenced_value } ' ) df_wk . iloc [:, 1 ] Differenced value : 30 . 576000000000004 0 1 . 000000 1 - 0 . 200000 2 - 0 . 080000 3 - 0 . 048000 4 - 0 . 033600 5 - 0 . 025536 6 - 0 . 020429 Name : 0 . 2 , dtype : float64","title":"For d = 0.4d = 0.4 (fractional differencing)"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#for-d-1d-1-integer-differencing","text":"differenced_value = 100 * 1 + 99 * - 1 + 98 * 0 + 97 * 0 + 96 * 0 + 95 * 0 + 94 * 0 print ( f 'Differenced value: { differenced_value } ' ) df_wk . iloc [:, - 1 ] Differenced value : 1 0 1 . 0 1 - 1 . 0 2 0 . 0 3 0 . 0 4 0 . 0 5 0 . 0 6 0 . 0 Name : 1 . 0 , dtype : float64","title":"For d = 1d = 1 (integer differencing)"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#cumulative-sum-of-weights-behavior","text":"Higher differencing value would accelerate cumulative sum weight decay This causes: Less information taken from further lags More information loss from further lags df_wk . cumsum () . plot ( figsize = figsize ) plt . title ( 'Cumulative Sum of Lag weights ($\\sum_0^k w_k$) for various differencing values (d)' ) plt . legend ( title = 'Differencing value ($d$)' ) plt . ylabel ( 'Cumulative Sum of Weights ($\\sum_0^k w_k$)' ) plt . xlabel ( 'Lag (k)' );","title":"Cumulative Sum of Weights Behavior"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#fixed-window-fractional-differencing-cpu","text":"","title":"Fixed Window Fractional Differencing (CPU)"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#floored-weights-function","text":"For computational efficiency, we want to stop the calculation of weights when the weights are too small (below a certain small predetermined small value). Image we had a time series with 1,000,000 data points, we do not want to compute the weights up till 1m lag for our latest data point! We can simply calculate the weights till they're too small which is what this function does. def get_weights_floored ( d , num_k , floor = 1e-3 ): r \"\"\"Calculate weights ($w$) for each lag ($k$) through $w_k = -w_{k-1} \\frac{d - k + 1}{k}$ provided weight above a minimum value (floor) for the weights to prevent computation of weights for the entire time series. Args: d (int): differencing value. num_k (int): number of lags (typically length of timeseries) to calculate w. floor (float): minimum value for the weights for computational efficiency. \"\"\" w_k = np . array ([ 1 ]) k = 1 while k < num_k : w_k_latest = - w_k [ - 1 ] * (( d - k + 1 )) / k if abs ( w_k_latest ) <= floor : break w_k = np . append ( w_k , w_k_latest ) k += 1 w_k = w_k . reshape ( - 1 , 1 ) return w_k # Show large time series being stopped # Notice how max lag is at 9? d = 0.5 num_k = 1000 weights = get_weights_floored ( d = d , num_k = num_k ) pd . DataFrame ( weights ) . plot ( legend = False , figsize = figsize ) # Plot plt . title ( f 'Lag weights ($w_k$) for differencing value $d= { d } $' ) plt . ylabel ( 'Weights (w)' ) plt . xlabel ( 'Lag (k)' );","title":"Floored Weights Function"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#fixed-window-fractional-differencing-function-cpu","text":"Here, we use the new floored weights function compared to our normal weights function Calculate our differenced values through the dot product of our transposed weights matrix and our original time series within that window Keep repeating by shifting the window by one time step until the end of the time series def frac_diff ( df , d , floor = 1e-3 ): r \"\"\"Fractionally difference time series via CPU. Args: df (pd.DataFrame): dataframe of raw time series values. d (float): differencing value from 0 to 1 where > 1 has no FD. floor (float): minimum value of weights, ignoring anything smaller. \"\"\" # Get weights window weights = get_weights_floored ( d = d , num_k = len ( df ), floor = floor ) weights_window_size = len ( weights ) # Reverse weights weights = weights [:: - 1 ] # Blank fractionally differenced series to be filled df_fd = [] # Slide window of time series, to calculated fractionally differenced values # per window for idx in range ( weights_window_size , df . shape [ 0 ]): # Dot product of weights and original values # to get fractionally differenced values date_idx = df . index [ idx ] df_fd . append ( np . dot ( weights . T , df . iloc [ idx - weights_window_size : idx ]) . item ()) # Return FD values and weights df_fd = pd . DataFrame ( df_fd ) return df_fd , weights # Start timer start = time . time () df_raw_fd , weights = frac_diff ( df_raw , d = 0.5 , floor = 5e-5 ) # End timer end = time . time () print ( f 'Time { end - start } s' ) # Plot df_raw_fd . plot ( figsize = figsize ); Time 0.5417799949645996 s import multiprocessing multiprocessing . cpu_count () 4 Existing CPU-based implementations are inefficient for running fractional differencing on many large-scale time-series. GPU-based implementations provide an avenue to adapt to this century\u2019s big data requirements.","title":"Fixed Window Fractional Differencing Function (CPU)"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#fixed-window-fractional-differencing-function-gpu","text":"This gives a slight 1.5x to 2x speed-up on an NVIDIA T4 on Google Colab for this tiny dataset For larger time series in line with real-world situations, it can easily be up to 100x to 1000x dataset (shown below). def moving_dot_product_kernel ( in_data , out , window_size , weights ): # Set the first window_size-1 rows in each chunk to np.nan due # insufficient history for i in range ( cuda . threadIdx . x , window_size - 1 , cuda . blockDim . x ): out [ i ] = np . nan # Compute dot product of preceding window_size rows for i in range ( cuda . threadIdx . x + window_size - 1 , in_data . size , cuda . blockDim . x ): rolling_dot_product = 0.0 k = 0 for j in range ( i - window_size + 1 , i + 1 ): rolling_dot_product += in_data [ j ] * weights [ k ][ 0 ] k += 1 out [ i ] = rolling_dot_product def frac_diff_gpu ( df , d , floor = 1e-3 ): r \"\"\"Fractionally difference time series via GPU. Args: df (pd.DataFrame): dataframe of raw time series values. d (float): differencing value from 0 to 1 where > 1 has no FD. floor (float): minimum value of weights, ignoring anything smaller. \"\"\" # Bring dataframe to GPU, reset index for GPU dot product kernel gdf_raw = cudf . from_pandas ( df ) . reset_index ( drop = True ) gdf_raw . columns = [ 'in_data' ] # Get weights window weights = get_weights_floored ( d = d , num_k = len ( gdf_raw ), floor = floor ) weights_window_size = len ( weights ) # Reverse weights and as contiguous weights = np . ascontiguousarray ( weights [:: - 1 ]) # Bring weights to GPU gdf_weights = cudf . DataFrame () gdf_weights [ gdf_raw . columns [ 0 ]] = weights . reshape ( - 1 ) # Length of data data_length = len ( gdf_raw ) # T4: max of 518 threads per block. # V100: max 1024 threads per block threads_per_block = 518 # Chunk size split # This has to be improved, but as a v0.1, it's sufficient to show speed-up # Up to easily 100 million data points trunk_size = data_length # Get fractionally differenced time series through GPU function gdf_raw_fd = gdf_raw . apply_chunks ( moving_dot_product_kernel , incols = [ 'in_data' ], outcols = dict ( out = np . float64 ), kwargs = dict ( window_size = weights_window_size , weights = weights ), chunks = list ( range ( 0 , data_length , trunk_size )) + [ data_length ], tpb = threads_per_block ) # Bring to CPU for normal manipulation df_raw_fd = gdf_raw_fd . to_pandas () . dropna () . iloc [: - 1 , 1 ] return df_raw_fd , weights # Start timer start = time . time () df_raw_fd_from_gpu , weights = frac_diff_gpu ( df_raw , d = 0.5 , floor = 5e-5 ) # End timer end = time . time () print ( f 'Time { end - start } s' ) # Plot df_raw_fd_from_gpu . plot ( figsize = figsize ); Time 0.27262187004089355 s","title":"Fixed Window Fractional Differencing Function (GPU)"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#check-values","text":"# Compare tail values print ( 'Tail values check' ) print ( df_raw_fd_from_gpu . tail () . values ) print ( df_raw_fd . tail () . values . reshape ( - 1 ,)) Tail values check [107.00340988 133.85520208 117.92878691 109.44132697 79.32562638] [107.00340988 133.85520208 117.92878691 109.44132697 79.32562638] # Compare tail values print ( 'Head values check' ) print ( df_raw_fd_from_gpu . head () . values ) print ( df_raw_fd . head () . values . reshape ( - 1 ,)) Head values check [56.74989213 52.56766288 47.32421832 45.89772154 37.74401501] [56.74989213 52.56766288 47.32421832 45.89772154 37.74401501] num_rows_true = ( df_raw_fd . values . astype ( np . float32 ) == df_raw_fd_from_gpu . values . astype ( np . float32 )) . astype ( int ) . sum () total_rows = df_raw_fd . shape [ 0 ] print ( f 'Number of rows equal: { num_rows_true } ' ) print ( f 'Total number of rows: { total_rows } ' ) Number of rows equal: 2093 Total number of rows: 2093","title":"Check Values"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#stationarity","text":"\"A stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time\", Robert Nau, Duke University Essentially what we are trying to do here with fractional differencing is to attempt to have a stationary time series without losing too much memory. There are many ways to check if a time series is stationary, but we will be using 1 test here (they're more, but it's not the purpose of this tutorial) Augmented Dickey\u2013Fuller (ADF) test: check for unit root","title":"Stationarity"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#adf-test","text":"# Import adf/kpss from statsmodels.tsa.stattools import adfuller import warnings warnings . filterwarnings ( \"ignore\" ) # Ignore deprecations for cleaner output","title":"ADF Test"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#adf-raw-data-d0","text":"# ADF on raw data print ( 'Stationarity Test for SPX Absolute Levels (d=0)' ) print ( '-' * 50 ) result = adfuller ( df_raw [ 'SPX' ], regression = 'c' ) print ( f 't-stat \\n\\t { result [ 0 ] : .2f } ' ) print ( f 'p-value \\n\\t { result [ 1 ] : .2f } ' ) print ( f 'Critical Values' ) for key , value in result [ 4 ] . items (): print ( f ' \\t { key } : { value : .2f } ' ) # test statistic > test statistic (1%) AND p-value > 0.01 # Fail to reject the null hypothesis that there's unit root at the 1% significance level # ==> Fail to reject H0 ==> Unit root ==> Non-stationary Stationarity Test for SPX Absolute Levels (d=0) -------------------------------------------------- t-stat -0.11 p-value 0.95 Critical Values 1%: -3.43 5%: -2.86 10%: -2.57","title":"ADF: raw data (d=0)"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#adf-daily-return-data-d1","text":"# ADF on 1 day returns (d=1) print ( 'Stationarity Test for SPX Daily Returns (d=1)' ) print ( '-' * 50 ) result = adfuller ( df_raw [ 'SPX' ] . pct_change ( 1 ) . dropna (), regression = 'c' ) print ( f 't-stat \\n\\t { result [ 0 ] : .2f } ' ) print ( f 'p-value \\n\\t { result [ 1 ] : .2f } ' ) print ( f 'Critical Values' ) for key , value in result [ 4 ] . items (): print ( f ' \\t { key } : { value : .2f } ' ) # test statistic < test statistic (1%) AND p-value < 0.01 # Reject the null hypothesis that there's unit root at the 1% significance level # ==> Reject H0 ==> No unit root ==> Stationary Stationarity Test for SPX Daily Returns (d=1) -------------------------------------------------- t-stat -11.12 p-value 0.00 Critical Values 1%: -3.43 5%: -2.86 10%: -2.57","title":"ADF: daily return data (d=1)"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#adf-fractionally-differenced-data-d05","text":"# ADF on fractionally differenced values print ( 'Stationarity Test (ADF) for SPX Fractionally Differenced (d=0.5)' ) print ( '-' * 50 ) df_raw_fd_from_gpu , weights = frac_diff_gpu ( df_raw , d = 0.5 , floor = 5e-5 ) result = adfuller ( df_raw_fd_from_gpu , regression = 'c' ) print ( f 't-stat \\n\\t { result [ 0 ] : .2f } ' ) print ( f 'p-value \\n\\t { result [ 1 ] : .2f } ' ) print ( f 'Critical Values' ) for key , value in result [ 4 ] . items (): print ( f ' \\t { key } : { value : .2f } ' ) # test statistic < test statistic (1%) AND p-value < 0.01 # Reject the null hypothesis that there's unit root at the 1% significance level # ==> Reject H0 ==> No unit root ==> Stationary Stationarity Test (ADF) for SPX Fractionally Differenced (d=0.5) -------------------------------------------------- t-stat -3.86 p-value 0.00 Critical Values 1%: -3.43 5%: -2.86 10%: -2.57","title":"ADF: fractionally differenced data (d=0.5)"},{"location":"machine_learning/gpu/gpu_fractional_differencing/#large-scale-rapid-fractional-differencing","text":"Here we test on a 100k, 1m, 10m and 100m datapoint dataframe. # Create 100m data points large_time_series_length = int ( 1e6 ) print ( large_time_series_length ) 1000000 # Start timer start = time . time () df_raw = pd . DataFrame ( np . arange ( large_time_series_length ) * np . random . rand ( large_time_series_length )) df_raw_fd_from_gpu , weights = frac_diff_gpu ( df_raw , d = 0.5 , floor = 5e-5 ) # End timer end = time . time () print ( f 'Time { end - start } s' ) # Check print ( 'FD Shape' , df_raw_fd_from_gpu . shape [ 0 ]) print ( 'Correct Shape' , df_raw . shape [ 0 ] - weights . shape [ 0 ]) print ( 'Original Shape' , df_raw . shape [ 0 ]) Time 0.4671785831451416 s FD Shape 999682 Correct Shape 999682 Original Shape 1000000 # Start timer start = time . time () # Create 100m data points df_raw = pd . DataFrame ( np . arange ( large_time_series_length ) * np . random . rand ( large_time_series_length )) df_raw_fd , weights = frac_diff ( df_raw , d = 0.5 , floor = 5e-5 ) # End timer end = time . time () print ( f 'Time { end - start } s' ) # Check print ( 'FD Shape' , df_raw_fd_from_gpu . shape [ 0 ]) print ( 'Correct Shape' , df_raw . shape [ 0 ] - weights . shape [ 0 ]) print ( 'Original Shape' , df_raw . shape [ 0 ]) Time 128.29733324050903 s FD Shape 999682 Correct Shape 999682 Original Shape 1000000","title":"Large-scale Rapid Fractional Differencing"},{"location":"machine_learning/gpu/rapids_cudf/","text":"RAPIDS cuDF \u00b6 Environment Setup \u00b6 Check Version \u00b6 Python Version \u00b6 # Check Python Version ! python -- version Python 3.6.7 Ubuntu Version \u00b6 # Check Ubuntu Version ! lsb_release - a No LSB modules are available. Distributor ID: Ubuntu Description: Ubuntu 18.04.2 LTS Release: 18.04 Codename: bionic Check CUDA Version \u00b6 # Check CUDA/cuDNN Version ! nvcc - V && which nvcc nvcc : NVIDIA ( R ) Cuda compiler driver Copyright ( c ) 2005 - 2018 NVIDIA Corporation Built on Sat_Aug_25_21 : 08 : 01 _CDT_2018 Cuda compilation tools , release 10.0 , V10 . 0.130 /usr/local/cuda/bin/ nvcc Check GPU Version \u00b6 # Check GPU ! nvidia - smi Mon May 13 09:31:40 2019 +-----------------------------------------------------------------------------+ | NVIDIA - SMI 418 . 56 Driver Version: 410 . 79 CUDA Version: 10 . 0 | | -------------------------------+----------------------+----------------------+ | GPU Name Persistence - M| Bus - Id Disp . A | Volatile Uncorr . ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory - Usage | GPU - Util Compute M . | |=============================== + ====================== + ======================| | 0 Tesla T4 Off | 00000000:00:04 . 0 Off | 0 | | N/A 67C P8 17W / 70W | 0MiB / 15079MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ Check GPU if You've Right Version (T4) \u00b6 Many thanks to NVIDIA team for this snippet of code to automatically set up everything. import pynvml pynvml . nvmlInit () handle = pynvml . nvmlDeviceGetHandleByIndex ( 0 ) device_name = pynvml . nvmlDeviceGetName ( handle ) if device_name != b 'Tesla T4' : raise Exception ( \"\"\" Unfortunately this instance does not have a T4 GPU. Please make sure you've configured Colab to request a GPU instance type. Sometimes Colab allocates a Tesla K80 instead of a T4. Resetting the instance. If you get a K80 GPU, try Runtime -> Reset all runtimes... \"\"\" ) else : print ( 'Woo! You got the right kind of GPU!' ) Woo! You got the right kind of GPU! Installation of cuDF/cuML \u00b6 Many thanks to NVIDIA team for this snippet of code to automatically set up everything. # intall miniconda ! wget - c https : // repo . continuum . io / miniconda / Miniconda3 - 4.5 . 4 - Linux - x86_64 . sh ! chmod + x Miniconda3 - 4.5 . 4 - Linux - x86_64 . sh ! bash ./ Miniconda3 - 4.5 . 4 - Linux - x86_64 . sh - b - f - p / usr / local # install RAPIDS packages ! conda install - q - y -- prefix / usr / local - c conda - forge \\ - c rapidsai - nightly / label / cuda10 . 0 - c nvidia / label / cuda10 . 0 \\ cudf cuml # set environment vars import sys , os , shutil sys . path . append ( '/usr/local/lib/python3.6/site-packages/' ) os . environ [ 'NUMBAPRO_NVVM' ] = '/usr/local/cuda/nvvm/lib64/libnvvm.so' os . environ [ 'NUMBAPRO_LIBDEVICE' ] = '/usr/local/cuda/nvvm/libdevice/' # copy .so files to current working dir for fn in [ 'libcudf.so' , 'librmm.so' ]: shutil . copy ( '/usr/local/lib/' + fn , os . getcwd ()) -- 2019 - 05 - 13 09 : 31 : 42 -- https : // repo . continuum . io / miniconda / Miniconda3 - 4.5 . 4 - Linux - x86_64 . sh Resolving repo . continuum . io ( repo . continuum . io ) ... 104.18 . 201.79 , 104.18 . 200.79 , 2606 : 4700 :: 6812 : c84f , ... Connecting to repo . continuum . io ( repo . continuum . io ) | 104.18 . 201.79 | : 443. .. connected . HTTP request sent , awaiting response ... 200 OK Length : 58468498 ( 56 M ) [ application / x - sh ] Saving to : \u2018 Miniconda3 - 4.5 . 4 - Linux - x86_64 . sh \u2019 Miniconda3 - 4.5 . 4 - Li 100 % [ ===================> ] 55.76 M 64.9 MB / s in 0.9 s 2019 - 05 - 13 09 : 31 : 48 ( 64.9 MB / s ) - \u2018 Miniconda3 - 4.5 . 4 - Linux - x86_64 . sh \u2019 saved [ 58468498 / 58468498 ] PREFIX =/ usr / local installing : python - 3.6 . 5 - hc3d631a_2 ... Python 3.6 . 5 :: Anaconda , Inc . installing : ca - certificates - 2018.03 . 07 - 0 ... installing : conda - env - 2.6 . 0 - h36134e3_1 ... installing : libgcc - ng - 7.2 . 0 - hdf63c60_3 ... installing : libstdcxx - ng - 7.2 . 0 - hdf63c60_3 ... installing : libffi - 3.2 . 1 - hd88cf55_4 ... installing : ncurses - 6.1 - hf484d3e_0 ... installing : openssl - 1.0 . 2 o - h20670df_0 ... installing : tk - 8.6 . 7 - hc745277_3 ... installing : xz - 5.2 . 4 - h14c3975_4 ... installing : yaml - 0.1 . 7 - had09818_2 ... installing : zlib - 1.2 . 11 - ha838bed_2 ... installing : libedit - 3.1 . 20170329 - h6b74fdf_2 ... installing : readline - 7.0 - ha6073c6_4 ... installing : sqlite - 3.23 . 1 - he433501_0 ... installing : asn1crypto - 0.24 . 0 - py36_0 ... installing : certifi - 2018.4 . 16 - py36_0 ... installing : chardet - 3.0 . 4 - py36h0f667ec_1 ... installing : idna - 2.6 - py36h82fb2a8_1 ... installing : pycosat - 0.6 . 3 - py36h0a5515d_0 ... installing : pycparser - 2.18 - py36hf9f622e_1 ... installing : pysocks - 1.6 . 8 - py36_0 ... installing : ruamel_yaml - 0.15 . 37 - py36h14c3975_2 ... installing : six - 1.11 . 0 - py36h372c433_1 ... installing : cffi - 1.11 . 5 - py36h9745a5d_0 ... installing : setuptools - 39.2 . 0 - py36_0 ... installing : cryptography - 2.2 . 2 - py36h14c3975_0 ... installing : wheel - 0.31 . 1 - py36_0 ... installing : pip - 10.0 . 1 - py36_0 ... installing : pyopenssl - 18.0 . 0 - py36_0 ... installing : urllib3 - 1.22 - py36hbe7ace6_0 ... installing : requests - 2.18 . 4 - py36he2e5f8d_1 ... installing : conda - 4.5 . 4 - py36_0 ... installation finished . WARNING : You currently have a PYTHONPATH environment variable set . This may cause unexpected behavior when running the Python interpreter in Miniconda3 . For best results , please verify that your PYTHONPATH only points to directories of packages that are compatible with the Python interpreter in Miniconda3 : / usr / local Solving environment : ... working ... done ## Package Plan ## environment location : / usr / local added / updated specs : - cudf - cuml The following packages will be downloaded : package | build ---------------------------|----------------- icu - 58.2 | hf484d3e_1000 22.6 MB conda - forge libgcc - ng - 8.2 . 0 | hdf63c60_1 7.6 MB sqlite - 3.26 . 0 | h67949de_1001 1.9 MB conda - forge python - 3.6 . 7 | h381d211_1004 34.5 MB conda - forge numpy - 1.16 . 3 | py36he5ce36f_0 4.3 MB conda - forge cudatoolkit - 10.0 . 130 | 0 380.0 MB libcudf - 0.7 . 0. dev0 | cuda10 . 0 _1567 21.0 MB rapidsai - nightly / label / cuda10 . 0 parquet - cpp - 1.5 . 1 | 4 3 KB conda - forge arrow - cpp - 0.12 . 1 | py36h0e61e49_0 6.9 MB conda - forge liblapack - 3.8 . 0 | 9 _openblas 6 KB conda - forge libstdcxx - ng - 8.2 . 0 | hdf63c60_1 2.9 MB libblas - 3.8 . 0 | 9 _openblas 6 KB conda - forge openssl - 1.1 . 1 b | h14c3975_1 4.0 MB conda - forge cython - 0.29 . 7 | py36he1b5a44_0 2.2 MB conda - forge cryptography - 2.6 . 1 | py36h72c5cf5_0 606 KB conda - forge certifi - 2019.3 . 9 | py36_0 149 KB conda - forge ca - certificates - 2019.3 . 9 | hecc5488_0 146 KB conda - forge tk - 8.6 . 9 | h84994c4_1001 3.2 MB conda - forge libcuml - 0.8 . 0 a | cuda10 . 0 _610 24.4 MB rapidsai - nightly / label / cuda10 . 0 bzip2 - 1.0 . 6 | h14c3975_1002 415 KB conda - forge cuml - 0.8 . 0 a | cuda10 . 0 _py36_610 3.0 MB rapidsai - nightly / label / cuda10 . 0 python - dateutil - 2.8 . 0 | py_0 219 KB conda - forge conda - 4.6 . 14 | py36_0 2.1 MB conda - forge openblas - 0.3 . 6 | h6e990d7_2 15.8 MB conda - forge thrift - cpp - 0.12 . 0 | h0a07b25_1002 2.4 MB conda - forge rmm - 0.7 . 0. dev0 | py36_54 14 KB rapidsai - nightly / label / cuda10 . 0 pyarrow - 0.12 . 1 | py36hbbcf98d_0 2.2 MB conda - forge libprotobuf - 3.6 . 1 | hdbcaa40_1001 4.0 MB conda - forge nvstrings - 0.7 . 0. dev0 | py36_143 88 KB rapidsai - nightly / label / cuda10 . 0 librmm - 0.7 . 0. dev0 | cuda10 . 0 _54 39 KB rapidsai - nightly / label / cuda10 . 0 libcblas - 3.8 . 0 | 9 _openblas 6 KB conda - forge libgfortran - ng - 7.3 . 0 | hdf63c60_0 1.3 MB libnvstrings - 0.7 . 0. dev0 | cuda10 . 0 _132 9.3 MB rapidsai - nightly / label / cuda10 . 0 llvmlite - 0.28 . 0 | py36hdbcaa40_0 20.2 MB conda - forge cudf - 0.7 . 0. dev0 | py36_1568 2.7 MB rapidsai - nightly / label / cuda10 . 0 pytz - 2019.1 | py_0 227 KB conda - forge numba - 0.43 . 1 | py36hf2d7682_0 2.9 MB conda - forge boost - cpp - 1.68 . 0 | h11c811c_1000 20.5 MB conda - forge pandas - 0.24 . 2 | py36hf484d3e_0 11.1 MB conda - forge libcumlmg - 0.0 . 0. dev0 | cuda10 . 0 _373 955 KB nvidia / label / cuda10 . 0 ------------------------------------------------------------ Total : 615.7 MB The following NEW packages will be INSTALLED : arrow - cpp : 0.12 . 1 - py36h0e61e49_0 conda - forge boost - cpp : 1.68 . 0 - h11c811c_1000 conda - forge bzip2 : 1.0 . 6 - h14c3975_1002 conda - forge cudatoolkit : 10.0 . 130 - 0 cudf : 0.7 . 0. dev0 - py36_1568 rapidsai - nightly / label / cuda10 . 0 cuml : 0.8 . 0 a - cuda10 . 0 _py36_610 rapidsai - nightly / label / cuda10 . 0 cython : 0.29 . 7 - py36he1b5a44_0 conda - forge icu : 58.2 - hf484d3e_1000 conda - forge libblas : 3.8 . 0 - 9 _openblas conda - forge libcblas : 3.8 . 0 - 9 _openblas conda - forge libcudf : 0.7 . 0. dev0 - cuda10 . 0 _1567 rapidsai - nightly / label / cuda10 . 0 libcuml : 0.8 . 0 a - cuda10 . 0 _610 rapidsai - nightly / label / cuda10 . 0 libcumlmg : 0.0 . 0. dev0 - cuda10 . 0 _373 nvidia / label / cuda10 . 0 libgfortran - ng : 7.3 . 0 - hdf63c60_0 liblapack : 3.8 . 0 - 9 _openblas conda - forge libnvstrings : 0.7 . 0. dev0 - cuda10 . 0 _132 rapidsai - nightly / label / cuda10 . 0 libprotobuf : 3.6 . 1 - hdbcaa40_1001 conda - forge librmm : 0.7 . 0. dev0 - cuda10 . 0 _54 rapidsai - nightly / label / cuda10 . 0 llvmlite : 0.28 . 0 - py36hdbcaa40_0 conda - forge numba : 0.43 . 1 - py36hf2d7682_0 conda - forge numpy : 1.16 . 3 - py36he5ce36f_0 conda - forge nvstrings : 0.7 . 0. dev0 - py36_143 rapidsai - nightly / label / cuda10 . 0 openblas : 0.3 . 6 - h6e990d7_2 conda - forge pandas : 0.24 . 2 - py36hf484d3e_0 conda - forge parquet - cpp : 1.5 . 1 - 4 conda - forge pyarrow : 0.12 . 1 - py36hbbcf98d_0 conda - forge python - dateutil : 2.8 . 0 - py_0 conda - forge pytz : 2019.1 - py_0 conda - forge rmm : 0.7 . 0. dev0 - py36_54 rapidsai - nightly / label / cuda10 . 0 thrift - cpp : 0.12 . 0 - h0a07b25_1002 conda - forge The following packages will be UPDATED : ca - certificates : 2018.03 . 07 - 0 --> 2019.3 . 9 - hecc5488_0 conda - forge certifi : 2018.4 . 16 - py36_0 --> 2019.3 . 9 - py36_0 conda - forge conda : 4.5 . 4 - py36_0 --> 4.6 . 14 - py36_0 conda - forge cryptography : 2.2 . 2 - py36h14c3975_0 --> 2.6 . 1 - py36h72c5cf5_0 conda - forge libgcc - ng : 7.2 . 0 - hdf63c60_3 --> 8.2 . 0 - hdf63c60_1 libstdcxx - ng : 7.2 . 0 - hdf63c60_3 --> 8.2 . 0 - hdf63c60_1 openssl : 1.0 . 2 o - h20670df_0 --> 1.1 . 1 b - h14c3975_1 conda - forge python : 3.6 . 5 - hc3d631a_2 --> 3.6 . 7 - h381d211_1004 conda - forge sqlite : 3.23 . 1 - he433501_0 --> 3.26 . 0 - h67949de_1001 conda - forge tk : 8.6 . 7 - hc745277_3 --> 8.6 . 9 - h84994c4_1001 conda - forge Preparing transaction : ... working ... done Verifying transaction : ... working ... done Executing transaction : ... working ... done Critical Imports \u00b6 # Critical imports import nvstrings , nvcategory , cudf import cuml import os import numpy as np import pandas as pd Creating \u00b6 Create a Series of integers \u00b6 gdf = cudf . Series ([ 1 , 2 , 3 , 4 , 5 , 6 ]) print ( gdf ) print ( type ( gdf )) 0 1 1 2 2 3 3 4 4 5 5 6 dtype: int64 <class 'cudf.dataframe.series.Series'> Create a Series of floats \u00b6 gdf = cudf . Series ([ 1. , 2. , 3. , 4. , 5. , 6. ]) print ( gdf ) 0 1.0 1 2.0 2 3.0 3 4.0 4 5.0 5 6.0 dtype: float64 Create a Series of strings \u00b6 gdf = cudf . Series ([ 'a' , 'b' , 'c' ]) print ( gdf ) 0 a 1 b 2 c dtype: object Create 3 column DataFrame \u00b6 Consisting of dates, integers and floats # Import import datetime as dt # Using a list of tuples # Each element in the list represents a category # The first element of the tuple is the category's name # The second element of the tuple is a list of the values in that category gdf = cudf . DataFrame ([ # Create 10 busindates ess from 1st January 2019 via pandas ( 'dates' , pd . date_range ( '1/1/2019' , periods = 10 , freq = 'B' )), # Integers ( 'integers' , [ i for i in range ( 10 )]), # Floats ( 'floats' , [ float ( i ) for i in range ( 10 )]) ]) # Print dataframe print ( gdf ) dates integers floats 0 2019-01-01T00:00:00.000 0 0.0 1 2019-01-02T00:00:00.000 1 1.0 2 2019-01-03T00:00:00.000 2 2.0 3 2019-01-04T00:00:00.000 3 3.0 4 2019-01-07T00:00:00.000 4 4.0 5 2019-01-08T00:00:00.000 5 5.0 6 2019-01-09T00:00:00.000 6 6.0 7 2019-01-10T00:00:00.000 7 7.0 8 2019-01-11T00:00:00.000 8 8.0 9 2019-01-14T00:00:00.000 9 9.0 Create 2 column Dataframe \u00b6 Consisting of integers and string category # Using a list of tuples # Each element in the list represents a category # The first element of the tuple is the category's name # The second element of the tuple is a list of the values in that category gdf = cudf . DataFrame ([ ( 'integers' , [ 1 , 2 , 3 , 4 ]), ( 'string' , [ 'a' , 'b' , 'c' , 'd' ]) ]) print ( gdf ) integers string 0 1 a 1 2 b 2 3 c 3 4 d Create a 2 Column Dataframe with Pandas Bridge \u00b6 Consisting of integers and string category For all string columns, you must convert them to type category for filtering functions to work intuitively (for now) # Create pandas dataframe pandas_df = pd . DataFrame ({ 'integers' : [ 1 , 2 , 3 , 4 ], 'strings' : [ 'a' , 'b' , 'c' , 'd' ] }) # Convert string column to category format pandas_df [ 'strings' ] = pandas_df [ 'strings' ] . astype ( 'category' ) # Bridge from pandas to cudf gdf = cudf . DataFrame . from_pandas ( pandas_df ) # Print dataframe print ( gdf ) integers strings 0 1 a 1 2 b 2 3 c 3 4 d Viewing \u00b6 Printing Column Names \u00b6 gdf . columns Index(['integers', 'strings'], dtype='object') Viewing Top of DataFrame \u00b6 num_of_rows_to_view = 2 print ( gdf . head ( num_of_rows_to_view )) integers strings 0 1 a 1 2 b Viewing Bottom of DataFrame \u00b6 num_of_rows_to_view = 3 print ( gdf . tail ( num_of_rows_to_view )) integers strings 1 2 b 2 3 c 3 4 d Filtering \u00b6 Method 1: Query \u00b6 Filtering Integers/Floats by Column Values \u00b6 This only works for floats and integers, not for strings print ( gdf . query ( 'integers == 1' )) integers strings 0 1 a Filtering Strings by Column Values \u00b6 This only works for floats and integers, not for strings so this will return an error! print ( gdf . query ( 'strings == a' )) --------------------------------------------------------------------------- KeyError Traceback ( most recent call last ) < ipython - input - 33 - 5 cfd0345d51c > in < module > () ----> 1 print ( gdf . query ( 'strings == a' )) / usr / local / lib / python3 .6 / site - packages / cudf / dataframe / dataframe . py in query ( self , expr , local_dict ) 1905 } 1906 # Run query -> 1907 boolmask = queryutils . query_execute ( self , expr , callenv ) 1908 1909 selected = Series ( boolmask ) / usr / local / lib / python3 .6 / site - packages / cudf / utils / queryutils . py in query_execute ( df , expr , callenv ) 215 envargs . append ( val ) 216 # prepare col args --> 217 colarrays = [ df[col ] . to_gpu_array () for col in compiled [ 'colnames' ] ] 218 # allocate output buffer 219 nrows = len ( df ) / usr / local / lib / python3 .6 / site - packages / cudf / utils / queryutils . py in < listcomp > ( .0 ) 215 envargs . append ( val ) 216 # prepare col args --> 217 colarrays = [ df[col ] . to_gpu_array () for col in compiled [ 'colnames' ] ] 218 # allocate output buffer 219 nrows = len ( df ) / usr / local / lib / python3 .6 / site - packages / cudf / dataframe / dataframe . py in __getitem__ ( self , arg ) 230 return self . columns . _get_column_major ( self , arg ) 231 if isinstance ( arg , ( str , numbers . Number )) or isinstance ( arg , tuple ) : --> 232 s = self . _cols [ arg ] 233 s . name = arg 234 s . index = self . index KeyError : 'a' Method 2: Simple Columns \u00b6 Filtering Strings by Column Values \u00b6 # Filtering based on the string column print ( gdf [ gdf . strings == 'b' ]) integers strings 1 2 b Filtering Integers/Floats by Column Values \u00b6 # Filtering based on the string column print ( gdf [ gdf . integers == 2 ]) integers strings 1 2 b Method 2: Simple Rows \u00b6 Filtering by Row Numbers \u00b6 # Filter rows 0 to 2 (not inclusive of the third row with the index 2) print ( gdf [ 0 : 2 ]) integers strings 0 1 a 1 2 b Method 3: loc[rows, columns] \u00b6 # The syntax is as follows loc[rows, columns] allowing you to choose rows and columns accordingly # The example allows us to filter the first 3 rows (inclusive) of the column integers print ( gdf . loc [ 0 : 2 , [ 'integers' ]]) integers 0 1 1 2 2 3","title":"GPU DataFrames"},{"location":"machine_learning/gpu/rapids_cudf/#rapids-cudf","text":"","title":"RAPIDS cuDF"},{"location":"machine_learning/gpu/rapids_cudf/#environment-setup","text":"","title":"Environment Setup"},{"location":"machine_learning/gpu/rapids_cudf/#check-version","text":"","title":"Check Version"},{"location":"machine_learning/gpu/rapids_cudf/#python-version","text":"# Check Python Version ! python -- version Python 3.6.7","title":"Python Version"},{"location":"machine_learning/gpu/rapids_cudf/#ubuntu-version","text":"# Check Ubuntu Version ! lsb_release - a No LSB modules are available. Distributor ID: Ubuntu Description: Ubuntu 18.04.2 LTS Release: 18.04 Codename: bionic","title":"Ubuntu Version"},{"location":"machine_learning/gpu/rapids_cudf/#check-cuda-version","text":"# Check CUDA/cuDNN Version ! nvcc - V && which nvcc nvcc : NVIDIA ( R ) Cuda compiler driver Copyright ( c ) 2005 - 2018 NVIDIA Corporation Built on Sat_Aug_25_21 : 08 : 01 _CDT_2018 Cuda compilation tools , release 10.0 , V10 . 0.130 /usr/local/cuda/bin/ nvcc","title":"Check CUDA Version"},{"location":"machine_learning/gpu/rapids_cudf/#check-gpu-version","text":"# Check GPU ! nvidia - smi Mon May 13 09:31:40 2019 +-----------------------------------------------------------------------------+ | NVIDIA - SMI 418 . 56 Driver Version: 410 . 79 CUDA Version: 10 . 0 | | -------------------------------+----------------------+----------------------+ | GPU Name Persistence - M| Bus - Id Disp . A | Volatile Uncorr . ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory - Usage | GPU - Util Compute M . | |=============================== + ====================== + ======================| | 0 Tesla T4 Off | 00000000:00:04 . 0 Off | 0 | | N/A 67C P8 17W / 70W | 0MiB / 15079MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+","title":"Check GPU Version"},{"location":"machine_learning/gpu/rapids_cudf/#check-gpu-if-youve-right-version-t4","text":"Many thanks to NVIDIA team for this snippet of code to automatically set up everything. import pynvml pynvml . nvmlInit () handle = pynvml . nvmlDeviceGetHandleByIndex ( 0 ) device_name = pynvml . nvmlDeviceGetName ( handle ) if device_name != b 'Tesla T4' : raise Exception ( \"\"\" Unfortunately this instance does not have a T4 GPU. Please make sure you've configured Colab to request a GPU instance type. Sometimes Colab allocates a Tesla K80 instead of a T4. Resetting the instance. If you get a K80 GPU, try Runtime -> Reset all runtimes... \"\"\" ) else : print ( 'Woo! You got the right kind of GPU!' ) Woo! You got the right kind of GPU!","title":"Check GPU if You've Right Version (T4)"},{"location":"machine_learning/gpu/rapids_cudf/#installation-of-cudfcuml","text":"Many thanks to NVIDIA team for this snippet of code to automatically set up everything. # intall miniconda ! wget - c https : // repo . continuum . io / miniconda / Miniconda3 - 4.5 . 4 - Linux - x86_64 . sh ! chmod + x Miniconda3 - 4.5 . 4 - Linux - x86_64 . sh ! bash ./ Miniconda3 - 4.5 . 4 - Linux - x86_64 . sh - b - f - p / usr / local # install RAPIDS packages ! conda install - q - y -- prefix / usr / local - c conda - forge \\ - c rapidsai - nightly / label / cuda10 . 0 - c nvidia / label / cuda10 . 0 \\ cudf cuml # set environment vars import sys , os , shutil sys . path . append ( '/usr/local/lib/python3.6/site-packages/' ) os . environ [ 'NUMBAPRO_NVVM' ] = '/usr/local/cuda/nvvm/lib64/libnvvm.so' os . environ [ 'NUMBAPRO_LIBDEVICE' ] = '/usr/local/cuda/nvvm/libdevice/' # copy .so files to current working dir for fn in [ 'libcudf.so' , 'librmm.so' ]: shutil . copy ( '/usr/local/lib/' + fn , os . getcwd ()) -- 2019 - 05 - 13 09 : 31 : 42 -- https : // repo . continuum . io / miniconda / Miniconda3 - 4.5 . 4 - Linux - x86_64 . sh Resolving repo . continuum . io ( repo . continuum . io ) ... 104.18 . 201.79 , 104.18 . 200.79 , 2606 : 4700 :: 6812 : c84f , ... Connecting to repo . continuum . io ( repo . continuum . io ) | 104.18 . 201.79 | : 443. .. connected . HTTP request sent , awaiting response ... 200 OK Length : 58468498 ( 56 M ) [ application / x - sh ] Saving to : \u2018 Miniconda3 - 4.5 . 4 - Linux - x86_64 . sh \u2019 Miniconda3 - 4.5 . 4 - Li 100 % [ ===================> ] 55.76 M 64.9 MB / s in 0.9 s 2019 - 05 - 13 09 : 31 : 48 ( 64.9 MB / s ) - \u2018 Miniconda3 - 4.5 . 4 - Linux - x86_64 . sh \u2019 saved [ 58468498 / 58468498 ] PREFIX =/ usr / local installing : python - 3.6 . 5 - hc3d631a_2 ... Python 3.6 . 5 :: Anaconda , Inc . installing : ca - certificates - 2018.03 . 07 - 0 ... installing : conda - env - 2.6 . 0 - h36134e3_1 ... installing : libgcc - ng - 7.2 . 0 - hdf63c60_3 ... installing : libstdcxx - ng - 7.2 . 0 - hdf63c60_3 ... installing : libffi - 3.2 . 1 - hd88cf55_4 ... installing : ncurses - 6.1 - hf484d3e_0 ... installing : openssl - 1.0 . 2 o - h20670df_0 ... installing : tk - 8.6 . 7 - hc745277_3 ... installing : xz - 5.2 . 4 - h14c3975_4 ... installing : yaml - 0.1 . 7 - had09818_2 ... installing : zlib - 1.2 . 11 - ha838bed_2 ... installing : libedit - 3.1 . 20170329 - h6b74fdf_2 ... installing : readline - 7.0 - ha6073c6_4 ... installing : sqlite - 3.23 . 1 - he433501_0 ... installing : asn1crypto - 0.24 . 0 - py36_0 ... installing : certifi - 2018.4 . 16 - py36_0 ... installing : chardet - 3.0 . 4 - py36h0f667ec_1 ... installing : idna - 2.6 - py36h82fb2a8_1 ... installing : pycosat - 0.6 . 3 - py36h0a5515d_0 ... installing : pycparser - 2.18 - py36hf9f622e_1 ... installing : pysocks - 1.6 . 8 - py36_0 ... installing : ruamel_yaml - 0.15 . 37 - py36h14c3975_2 ... installing : six - 1.11 . 0 - py36h372c433_1 ... installing : cffi - 1.11 . 5 - py36h9745a5d_0 ... installing : setuptools - 39.2 . 0 - py36_0 ... installing : cryptography - 2.2 . 2 - py36h14c3975_0 ... installing : wheel - 0.31 . 1 - py36_0 ... installing : pip - 10.0 . 1 - py36_0 ... installing : pyopenssl - 18.0 . 0 - py36_0 ... installing : urllib3 - 1.22 - py36hbe7ace6_0 ... installing : requests - 2.18 . 4 - py36he2e5f8d_1 ... installing : conda - 4.5 . 4 - py36_0 ... installation finished . WARNING : You currently have a PYTHONPATH environment variable set . This may cause unexpected behavior when running the Python interpreter in Miniconda3 . For best results , please verify that your PYTHONPATH only points to directories of packages that are compatible with the Python interpreter in Miniconda3 : / usr / local Solving environment : ... working ... done ## Package Plan ## environment location : / usr / local added / updated specs : - cudf - cuml The following packages will be downloaded : package | build ---------------------------|----------------- icu - 58.2 | hf484d3e_1000 22.6 MB conda - forge libgcc - ng - 8.2 . 0 | hdf63c60_1 7.6 MB sqlite - 3.26 . 0 | h67949de_1001 1.9 MB conda - forge python - 3.6 . 7 | h381d211_1004 34.5 MB conda - forge numpy - 1.16 . 3 | py36he5ce36f_0 4.3 MB conda - forge cudatoolkit - 10.0 . 130 | 0 380.0 MB libcudf - 0.7 . 0. dev0 | cuda10 . 0 _1567 21.0 MB rapidsai - nightly / label / cuda10 . 0 parquet - cpp - 1.5 . 1 | 4 3 KB conda - forge arrow - cpp - 0.12 . 1 | py36h0e61e49_0 6.9 MB conda - forge liblapack - 3.8 . 0 | 9 _openblas 6 KB conda - forge libstdcxx - ng - 8.2 . 0 | hdf63c60_1 2.9 MB libblas - 3.8 . 0 | 9 _openblas 6 KB conda - forge openssl - 1.1 . 1 b | h14c3975_1 4.0 MB conda - forge cython - 0.29 . 7 | py36he1b5a44_0 2.2 MB conda - forge cryptography - 2.6 . 1 | py36h72c5cf5_0 606 KB conda - forge certifi - 2019.3 . 9 | py36_0 149 KB conda - forge ca - certificates - 2019.3 . 9 | hecc5488_0 146 KB conda - forge tk - 8.6 . 9 | h84994c4_1001 3.2 MB conda - forge libcuml - 0.8 . 0 a | cuda10 . 0 _610 24.4 MB rapidsai - nightly / label / cuda10 . 0 bzip2 - 1.0 . 6 | h14c3975_1002 415 KB conda - forge cuml - 0.8 . 0 a | cuda10 . 0 _py36_610 3.0 MB rapidsai - nightly / label / cuda10 . 0 python - dateutil - 2.8 . 0 | py_0 219 KB conda - forge conda - 4.6 . 14 | py36_0 2.1 MB conda - forge openblas - 0.3 . 6 | h6e990d7_2 15.8 MB conda - forge thrift - cpp - 0.12 . 0 | h0a07b25_1002 2.4 MB conda - forge rmm - 0.7 . 0. dev0 | py36_54 14 KB rapidsai - nightly / label / cuda10 . 0 pyarrow - 0.12 . 1 | py36hbbcf98d_0 2.2 MB conda - forge libprotobuf - 3.6 . 1 | hdbcaa40_1001 4.0 MB conda - forge nvstrings - 0.7 . 0. dev0 | py36_143 88 KB rapidsai - nightly / label / cuda10 . 0 librmm - 0.7 . 0. dev0 | cuda10 . 0 _54 39 KB rapidsai - nightly / label / cuda10 . 0 libcblas - 3.8 . 0 | 9 _openblas 6 KB conda - forge libgfortran - ng - 7.3 . 0 | hdf63c60_0 1.3 MB libnvstrings - 0.7 . 0. dev0 | cuda10 . 0 _132 9.3 MB rapidsai - nightly / label / cuda10 . 0 llvmlite - 0.28 . 0 | py36hdbcaa40_0 20.2 MB conda - forge cudf - 0.7 . 0. dev0 | py36_1568 2.7 MB rapidsai - nightly / label / cuda10 . 0 pytz - 2019.1 | py_0 227 KB conda - forge numba - 0.43 . 1 | py36hf2d7682_0 2.9 MB conda - forge boost - cpp - 1.68 . 0 | h11c811c_1000 20.5 MB conda - forge pandas - 0.24 . 2 | py36hf484d3e_0 11.1 MB conda - forge libcumlmg - 0.0 . 0. dev0 | cuda10 . 0 _373 955 KB nvidia / label / cuda10 . 0 ------------------------------------------------------------ Total : 615.7 MB The following NEW packages will be INSTALLED : arrow - cpp : 0.12 . 1 - py36h0e61e49_0 conda - forge boost - cpp : 1.68 . 0 - h11c811c_1000 conda - forge bzip2 : 1.0 . 6 - h14c3975_1002 conda - forge cudatoolkit : 10.0 . 130 - 0 cudf : 0.7 . 0. dev0 - py36_1568 rapidsai - nightly / label / cuda10 . 0 cuml : 0.8 . 0 a - cuda10 . 0 _py36_610 rapidsai - nightly / label / cuda10 . 0 cython : 0.29 . 7 - py36he1b5a44_0 conda - forge icu : 58.2 - hf484d3e_1000 conda - forge libblas : 3.8 . 0 - 9 _openblas conda - forge libcblas : 3.8 . 0 - 9 _openblas conda - forge libcudf : 0.7 . 0. dev0 - cuda10 . 0 _1567 rapidsai - nightly / label / cuda10 . 0 libcuml : 0.8 . 0 a - cuda10 . 0 _610 rapidsai - nightly / label / cuda10 . 0 libcumlmg : 0.0 . 0. dev0 - cuda10 . 0 _373 nvidia / label / cuda10 . 0 libgfortran - ng : 7.3 . 0 - hdf63c60_0 liblapack : 3.8 . 0 - 9 _openblas conda - forge libnvstrings : 0.7 . 0. dev0 - cuda10 . 0 _132 rapidsai - nightly / label / cuda10 . 0 libprotobuf : 3.6 . 1 - hdbcaa40_1001 conda - forge librmm : 0.7 . 0. dev0 - cuda10 . 0 _54 rapidsai - nightly / label / cuda10 . 0 llvmlite : 0.28 . 0 - py36hdbcaa40_0 conda - forge numba : 0.43 . 1 - py36hf2d7682_0 conda - forge numpy : 1.16 . 3 - py36he5ce36f_0 conda - forge nvstrings : 0.7 . 0. dev0 - py36_143 rapidsai - nightly / label / cuda10 . 0 openblas : 0.3 . 6 - h6e990d7_2 conda - forge pandas : 0.24 . 2 - py36hf484d3e_0 conda - forge parquet - cpp : 1.5 . 1 - 4 conda - forge pyarrow : 0.12 . 1 - py36hbbcf98d_0 conda - forge python - dateutil : 2.8 . 0 - py_0 conda - forge pytz : 2019.1 - py_0 conda - forge rmm : 0.7 . 0. dev0 - py36_54 rapidsai - nightly / label / cuda10 . 0 thrift - cpp : 0.12 . 0 - h0a07b25_1002 conda - forge The following packages will be UPDATED : ca - certificates : 2018.03 . 07 - 0 --> 2019.3 . 9 - hecc5488_0 conda - forge certifi : 2018.4 . 16 - py36_0 --> 2019.3 . 9 - py36_0 conda - forge conda : 4.5 . 4 - py36_0 --> 4.6 . 14 - py36_0 conda - forge cryptography : 2.2 . 2 - py36h14c3975_0 --> 2.6 . 1 - py36h72c5cf5_0 conda - forge libgcc - ng : 7.2 . 0 - hdf63c60_3 --> 8.2 . 0 - hdf63c60_1 libstdcxx - ng : 7.2 . 0 - hdf63c60_3 --> 8.2 . 0 - hdf63c60_1 openssl : 1.0 . 2 o - h20670df_0 --> 1.1 . 1 b - h14c3975_1 conda - forge python : 3.6 . 5 - hc3d631a_2 --> 3.6 . 7 - h381d211_1004 conda - forge sqlite : 3.23 . 1 - he433501_0 --> 3.26 . 0 - h67949de_1001 conda - forge tk : 8.6 . 7 - hc745277_3 --> 8.6 . 9 - h84994c4_1001 conda - forge Preparing transaction : ... working ... done Verifying transaction : ... working ... done Executing transaction : ... working ... done","title":"Installation of cuDF/cuML"},{"location":"machine_learning/gpu/rapids_cudf/#critical-imports","text":"# Critical imports import nvstrings , nvcategory , cudf import cuml import os import numpy as np import pandas as pd","title":"Critical Imports"},{"location":"machine_learning/gpu/rapids_cudf/#creating","text":"","title":"Creating"},{"location":"machine_learning/gpu/rapids_cudf/#create-a-series-of-integers","text":"gdf = cudf . Series ([ 1 , 2 , 3 , 4 , 5 , 6 ]) print ( gdf ) print ( type ( gdf )) 0 1 1 2 2 3 3 4 4 5 5 6 dtype: int64 <class 'cudf.dataframe.series.Series'>","title":"Create a Series of integers"},{"location":"machine_learning/gpu/rapids_cudf/#create-a-series-of-floats","text":"gdf = cudf . Series ([ 1. , 2. , 3. , 4. , 5. , 6. ]) print ( gdf ) 0 1.0 1 2.0 2 3.0 3 4.0 4 5.0 5 6.0 dtype: float64","title":"Create a Series of floats"},{"location":"machine_learning/gpu/rapids_cudf/#create-a-series-of-strings","text":"gdf = cudf . Series ([ 'a' , 'b' , 'c' ]) print ( gdf ) 0 a 1 b 2 c dtype: object","title":"Create a  Series of strings"},{"location":"machine_learning/gpu/rapids_cudf/#create-3-column-dataframe","text":"Consisting of dates, integers and floats # Import import datetime as dt # Using a list of tuples # Each element in the list represents a category # The first element of the tuple is the category's name # The second element of the tuple is a list of the values in that category gdf = cudf . DataFrame ([ # Create 10 busindates ess from 1st January 2019 via pandas ( 'dates' , pd . date_range ( '1/1/2019' , periods = 10 , freq = 'B' )), # Integers ( 'integers' , [ i for i in range ( 10 )]), # Floats ( 'floats' , [ float ( i ) for i in range ( 10 )]) ]) # Print dataframe print ( gdf ) dates integers floats 0 2019-01-01T00:00:00.000 0 0.0 1 2019-01-02T00:00:00.000 1 1.0 2 2019-01-03T00:00:00.000 2 2.0 3 2019-01-04T00:00:00.000 3 3.0 4 2019-01-07T00:00:00.000 4 4.0 5 2019-01-08T00:00:00.000 5 5.0 6 2019-01-09T00:00:00.000 6 6.0 7 2019-01-10T00:00:00.000 7 7.0 8 2019-01-11T00:00:00.000 8 8.0 9 2019-01-14T00:00:00.000 9 9.0","title":"Create 3 column DataFrame"},{"location":"machine_learning/gpu/rapids_cudf/#create-2-column-dataframe","text":"Consisting of integers and string category # Using a list of tuples # Each element in the list represents a category # The first element of the tuple is the category's name # The second element of the tuple is a list of the values in that category gdf = cudf . DataFrame ([ ( 'integers' , [ 1 , 2 , 3 , 4 ]), ( 'string' , [ 'a' , 'b' , 'c' , 'd' ]) ]) print ( gdf ) integers string 0 1 a 1 2 b 2 3 c 3 4 d","title":"Create 2 column Dataframe"},{"location":"machine_learning/gpu/rapids_cudf/#create-a-2-column-dataframe-with-pandas-bridge","text":"Consisting of integers and string category For all string columns, you must convert them to type category for filtering functions to work intuitively (for now) # Create pandas dataframe pandas_df = pd . DataFrame ({ 'integers' : [ 1 , 2 , 3 , 4 ], 'strings' : [ 'a' , 'b' , 'c' , 'd' ] }) # Convert string column to category format pandas_df [ 'strings' ] = pandas_df [ 'strings' ] . astype ( 'category' ) # Bridge from pandas to cudf gdf = cudf . DataFrame . from_pandas ( pandas_df ) # Print dataframe print ( gdf ) integers strings 0 1 a 1 2 b 2 3 c 3 4 d","title":"Create a 2 Column  Dataframe with Pandas Bridge"},{"location":"machine_learning/gpu/rapids_cudf/#viewing","text":"","title":"Viewing"},{"location":"machine_learning/gpu/rapids_cudf/#printing-column-names","text":"gdf . columns Index(['integers', 'strings'], dtype='object')","title":"Printing Column Names"},{"location":"machine_learning/gpu/rapids_cudf/#viewing-top-of-dataframe","text":"num_of_rows_to_view = 2 print ( gdf . head ( num_of_rows_to_view )) integers strings 0 1 a 1 2 b","title":"Viewing Top of DataFrame"},{"location":"machine_learning/gpu/rapids_cudf/#viewing-bottom-of-dataframe","text":"num_of_rows_to_view = 3 print ( gdf . tail ( num_of_rows_to_view )) integers strings 1 2 b 2 3 c 3 4 d","title":"Viewing Bottom of DataFrame"},{"location":"machine_learning/gpu/rapids_cudf/#filtering","text":"","title":"Filtering"},{"location":"machine_learning/gpu/rapids_cudf/#method-1-query","text":"","title":"Method 1: Query"},{"location":"machine_learning/gpu/rapids_cudf/#filtering-integersfloats-by-column-values","text":"This only works for floats and integers, not for strings print ( gdf . query ( 'integers == 1' )) integers strings 0 1 a","title":"Filtering Integers/Floats by Column Values"},{"location":"machine_learning/gpu/rapids_cudf/#filtering-strings-by-column-values","text":"This only works for floats and integers, not for strings so this will return an error! print ( gdf . query ( 'strings == a' )) --------------------------------------------------------------------------- KeyError Traceback ( most recent call last ) < ipython - input - 33 - 5 cfd0345d51c > in < module > () ----> 1 print ( gdf . query ( 'strings == a' )) / usr / local / lib / python3 .6 / site - packages / cudf / dataframe / dataframe . py in query ( self , expr , local_dict ) 1905 } 1906 # Run query -> 1907 boolmask = queryutils . query_execute ( self , expr , callenv ) 1908 1909 selected = Series ( boolmask ) / usr / local / lib / python3 .6 / site - packages / cudf / utils / queryutils . py in query_execute ( df , expr , callenv ) 215 envargs . append ( val ) 216 # prepare col args --> 217 colarrays = [ df[col ] . to_gpu_array () for col in compiled [ 'colnames' ] ] 218 # allocate output buffer 219 nrows = len ( df ) / usr / local / lib / python3 .6 / site - packages / cudf / utils / queryutils . py in < listcomp > ( .0 ) 215 envargs . append ( val ) 216 # prepare col args --> 217 colarrays = [ df[col ] . to_gpu_array () for col in compiled [ 'colnames' ] ] 218 # allocate output buffer 219 nrows = len ( df ) / usr / local / lib / python3 .6 / site - packages / cudf / dataframe / dataframe . py in __getitem__ ( self , arg ) 230 return self . columns . _get_column_major ( self , arg ) 231 if isinstance ( arg , ( str , numbers . Number )) or isinstance ( arg , tuple ) : --> 232 s = self . _cols [ arg ] 233 s . name = arg 234 s . index = self . index KeyError : 'a'","title":"Filtering Strings by Column Values"},{"location":"machine_learning/gpu/rapids_cudf/#method-2-simple-columns","text":"","title":"Method 2:  Simple Columns"},{"location":"machine_learning/gpu/rapids_cudf/#filtering-strings-by-column-values_1","text":"# Filtering based on the string column print ( gdf [ gdf . strings == 'b' ]) integers strings 1 2 b","title":"Filtering Strings by Column Values"},{"location":"machine_learning/gpu/rapids_cudf/#filtering-integersfloats-by-column-values_1","text":"# Filtering based on the string column print ( gdf [ gdf . integers == 2 ]) integers strings 1 2 b","title":"Filtering Integers/Floats by Column Values"},{"location":"machine_learning/gpu/rapids_cudf/#method-2-simple-rows","text":"","title":"Method 2:  Simple Rows"},{"location":"machine_learning/gpu/rapids_cudf/#filtering-by-row-numbers","text":"# Filter rows 0 to 2 (not inclusive of the third row with the index 2) print ( gdf [ 0 : 2 ]) integers strings 0 1 a 1 2 b","title":"Filtering by Row Numbers"},{"location":"machine_learning/gpu/rapids_cudf/#method-3-locrows-columns","text":"# The syntax is as follows loc[rows, columns] allowing you to choose rows and columns accordingly # The example allows us to filter the first 3 rows (inclusive) of the column integers print ( gdf . loc [ 0 : 2 , [ 'integers' ]]) integers 0 1 1 2 2 3","title":"Method 3:  loc[rows, columns]"},{"location":"news/ammi_facebook_google_recap_2018_11_21/","text":"AMMI (AIMS) Recap Supported by Facebook and Google \u00b6 To the future of AI in and beyond Africa \u00b6 Helped to teach these students from The African Masters of Machine Intelligence (AMMI) by The African Institute for Mathematical Sciences (AIMS) with Alfredo Canziani. A unique pedagogy is required for deep learning beginning with intuition, culminating in just enough math, and ending with programming. This approached has worked when I delivered my deep learning course to over 3000 students over 120 countries. And I continue to champion this pedagogy. I\u2019m very pleased that I share the same passion with Alfedo when he delivered his interactive lectures. Importantly, I see great potential when I personally interacted with students here. I see future deep learning researchers and applied machine intelligence experts who have the potential to make groundbreaking changes in the fields of healthcare, agriculture and education throughout Africa. With the right education, mentorship and opportunities, they would be pioneers. Simply put, deep learning talent is underexplored in this continent. I am thankful to Alfredo Canziani who has been my best pal and inspirational guy while I was there, Moustapha Cisse for creating these opportunities for young African students to thrive, #Facebook and #Google for supporting this program, and Pedro Antonio Mart\u00ednez Mediano and Marc Deisenroff. Coincidentally, I would also like to thank Yoshua Bengio for holding ICLR in Africa in 2020 for the first time that he announced recently. Finally, really appreciate how #PyTorch has made programming neural networks more approachable and it was made possible with the amazing PyTorch community sparked by Soumith Chintala. Speaking about community, I got to contribute to the deep learning community with the help from supportive groups and individuals from the global deep learning community. Likewise, we should continually help these young Africans like any other. To the future of AI in and beyond Africa. Looking forward to help Moustapha in any way I can to nurture AI talent \ud83d\ude42 I'm being a fanboy here, but Yann Lecun liked my post on Facebook ! Cheers, Ritchie","title":"AMMI (AIMS) supported by Facebook and Google, November 2018"},{"location":"news/ammi_facebook_google_recap_2018_11_21/#ammi-aims-recap-supported-by-facebook-and-google","text":"","title":"AMMI (AIMS) Recap Supported by Facebook and Google"},{"location":"news/ammi_facebook_google_recap_2018_11_21/#to-the-future-of-ai-in-and-beyond-africa","text":"Helped to teach these students from The African Masters of Machine Intelligence (AMMI) by The African Institute for Mathematical Sciences (AIMS) with Alfredo Canziani. A unique pedagogy is required for deep learning beginning with intuition, culminating in just enough math, and ending with programming. This approached has worked when I delivered my deep learning course to over 3000 students over 120 countries. And I continue to champion this pedagogy. I\u2019m very pleased that I share the same passion with Alfedo when he delivered his interactive lectures. Importantly, I see great potential when I personally interacted with students here. I see future deep learning researchers and applied machine intelligence experts who have the potential to make groundbreaking changes in the fields of healthcare, agriculture and education throughout Africa. With the right education, mentorship and opportunities, they would be pioneers. Simply put, deep learning talent is underexplored in this continent. I am thankful to Alfredo Canziani who has been my best pal and inspirational guy while I was there, Moustapha Cisse for creating these opportunities for young African students to thrive, #Facebook and #Google for supporting this program, and Pedro Antonio Mart\u00ednez Mediano and Marc Deisenroff. Coincidentally, I would also like to thank Yoshua Bengio for holding ICLR in Africa in 2020 for the first time that he announced recently. Finally, really appreciate how #PyTorch has made programming neural networks more approachable and it was made possible with the amazing PyTorch community sparked by Soumith Chintala. Speaking about community, I got to contribute to the deep learning community with the help from supportive groups and individuals from the global deep learning community. Likewise, we should continually help these young Africans like any other. To the future of AI in and beyond Africa. Looking forward to help Moustapha in any way I can to nurture AI talent \ud83d\ude42 I'm being a fanboy here, but Yann Lecun liked my post on Facebook ! Cheers, Ritchie","title":"To the future of AI in and beyond Africa"},{"location":"news/dbs_gpu_rapids_nvidia_ensemble_frac_diff/","text":"GPU Fractional Differencing with NVIDIA RAPIDS \u00b6 Introduction to Deep Learning \u00b6 I created and led a workshop on accelerating fractional differencing with NVIDIA's RAPIDS in DBS. This is our article on medium with NVIDIA on our work. You can check out our presentation and open-source code on Github. Thanks to our industry partners DBS, NVIDIA and ensemblecap.ai Cheers, Ritchie","title":"Fractional Differencing with GPU (GFD), DBS and NVIDIA, September 2019"},{"location":"news/dbs_gpu_rapids_nvidia_ensemble_frac_diff/#gpu-fractional-differencing-with-nvidia-rapids","text":"","title":"GPU Fractional Differencing with NVIDIA RAPIDS"},{"location":"news/dbs_gpu_rapids_nvidia_ensemble_frac_diff/#introduction-to-deep-learning","text":"I created and led a workshop on accelerating fractional differencing with NVIDIA's RAPIDS in DBS. This is our article on medium with NVIDIA on our work. You can check out our presentation and open-source code on Github. Thanks to our industry partners DBS, NVIDIA and ensemblecap.ai Cheers, Ritchie","title":"Introduction to Deep Learning"},{"location":"news/deep_learning_wizard_1y_2018_06_01/","text":"Featured on PyTorch Website \u00b6 PyTorch a Year Later \u00b6 We are featured on PyTorch website's post I used PyTorch from day 1 and I fell in love with it. One year later, PyTorch's community has grown tremendously. I thank Facebook AI Research (FAIR) and everyone who contributed to building one of the best deep learning framework. In particular Soumith Chintala from FAIR where he is continually fixing bugs and making PyTorch better everyday. A year later, Deep Learning Wizard has taught more than 2000 students across 100+ countries, and we continue to strive to release new and updated content to enable anyone to build customized deep learning systems to solve many problems. And I'm trying to push for PyTorch workshops at NVIDIA Deep Learning Institute in Singapore and across the world, so stay tuned! A big shoutout for Alfredo Canziani who's currently a postdoc at NYU co-teaching with Yann LeCun on deep learning classes and he's releasing another technical course on PyTorch that is going to be awesome. To more great years ahead for PyTorch Cheers, Ritchie Ng","title":"Featured on PyTorch Website 2018"},{"location":"news/deep_learning_wizard_1y_2018_06_01/#featured-on-pytorch-website","text":"","title":"Featured on PyTorch Website"},{"location":"news/deep_learning_wizard_1y_2018_06_01/#pytorch-a-year-later","text":"We are featured on PyTorch website's post I used PyTorch from day 1 and I fell in love with it. One year later, PyTorch's community has grown tremendously. I thank Facebook AI Research (FAIR) and everyone who contributed to building one of the best deep learning framework. In particular Soumith Chintala from FAIR where he is continually fixing bugs and making PyTorch better everyday. A year later, Deep Learning Wizard has taught more than 2000 students across 100+ countries, and we continue to strive to release new and updated content to enable anyone to build customized deep learning systems to solve many problems. And I'm trying to push for PyTorch workshops at NVIDIA Deep Learning Institute in Singapore and across the world, so stay tuned! A big shoutout for Alfredo Canziani who's currently a postdoc at NYU co-teaching with Yann LeCun on deep learning classes and he's releasing another technical course on PyTorch that is going to be awesome. To more great years ahead for PyTorch Cheers, Ritchie Ng","title":"PyTorch a Year Later"},{"location":"news/deep_learning_wizard_nvidia_inception_2018_05_01/","text":"We Are an NVIDIA Inception Partner \u00b6 We did it! \u00b6 After almost a year, we are an NVIDIA Inception Partner now! \"NVIDIA Inception Partner Deep Learning Wizard is proudly an NVIDIA Inception Partner, empowering people across the world to leverage on deep learning via open-source programming languages and frameworks to solve problems. Cheers, Ritchie Ng","title":"NVIDIA Inception Partner Status, Singapore, May 2017"},{"location":"news/deep_learning_wizard_nvidia_inception_2018_05_01/#we-are-an-nvidia-inception-partner","text":"","title":"We Are an NVIDIA Inception Partner"},{"location":"news/deep_learning_wizard_nvidia_inception_2018_05_01/#we-did-it","text":"After almost a year, we are an NVIDIA Inception Partner now! \"NVIDIA Inception Partner Deep Learning Wizard is proudly an NVIDIA Inception Partner, empowering people across the world to leverage on deep learning via open-source programming languages and frameworks to solve problems. Cheers, Ritchie Ng","title":"We did it!"},{"location":"news/defence_and_science_technology_agency_dsta_nvidia_talk_2016_06/","text":"Deep Learning Demystified, Defence and Science Technology Agency (DSTA) \u00b6 Introduction to Deep Learning \u00b6 I gave an introductory talk on deep learning to more than 1000 students at BrainHack held by DSTA in June 2019 . I'm really excited for Singapore's progress in leveraging on deep learning for the defense sector. There are a lot of applications and areas of research. Special thanks to DSTA, NVIDIA and ensemblecap.ai Cheers, Ritchie","title":"Deep Learning Introduction, Defence and Science Technology Agency (DSTA) and NVIDIA, June 2019"},{"location":"news/defence_and_science_technology_agency_dsta_nvidia_talk_2016_06/#deep-learning-demystified-defence-and-science-technology-agency-dsta","text":"","title":"Deep Learning Demystified, Defence and Science Technology Agency (DSTA)"},{"location":"news/defence_and_science_technology_agency_dsta_nvidia_talk_2016_06/#introduction-to-deep-learning","text":"I gave an introductory talk on deep learning to more than 1000 students at BrainHack held by DSTA in June 2019 . I'm really excited for Singapore's progress in leveraging on deep learning for the defense sector. There are a lot of applications and areas of research. Special thanks to DSTA, NVIDIA and ensemblecap.ai Cheers, Ritchie","title":"Introduction to Deep Learning"},{"location":"news/detect_waterbone_debris_ai_for_social_good_icml_2019_06/","text":"Oral Presentation for AI for Social Good Workshop ICML \u00b6 Detecting Waterborne Debris with Sim2Real and Randomization \u00b6 We'll be presenting our work in ICML this June. Special thanks to everyone working on this project: Jie Fu (MILA, Polytechnic Montr\u00e9al); Ritchie Ng (National University of Singapore); Mirgahney Mohamed (The African Institute for Mathematical Sciences); Yi Tay (Nanyang Technological University); Kris Sankaran (Montreal Institute for Learning Algorithms); Shangbang Long (Peking University); Alfredo Canziani (New York University); Chris J Pal (MILA, Polytechnique Montr\u00e9al, Element AI); Moustapha Cisse (Google Research) Cheers, Ritchie","title":"Oral Presentation for AI for Social Good Workshop ICML, June 2019"},{"location":"news/detect_waterbone_debris_ai_for_social_good_icml_2019_06/#oral-presentation-for-ai-for-social-good-workshop-icml","text":"","title":"Oral Presentation for AI for Social Good Workshop ICML"},{"location":"news/detect_waterbone_debris_ai_for_social_good_icml_2019_06/#detecting-waterborne-debris-with-sim2real-and-randomization","text":"We'll be presenting our work in ICML this June. Special thanks to everyone working on this project: Jie Fu (MILA, Polytechnic Montr\u00e9al); Ritchie Ng (National University of Singapore); Mirgahney Mohamed (The African Institute for Mathematical Sciences); Yi Tay (Nanyang Technological University); Kris Sankaran (Montreal Institute for Learning Algorithms); Shangbang Long (Peking University); Alfredo Canziani (New York University); Chris J Pal (MILA, Polytechnique Montr\u00e9al, Element AI); Moustapha Cisse (Google Research) Cheers, Ritchie","title":"Detecting Waterborne Debris with Sim2Real and Randomization"},{"location":"news/facebook_pytorch_devcon_recap_2018_10_02/","text":"Recap of Facebook PyTorch Devcon \u00b6 Woo! \u00b6 Proud of how far PyTorch has come. Thanks Soumith Chintala, it is a great journey and it just started. And thanks to Adam Paszke too! Finally got to catch up with Soumith Chintala, Alfredo Canziani and Marek Bardo\u0144ski and found Andrej Karpathy! Back to Singapore tonight \ud83e\udd17 Here are some photos of my trip! Cheers, Ritchie","title":"Recap of Facebook PyTorch Developer Conference, San Francisco, September 2018"},{"location":"news/facebook_pytorch_devcon_recap_2018_10_02/#recap-of-facebook-pytorch-devcon","text":"","title":"Recap of Facebook PyTorch Devcon"},{"location":"news/facebook_pytorch_devcon_recap_2018_10_02/#woo","text":"Proud of how far PyTorch has come. Thanks Soumith Chintala, it is a great journey and it just started. And thanks to Adam Paszke too! Finally got to catch up with Soumith Chintala, Alfredo Canziani and Marek Bardo\u0144ski and found Andrej Karpathy! Back to Singapore tonight \ud83e\udd17 Here are some photos of my trip! Cheers, Ritchie","title":"Woo!"},{"location":"news/facebook_pytorch_developer_conference_2018_09_05/","text":"Facebook PyTorch Developer Conference \u00b6 We are heading down! \u00b6 In barely 2 short years, PyTorch (Facebook) will be hosting their first PyTorch Developer Conference in San Francisco, USA. I will be heading down thanks to Soumith Chintala for the invite and arrangements. Looking forward to meet anyone there. The PyTorch ecosystem has grown tremendously from when I first started using it. To this date, I've taught more than 3000 students worldwide in 120+ countries and every single wizard has fallen in love with it! Cheers, Ritchie Ng","title":"Facebook PyTorch Developer Conference, San Francisco, September 2018"},{"location":"news/facebook_pytorch_developer_conference_2018_09_05/#facebook-pytorch-developer-conference","text":"","title":"Facebook PyTorch Developer Conference"},{"location":"news/facebook_pytorch_developer_conference_2018_09_05/#we-are-heading-down","text":"In barely 2 short years, PyTorch (Facebook) will be hosting their first PyTorch Developer Conference in San Francisco, USA. I will be heading down thanks to Soumith Chintala for the invite and arrangements. Looking forward to meet anyone there. The PyTorch ecosystem has grown tremendously from when I first started using it. To this date, I've taught more than 3000 students worldwide in 120+ countries and every single wizard has fallen in love with it! Cheers, Ritchie Ng","title":"We are heading down!"},{"location":"news/it_youth_leader_2019_03_11/","text":"IT Youth Leader of the Year 2019 Award \u00b6 We got it! \u00b6 I am very happy to be the IT Youth Leader of the Year 2019 in Singapore for my industry and non-profit contribution in AI at Deep Learning Wizard , ensemblecap.ai, NUS, and NVIDIA! I would like to thank the professor who changed my life, believed in me and nominated me for consideration, Professor Chua Tat-Seng from NExT Centre, NUS School of Computing. Big thanks to everyone who helped for this in NExT Centre particularly Choon Meng, Tek Min, Chua Yi Hao, and Eugene Tan. Special thanks to an important friend and associate who stuck by with me all the way, Jie Fu from our endless days of programming (and scribbling equations) during your PhD years to now as a postdoc in MILA \ud83d\ude42And Ilija Ilievski from those years till now too! Big thanks to the people who believed in the future of AI and asset management and made this happen with the original team Damien Loh, Atsuo Ogaki, and Samuel Chen from ensemblecap.ai Thanks to new found close friend Alfredo Canziani who motivated me on open-source education further and supporting each other \ud83d\ude42 And thanks to Moustapha Cisse for setting up an amazing initiative in Africa with #AMMI to kickstart a future of AI by Africa and trusted your students with our teaching. And I'm on my starting journey to help as many students of your as possible by guiding them on learning like Merghaney Mohammed and Enoch Tetteh. Thanks to Soumith Chintala for spearheading #PyTorch and it's now in production running critical functions in finance (like my firm) and tech. Thanks to Marek Bardo\u0144ski where our friendship started when you were in #NVIDIA and we've supported each other ever since in our learning journey. Thanks to #NVIDIA for allowing me to lead deep learning workshops under DLI in NUS all these years. Thanks to my mentors and everyone in #PhilipYeoInitiative like Abel Ang, Seok Lin Khoo, Kiren Kumar, Claire Cheong, Chong Siak Ching, and Kai Hoe Tan Last thanks to everyone in #SingaporeComputerSociety like Jason Chen for being there when I was nervous and Howie Lau for spearheading tech from start till now. And to a senior big award winner I've a lot to learn and also from NUS: Jeffrey Tiong! Thanks to forward-looking civil servants and minister like Minister S Iswaran for gracing the event and delivering a speech on digital readiness and skills empowerment for Singaporeans. Thanks to my family for being by my side Carine Chng, Ng William, and Adabelle Ng for giving me the freedom and support to pursue whatever I wanted even from young. To the future of AI! Cheers, Ritchie","title":"IT Youth Leader of The Year 2019, March 2019"},{"location":"news/it_youth_leader_2019_03_11/#it-youth-leader-of-the-year-2019-award","text":"","title":"IT Youth Leader of the Year 2019 Award"},{"location":"news/it_youth_leader_2019_03_11/#we-got-it","text":"I am very happy to be the IT Youth Leader of the Year 2019 in Singapore for my industry and non-profit contribution in AI at Deep Learning Wizard , ensemblecap.ai, NUS, and NVIDIA! I would like to thank the professor who changed my life, believed in me and nominated me for consideration, Professor Chua Tat-Seng from NExT Centre, NUS School of Computing. Big thanks to everyone who helped for this in NExT Centre particularly Choon Meng, Tek Min, Chua Yi Hao, and Eugene Tan. Special thanks to an important friend and associate who stuck by with me all the way, Jie Fu from our endless days of programming (and scribbling equations) during your PhD years to now as a postdoc in MILA \ud83d\ude42And Ilija Ilievski from those years till now too! Big thanks to the people who believed in the future of AI and asset management and made this happen with the original team Damien Loh, Atsuo Ogaki, and Samuel Chen from ensemblecap.ai Thanks to new found close friend Alfredo Canziani who motivated me on open-source education further and supporting each other \ud83d\ude42 And thanks to Moustapha Cisse for setting up an amazing initiative in Africa with #AMMI to kickstart a future of AI by Africa and trusted your students with our teaching. And I'm on my starting journey to help as many students of your as possible by guiding them on learning like Merghaney Mohammed and Enoch Tetteh. Thanks to Soumith Chintala for spearheading #PyTorch and it's now in production running critical functions in finance (like my firm) and tech. Thanks to Marek Bardo\u0144ski where our friendship started when you were in #NVIDIA and we've supported each other ever since in our learning journey. Thanks to #NVIDIA for allowing me to lead deep learning workshops under DLI in NUS all these years. Thanks to my mentors and everyone in #PhilipYeoInitiative like Abel Ang, Seok Lin Khoo, Kiren Kumar, Claire Cheong, Chong Siak Ching, and Kai Hoe Tan Last thanks to everyone in #SingaporeComputerSociety like Jason Chen for being there when I was nervous and Howie Lau for spearheading tech from start till now. And to a senior big award winner I've a lot to learn and also from NUS: Jeffrey Tiong! Thanks to forward-looking civil servants and minister like Minister S Iswaran for gracing the event and delivering a speech on digital readiness and skills empowerment for Singaporeans. Thanks to my family for being by my side Carine Chng, Ng William, and Adabelle Ng for giving me the freedom and support to pursue whatever I wanted even from young. To the future of AI! Cheers, Ritchie","title":"We got it!"},{"location":"news/nanjing_next_nus_tsinghua_ai_finance_healthcare_2018_11_01/","text":"NExT++ AI in Finance and Healthcare Workshop 2018 \u00b6 Recap \u00b6 This is NExT++\u2019s 3 rd Workshop and this time we are looking at the applications and development of AI related technologies in the healthcare and finance verticals with the theme \u201cAI in Health and Finance\u201d Visual summary of my talk on AI and Unstructured Analytics in Fintech. Doesn't contain everything, but whatever the non-technical designers could come up with in real-time. Really amazing so kudos to them. Really thankful to the whole team in my former lab, NExT++ and a close friend, Choon Meng, for making all the arrangements. Also thanks to Tek Min, Yi Hao and everyone else. Super happy to catch up with Prof Tat-Seng Chua (KITHCT Chair Professor at the School of Computing), Prof Sun Maosong (Dean of Department of Computer Science and Technology, Tsinghua University), and Prof Dame Wendy Hall (Director of the Web Science Institute, University of Southampton). Cheers, Ritchie","title":"NExT++ AI in Healthcare and Finance, Nanjing, November 2018"},{"location":"news/nanjing_next_nus_tsinghua_ai_finance_healthcare_2018_11_01/#next-ai-in-finance-and-healthcare-workshop-2018","text":"","title":"NExT++ AI in Finance and Healthcare Workshop 2018"},{"location":"news/nanjing_next_nus_tsinghua_ai_finance_healthcare_2018_11_01/#recap","text":"This is NExT++\u2019s 3 rd Workshop and this time we are looking at the applications and development of AI related technologies in the healthcare and finance verticals with the theme \u201cAI in Health and Finance\u201d Visual summary of my talk on AI and Unstructured Analytics in Fintech. Doesn't contain everything, but whatever the non-technical designers could come up with in real-time. Really amazing so kudos to them. Really thankful to the whole team in my former lab, NExT++ and a close friend, Choon Meng, for making all the arrangements. Also thanks to Tek Min, Yi Hao and everyone else. Super happy to catch up with Prof Tat-Seng Chua (KITHCT Chair Professor at the School of Computing), Prof Sun Maosong (Dean of Department of Computer Science and Technology, Tsinghua University), and Prof Dame Wendy Hall (Director of the Web Science Institute, University of Southampton). Cheers, Ritchie","title":"Recap"},{"location":"news/news/","text":"Welcome to our Blog \u00b6 Here, we post news related to Deep Learning Wizard's releases, features and achievements Notable News \u00b6 Oral Presentation for AI for Social Good Workshop ICML, Long Beach, Los Angeles, USA, June 2019 IT Youth Leader of The Year, Singapore Computer Society (SCS), Singapore, March 2019 Foundations of Deep Learning, African Masters of Machine Intelligence (AMMI), Google & Facebook, Kigali, Rwanda, November 2018 Invited to Facebook PyTorch Developer Conference, San Francisco, USA, September 2018 Led NUS-MIT-NUHS Datathon NVIDIA Image Recognition Workshop, Singapore, July 2018 Featured on PyTorch Website, January 2018 Led NVIDIA Self-Driving Cars and Healthcare Talk, Singapore, June 2017 Clinched NVIDIA Inception Partner Status, Singapore, May 2017","title":"Welcome"},{"location":"news/news/#welcome-to-our-blog","text":"Here, we post news related to Deep Learning Wizard's releases, features and achievements","title":"Welcome to our Blog"},{"location":"news/news/#notable-news","text":"Oral Presentation for AI for Social Good Workshop ICML, Long Beach, Los Angeles, USA, June 2019 IT Youth Leader of The Year, Singapore Computer Society (SCS), Singapore, March 2019 Foundations of Deep Learning, African Masters of Machine Intelligence (AMMI), Google & Facebook, Kigali, Rwanda, November 2018 Invited to Facebook PyTorch Developer Conference, San Francisco, USA, September 2018 Led NUS-MIT-NUHS Datathon NVIDIA Image Recognition Workshop, Singapore, July 2018 Featured on PyTorch Website, January 2018 Led NVIDIA Self-Driving Cars and Healthcare Talk, Singapore, June 2017 Clinched NVIDIA Inception Partner Status, Singapore, May 2017","title":"Notable News"},{"location":"news/nvidia_nus_mit_datathon_2018_07_05/","text":"NVIDIA Workshop at NUS-MIT-NUHS Datathon \u00b6 Image Recognition Workshop by Ritchie Ng \u00b6 The NVIDIA Deep Learning Institute (DLI) trains developers, data scientists, and researchers on how to use artificial intelligence to solve real-world problems across a wide range of domains. In the deep learning courses, you\u2019ll learn how to train, optimize, and deploy neural networks using the latest tools, frameworks, and techniques for deep learning. In \u201cImage Classification with DIGITS\u201d mini-course, you will learn how to train a deep neural network to recognize handwritten digits by loading image data into a training environment, choosing and training a network, testing with new data, and iterating to improve performance. Link to the NUS-MIT-NUHS Datathon workshop .","title":"NUS-MIT-NUHS NVIDIA Image Recognition Workshop, Singapore, July 2018"},{"location":"news/nvidia_nus_mit_datathon_2018_07_05/#nvidia-workshop-at-nus-mit-nuhs-datathon","text":"","title":"NVIDIA Workshop at NUS-MIT-NUHS Datathon"},{"location":"news/nvidia_nus_mit_datathon_2018_07_05/#image-recognition-workshop-by-ritchie-ng","text":"The NVIDIA Deep Learning Institute (DLI) trains developers, data scientists, and researchers on how to use artificial intelligence to solve real-world problems across a wide range of domains. In the deep learning courses, you\u2019ll learn how to train, optimize, and deploy neural networks using the latest tools, frameworks, and techniques for deep learning. In \u201cImage Classification with DIGITS\u201d mini-course, you will learn how to train a deep neural network to recognize handwritten digits by loading image data into a training environment, choosing and training a network, testing with new data, and iterating to improve performance. Link to the NUS-MIT-NUHS Datathon workshop .","title":"Image Recognition Workshop by Ritchie Ng"},{"location":"news/nvidia_self_driving_cars_talk_2017_06_21/","text":"NVIDIA Self-Driving Cars and Healthcare Workshop \u00b6 Hosted by Ritchie Ng \u00b6 A talk by Marek Bardo\u0144ski, Senior Deep Learning Research Engineer, NVIDIA Switzerland. This is hosted by Ritchie Ng, Deep Learning Researcher, NExT Search Centre, NUS. We will be touching on cutting-edge applications of deep learning for self-driving cars and medical diagnostics. Also there will be a tutorial followed by networking with deep learning researchers from NUS and NVIDIA. Details: Wednesday, June 21 st 1:00 PM to 3:30 PM The Hangar by NUS Enterprise 21 Heng Mui Keng Terrace, Singapore 119613","title":"NVIDIA Self Driving Cars & Healthcare Talk, Singapore, June 2017"},{"location":"news/nvidia_self_driving_cars_talk_2017_06_21/#nvidia-self-driving-cars-and-healthcare-workshop","text":"","title":"NVIDIA Self-Driving Cars and Healthcare Workshop"},{"location":"news/nvidia_self_driving_cars_talk_2017_06_21/#hosted-by-ritchie-ng","text":"A talk by Marek Bardo\u0144ski, Senior Deep Learning Research Engineer, NVIDIA Switzerland. This is hosted by Ritchie Ng, Deep Learning Researcher, NExT Search Centre, NUS. We will be touching on cutting-edge applications of deep learning for self-driving cars and medical diagnostics. Also there will be a tutorial followed by networking with deep learning researchers from NUS and NVIDIA. Details: Wednesday, June 21 st 1:00 PM to 3:30 PM The Hangar by NUS Enterprise 21 Heng Mui Keng Terrace, Singapore 119613","title":"Hosted by Ritchie Ng"},{"location":"ood/intro/","text":"Out-of-distribution Data \u00b6 This is a persistent and critical production issue present in any machine learning and deep learning systems, no matter how good the models were trained. A single out-of-distribution sample data fed into a well-trained model can result in a few problematic outcomes, and in production-level systems they become fat-tailed critical risks that have outsized consequences. Problematic consequences of out-of-distribution data False positive & False Negative : prediction does not match the ground truth. Unreliable True positive & True Negative : prediction matches the ground truth but due to samples being out-of-distribution, the performance of these predictions becomes very erratic.","title":"Introduction"},{"location":"ood/intro/#out-of-distribution-data","text":"This is a persistent and critical production issue present in any machine learning and deep learning systems, no matter how good the models were trained. A single out-of-distribution sample data fed into a well-trained model can result in a few problematic outcomes, and in production-level systems they become fat-tailed critical risks that have outsized consequences. Problematic consequences of out-of-distribution data False positive & False Negative : prediction does not match the ground truth. Unreliable True positive & True Negative : prediction matches the ground truth but due to samples being out-of-distribution, the performance of these predictions becomes very erratic.","title":"Out-of-distribution Data"},{"location":"programming/intro/","text":"Programming \u00b6 We'll be covering C++, Python, Bash and more for end-to-end AI deployments. Packages and Languages you will learn to use C++ Python Bash Matplotlib Javascript Electron Work in progress This open-source portion is still a work in progress. Stay tuned while we gradually upload our tutorials and notes. Feel free to contact Ritchie Ng if you would like to contribute via our Facebook page.","title":"Introduction"},{"location":"programming/intro/#programming","text":"We'll be covering C++, Python, Bash and more for end-to-end AI deployments. Packages and Languages you will learn to use C++ Python Bash Matplotlib Javascript Electron Work in progress This open-source portion is still a work in progress. Stay tuned while we gradually upload our tutorials and notes. Feel free to contact Ritchie Ng if you would like to contribute via our Facebook page.","title":"Programming"},{"location":"programming/bash/bash/","text":"Bash \u00b6 Run Bash Files You can find the bash code files for this section in this link . Creating and Running a Bash File \u00b6 Create bash file \u00b6 touch hello_world.sh Edit bash file with Hello World \u00b6 You can edit with anything like Vim, Sublime, etc. echo Hello World! Run bash file \u00b6 bash hello_world.sh This will print out, in your bash: Hello World! Calculating \u00b6 # Add echo $(( 10 + 10 )) # Subtract echo $(( 10 - 10 )) # Divide echo $(( 10 / 10 )) # Multiple echo $(( 10 * 10 )) # Modulo echo $(( 10 % 10 )) # Multiple Operations: Divide and Add echo $(( 10 / 10 + 10 )) 20 0 1 100 0 11 Loops and Conditional \u00b6 For Loop \u00b6 for i in 'A' 'B' 'C' do echo $i done A B C For Loop With Range \u00b6 This will echo the digits 0 to 10 without explicitly requiring to define the whole range of numbers/alphabets like above. for (( i = 0 ; i< = 10 ; i++ )) ; do echo $i done 0 1 2 3 4 5 6 7 8 9 10 If Else Conditional \u00b6 This is a simple if-else to check if the day of the week is 5, meaning if it is a Friday. day = $( date +%u ) if [ $day == 5 ] ; then echo \"Friday is here!\" else echo \"Friday is not here :(\" echo \"Today is day $day of the week\" fi Sequentially Running of Python Scripts \u00b6 This snippet allows you to to run 3 python scripts sequentially, waiting for each to finish before proceeding to the next. python script_1.py wait python script_2.py wait python script_3.py wait echo \"Finished running all 3 scripts in sequence!\" Parallel Running of Python Scripts \u00b6 python script_1.py && script_2.py && script_3.py wait echo \"Finished running all 3 scripts in parallel in sequence\" Reading and Writing Operations \u00b6 Reading logs and texts \u00b6 Create a text file called random_text.txt with the following contents Row 1 Row 2 Row 3 Row 4 Row 5 Row 6 Row 7 Row 8 Row 9 Row 10 Then run the following command to read it in bash then print it. text_file = $( cat random_text.txt ) echo $text_file Row 1 Row 2 Row 3 Row 4 Row 5 Row 6 Row 7 Row 8 Row 9 Row 10 Date Operations \u00b6 Getting Current Date \u00b6 This will return the date in the format YYYY-MM-DD for example 2019-06-03. DATE = ` date +%Y-%m-%d ` echo $DATE Getting Current Day of Week \u00b6 This will return 1, 2, 3, 4, 5, 6, 7 depending on the day of the week. DAY = $( date +%u ) echo $DAY Changing System Dates By 1 Day \u00b6 You can change system dates based on this. Surprisingly, you'll find it useful for testing an environment for deployments in the next day and then shifting it back to the actual day. sudo date -s 'next day' sudo date -s 'yesterday' If you are running some tests via bash and want to disable typing in password you can edit the sudoer file via sudo visudo and adding the following line. Only use sudo visudo and nothing else, as they've a special syntax. <username> ALL =( ALL ) NOPASSWD: /bin/date To find out your username, simply just run the command whoami . Jupyter Utility Commands \u00b6 Convert Notebook to HTML/Markdown \u00b6 jupyter nbconvert --to markdown python.ipynb jupyter nbconvert --to html python.ipynb Bash Convenient Commands \u00b6 List directories only \u00b6 ls -d */ List non-directories only \u00b6 ls -p | grep -v '/$' Check IP \u00b6 ifconfig | sed -En \"s/127.0.0.1//;s/.*inet (addr:)?(([0-9]*\\.){3}[0-9]*).*/\\2/p\" Check internet speed \u00b6 ethtool eno1 Check disk space \u00b6 df -h Check ubuntu version \u00b6 lsb_release -a Check truncated system logs \u00b6 tail /var/log/syslog Check CUDA version \u00b6 nvcc -V Check cuDNN version \u00b6 cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 Check username \u00b6 whoami Untar file \u00b6 tar -xvzf file_name Open PDF file \u00b6 gvfs-open file_name Download file from link rapidly with aria \u00b6 aria2c -x16 -c url_link Kill all python processes \u00b6 ps ax | grep python | cut -c1-5 | xargs kill -9 Install .deb files \u00b6 sudo apt-get install -f file_name.deb Empty thrash \u00b6 sudo apt-get install trash-cli thrash-empty Conda Commands \u00b6 Check conda environment \u00b6 conda env list Create conda kernel \u00b6 conda create -n kernel_name python = 3 .6 source activate kernel_name Install conda kernel \u00b6 conda install ipykernel source activate kernel_name python -m ipykernel install --user --name kernel_name --display-name kernel_name Remove conda kernel \u00b6 conda env remove -n kernel_name Recovering problematic conda installation \u00b6 # Download miniconda according to your environment # Link: https://docs.conda.io/en/latest/miniconda.html # Backup existing miniconda environment that may have problems mv miniconda3 miniconda3_backup # Install miniconda bash Miniconda3-latest-Linux-x86_64.sh # Restore old environment settings rsync -a miniconda3_backup/ miniconda3/ Internet Operations \u00b6 Checking Internet Availability \u00b6 This script will return whether your internet is fine or not without using pings. Pings can often be rendered unusable when the network administrator disables ICMP to prevent the origination of ping floods from the data centre. if nc -zw1 google.com 443 ; then echo \"INTERNET: OK\" else echo \"INTERNET: NOT OK\" fi Cron Operations \u00b6 Edit Cron \u00b6 Formatting follows this syntax with full credits on this beautiful diagram to fedorqui from Stack Overflow : \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59) \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23) \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500 day of month (1 - 31) \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500 month (1 - 12) \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500 day of week (0 - 6 => Sunday - Saturday, or \u2502 \u2502 \u2502 \u2502 \u2502 1 - 7 => Monday - Sunday) \u2193 \u2193 \u2193 \u2193 \u2193 * * * * * command to be executed Edit cron with this command: sudo crontab -e List Cron \u00b6 sudo crontab -l Status, Start, Stop and Restart \u00b6 sudo service cron status sudo service cron stop sudo service cron start sudo service cron restart Cron Debugging \u00b6 Install postfix for local routing of errors (choose local option): sudo apt-get install postfix Restart cron to see for any errors posted (if not errors, there will be no file, be patient before errors are posted): sudo cat /var/mail/root Cron Bash Fix \u00b6 Cron uses /bin/sh as the default shell. Typically you would realize you're using /bin/bash as your shell, so this typically needs to be rectified before you can use cron to schedule commands as if it were your usual bash. Edit your cron file via sudo crontab -e and paste the following lines at the end of the file prior to your command like so. Take note for PATH , you've to paste the output of echo PATH=$PATH in there instead! SHELL=/bin/bash PATH=/usr/lib.... # Your command schedule here! Cron Conda Environment \u00b6 This is an example of enabling an anaconda environment, for example the default base , and running a python script. Take note you need to put your python script in the right directory or simply navigate to that path with cd prior to \"$(conda shell.bash hook)\" . SHELL=/bin/bash PATH=/usr/lib.... * * * * 1-5 eval \"$(conda shell.bash hook)\" && conda activate base && python python_script_name.py Cron Running Processes \u00b6 Some times you want to see the status of running tasks and may want to get the PID to end it. This is a very handy command. ps - o pid , sess , cmd afx | egrep - A20 \"( |/)cron( -f)?$\" You can get the PID of the cron process and then end it with sudo pkill -s <PID> Hardware Information \u00b6 Comprehensive CPU Information \u00b6 cat /proc/cpuinfo Number of CPU Threads \u00b6 !grep -c ^processor /proc/cpuinfo or nproc CPU Model Name \u00b6 !cat /proc/cpuinfo | grep \"model name\" Check Available RAM \u00b6 In MB \u00b6 free -m In GB \u00b6 free -g","title":"Bash"},{"location":"programming/bash/bash/#bash","text":"Run Bash Files You can find the bash code files for this section in this link .","title":"Bash"},{"location":"programming/bash/bash/#creating-and-running-a-bash-file","text":"","title":"Creating and Running a Bash File"},{"location":"programming/bash/bash/#create-bash-file","text":"touch hello_world.sh","title":"Create bash file"},{"location":"programming/bash/bash/#edit-bash-file-with-hello-world","text":"You can edit with anything like Vim, Sublime, etc. echo Hello World!","title":"Edit bash file with Hello World"},{"location":"programming/bash/bash/#run-bash-file","text":"bash hello_world.sh This will print out, in your bash: Hello World!","title":"Run bash file"},{"location":"programming/bash/bash/#calculating","text":"# Add echo $(( 10 + 10 )) # Subtract echo $(( 10 - 10 )) # Divide echo $(( 10 / 10 )) # Multiple echo $(( 10 * 10 )) # Modulo echo $(( 10 % 10 )) # Multiple Operations: Divide and Add echo $(( 10 / 10 + 10 )) 20 0 1 100 0 11","title":"Calculating"},{"location":"programming/bash/bash/#loops-and-conditional","text":"","title":"Loops and Conditional"},{"location":"programming/bash/bash/#for-loop","text":"for i in 'A' 'B' 'C' do echo $i done A B C","title":"For Loop"},{"location":"programming/bash/bash/#for-loop-with-range","text":"This will echo the digits 0 to 10 without explicitly requiring to define the whole range of numbers/alphabets like above. for (( i = 0 ; i< = 10 ; i++ )) ; do echo $i done 0 1 2 3 4 5 6 7 8 9 10","title":"For Loop With Range"},{"location":"programming/bash/bash/#if-else-conditional","text":"This is a simple if-else to check if the day of the week is 5, meaning if it is a Friday. day = $( date +%u ) if [ $day == 5 ] ; then echo \"Friday is here!\" else echo \"Friday is not here :(\" echo \"Today is day $day of the week\" fi","title":"If Else Conditional"},{"location":"programming/bash/bash/#sequentially-running-of-python-scripts","text":"This snippet allows you to to run 3 python scripts sequentially, waiting for each to finish before proceeding to the next. python script_1.py wait python script_2.py wait python script_3.py wait echo \"Finished running all 3 scripts in sequence!\"","title":"Sequentially Running of Python Scripts"},{"location":"programming/bash/bash/#parallel-running-of-python-scripts","text":"python script_1.py && script_2.py && script_3.py wait echo \"Finished running all 3 scripts in parallel in sequence\"","title":"Parallel Running of Python Scripts"},{"location":"programming/bash/bash/#reading-and-writing-operations","text":"","title":"Reading and Writing Operations"},{"location":"programming/bash/bash/#reading-logs-and-texts","text":"Create a text file called random_text.txt with the following contents Row 1 Row 2 Row 3 Row 4 Row 5 Row 6 Row 7 Row 8 Row 9 Row 10 Then run the following command to read it in bash then print it. text_file = $( cat random_text.txt ) echo $text_file Row 1 Row 2 Row 3 Row 4 Row 5 Row 6 Row 7 Row 8 Row 9 Row 10","title":"Reading logs and texts"},{"location":"programming/bash/bash/#date-operations","text":"","title":"Date Operations"},{"location":"programming/bash/bash/#getting-current-date","text":"This will return the date in the format YYYY-MM-DD for example 2019-06-03. DATE = ` date +%Y-%m-%d ` echo $DATE","title":"Getting Current Date"},{"location":"programming/bash/bash/#getting-current-day-of-week","text":"This will return 1, 2, 3, 4, 5, 6, 7 depending on the day of the week. DAY = $( date +%u ) echo $DAY","title":"Getting Current Day of Week"},{"location":"programming/bash/bash/#changing-system-dates-by-1-day","text":"You can change system dates based on this. Surprisingly, you'll find it useful for testing an environment for deployments in the next day and then shifting it back to the actual day. sudo date -s 'next day' sudo date -s 'yesterday' If you are running some tests via bash and want to disable typing in password you can edit the sudoer file via sudo visudo and adding the following line. Only use sudo visudo and nothing else, as they've a special syntax. <username> ALL =( ALL ) NOPASSWD: /bin/date To find out your username, simply just run the command whoami .","title":"Changing System Dates By 1 Day"},{"location":"programming/bash/bash/#jupyter-utility-commands","text":"","title":"Jupyter Utility Commands"},{"location":"programming/bash/bash/#convert-notebook-to-htmlmarkdown","text":"jupyter nbconvert --to markdown python.ipynb jupyter nbconvert --to html python.ipynb","title":"Convert Notebook to HTML/Markdown"},{"location":"programming/bash/bash/#bash-convenient-commands","text":"","title":"Bash Convenient Commands"},{"location":"programming/bash/bash/#list-directories-only","text":"ls -d */","title":"List directories only"},{"location":"programming/bash/bash/#list-non-directories-only","text":"ls -p | grep -v '/$'","title":"List non-directories only"},{"location":"programming/bash/bash/#check-ip","text":"ifconfig | sed -En \"s/127.0.0.1//;s/.*inet (addr:)?(([0-9]*\\.){3}[0-9]*).*/\\2/p\"","title":"Check IP"},{"location":"programming/bash/bash/#check-internet-speed","text":"ethtool eno1","title":"Check internet speed"},{"location":"programming/bash/bash/#check-disk-space","text":"df -h","title":"Check disk space"},{"location":"programming/bash/bash/#check-ubuntu-version","text":"lsb_release -a","title":"Check ubuntu version"},{"location":"programming/bash/bash/#check-truncated-system-logs","text":"tail /var/log/syslog","title":"Check truncated system logs"},{"location":"programming/bash/bash/#check-cuda-version","text":"nvcc -V","title":"Check CUDA version"},{"location":"programming/bash/bash/#check-cudnn-version","text":"cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2","title":"Check cuDNN version"},{"location":"programming/bash/bash/#check-username","text":"whoami","title":"Check username"},{"location":"programming/bash/bash/#untar-file","text":"tar -xvzf file_name","title":"Untar file"},{"location":"programming/bash/bash/#open-pdf-file","text":"gvfs-open file_name","title":"Open PDF file"},{"location":"programming/bash/bash/#download-file-from-link-rapidly-with-aria","text":"aria2c -x16 -c url_link","title":"Download file from link rapidly with aria"},{"location":"programming/bash/bash/#kill-all-python-processes","text":"ps ax | grep python | cut -c1-5 | xargs kill -9","title":"Kill all python processes"},{"location":"programming/bash/bash/#install-deb-files","text":"sudo apt-get install -f file_name.deb","title":"Install .deb files"},{"location":"programming/bash/bash/#empty-thrash","text":"sudo apt-get install trash-cli thrash-empty","title":"Empty thrash"},{"location":"programming/bash/bash/#conda-commands","text":"","title":"Conda Commands"},{"location":"programming/bash/bash/#check-conda-environment","text":"conda env list","title":"Check conda environment"},{"location":"programming/bash/bash/#create-conda-kernel","text":"conda create -n kernel_name python = 3 .6 source activate kernel_name","title":"Create conda kernel"},{"location":"programming/bash/bash/#install-conda-kernel","text":"conda install ipykernel source activate kernel_name python -m ipykernel install --user --name kernel_name --display-name kernel_name","title":"Install conda kernel"},{"location":"programming/bash/bash/#remove-conda-kernel","text":"conda env remove -n kernel_name","title":"Remove conda kernel"},{"location":"programming/bash/bash/#recovering-problematic-conda-installation","text":"# Download miniconda according to your environment # Link: https://docs.conda.io/en/latest/miniconda.html # Backup existing miniconda environment that may have problems mv miniconda3 miniconda3_backup # Install miniconda bash Miniconda3-latest-Linux-x86_64.sh # Restore old environment settings rsync -a miniconda3_backup/ miniconda3/","title":"Recovering problematic conda installation"},{"location":"programming/bash/bash/#internet-operations","text":"","title":"Internet Operations"},{"location":"programming/bash/bash/#checking-internet-availability","text":"This script will return whether your internet is fine or not without using pings. Pings can often be rendered unusable when the network administrator disables ICMP to prevent the origination of ping floods from the data centre. if nc -zw1 google.com 443 ; then echo \"INTERNET: OK\" else echo \"INTERNET: NOT OK\" fi","title":"Checking Internet Availability"},{"location":"programming/bash/bash/#cron-operations","text":"","title":"Cron Operations"},{"location":"programming/bash/bash/#edit-cron","text":"Formatting follows this syntax with full credits on this beautiful diagram to fedorqui from Stack Overflow : \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59) \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23) \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500 day of month (1 - 31) \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500 month (1 - 12) \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500 day of week (0 - 6 => Sunday - Saturday, or \u2502 \u2502 \u2502 \u2502 \u2502 1 - 7 => Monday - Sunday) \u2193 \u2193 \u2193 \u2193 \u2193 * * * * * command to be executed Edit cron with this command: sudo crontab -e","title":"Edit Cron"},{"location":"programming/bash/bash/#list-cron","text":"sudo crontab -l","title":"List Cron"},{"location":"programming/bash/bash/#status-start-stop-and-restart","text":"sudo service cron status sudo service cron stop sudo service cron start sudo service cron restart","title":"Status, Start, Stop and Restart"},{"location":"programming/bash/bash/#cron-debugging","text":"Install postfix for local routing of errors (choose local option): sudo apt-get install postfix Restart cron to see for any errors posted (if not errors, there will be no file, be patient before errors are posted): sudo cat /var/mail/root","title":"Cron Debugging"},{"location":"programming/bash/bash/#cron-bash-fix","text":"Cron uses /bin/sh as the default shell. Typically you would realize you're using /bin/bash as your shell, so this typically needs to be rectified before you can use cron to schedule commands as if it were your usual bash. Edit your cron file via sudo crontab -e and paste the following lines at the end of the file prior to your command like so. Take note for PATH , you've to paste the output of echo PATH=$PATH in there instead! SHELL=/bin/bash PATH=/usr/lib.... # Your command schedule here!","title":"Cron Bash Fix"},{"location":"programming/bash/bash/#cron-conda-environment","text":"This is an example of enabling an anaconda environment, for example the default base , and running a python script. Take note you need to put your python script in the right directory or simply navigate to that path with cd prior to \"$(conda shell.bash hook)\" . SHELL=/bin/bash PATH=/usr/lib.... * * * * 1-5 eval \"$(conda shell.bash hook)\" && conda activate base && python python_script_name.py","title":"Cron Conda Environment"},{"location":"programming/bash/bash/#cron-running-processes","text":"Some times you want to see the status of running tasks and may want to get the PID to end it. This is a very handy command. ps - o pid , sess , cmd afx | egrep - A20 \"( |/)cron( -f)?$\" You can get the PID of the cron process and then end it with sudo pkill -s <PID>","title":"Cron Running Processes"},{"location":"programming/bash/bash/#hardware-information","text":"","title":"Hardware Information"},{"location":"programming/bash/bash/#comprehensive-cpu-information","text":"cat /proc/cpuinfo","title":"Comprehensive CPU Information"},{"location":"programming/bash/bash/#number-of-cpu-threads","text":"!grep -c ^processor /proc/cpuinfo or nproc","title":"Number of CPU Threads"},{"location":"programming/bash/bash/#cpu-model-name","text":"!cat /proc/cpuinfo | grep \"model name\"","title":"CPU Model Name"},{"location":"programming/bash/bash/#check-available-ram","text":"","title":"Check Available RAM"},{"location":"programming/bash/bash/#in-mb","text":"free -m","title":"In MB"},{"location":"programming/bash/bash/#in-gb","text":"free -g","title":"In GB"},{"location":"programming/cpp/cpp/","text":"C++ \u00b6 Run Jupyter Notebook You can run the code for this section in this jupyter notebook link . Installation of Interactive C++17 \u00b6 Xeus-Cling is a game-changer where similar to Python Jupyter Notebooks, we can run C++ Jupyter Notebooks now. Run the following bash commands in sequence to create a C++ kernel for Jupyter Notebook. conda create -n cpp source activate cpp conda install -c conda-forge xeus-cling jupyter kernelspec install --user /home/ritchie/miniconda3/envs/cpp/share/jupyter/kernels/xcpp17 jupyter notebook Printing \u00b6 Printing Single Line \u00b6 // When you compile, the preprocessor runs and acts on all line with the pound key first // This is a preprocessor instruction that // essentially places the file with the name iostream into this spot #include <iostream> // Print one line std :: cout << \"Using Xeus Cling\" << std :: endl ; Using Xeus Cling Printing 2 Lines \u00b6 // Print two lines std :: cout << \"First Line \\n Second Line\" << std :: endl ; First Line Second Line Printing with Tabs \u00b6 // Print numbers with nicely formatted tabs std :: cout << \"One hundred: \\t \" ; std :: cout << ( float ) 1000 / 10 << std :: endl ; std :: cout << \"Two hundred: \\t \" ; std :: cout << ( double ) 2000 / 10 << std :: endl ; std :: cout << \"Three hundred: \\t \" ; std :: cout << ( double ) 3000 / 10 << std :: endl ; One hundred: 100 Two hundred: 200 Three hundred: 300 Easier Printing (subjective) with Namespaces \u00b6 Gets irritating to use std in front of cout and endl to keep printing so we can use namespaces using namespace std ; cout << \"No need for messy std::\" << endl ; No need for messy std:: Variables \u00b6 General memory management \u00b6 In C++, you always need to determine each variable's type so the compiler will know how much memory (RAM) to allocate to the variable Types of Variables \u00b6 Type Bytes (Size) Range of Values char 1 0 to 255 or -127 to 127 unsigned char 1 -127 to 127 signed char 1 0 to 255 bool 1 True or False int 4 -2,147,483,648 to 2,147,483,647 unsigned int 4 0 to 4,294,967,295 signed int 4 -2,147,483,648 to 2,147,483,647 short int 2 -32,768 to 32,767 long int 4 -2,147,483,648 to 2,147,483,647 float 4 3.4e-38 to 3.4e38 double 8 1.7e-308 to 1.7e-08 long double 8 1.7e-308 to 1.7e-08 Integer sizes \u00b6 Typical sizes Short integer: 2 bytes (will be smaller than long) Limitation is that the value of this short integer has a max value, you should be cautious of using short integers Long integer: 4 bytes Integer: 2 or 4 bytes Notes Technically these sizes can vary depending on your processor (32/64 bit) and compiler You should not assume the sizes but the hierarchy of sizes will not change (short memory < long memory) Integer size \u00b6 cout << \"Size of an integer: \" << sizeof ( int ); Size of an integer: 4 Short integer size \u00b6 cout << \"Size of a short integer: \" << sizeof ( short int ); Size of a short integer: 2 Long integer size \u00b6 cout << \"Size of an long integer: \" << sizeof ( long int ); Size of an long integer: 8 Unsigned or signed integers \u00b6 Unsigned integers: only can hold positive integers Signed integers: can hold positive/negative integers Signed short integer \u00b6 cout << \"Size of an signed short integer: \" << sizeof ( signed short int ); Size of an signed short integer: 2 Unsigned short integer \u00b6 cout << \"Size of an unsigned short integer: \" << sizeof ( unsigned short int ); Size of an unsigned short integer: 2 Signed long integer \u00b6 cout << \"Size of an signed long integer: \" << sizeof ( signed long int ); Size of an signed long integer: 8 Unsigned long integer \u00b6 cout << \"Size of an unsigned long integer: \" << sizeof ( unsigned long int ); Size of an unsigned long integer: 8 Constants \u00b6 Literal Constants \u00b6 int varOne = 20 ; std :: cout << varOne << std :: endl ; 20 Enumerated Constants \u00b6 This enables you to create a new type! In this example we create a new type directions containing Up , Down , Left , and Right . enum directions { Up , Down , Left , Right }; directions goWhere ; goWhere = Right ; if ( goWhere == Right ) std :: cout << \"Go right\" << std :: endl ; Go right Functions \u00b6 Function Without Return Value \u00b6 Syntax generally follows void FunctionName(argOne, argTwo) to define the function followed by FuntionName() to call the function. // Usage of void when the function does not return anything // In this example, this function prints out the multiplication result of two given numbers void MultiplyTwoNumbers ( int firstNum , int secondNum ) { // Define variable as integer type long int value ; value = firstNum * secondNum ; std :: cout << value << std :: endl ; } Multiply Two Numbers 3 and 2 \u00b6 MultiplyTwoNumbers ( 3 , 2 ) 6 Multiply Two Numbers 6 and 2 \u00b6 MultiplyTwoNumbers ( 6 , 2 ) 12 Aliases Function \u00b6 Say we want the variable value to be of type unsigned short int such that it's 2 bytes and can hold 2x the range of values compared to just short int . We can use typedef as an alias. Type Bytes (Size) Range of Values short int 2 -32,768 to 32,767 unsigned short int 2 0 to 65,536 // Usage of void when the function does not return anything // In this exmaple, this function prints out the multiplication result of two given numbers void MultiplyTwoNumbersWithAlias ( int firstNum , int secondNum ) { // Using an alias typedef unsigned short int ushortint ; // initializing value variable with ushortint type ushortint value ; value = firstNum * secondNum ; std :: cout << value << std :: endl ; } Multiply Two Numbers 10 and 10 \u00b6 MultiplyTwoNumbersWithAlias ( 10 , 10 ) 100 Multiply Two Numbers 1000 and 65 \u00b6 MultiplyTwoNumbersWithAlias ( 1000 , 65 ) 65000 Multiply Two Numbers 1000 and 67 \u00b6 Notice how you don't get 67,000? This is because our variable value of ushortint type can only hold values up to the integer 65,536. What this returns is the remainder of 67,000 - 65,536 = 1464 MultiplyTwoNumbersWithAlias ( 1000 , 67 ) 1464 std :: cout << 67 * 1000 - 65536 << std :: endl ; 1464 Function with Return Value \u00b6 Unlike functions without return values where we use void to declare the function, here we use int to declare our function that returns values. // In this exmaple, this function returns the value of the multiplication of two numbers int MultiplyTwoNumbersNoPrint ( int firstNum , int secondNum ) { // Define variable as integer type long int value ; value = firstNum * secondNum ; return value ; } Call Function \u00b6 // Declare variable with type long int returnValue ; // Call function returnValue = MultiplyTwoNumbersNoPrint ( 10 , 2 ); // Print variable std :: cout << returnValue << std :: endl ; 20 Function Inner Workings \u00b6 Essentially our lines of codes translates to instruction pointers with unique memory addresses. Execution of instruction pointers operates on a \"LIFO\" basis, last in first out. Oversimplifying here, in our example, the last line is taken off first and it follows up // Code Space int varOneTest = 10 ; // Instruction pointer 100 std :: cout << varOneTest << std :: endl ; // Instruction Pointer 102 10 @0x7fa6b7de5460 Arrays \u00b6 An array contains a sequence of elements with the same data type. Creating an Array \u00b6 // This is how you declare an array of 50 elements each of type double double DoubleArray [ 50 ]; Accessing Array's Elements \u00b6 First Element \u00b6 // This access the first array std :: cout << DoubleArray [ 0 ] << std :: endl ; 0 Last Element \u00b6 std :: cout << DoubleArray [ 49 ] << std :: endl ; 0 First 10 Elements \u00b6 // In steps of 1 for ( int i = 0 ; i < 10 ; i ++ ) { // This is how you print a mix of characters and declared variables std :: cout << \"Element \" << i << \" contains \" << DoubleArray [ i ] << std :: endl ; } Element 0 contains 0 Element 1 contains 0 Element 2 contains 0 Element 3 contains 0 Element 4 contains 0 Element 5 contains 0 Element 6 contains 0 Element 7 contains 0 Element 8 contains 0 Element 9 contains 0 // In steps of 2 for ( int i = 0 ; i < 10 ; i += 2 ) { // This is how you print a mix of characters and declared variables std :: cout << \"Element \" << i << \" contains \" << DoubleArray [ i ] << std :: endl ; } Element 0 contains 0 Element 2 contains 0 Element 4 contains 0 Element 6 contains 0 Element 8 contains 0 Going Beyond The Array's Length \u00b6 This will return a warning that it's past the end of the array std :: cout << DoubleArray [ 50 ] << std :: endl ; input_line_36:2:15: warning: array index 50 is past the end of the array ( which contains 50 elements ) [ -Warray-bounds ] std::cout << DoubleArray[50] << std::endl; ^ ~~ input_line_32:3:1: note: array 'DoubleArray ' declared here double DoubleArray [ 50 ] ; ^ 4 .94066e-323 Arrays with Enumeration \u00b6 enum directionsNew { up , down , left , right , individualDirections }; int directionsArray [ individualDirections ] = { 1 , 2 , 3 , 4 }; std :: cout << \"Up value: \\t \" << directionsArray [ up ]; std :: cout << \" \\n Down value: \\t \" << directionsArray [ down ]; std :: cout << \" \\n Left value: \\t \" << directionsArray [ left ]; std :: cout << \" \\n Right value: \\t \" << directionsArray [ right ]; // This is the number of elements in the array std :: cout << \" \\n Num value: \\t \" << sizeof ( directionsArray ) / sizeof ( directionsArray [ 0 ]) << std :: endl ; Up value: 1 Down value: 2 Left value: 3 Right value: 4 Num value: 4 Arrays with >1 Dimension (Tensors) \u00b6 Multi Dimension Array with Numbers \u00b6 // This is how you declare a multi-dimensional array of 5x5 elements each of type double double multiDimArray [ 5 ][ 5 ] = { { 1 , 2 , 3 , 4 , 5 }, { 2 , 2 , 3 , 4 , 5 }, { 3 , 2 , 3 , 4 , 5 }, { 4 , 2 , 3 , 4 , 5 }, { 5 , 2 , 3 , 4 , 5 } }; // Print each row of our 5x5 multi-dimensional array for ( int i = 0 ; i < 5 ; i ++ ) { for ( int j = 0 ; j < 5 ; j ++ ) { std :: cout << multiDimArray [ i ][ j ]; }; std :: cout << \" \\n \" << std :: endl ; }; 12345 22345 32345 42345 52345 Multi Dimension Array with Characters \u00b6 // This is how you declare a multi-dimensional array of 5x5 elements each of type char char multiDimArrayChars [ 5 ][ 5 ] = { { 'a' , 'b' , 'c' , 'd' , 'e' }, { 'b' , 'b' , 'c' , 'd' , 'e' }, { 'c' , 'b' , 'c' , 'd' , 'e' }, { 'd' , 'b' , 'c' , 'd' , 'e' }, { 'e' , 'b' , 'c' , 'd' , 'e' }, }; // Print each row of our 5x5 multi-dimensional array for ( int i = 0 ; i < 5 ; i ++ ) { for ( int j = 0 ; j < 5 ; j ++ ) { std :: cout << multiDimArrayChars [ i ][ j ]; }; std :: cout << \" \\n \" << std :: endl ; }; abcde bbcde cbcde dbcde ebcde Copy Arrays \u00b6 Copy Number Arrays \u00b6 double ArrayNumOne [] = { 1 , 2 , 3 , 4 , 5 }; double ArrayNumTwo [ 5 ]; // Use namespace std so it's cleaner using namespace std ; // Copy array with copy() copy ( begin ( ArrayNumOne ), end ( ArrayNumOne ), begin ( ArrayNumTwo )); // Print double type array with copy() copy ( begin ( ArrayNumTwo ), end ( ArrayNumTwo ), ostream_iterator < double > ( cout , \" \\n \" )); 1 2 3 4 5 Copy String Arrays \u00b6 char ArrayCharOne [] = { 'a' , 'b' , 'c' , 'd' , 'e' }; char ArrayCharTwo [ 5 ]; // Use namespace std so it's cleaner using namespace std ; // Copy array with copy() copy ( begin ( ArrayCharOne ), end ( ArrayCharOne ), begin ( ArrayCharTwo )); // Print char type array with copy() copy ( begin ( ArrayCharTwo ), end ( ArrayCharTwo ), ostream_iterator < char > ( cout , \" \\n \" )); a b c d e Mathematical Operators \u00b6 Add, subtract, multiply, divide and modulus Add \u00b6 double addStatement = 5 + 5 ; std :: cout << addStatement ; 10 Subtract \u00b6 double subtractStatement = 5 - 5 ; std :: cout << subtractStatement ; 0 Divide \u00b6 double divideStatement = 5 / 5 ; std :: cout << divideStatement ; 1 Multiply \u00b6 double multiplyStatement = 5 * 5 ; std :: cout << multiplyStatement ; 25 Modulus \u00b6 Gets the remainder of the division double modulusStatement = 8 % 5 ; std :: cout << modulusStatement ; 3 Exponent \u00b6 Base to the power of something, this requires a new package called <cmath> that we want to include. #include <cmath> void SquareNumber ( int baseNum , int exponentNum ) { // Square the locally scoped variable with 2 int squaredNumber ; squaredNumber = pow ( baseNum , exponentNum ); std :: cout << \"Base of 2 with exponent of 2 gives: \" << squaredNumber << std :: endl ; } SquareNumber ( 2 , 2 ) Base of 2 with exponent of 2 gives: 4 Incrementing/Decrementing \u00b6 3 ways to do this, from least to most verbose Methods \u00b6 Method 1 \u00b6 double idx = 1 ; idx ++ ; std :: cout << idx ; 2 Method 2 \u00b6 idx += 1 ; std :: cout << idx ; 3 Method 3 \u00b6 idx = idx + 1 ; std :: cout << idx ; 4 Prefix/Postfix \u00b6 Prefix \u00b6 This will change both incremented variable and the new variable you assign the incremented variable to Summary: both variables will have the same values // Instantiate double a = 1 ; double b = 1 ; // Print original values cout << \"Old a: \\t \" << a << \" \\n \" ; cout << \"Old b: \\t \" << b << \" \\n \" ; // Prefix increment a = ++ b ; // Print new values cout << \"New a: \\t \" << a << \" \\n \" ; cout << \"New b: \\t \" << b << \" \\n \" ; Old a: 1 Old b: 1 New a: 2 New b: 2 Postfix \u00b6 This will change only the incremented variable but not the variable it's assigned to Summary: incremented variable will change but not the variable it was assigned to // Instantiate double c = 2 ; double d = 2 ; // Print original values cout << \"Old c: \\t \" << c << \" \\n \" ; cout << \"Old d: \\t \" << d << \" \\n \" ; // Prefix increment c = d -- ; // Print new values, notice how only d decremented? c which is what d is assigned to doesn't change. cout << \"New c: \\t \" << c << \" \\n \" ; cout << \"New d: \\t \" << d << \" \\n \" ; Old c: 2 Old d: 2 New c: 2 New d: 1 Conditional Statements \u00b6 If \u00b6 int maxValue = 10 ; // Increment till 10 for ( int i = 0 ; i <= 10 ; i += 2 ) { // Stop if the number reaches 10 (inclusive) if ( i == maxValue ) { cout << \"Reached max value!\" ; cout << \" \\n Value is \" << i << endl ; }; }; Reached max value! Value is 10 Else \u00b6 int newMaxValue = 20 ; // Increment till 10 for ( int i = 0 ; i <= 10 ; i += 2 ) { // Stop if the number reaches 10 (inclusive) if ( i == newMaxValue ) { cout << \"Reached max value!\" ; cout << \" \\n Value is \" << i << endl ; } // Else print current value else { cout << \" \\n Current Value is \" << i << endl ; } } Current Value is 0 Current Value is 2 Current Value is 4 Current Value is 6 Current Value is 8 Current Value is 10 Logical Operators \u00b6 And \u00b6 int varOneNew = 10 ; int varTwo = 10 ; int varCheckOne = 10 ; int varCheckTwo = 5 ; // This should print out if (( varOneNew == varCheckOne ) && ( varTwo == varCheckOne )) { std :: cout << \"Both values equal to 10!\" << std :: endl ; } // This should not print out as varTwo does not equal to 5 if (( varOneNew == varCheckOne ) && ( varTwo == varCheckTwo )) { std :: cout << \"VarOneNew equals to 10, VarTwo equals to 5\" << std :: endl ; } Both values equal to 10! Or \u00b6 // On the contrary, this exact same statement would print out // as VarOne is equal to 10 and we are using an OR operator if (( varOneNew == varCheckOne ) || ( varTwo == varCheckTwo )) { std :: cout << \"VarOneNew equals to 10 or VarTwo equals to 5\" << std :: endl ; } VarOneNew equals to 10 or VarTwo equals to 5 Not \u00b6 // This would print out as VarTwo is not equal to 5 if ( varTwo != varCheckTwo ) { std :: cout << \"VarTwo (10) is not equal to VarCheckTwo (5).\" << std :: endl ; } VarTwo (10) is not equal to VarCheckTwo (5). Getting User Input \u00b6 using namespace std ; long double inputOne , inputTwo ; cout << \"This program multiplies 2 given numbers \\n \" ; cout << \"Enter first number: \\n \" ; cin >> inputOne ; cout << \"Enter second number: \\n \" ; cin >> inputTwo ; cout << \"Multiplication value: \" << inputOne * inputTwo << endl ; This program multiplies 2 given numbers Enter first number: 10 Enter second number: 10 Multiplication value: 100 Loops \u00b6 For Loop \u00b6 for ( int i = 0 ; i < 10 ; i += 1 ) { cout << \"Value of i is: \" << i << endl ; } Value of i is: 0 Value of i is: 1 Value of i is: 2 Value of i is: 3 Value of i is: 4 Value of i is: 5 Value of i is: 6 Value of i is: 7 Value of i is: 8 Value of i is: 9 While Loop \u00b6 int idxWhile = 0 ; while ( idxWhile < 10 ) { idxWhile += 1 ; cout << \"Value of while loop i is: \" << idxWhile << endl ; } Value of while loop i is: 1 Value of while loop i is: 2 Value of while loop i is: 3 Value of while loop i is: 4 Value of while loop i is: 5 Value of while loop i is: 6 Value of while loop i is: 7 Value of while loop i is: 8 Value of while loop i is: 9 Value of while loop i is: 10 While Loop with Continue/Break \u00b6 int idxWhileNew = 0 ; while ( idxWhileNew < 100 ) { idxWhileNew += 1 ; cout << \"Value of while loop i is: \" << idxWhile << endl ; if ( idxWhileNew == 10 ) { cout << \"Max value of 10 reached!\" << endl ; break ; } } Value of while loop i is: 10 Value of while loop i is: 10 Value of while loop i is: 10 Value of while loop i is: 10 Value of while loop i is: 10 Value of while loop i is: 10 Value of while loop i is: 10 Value of while loop i is: 10 Value of while loop i is: 10 Value of while loop i is: 10 Max value of 10 reached!","title":"C++"},{"location":"programming/cpp/cpp/#c","text":"Run Jupyter Notebook You can run the code for this section in this jupyter notebook link .","title":"C++"},{"location":"programming/cpp/cpp/#installation-of-interactive-c17","text":"Xeus-Cling is a game-changer where similar to Python Jupyter Notebooks, we can run C++ Jupyter Notebooks now. Run the following bash commands in sequence to create a C++ kernel for Jupyter Notebook. conda create -n cpp source activate cpp conda install -c conda-forge xeus-cling jupyter kernelspec install --user /home/ritchie/miniconda3/envs/cpp/share/jupyter/kernels/xcpp17 jupyter notebook","title":"Installation of Interactive C++17"},{"location":"programming/cpp/cpp/#printing","text":"","title":"Printing"},{"location":"programming/cpp/cpp/#printing-single-line","text":"// When you compile, the preprocessor runs and acts on all line with the pound key first // This is a preprocessor instruction that // essentially places the file with the name iostream into this spot #include <iostream> // Print one line std :: cout << \"Using Xeus Cling\" << std :: endl ; Using Xeus Cling","title":"Printing Single Line"},{"location":"programming/cpp/cpp/#printing-2-lines","text":"// Print two lines std :: cout << \"First Line \\n Second Line\" << std :: endl ; First Line Second Line","title":"Printing 2 Lines"},{"location":"programming/cpp/cpp/#printing-with-tabs","text":"// Print numbers with nicely formatted tabs std :: cout << \"One hundred: \\t \" ; std :: cout << ( float ) 1000 / 10 << std :: endl ; std :: cout << \"Two hundred: \\t \" ; std :: cout << ( double ) 2000 / 10 << std :: endl ; std :: cout << \"Three hundred: \\t \" ; std :: cout << ( double ) 3000 / 10 << std :: endl ; One hundred: 100 Two hundred: 200 Three hundred: 300","title":"Printing with Tabs"},{"location":"programming/cpp/cpp/#easier-printing-subjective-with-namespaces","text":"Gets irritating to use std in front of cout and endl to keep printing so we can use namespaces using namespace std ; cout << \"No need for messy std::\" << endl ; No need for messy std::","title":"Easier Printing (subjective) with Namespaces"},{"location":"programming/cpp/cpp/#variables","text":"","title":"Variables"},{"location":"programming/cpp/cpp/#general-memory-management","text":"In C++, you always need to determine each variable's type so the compiler will know how much memory (RAM) to allocate to the variable","title":"General memory management"},{"location":"programming/cpp/cpp/#types-of-variables","text":"Type Bytes (Size) Range of Values char 1 0 to 255 or -127 to 127 unsigned char 1 -127 to 127 signed char 1 0 to 255 bool 1 True or False int 4 -2,147,483,648 to 2,147,483,647 unsigned int 4 0 to 4,294,967,295 signed int 4 -2,147,483,648 to 2,147,483,647 short int 2 -32,768 to 32,767 long int 4 -2,147,483,648 to 2,147,483,647 float 4 3.4e-38 to 3.4e38 double 8 1.7e-308 to 1.7e-08 long double 8 1.7e-308 to 1.7e-08","title":"Types of Variables"},{"location":"programming/cpp/cpp/#integer-sizes","text":"Typical sizes Short integer: 2 bytes (will be smaller than long) Limitation is that the value of this short integer has a max value, you should be cautious of using short integers Long integer: 4 bytes Integer: 2 or 4 bytes Notes Technically these sizes can vary depending on your processor (32/64 bit) and compiler You should not assume the sizes but the hierarchy of sizes will not change (short memory < long memory)","title":"Integer sizes"},{"location":"programming/cpp/cpp/#integer-size","text":"cout << \"Size of an integer: \" << sizeof ( int ); Size of an integer: 4","title":"Integer size"},{"location":"programming/cpp/cpp/#short-integer-size","text":"cout << \"Size of a short integer: \" << sizeof ( short int ); Size of a short integer: 2","title":"Short integer size"},{"location":"programming/cpp/cpp/#long-integer-size","text":"cout << \"Size of an long integer: \" << sizeof ( long int ); Size of an long integer: 8","title":"Long integer size"},{"location":"programming/cpp/cpp/#unsigned-or-signed-integers","text":"Unsigned integers: only can hold positive integers Signed integers: can hold positive/negative integers","title":"Unsigned or signed integers"},{"location":"programming/cpp/cpp/#signed-short-integer","text":"cout << \"Size of an signed short integer: \" << sizeof ( signed short int ); Size of an signed short integer: 2","title":"Signed short integer"},{"location":"programming/cpp/cpp/#unsigned-short-integer","text":"cout << \"Size of an unsigned short integer: \" << sizeof ( unsigned short int ); Size of an unsigned short integer: 2","title":"Unsigned short integer"},{"location":"programming/cpp/cpp/#signed-long-integer","text":"cout << \"Size of an signed long integer: \" << sizeof ( signed long int ); Size of an signed long integer: 8","title":"Signed long integer"},{"location":"programming/cpp/cpp/#unsigned-long-integer","text":"cout << \"Size of an unsigned long integer: \" << sizeof ( unsigned long int ); Size of an unsigned long integer: 8","title":"Unsigned long integer"},{"location":"programming/cpp/cpp/#constants","text":"","title":"Constants"},{"location":"programming/cpp/cpp/#literal-constants","text":"int varOne = 20 ; std :: cout << varOne << std :: endl ; 20","title":"Literal Constants"},{"location":"programming/cpp/cpp/#enumerated-constants","text":"This enables you to create a new type! In this example we create a new type directions containing Up , Down , Left , and Right . enum directions { Up , Down , Left , Right }; directions goWhere ; goWhere = Right ; if ( goWhere == Right ) std :: cout << \"Go right\" << std :: endl ; Go right","title":"Enumerated Constants"},{"location":"programming/cpp/cpp/#functions","text":"","title":"Functions"},{"location":"programming/cpp/cpp/#function-without-return-value","text":"Syntax generally follows void FunctionName(argOne, argTwo) to define the function followed by FuntionName() to call the function. // Usage of void when the function does not return anything // In this example, this function prints out the multiplication result of two given numbers void MultiplyTwoNumbers ( int firstNum , int secondNum ) { // Define variable as integer type long int value ; value = firstNum * secondNum ; std :: cout << value << std :: endl ; }","title":"Function Without Return Value"},{"location":"programming/cpp/cpp/#multiply-two-numbers-3-and-2","text":"MultiplyTwoNumbers ( 3 , 2 ) 6","title":"Multiply Two Numbers 3 and 2"},{"location":"programming/cpp/cpp/#multiply-two-numbers-6-and-2","text":"MultiplyTwoNumbers ( 6 , 2 ) 12","title":"Multiply Two Numbers 6 and 2"},{"location":"programming/cpp/cpp/#aliases-function","text":"Say we want the variable value to be of type unsigned short int such that it's 2 bytes and can hold 2x the range of values compared to just short int . We can use typedef as an alias. Type Bytes (Size) Range of Values short int 2 -32,768 to 32,767 unsigned short int 2 0 to 65,536 // Usage of void when the function does not return anything // In this exmaple, this function prints out the multiplication result of two given numbers void MultiplyTwoNumbersWithAlias ( int firstNum , int secondNum ) { // Using an alias typedef unsigned short int ushortint ; // initializing value variable with ushortint type ushortint value ; value = firstNum * secondNum ; std :: cout << value << std :: endl ; }","title":"Aliases Function"},{"location":"programming/cpp/cpp/#multiply-two-numbers-10-and-10","text":"MultiplyTwoNumbersWithAlias ( 10 , 10 ) 100","title":"Multiply Two Numbers 10 and 10"},{"location":"programming/cpp/cpp/#multiply-two-numbers-1000-and-65","text":"MultiplyTwoNumbersWithAlias ( 1000 , 65 ) 65000","title":"Multiply Two Numbers 1000 and 65"},{"location":"programming/cpp/cpp/#multiply-two-numbers-1000-and-67","text":"Notice how you don't get 67,000? This is because our variable value of ushortint type can only hold values up to the integer 65,536. What this returns is the remainder of 67,000 - 65,536 = 1464 MultiplyTwoNumbersWithAlias ( 1000 , 67 ) 1464 std :: cout << 67 * 1000 - 65536 << std :: endl ; 1464","title":"Multiply Two Numbers 1000 and 67"},{"location":"programming/cpp/cpp/#function-with-return-value","text":"Unlike functions without return values where we use void to declare the function, here we use int to declare our function that returns values. // In this exmaple, this function returns the value of the multiplication of two numbers int MultiplyTwoNumbersNoPrint ( int firstNum , int secondNum ) { // Define variable as integer type long int value ; value = firstNum * secondNum ; return value ; }","title":"Function with Return Value"},{"location":"programming/cpp/cpp/#call-function","text":"// Declare variable with type long int returnValue ; // Call function returnValue = MultiplyTwoNumbersNoPrint ( 10 , 2 ); // Print variable std :: cout << returnValue << std :: endl ; 20","title":"Call Function"},{"location":"programming/cpp/cpp/#function-inner-workings","text":"Essentially our lines of codes translates to instruction pointers with unique memory addresses. Execution of instruction pointers operates on a \"LIFO\" basis, last in first out. Oversimplifying here, in our example, the last line is taken off first and it follows up // Code Space int varOneTest = 10 ; // Instruction pointer 100 std :: cout << varOneTest << std :: endl ; // Instruction Pointer 102 10 @0x7fa6b7de5460","title":"Function Inner Workings"},{"location":"programming/cpp/cpp/#arrays","text":"An array contains a sequence of elements with the same data type.","title":"Arrays"},{"location":"programming/cpp/cpp/#creating-an-array","text":"// This is how you declare an array of 50 elements each of type double double DoubleArray [ 50 ];","title":"Creating an Array"},{"location":"programming/cpp/cpp/#accessing-arrays-elements","text":"","title":"Accessing Array's Elements"},{"location":"programming/cpp/cpp/#first-element","text":"// This access the first array std :: cout << DoubleArray [ 0 ] << std :: endl ; 0","title":"First Element"},{"location":"programming/cpp/cpp/#last-element","text":"std :: cout << DoubleArray [ 49 ] << std :: endl ; 0","title":"Last Element"},{"location":"programming/cpp/cpp/#first-10-elements","text":"// In steps of 1 for ( int i = 0 ; i < 10 ; i ++ ) { // This is how you print a mix of characters and declared variables std :: cout << \"Element \" << i << \" contains \" << DoubleArray [ i ] << std :: endl ; } Element 0 contains 0 Element 1 contains 0 Element 2 contains 0 Element 3 contains 0 Element 4 contains 0 Element 5 contains 0 Element 6 contains 0 Element 7 contains 0 Element 8 contains 0 Element 9 contains 0 // In steps of 2 for ( int i = 0 ; i < 10 ; i += 2 ) { // This is how you print a mix of characters and declared variables std :: cout << \"Element \" << i << \" contains \" << DoubleArray [ i ] << std :: endl ; } Element 0 contains 0 Element 2 contains 0 Element 4 contains 0 Element 6 contains 0 Element 8 contains 0","title":"First 10 Elements"},{"location":"programming/cpp/cpp/#going-beyond-the-arrays-length","text":"This will return a warning that it's past the end of the array std :: cout << DoubleArray [ 50 ] << std :: endl ; input_line_36:2:15: warning: array index 50 is past the end of the array ( which contains 50 elements ) [ -Warray-bounds ] std::cout << DoubleArray[50] << std::endl; ^ ~~ input_line_32:3:1: note: array 'DoubleArray ' declared here double DoubleArray [ 50 ] ; ^ 4 .94066e-323","title":"Going Beyond The Array's Length"},{"location":"programming/cpp/cpp/#arrays-with-enumeration","text":"enum directionsNew { up , down , left , right , individualDirections }; int directionsArray [ individualDirections ] = { 1 , 2 , 3 , 4 }; std :: cout << \"Up value: \\t \" << directionsArray [ up ]; std :: cout << \" \\n Down value: \\t \" << directionsArray [ down ]; std :: cout << \" \\n Left value: \\t \" << directionsArray [ left ]; std :: cout << \" \\n Right value: \\t \" << directionsArray [ right ]; // This is the number of elements in the array std :: cout << \" \\n Num value: \\t \" << sizeof ( directionsArray ) / sizeof ( directionsArray [ 0 ]) << std :: endl ; Up value: 1 Down value: 2 Left value: 3 Right value: 4 Num value: 4","title":"Arrays with Enumeration"},{"location":"programming/cpp/cpp/#arrays-with-1-dimension-tensors","text":"","title":"Arrays with &gt;1 Dimension (Tensors)"},{"location":"programming/cpp/cpp/#multi-dimension-array-with-numbers","text":"// This is how you declare a multi-dimensional array of 5x5 elements each of type double double multiDimArray [ 5 ][ 5 ] = { { 1 , 2 , 3 , 4 , 5 }, { 2 , 2 , 3 , 4 , 5 }, { 3 , 2 , 3 , 4 , 5 }, { 4 , 2 , 3 , 4 , 5 }, { 5 , 2 , 3 , 4 , 5 } }; // Print each row of our 5x5 multi-dimensional array for ( int i = 0 ; i < 5 ; i ++ ) { for ( int j = 0 ; j < 5 ; j ++ ) { std :: cout << multiDimArray [ i ][ j ]; }; std :: cout << \" \\n \" << std :: endl ; }; 12345 22345 32345 42345 52345","title":"Multi Dimension Array with Numbers"},{"location":"programming/cpp/cpp/#multi-dimension-array-with-characters","text":"// This is how you declare a multi-dimensional array of 5x5 elements each of type char char multiDimArrayChars [ 5 ][ 5 ] = { { 'a' , 'b' , 'c' , 'd' , 'e' }, { 'b' , 'b' , 'c' , 'd' , 'e' }, { 'c' , 'b' , 'c' , 'd' , 'e' }, { 'd' , 'b' , 'c' , 'd' , 'e' }, { 'e' , 'b' , 'c' , 'd' , 'e' }, }; // Print each row of our 5x5 multi-dimensional array for ( int i = 0 ; i < 5 ; i ++ ) { for ( int j = 0 ; j < 5 ; j ++ ) { std :: cout << multiDimArrayChars [ i ][ j ]; }; std :: cout << \" \\n \" << std :: endl ; }; abcde bbcde cbcde dbcde ebcde","title":"Multi Dimension Array with Characters"},{"location":"programming/cpp/cpp/#copy-arrays","text":"","title":"Copy Arrays"},{"location":"programming/cpp/cpp/#copy-number-arrays","text":"double ArrayNumOne [] = { 1 , 2 , 3 , 4 , 5 }; double ArrayNumTwo [ 5 ]; // Use namespace std so it's cleaner using namespace std ; // Copy array with copy() copy ( begin ( ArrayNumOne ), end ( ArrayNumOne ), begin ( ArrayNumTwo )); // Print double type array with copy() copy ( begin ( ArrayNumTwo ), end ( ArrayNumTwo ), ostream_iterator < double > ( cout , \" \\n \" )); 1 2 3 4 5","title":"Copy Number Arrays"},{"location":"programming/cpp/cpp/#copy-string-arrays","text":"char ArrayCharOne [] = { 'a' , 'b' , 'c' , 'd' , 'e' }; char ArrayCharTwo [ 5 ]; // Use namespace std so it's cleaner using namespace std ; // Copy array with copy() copy ( begin ( ArrayCharOne ), end ( ArrayCharOne ), begin ( ArrayCharTwo )); // Print char type array with copy() copy ( begin ( ArrayCharTwo ), end ( ArrayCharTwo ), ostream_iterator < char > ( cout , \" \\n \" )); a b c d e","title":"Copy String Arrays"},{"location":"programming/cpp/cpp/#mathematical-operators","text":"Add, subtract, multiply, divide and modulus","title":"Mathematical Operators"},{"location":"programming/cpp/cpp/#add","text":"double addStatement = 5 + 5 ; std :: cout << addStatement ; 10","title":"Add"},{"location":"programming/cpp/cpp/#subtract","text":"double subtractStatement = 5 - 5 ; std :: cout << subtractStatement ; 0","title":"Subtract"},{"location":"programming/cpp/cpp/#divide","text":"double divideStatement = 5 / 5 ; std :: cout << divideStatement ; 1","title":"Divide"},{"location":"programming/cpp/cpp/#multiply","text":"double multiplyStatement = 5 * 5 ; std :: cout << multiplyStatement ; 25","title":"Multiply"},{"location":"programming/cpp/cpp/#modulus","text":"Gets the remainder of the division double modulusStatement = 8 % 5 ; std :: cout << modulusStatement ; 3","title":"Modulus"},{"location":"programming/cpp/cpp/#exponent","text":"Base to the power of something, this requires a new package called <cmath> that we want to include. #include <cmath> void SquareNumber ( int baseNum , int exponentNum ) { // Square the locally scoped variable with 2 int squaredNumber ; squaredNumber = pow ( baseNum , exponentNum ); std :: cout << \"Base of 2 with exponent of 2 gives: \" << squaredNumber << std :: endl ; } SquareNumber ( 2 , 2 ) Base of 2 with exponent of 2 gives: 4","title":"Exponent"},{"location":"programming/cpp/cpp/#incrementingdecrementing","text":"3 ways to do this, from least to most verbose","title":"Incrementing/Decrementing"},{"location":"programming/cpp/cpp/#methods","text":"","title":"Methods"},{"location":"programming/cpp/cpp/#method-1","text":"double idx = 1 ; idx ++ ; std :: cout << idx ; 2","title":"Method 1"},{"location":"programming/cpp/cpp/#method-2","text":"idx += 1 ; std :: cout << idx ; 3","title":"Method 2"},{"location":"programming/cpp/cpp/#method-3","text":"idx = idx + 1 ; std :: cout << idx ; 4","title":"Method 3"},{"location":"programming/cpp/cpp/#prefixpostfix","text":"","title":"Prefix/Postfix"},{"location":"programming/cpp/cpp/#prefix","text":"This will change both incremented variable and the new variable you assign the incremented variable to Summary: both variables will have the same values // Instantiate double a = 1 ; double b = 1 ; // Print original values cout << \"Old a: \\t \" << a << \" \\n \" ; cout << \"Old b: \\t \" << b << \" \\n \" ; // Prefix increment a = ++ b ; // Print new values cout << \"New a: \\t \" << a << \" \\n \" ; cout << \"New b: \\t \" << b << \" \\n \" ; Old a: 1 Old b: 1 New a: 2 New b: 2","title":"Prefix"},{"location":"programming/cpp/cpp/#postfix","text":"This will change only the incremented variable but not the variable it's assigned to Summary: incremented variable will change but not the variable it was assigned to // Instantiate double c = 2 ; double d = 2 ; // Print original values cout << \"Old c: \\t \" << c << \" \\n \" ; cout << \"Old d: \\t \" << d << \" \\n \" ; // Prefix increment c = d -- ; // Print new values, notice how only d decremented? c which is what d is assigned to doesn't change. cout << \"New c: \\t \" << c << \" \\n \" ; cout << \"New d: \\t \" << d << \" \\n \" ; Old c: 2 Old d: 2 New c: 2 New d: 1","title":"Postfix"},{"location":"programming/cpp/cpp/#conditional-statements","text":"","title":"Conditional Statements"},{"location":"programming/cpp/cpp/#if","text":"int maxValue = 10 ; // Increment till 10 for ( int i = 0 ; i <= 10 ; i += 2 ) { // Stop if the number reaches 10 (inclusive) if ( i == maxValue ) { cout << \"Reached max value!\" ; cout << \" \\n Value is \" << i << endl ; }; }; Reached max value! Value is 10","title":"If"},{"location":"programming/cpp/cpp/#else","text":"int newMaxValue = 20 ; // Increment till 10 for ( int i = 0 ; i <= 10 ; i += 2 ) { // Stop if the number reaches 10 (inclusive) if ( i == newMaxValue ) { cout << \"Reached max value!\" ; cout << \" \\n Value is \" << i << endl ; } // Else print current value else { cout << \" \\n Current Value is \" << i << endl ; } } Current Value is 0 Current Value is 2 Current Value is 4 Current Value is 6 Current Value is 8 Current Value is 10","title":"Else"},{"location":"programming/cpp/cpp/#logical-operators","text":"","title":"Logical Operators"},{"location":"programming/cpp/cpp/#and","text":"int varOneNew = 10 ; int varTwo = 10 ; int varCheckOne = 10 ; int varCheckTwo = 5 ; // This should print out if (( varOneNew == varCheckOne ) && ( varTwo == varCheckOne )) { std :: cout << \"Both values equal to 10!\" << std :: endl ; } // This should not print out as varTwo does not equal to 5 if (( varOneNew == varCheckOne ) && ( varTwo == varCheckTwo )) { std :: cout << \"VarOneNew equals to 10, VarTwo equals to 5\" << std :: endl ; } Both values equal to 10!","title":"And"},{"location":"programming/cpp/cpp/#or","text":"// On the contrary, this exact same statement would print out // as VarOne is equal to 10 and we are using an OR operator if (( varOneNew == varCheckOne ) || ( varTwo == varCheckTwo )) { std :: cout << \"VarOneNew equals to 10 or VarTwo equals to 5\" << std :: endl ; } VarOneNew equals to 10 or VarTwo equals to 5","title":"Or"},{"location":"programming/cpp/cpp/#not","text":"// This would print out as VarTwo is not equal to 5 if ( varTwo != varCheckTwo ) { std :: cout << \"VarTwo (10) is not equal to VarCheckTwo (5).\" << std :: endl ; } VarTwo (10) is not equal to VarCheckTwo (5).","title":"Not"},{"location":"programming/cpp/cpp/#getting-user-input","text":"using namespace std ; long double inputOne , inputTwo ; cout << \"This program multiplies 2 given numbers \\n \" ; cout << \"Enter first number: \\n \" ; cin >> inputOne ; cout << \"Enter second number: \\n \" ; cin >> inputTwo ; cout << \"Multiplication value: \" << inputOne * inputTwo << endl ; This program multiplies 2 given numbers Enter first number: 10 Enter second number: 10 Multiplication value: 100","title":"Getting User Input"},{"location":"programming/cpp/cpp/#loops","text":"","title":"Loops"},{"location":"programming/cpp/cpp/#for-loop","text":"for ( int i = 0 ; i < 10 ; i += 1 ) { cout << \"Value of i is: \" << i << endl ; } Value of i is: 0 Value of i is: 1 Value of i is: 2 Value of i is: 3 Value of i is: 4 Value of i is: 5 Value of i is: 6 Value of i is: 7 Value of i is: 8 Value of i is: 9","title":"For Loop"},{"location":"programming/cpp/cpp/#while-loop","text":"int idxWhile = 0 ; while ( idxWhile < 10 ) { idxWhile += 1 ; cout << \"Value of while loop i is: \" << idxWhile << endl ; } Value of while loop i is: 1 Value of while loop i is: 2 Value of while loop i is: 3 Value of while loop i is: 4 Value of while loop i is: 5 Value of while loop i is: 6 Value of while loop i is: 7 Value of while loop i is: 8 Value of while loop i is: 9 Value of while loop i is: 10","title":"While Loop"},{"location":"programming/cpp/cpp/#while-loop-with-continuebreak","text":"int idxWhileNew = 0 ; while ( idxWhileNew < 100 ) { idxWhileNew += 1 ; cout << \"Value of while loop i is: \" << idxWhile << endl ; if ( idxWhileNew == 10 ) { cout << \"Max value of 10 reached!\" << endl ; break ; } } Value of while loop i is: 10 Value of while loop i is: 10 Value of while loop i is: 10 Value of while loop i is: 10 Value of while loop i is: 10 Value of while loop i is: 10 Value of while loop i is: 10 Value of while loop i is: 10 Value of while loop i is: 10 Value of while loop i is: 10 Max value of 10 reached!","title":"While Loop with Continue/Break"},{"location":"programming/electron/electron/","text":"Electron \u00b6 Why Electron \u00b6 We choose to cover electron as you can easily use it as a front-end application across any platforms (Windows, MacOS, Linux or even a mobile application) for your AI applications. Installation of Electron \u00b6 npm i -D electron@latest Creating Electron Project \u00b6 Critical Files \u00b6 You should have 3 base files package.json , main.js and index.html to have a basic application. mkdir app cd app npm init touch main.js touch index.html Edit package.json \u00b6 When you run npm init , it should create a package.json file. But we need to make some tiny changes to leverage on electron. Key fields name : name of your app, can be anything version : version of your app, can be anything main : main javascript file, we recommend using main.js scripts : here you want to copy the whole scripts section to leverage on electron devDependencies : electron version required { \"name\" : \"dlw\" , \"version\" : \"0.1.0\" , \"main\" : \"main.js\" , \"scripts\" : { \"start\" : \"electron .\" }, \"devDependencies\" : { \"electron\" : \"^6.0.8\" } } Edit main.js \u00b6 This beautiful boilerplate code is provided by Electron, full credits to them. const { app , BrowserWindow } = require ( 'electron' ) // Keep a global reference of the window object, if you don't, the window will // be closed automatically when the JavaScript object is garbage collected. let win function createWindow () { // Create the browser window. win = new BrowserWindow ({ width : 800 , height : 600 , webPreferences : { nodeIntegration : true } }) // and load the index.html of the app. win . loadFile ( 'index.html' ) // Open the DevTools. // win.webContents.openDevTools() // Emitted when the window is closed. win . on ( 'closed' , () => { // Dereference the window object, usually you would store windows // in an array if your app supports multi windows, this is the time // when you should delete the corresponding element. win = null }) } // This method will be called when Electron has finished // initialization and is ready to create browser windows. // Some APIs can only be used after this event occurs. app . on ( 'ready' , createWindow ) // Quit when all windows are closed. app . on ( 'window-all-closed' , () => { // On macOS it is common for applications and their menu bar // to stay active until the user quits explicitly with Cmd + Q if ( process . platform !== 'darwin' ) { app . quit () } }) app . on ( 'activate' , () => { // On macOS it's common to re-create a window in the app when the // dock icon is clicked and there are no other windows open. if ( win === null ) { createWindow () } }) // In this file you can include the rest of your app's specific main process // code. You can also put them in separate files and require them here. Edit index.html \u00b6 I modified this script from electron's boilerplate code where it will display critical dependencies' versions for your node, chrome and electron. <!DOCTYPE html> < html > < head > < meta charset = \"UTF-8\" > < title > Dashboard </ title > </ head > < body > < h1 > Dashboard </ h1 > < h2 > Environment </ h2 > < br /> Node: < script > document . write ( process . versions . node )</ script > < br /> Chrome: < script > document . write ( process . versions . chrome )</ script > < br /> Electron: < script > document . write ( process . versions . electron )</ script > </ body > </ html > Starting App \u00b6 This will start your electron application. npm start Packaging Electron App \u00b6 Wine \u00b6 The reason for installing Wine is being able to package Electron applications for the Windows platform, creating the executable file app.exe like any other application on Windows. The final aim of our tutorial is to package the app for Windows, MacOS and Ubuntu. Installation of Wine \u00b6 This assumes installation on Ubuntu 16.04 xenial , if you're on Ubuntu 18.04 or 19.04, change to bionic and disco respectively. Also, this works on 64-bit system architecture. cd ~ wget -qO - https://dl.winehq.org/wine-builds/winehq.key | sudo apt-key add - sudo apt-add-repository 'deb https://dl.winehq.org/wine-builds/ubuntu/ xenial main' sudo apt-get update sudo apt-get install --install-recommends winehq-stable sudo chown root:root ~/.wine Check Wine Version \u00b6 wine --version Packaging Windows Application \u00b6 This packages the application churning the necessary files and the executable app.exe for windows 64 bit. electron-packager ./app app --platform = win32 --arch = x64 Python Scripts \u00b6 Installing Python Node Package \u00b6 So we want to easily create Python scripts and run through Javascript in the Electron application. This can be done via python-shell npm package. sudo npm install --save python-shell Creating \"Hello from JS\" \u00b6 Javascript \u00b6 In your main.js file, you would want to add the following code. This leverages on the python-shell package to send a message to hello_world.py and receive the message subsequently. // Start Python shell let { PythonShell } = require ( 'python-shell' ) // Start shell for specific script for communicating let pyshell = new PythonShell ( './scripts/hello_world.py' ); // Send a message to the Python script via stdin pyshell . send ( 'Hello from JS' ); // Receive message from Python script pyshell . on ( 'message' , function ( message ) { console . log ( message ); }); // End the input stream and allow the process to exit pyshell . end ( function ( err , code , signal ) { if ( err ) throw err ; // console.log('The exit code was: ' + code); // console.log('The exit signal was: ' + signal); console . log ( 'finished' ); }); Python \u00b6 Create a folder scripts to hold all your Python scripts. Then create a Python file named hello_world.py with the following content. import sys msg_from_js = sys . stdin . read () print ( msg_from_js ) Run App \u00b6 Run via npm start and you'll see this in your bash output. Viola! We managed to call hello_world.py via main.js through the python-shell package. Next task, we will be passing this message to index.html . Hello from JS finished","title":"Electron"},{"location":"programming/electron/electron/#electron","text":"","title":"Electron"},{"location":"programming/electron/electron/#why-electron","text":"We choose to cover electron as you can easily use it as a front-end application across any platforms (Windows, MacOS, Linux or even a mobile application) for your AI applications.","title":"Why Electron"},{"location":"programming/electron/electron/#installation-of-electron","text":"npm i -D electron@latest","title":"Installation of Electron"},{"location":"programming/electron/electron/#creating-electron-project","text":"","title":"Creating Electron Project"},{"location":"programming/electron/electron/#critical-files","text":"You should have 3 base files package.json , main.js and index.html to have a basic application. mkdir app cd app npm init touch main.js touch index.html","title":"Critical Files"},{"location":"programming/electron/electron/#edit-packagejson","text":"When you run npm init , it should create a package.json file. But we need to make some tiny changes to leverage on electron. Key fields name : name of your app, can be anything version : version of your app, can be anything main : main javascript file, we recommend using main.js scripts : here you want to copy the whole scripts section to leverage on electron devDependencies : electron version required { \"name\" : \"dlw\" , \"version\" : \"0.1.0\" , \"main\" : \"main.js\" , \"scripts\" : { \"start\" : \"electron .\" }, \"devDependencies\" : { \"electron\" : \"^6.0.8\" } }","title":"Edit package.json"},{"location":"programming/electron/electron/#edit-mainjs","text":"This beautiful boilerplate code is provided by Electron, full credits to them. const { app , BrowserWindow } = require ( 'electron' ) // Keep a global reference of the window object, if you don't, the window will // be closed automatically when the JavaScript object is garbage collected. let win function createWindow () { // Create the browser window. win = new BrowserWindow ({ width : 800 , height : 600 , webPreferences : { nodeIntegration : true } }) // and load the index.html of the app. win . loadFile ( 'index.html' ) // Open the DevTools. // win.webContents.openDevTools() // Emitted when the window is closed. win . on ( 'closed' , () => { // Dereference the window object, usually you would store windows // in an array if your app supports multi windows, this is the time // when you should delete the corresponding element. win = null }) } // This method will be called when Electron has finished // initialization and is ready to create browser windows. // Some APIs can only be used after this event occurs. app . on ( 'ready' , createWindow ) // Quit when all windows are closed. app . on ( 'window-all-closed' , () => { // On macOS it is common for applications and their menu bar // to stay active until the user quits explicitly with Cmd + Q if ( process . platform !== 'darwin' ) { app . quit () } }) app . on ( 'activate' , () => { // On macOS it's common to re-create a window in the app when the // dock icon is clicked and there are no other windows open. if ( win === null ) { createWindow () } }) // In this file you can include the rest of your app's specific main process // code. You can also put them in separate files and require them here.","title":"Edit main.js"},{"location":"programming/electron/electron/#edit-indexhtml","text":"I modified this script from electron's boilerplate code where it will display critical dependencies' versions for your node, chrome and electron. <!DOCTYPE html> < html > < head > < meta charset = \"UTF-8\" > < title > Dashboard </ title > </ head > < body > < h1 > Dashboard </ h1 > < h2 > Environment </ h2 > < br /> Node: < script > document . write ( process . versions . node )</ script > < br /> Chrome: < script > document . write ( process . versions . chrome )</ script > < br /> Electron: < script > document . write ( process . versions . electron )</ script > </ body > </ html >","title":"Edit index.html"},{"location":"programming/electron/electron/#starting-app","text":"This will start your electron application. npm start","title":"Starting App"},{"location":"programming/electron/electron/#packaging-electron-app","text":"","title":"Packaging Electron App"},{"location":"programming/electron/electron/#wine","text":"The reason for installing Wine is being able to package Electron applications for the Windows platform, creating the executable file app.exe like any other application on Windows. The final aim of our tutorial is to package the app for Windows, MacOS and Ubuntu.","title":"Wine"},{"location":"programming/electron/electron/#installation-of-wine","text":"This assumes installation on Ubuntu 16.04 xenial , if you're on Ubuntu 18.04 or 19.04, change to bionic and disco respectively. Also, this works on 64-bit system architecture. cd ~ wget -qO - https://dl.winehq.org/wine-builds/winehq.key | sudo apt-key add - sudo apt-add-repository 'deb https://dl.winehq.org/wine-builds/ubuntu/ xenial main' sudo apt-get update sudo apt-get install --install-recommends winehq-stable sudo chown root:root ~/.wine","title":"Installation of Wine"},{"location":"programming/electron/electron/#check-wine-version","text":"wine --version","title":"Check Wine Version"},{"location":"programming/electron/electron/#packaging-windows-application","text":"This packages the application churning the necessary files and the executable app.exe for windows 64 bit. electron-packager ./app app --platform = win32 --arch = x64","title":"Packaging Windows Application"},{"location":"programming/electron/electron/#python-scripts","text":"","title":"Python Scripts"},{"location":"programming/electron/electron/#installing-python-node-package","text":"So we want to easily create Python scripts and run through Javascript in the Electron application. This can be done via python-shell npm package. sudo npm install --save python-shell","title":"Installing Python Node Package"},{"location":"programming/electron/electron/#creating-hello-from-js","text":"","title":"Creating \"Hello from JS\""},{"location":"programming/electron/electron/#javascript","text":"In your main.js file, you would want to add the following code. This leverages on the python-shell package to send a message to hello_world.py and receive the message subsequently. // Start Python shell let { PythonShell } = require ( 'python-shell' ) // Start shell for specific script for communicating let pyshell = new PythonShell ( './scripts/hello_world.py' ); // Send a message to the Python script via stdin pyshell . send ( 'Hello from JS' ); // Receive message from Python script pyshell . on ( 'message' , function ( message ) { console . log ( message ); }); // End the input stream and allow the process to exit pyshell . end ( function ( err , code , signal ) { if ( err ) throw err ; // console.log('The exit code was: ' + code); // console.log('The exit signal was: ' + signal); console . log ( 'finished' ); });","title":"Javascript"},{"location":"programming/electron/electron/#python","text":"Create a folder scripts to hold all your Python scripts. Then create a Python file named hello_world.py with the following content. import sys msg_from_js = sys . stdin . read () print ( msg_from_js )","title":"Python"},{"location":"programming/electron/electron/#run-app","text":"Run via npm start and you'll see this in your bash output. Viola! We managed to call hello_world.py via main.js through the python-shell package. Next task, we will be passing this message to index.html . Hello from JS finished","title":"Run App"},{"location":"programming/javascript/javascript/","text":"Javascript \u00b6 Run Jupyter Notebook You can run the code for this section in this jupyter notebook link . Installation of iJavascript \u00b6 This will enable the Javascript kernel to be installed in your jupyter notebook kernel list so you can play with Javascript easily. For now, the installation requires the ancient Python 2.7. Hopefully this changes in the future. conda create -n py27 python=2.7 conda activate py27 sudo npm install -g --unsafe-perm ijavascript conda install jupyter ijsinstall jupyter notebook Variables & Constants \u00b6 Variable \u00b6 Declaring Variable \u00b6 var randomNumber = 1.52 Read out Variable \u00b6 // Check value randomNumber 1.52 Read out via Console \u00b6 // Using the console via log and error console . log ( randomNumber ) console . error ( randomNumber ) 1 . 52 1 . 52 Constants \u00b6 Declaring Constant \u00b6 This is a weird one, because in Javascript when you declare a constant, you are essentially unable to overwrite the constant subsequently. It's useful when you want a fixed value. const cannotOverwriteNumber = 2.22 Read out Constant \u00b6 cannotOverwriteNumber 2.22 Overwrite Constant (Error) \u00b6 This will throw an error because this constant has been declared once and you cannot do it again. const cannotOverwriteNumber = 1.52 evalmachine . < anonymous > : 1 const cannotOverwriteNumber = 1.52 ^ SyntaxError : Identifier 'cannotOverwriteNumber' has already been declared at evalmachine . < anonymous > : 1 : 1 at Script . runInThisContext ( vm . js : 122 : 20 ) at Object . runInThisContext ( vm . js : 329 : 38 ) at run ( [ eval ] : 1054 : 15 ) at onRunRequest ( [ eval ] : 888 : 18 ) at onMessage ( [ eval ] : 848 : 13 ) at process . emit ( events . js : 198 : 13 ) at emit ( internal / child_process . js : 832 : 12 ) at process . _tickCallback ( internal / process / next_tick . js : 63 : 19 )","title":"Javascript"},{"location":"programming/javascript/javascript/#javascript","text":"Run Jupyter Notebook You can run the code for this section in this jupyter notebook link .","title":"Javascript"},{"location":"programming/javascript/javascript/#installation-of-ijavascript","text":"This will enable the Javascript kernel to be installed in your jupyter notebook kernel list so you can play with Javascript easily. For now, the installation requires the ancient Python 2.7. Hopefully this changes in the future. conda create -n py27 python=2.7 conda activate py27 sudo npm install -g --unsafe-perm ijavascript conda install jupyter ijsinstall jupyter notebook","title":"Installation of iJavascript"},{"location":"programming/javascript/javascript/#variables-constants","text":"","title":"Variables &amp; Constants"},{"location":"programming/javascript/javascript/#variable","text":"","title":"Variable"},{"location":"programming/javascript/javascript/#declaring-variable","text":"var randomNumber = 1.52","title":"Declaring Variable"},{"location":"programming/javascript/javascript/#read-out-variable","text":"// Check value randomNumber 1.52","title":"Read out Variable"},{"location":"programming/javascript/javascript/#read-out-via-console","text":"// Using the console via log and error console . log ( randomNumber ) console . error ( randomNumber ) 1 . 52 1 . 52","title":"Read out via Console"},{"location":"programming/javascript/javascript/#constants","text":"","title":"Constants"},{"location":"programming/javascript/javascript/#declaring-constant","text":"This is a weird one, because in Javascript when you declare a constant, you are essentially unable to overwrite the constant subsequently. It's useful when you want a fixed value. const cannotOverwriteNumber = 2.22","title":"Declaring Constant"},{"location":"programming/javascript/javascript/#read-out-constant","text":"cannotOverwriteNumber 2.22","title":"Read out Constant"},{"location":"programming/javascript/javascript/#overwrite-constant-error","text":"This will throw an error because this constant has been declared once and you cannot do it again. const cannotOverwriteNumber = 1.52 evalmachine . < anonymous > : 1 const cannotOverwriteNumber = 1.52 ^ SyntaxError : Identifier 'cannotOverwriteNumber' has already been declared at evalmachine . < anonymous > : 1 : 1 at Script . runInThisContext ( vm . js : 122 : 20 ) at Object . runInThisContext ( vm . js : 329 : 38 ) at run ( [ eval ] : 1054 : 15 ) at onRunRequest ( [ eval ] : 888 : 18 ) at onMessage ( [ eval ] : 848 : 13 ) at process . emit ( events . js : 198 : 13 ) at emit ( internal / child_process . js : 832 : 12 ) at process . _tickCallback ( internal / process / next_tick . js : 63 : 19 )","title":"Overwrite Constant (Error)"},{"location":"programming/plotting/intro/","text":"Plotting \u00b6 Packages you will learn to use Fast professional static plots: ggplot2 and rpy2 Complicated static plots: matplotlib Live stream and/or interactive plots: plotly Dashboard: streamlit Typically with tremendous mastery of matplotlib, you will be able to create any good looking plots static or interactive. However, it takes many more lines of code and is relatively complicated. Thus, before we go there, we will be covering the fastest way to create professional looking static plots with rpy2 and ggplot rapidly. It is unconventional but it yields the fastest result for time-sensitive learners. We'll then move backwards to then create more complicated plots with matplotlib. Subsequently we'll create live stream and interactive plots with plotly. Finally we will be bringing everything together with streamlit as a dashboard. As usual, you can go freestyle and just use any of our open-source material you think is useful and you do not need to follow the sequence we are suggesting.","title":"Plotting"},{"location":"programming/plotting/intro/#plotting","text":"Packages you will learn to use Fast professional static plots: ggplot2 and rpy2 Complicated static plots: matplotlib Live stream and/or interactive plots: plotly Dashboard: streamlit Typically with tremendous mastery of matplotlib, you will be able to create any good looking plots static or interactive. However, it takes many more lines of code and is relatively complicated. Thus, before we go there, we will be covering the fastest way to create professional looking static plots with rpy2 and ggplot rapidly. It is unconventional but it yields the fastest result for time-sensitive learners. We'll then move backwards to then create more complicated plots with matplotlib. Subsequently we'll create live stream and interactive plots with plotly. Finally we will be bringing everything together with streamlit as a dashboard. As usual, you can go freestyle and just use any of our open-source material you think is useful and you do not need to follow the sequence we are suggesting.","title":"Plotting"},{"location":"programming/python/python/","text":"Python \u00b6 Lists \u00b6 Creating List: Manual Fill \u00b6 lst = [ 0 , 1 , 2 , 3 ] print ( lst ) [0, 1, 2, 3] Creating List: List Comprehension \u00b6 lst = [ i for i in range ( 4 )] print ( lst ) [0, 1, 2, 3] Joining List with Blanks \u00b6 # To use .join(), your list needs to be of type string lst_to_string = list ( map ( str , lst )) # Join the list of strings lst_join = ' ' . join ( lst_to_string ) print ( lst_join ) 0 1 2 3 Joining List with Comma \u00b6 # Join the list of strings lst_join = ', ' . join ( lst_to_string ) print ( lst_join ) 0, 1, 2, 3 Checking Lists Equal: Method 1 \u00b6 Returns True if equal, and False if unequal lst_unequal = [ 1 , 1 , 2 , 3 , 4 , 4 ] lst_equal = [ 0 , 0 , 0 , 0 , 0 , 0 ] print ( '-' * 50 ) print ( 'Unequal List' ) print ( '-' * 50 ) print ( lst_unequal [ 1 :]) print ( lst_unequal [: - 1 ]) bool_equal = lst_unequal [ 1 :] == lst_unequal [: - 1 ] print ( bool_equal ) print ( '-' * 50 ) print ( 'Equal List' ) print ( '-' * 50 ) print ( lst_equal [ 1 :]) print ( lst_equal [: - 1 ]) bool_equal = lst_equal [ 1 :] == lst_equal [: - 1 ] print ( bool_equal ) -------------------------------------------------- Unequal List -------------------------------------------------- [ 1 , 2 , 3 , 4 , 4 ] [ 1 , 1 , 2 , 3 , 4 ] False -------------------------------------------------- Equal List -------------------------------------------------- [ 0 , 0 , 0 , 0 , 0 ] [ 0 , 0 , 0 , 0 , 0 ] True Checking Lists Equal: Method 2 \u00b6 Returns True if equal, and False if unequal. Here, all essentially checks that there is no False in the list. print ( '-' * 50 ) print ( 'Unequal List' ) print ( '-' * 50 ) lst_check = [ i == lst_unequal [ 0 ] for i in lst_unequal ] bool_equal = all ( lst_check ) print ( bool_equal ) print ( '-' * 50 ) print ( 'Equal List' ) print ( '-' * 50 ) lst_check = [ i == lst_equal [ 0 ] for i in lst_equal ] bool_equal = all ( lst_check ) print ( bool_equal ) -------------------------------------------------- Unequal List -------------------------------------------------- False -------------------------------------------------- Equal List -------------------------------------------------- True Sets \u00b6 Removing Duplicate from List \u00b6 Sets can be very useful for quickly removing duplicates from a list, essentially finding unique values lst_one = [ 1 , 2 , 3 , 5 ] lst_two = [ 1 , 1 , 2 , 4 ] lst_both = lst_one + lst_two lst_no_duplicate = list ( set ( lst_both )) print ( f 'Original Combined List { lst_both } ' ) print ( f 'No Duplicated Combined List { lst_no_duplicate } ' ) Original Combined List [1, 2, 3, 5, 1, 1, 2, 4] No Duplicated Combined List [1, 2, 3, 4, 5] Lambda, map, filter, reduce, partial \u00b6 Lambda \u00b6 The syntax is simple lambda your_variables: your_operation Add Function \u00b6 add = lambda x , y : x + y add ( 2 , 3 ) 5 Multiply Function \u00b6 multiply = lambda x , y : x * y multiply ( 2 , 3 ) 6 Map \u00b6 Create List \u00b6 lst = [ i for i in range ( 11 )] print ( lst ) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] Map Square Function to List \u00b6 square_element = map ( lambda x : x ** 2 , lst ) # This gives you a map object print ( square_element ) # You need to explicitly return a list print ( list ( square_element )) <map object at 0x7f08c8620438> [0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100] Create Multiple List \u00b6 lst_1 = [ 1 , 2 , 3 , 4 ] lst_2 = [ 2 , 4 , 6 , 8 ] lst_3 = [ 3 , 6 , 9 , 12 ] Map Add Function to Multiple Lists \u00b6 add_elements = map ( lambda x , y , z : x + y + z , lst_1 , lst_2 , lst_3 ) print ( list ( add_elements )) [6, 12, 18, 24] Filter \u00b6 Create List \u00b6 lst = [ i for i in range ( 10 )] print ( lst ) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] Filter multiples of 3 \u00b6 multiples_of_three = filter ( lambda x : x % 3 == 0 , lst ) print ( list ( multiples_of_three )) [0, 3, 6, 9] Reduce \u00b6 The syntax is reduce(function, sequence) . The function is applied to the elements in the list in a sequential manner. Meaning if lst = [1, 2, 3, 4] and you have a sum function, you would arrive with ((1+2) + 3) + 4 . from functools import reduce sum_all = reduce ( lambda x , y : x + y , lst ) # Here we've 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 print ( sum_all ) print ( 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 ) 45 45 Partial \u00b6 Allows us to predefine and freeze a function's argument. Combined with lambda, it allows us to have more flexibility beyond lambda's restriction of a single line. from functools import partial def display_sum_three ( a , b , c ): sum_all = a + b + c print ( f 'Sum is { sum_all } ' ) fixed_args_func = partial ( display_sum_three , b = 3 , c = 4 ) # Given fixed arguments b=3 and c=4 # We add the new variable against the fixed arguments var_int = 1 fixed_args_func ( var_int ) # More advanced mapping with partial # Add a variable from 0 to 9 to the constants print ( '-' * 50 ) _ = list ( map ( fixed_args_func , list ( range ( 10 )))) # How about using with lambda to modifying constants without # declaring your function again? print ( '-' * 50 ) _ = list ( map ( lambda x : fixed_args_func ( x , b = 2 ), list ( range ( 10 )))) Sum is 8 -------------------------------------------------- Sum is 7 Sum is 8 Sum is 9 Sum is 10 Sum is 11 Sum is 12 Sum is 13 Sum is 14 Sum is 15 Sum is 16 -------------------------------------------------- Sum is 6 Sum is 7 Sum is 8 Sum is 9 Sum is 10 Sum is 11 Sum is 12 Sum is 13 Sum is 14 Sum is 15 Generators \u00b6 Why: generators are typically more memory-efficient than using simple for loops Imagine wanting to sum digits 0 to 1 trillion, using a list containing those numbers and summing them would be very RAM memory-inefficient. Using a generator would allow you to sum one digit sequentially, staggering the RAM memory usage in steps. What: generator basically a function that returns an iterable object where we can iterate one bye one Types: generator functions and generator expressions Dependencies: we need to install a memory profiler, so install via pip install memory_profiler Simple custom generator function example: sum 1 to 1,000,000 \u00b6 What: let's create a simple generator, allowing us to iterate through the digits 1 to 1,000,000 (inclusive) one by one with an increment of 1 at each step and summing them How: 2 step process with a while and a yield # Load memory profiler % load_ext memory_profiler # Here we take a step from 1 def create_numbers ( end_number ): current_number = 1 # Step 1: while while current_number <= end_number : # Step 2: yield yield current_number # Add to current number current_number += 1 # Here we sum the digits 1 to 100 (inclusive) and time it % memit total = sum ( create_numbers ( 1e6 )) print ( total ) peak memory: 46.50 MiB, increment: 0.28 MiB 500000500000 Without generator function: sum with list \u00b6 Say we don't use a generator, and have a list of digits 0 to 1,000,000 (inclusive) in memory then sum them. Notice how this is double the memory than using a generator! % memit total = sum ( list ( range ( int ( 1e6 ) + 1 ))) print ( total ) peak memory: 85.14 MiB, increment: 38.38 MiB 500000500000 Without generator function: sum with for loop \u00b6 Say we don't use a generator and don't put all our numbers into a list Notiice how this is much better than summing a list but still worst than a generator in terms of memory? def sum_with_loop ( end_number ): total = 0 for i in range ( end_number + 1 ): i += 1 total += i return total % memit total = sum_with_loop ( int ( 1e6 )) print ( total ) peak memory: 54.49 MiB, increment: 0.00 MiB 500001500001 Generator expression \u00b6 Like list/dictionary expressions, we can have generator expressions too We can quickly create generators this way, allowing us to make computations on the fly rather than pre-compute on a whole list/array of numbers This is more memory efficient # Define the list list_of_numbers = list ( range ( 10 )) # Find square root using the list comprehension list_of_results = [ number ** 2 for number in list_of_numbers ] print ( list_of_results ) # Use generator expression to calculate the square root generator_of_results = ( number ** 2 for number in list_of_numbers ) print ( generator_of_results ) for idx in range ( 10 ): print ( next ( generator_of_results )) [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] <generator object <genexpr> at 0x7f08c85aa4f8> 0 1 4 9 16 25 36 49 64 81 Decorators \u00b6 This allows us to to modify our original function or even entirely replace it without changing the function's code. It sounds mind-boggling, but a simple case I would like to illustrate here is using decorators for consistent logging (formatted print statements). For us to understand decorators, we'll first need to understand: first class objects *args *kwargs First Class Objects \u00b6 def outer (): def inner (): print ( 'Inside inner() function.' ) # This returns a function. return inner # Here, we are assigning `outer()` function to the object `call_outer`. call_outer = outer () # Then we call `call_outer()` call_outer () Inside inner() function. *args \u00b6 This is used to indicate that positional arguments should be stored in the variable args * is for iterables and positional parameters # Define dummy function def dummy_func ( * args ): print ( args ) # * allows us to extract positional variables from an iterable when we are calling a function dummy_func ( * range ( 10 )) # If we do not use *, this would happen dummy_func ( range ( 10 )) # See how we can have varying arguments? dummy_func ( * range ( 2 )) (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) (range(0, 10),) (0, 1) **kwargs \u00b6 ** is for dictionaries & key/value pairs # New dummy function def dummy_func_new ( ** kwargs ): print ( kwargs ) # Call function with no arguments dummy_func_new () # Call function with 2 arguments dummy_func_new ( a = 0 , b = 1 ) # Again, there's no limit to the number of arguments. dummy_func_new ( a = 0 , b = 1 , c = 2 ) # Or we can just pass the whole dictionary object if we want new_dict = { 'a' : 0 , 'b' : 1 , 'c' : 2 , 'd' : 3 } dummy_func_new ( ** new_dict ) {} {'a': 0, 'b': 1} {'a': 0, 'b': 1, 'c': 2} {'a': 0, 'b': 1, 'c': 2, 'd': 3} Decorators as Logger and Debugging \u00b6 A simple way to remember the power of decorators is that the decorator (the nested function illustrated below) can (1) access the passed arguments of the decorated function and (2) access the decorated function Therefore this allows us to modify the decorated function without changing the decorated function # Create a nested function that will be our decorator def function_inspector ( func ): def inner ( * args , ** kwargs ): result = func ( * args , ** kwargs ) print ( f 'Function args: { args } ' ) print ( f 'Function kwargs: { kwargs } ' ) print ( f 'Function return result: { result } ' ) return result return inner # Decorate our multiply function with our logger for easy logging # Of arguments pass to the function and results returned @function_inspector def multiply_func ( num_one , num_two ): return num_one * num_two multiply_result = multiply_func ( num_one = 1 , num_two = 2 ) Function args: () Function kwargs: {'num_one': 1, 'num_two': 2} Function return result: 2 Dates \u00b6 Get Current Date \u00b6 import datetime now = datetime . datetime . now () print ( now ) 2019-08-12 14:20:45.604849 Get Clean String Current Date \u00b6 # YYYY-MM-DD now . date () . strftime ( '20%y-%m- %d ' ) '2019-08-12' Count Business Days \u00b6 # Number of business days in a month from Jan 2019 to Feb 2019 import numpy as np days = np . busday_count ( '2019-01' , '2019-02' ) print ( days ) 23 Progress Bars \u00b6 TQDM \u00b6 Simple progress bar via pip install tqdm from tqdm import tqdm import time for i in tqdm ( range ( 100 )): time . sleep ( 0.1 ) pass 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:10<00:00, 9.91it/s] Check Paths \u00b6 Check Path Exists \u00b6 Check if directory exists import os directory = 'new_dir' print ( os . path . exists ( directory )) # Magic function to list all folders ! ls - d */ False ls: cannot access '*/': No such file or directory Check Path Exists Otherwise Create Folder \u00b6 Check if directory exists, otherwise make folder if not os . path . exists ( directory ): os . makedirs ( directory ) # Magic function to list all folders ! ls - d */ # Remove directory ! rmdir new_dir new_dir/ Exception Handling \u00b6 Try, Except, Finally: Error \u00b6 This is very handy and often exploited to patch up (save) poorly written code You can use general exceptions or specific ones like ValueError , KeyboardInterrupt and MemoryError to name a few value_one = 'a' value_two = 2 # Try the following line of code try : final_sum = value_one / value_two print ( 'Code passed!' ) # If the code above fails, code nested under except will be executed except : print ( 'Code failed!' ) # This will run no matter whether the nested code in try or except is executed finally : print ( 'Ran code block regardless of error or not.' ) Code failed! Ran code block regardless of error or not. Try, Except, Finally: No Error \u00b6 There won't be errors because you can divide 4 with 2 value_one = 4 value_two = 2 # Try the following line of code try : final_sum = value_one / value_two print ( 'Code passed!' ) # If the code above fails, code nested under except will be executed except : print ( 'Code failed!' ) # This will run no matter whether the nested code in try or except is executed finally : print ( 'Ran code block regardless of error or not.' ) Code passed! Ran code block regardless of error or not. Assertion \u00b6 This comes in handy when you want to enforce strict requirmenets of a certain value, shape, value type, or others for i in range ( 10 ): assert i <= 5 , 'Value is more than 5, rejected' print ( f 'Passed assertion for value { i } ' ) Passed assertion for value 0 Passed assertion for value 1 Passed assertion for value 2 Passed assertion for value 3 Passed assertion for value 4 Passed assertion for value 5 --------------------------------------------------------------------------- AssertionError Traceback ( most recent call last ) < ipython - input - 2 - d9d077e139a9 > in < module > 1 for i in range ( 10 ): ----> 2 assert i <= 5, 'Value is more than 5, rejected' 3 print ( f 'Passed assertion for value {i}' ) AssertionError : Value is more than 5 , rejected Asynchronous \u00b6 Concurrency, Parallelism, Asynchronous \u00b6 Concurrency (single CPU core): multiple threads on a single core running in sequence , only 1 thread is making progress at any point Think of 1 human, packing a box then wrapping the box Parallelism (mutliple GPU cores): multiple threads on multiple cores running in parallel , multiple threads can be making progress Think of 2 humans, one packing a box, another wrapping the box Asynchronous: concurrency but with a more dynamic system that moves amongst threads more efficiently rather than waiting for a task to finish then moving to the next task Python's asyncio allows us to code asynchronously Benefits: Scales better if you need to wait on a lot of processes Less memory (easier in this sense) to wait on thousands of co-routines than running on thousands of threads Good for IO bound uses like reading/saving from databases while subsequently running other computation Easier management than multi-thread processing like in parallel programming In the sense that everything operates sequentially in the same memory space Asynchronous Key Components \u00b6 The three main parts are (1) coroutines and subroutines, (2) event loops, and (3) future. Co-routine and subroutines Subroutine: the usual function Coroutine: this allows us to maintain states with memory of where things stopped so we can swap amongst subroutines async declares a function as a coroutine await to call a coroutine Event loops Future Synchronous 2 Function Calls \u00b6 import timeit def add_numbers ( num_1 , num_2 ): print ( 'Adding' ) time . sleep ( 1 ) return num_1 + num_2 def display_sum ( num_1 , num_2 ): total_sum = add_numbers ( num_1 , num_2 ) print ( f 'Total sum { total_sum } ' ) def main (): display_sum ( 2 , 2 ) display_sum ( 2 , 2 ) start = timeit . default_timer () main () end = timeit . default_timer () total_time = end - start print ( f 'Total time { total_time : .2f } s' ) Adding Total sum 4 Adding Total sum 4 Total time 2.00s Parallel 2 Function Calls \u00b6 from multiprocessing import Pool from functools import partial start = timeit . default_timer () pool = Pool () result = pool . map ( partial ( display_sum , num_2 = 2 ), [ 2 , 2 ]) end = timeit . default_timer () total_time = end - start print ( f 'Total time { total_time : .2f } s' ) Adding Adding Total sum 4 Total sum 4 Total time 1.08s Asynchronous 2 Function Calls \u00b6 For this use case, it'll take half the time compared to a synchronous application and slightly faster than parallel application (although not always true for parallel except in this case) import asyncio import timeit import time async def add_numbers ( num_1 , num_2 ): print ( 'Adding' ) await asyncio . sleep ( 1 ) return num_1 + num_2 async def display_sum ( num_1 , num_2 ): total_sum = await add_numbers ( num_1 , num_2 ) print ( f 'Total sum { total_sum } ' ) async def main (): # .gather allows us to group subroutines await asyncio . gather ( display_sum ( 2 , 2 ), display_sum ( 2 , 2 )) start = timeit . default_timer () # For .ipynb, event loop already done await main () # For .py # asyncio.run(main()) end = timeit . default_timer () total_time = end - start print ( f 'Total time { total_time : .4f } s' ) Adding Adding Total sum 4 Total sum 4 Total time 1.0021s","title":"Python"},{"location":"programming/python/python/#python","text":"","title":"Python"},{"location":"programming/python/python/#lists","text":"","title":"Lists"},{"location":"programming/python/python/#creating-list-manual-fill","text":"lst = [ 0 , 1 , 2 , 3 ] print ( lst ) [0, 1, 2, 3]","title":"Creating List: Manual Fill"},{"location":"programming/python/python/#creating-list-list-comprehension","text":"lst = [ i for i in range ( 4 )] print ( lst ) [0, 1, 2, 3]","title":"Creating List: List Comprehension"},{"location":"programming/python/python/#joining-list-with-blanks","text":"# To use .join(), your list needs to be of type string lst_to_string = list ( map ( str , lst )) # Join the list of strings lst_join = ' ' . join ( lst_to_string ) print ( lst_join ) 0 1 2 3","title":"Joining List with Blanks"},{"location":"programming/python/python/#joining-list-with-comma","text":"# Join the list of strings lst_join = ', ' . join ( lst_to_string ) print ( lst_join ) 0, 1, 2, 3","title":"Joining List with Comma"},{"location":"programming/python/python/#checking-lists-equal-method-1","text":"Returns True if equal, and False if unequal lst_unequal = [ 1 , 1 , 2 , 3 , 4 , 4 ] lst_equal = [ 0 , 0 , 0 , 0 , 0 , 0 ] print ( '-' * 50 ) print ( 'Unequal List' ) print ( '-' * 50 ) print ( lst_unequal [ 1 :]) print ( lst_unequal [: - 1 ]) bool_equal = lst_unequal [ 1 :] == lst_unequal [: - 1 ] print ( bool_equal ) print ( '-' * 50 ) print ( 'Equal List' ) print ( '-' * 50 ) print ( lst_equal [ 1 :]) print ( lst_equal [: - 1 ]) bool_equal = lst_equal [ 1 :] == lst_equal [: - 1 ] print ( bool_equal ) -------------------------------------------------- Unequal List -------------------------------------------------- [ 1 , 2 , 3 , 4 , 4 ] [ 1 , 1 , 2 , 3 , 4 ] False -------------------------------------------------- Equal List -------------------------------------------------- [ 0 , 0 , 0 , 0 , 0 ] [ 0 , 0 , 0 , 0 , 0 ] True","title":"Checking Lists Equal: Method 1"},{"location":"programming/python/python/#checking-lists-equal-method-2","text":"Returns True if equal, and False if unequal. Here, all essentially checks that there is no False in the list. print ( '-' * 50 ) print ( 'Unequal List' ) print ( '-' * 50 ) lst_check = [ i == lst_unequal [ 0 ] for i in lst_unequal ] bool_equal = all ( lst_check ) print ( bool_equal ) print ( '-' * 50 ) print ( 'Equal List' ) print ( '-' * 50 ) lst_check = [ i == lst_equal [ 0 ] for i in lst_equal ] bool_equal = all ( lst_check ) print ( bool_equal ) -------------------------------------------------- Unequal List -------------------------------------------------- False -------------------------------------------------- Equal List -------------------------------------------------- True","title":"Checking Lists Equal: Method 2"},{"location":"programming/python/python/#sets","text":"","title":"Sets"},{"location":"programming/python/python/#removing-duplicate-from-list","text":"Sets can be very useful for quickly removing duplicates from a list, essentially finding unique values lst_one = [ 1 , 2 , 3 , 5 ] lst_two = [ 1 , 1 , 2 , 4 ] lst_both = lst_one + lst_two lst_no_duplicate = list ( set ( lst_both )) print ( f 'Original Combined List { lst_both } ' ) print ( f 'No Duplicated Combined List { lst_no_duplicate } ' ) Original Combined List [1, 2, 3, 5, 1, 1, 2, 4] No Duplicated Combined List [1, 2, 3, 4, 5]","title":"Removing Duplicate from List"},{"location":"programming/python/python/#lambda-map-filter-reduce-partial","text":"","title":"Lambda, map, filter, reduce, partial"},{"location":"programming/python/python/#lambda","text":"The syntax is simple lambda your_variables: your_operation","title":"Lambda"},{"location":"programming/python/python/#add-function","text":"add = lambda x , y : x + y add ( 2 , 3 ) 5","title":"Add Function"},{"location":"programming/python/python/#multiply-function","text":"multiply = lambda x , y : x * y multiply ( 2 , 3 ) 6","title":"Multiply Function"},{"location":"programming/python/python/#map","text":"","title":"Map"},{"location":"programming/python/python/#create-list","text":"lst = [ i for i in range ( 11 )] print ( lst ) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]","title":"Create List"},{"location":"programming/python/python/#map-square-function-to-list","text":"square_element = map ( lambda x : x ** 2 , lst ) # This gives you a map object print ( square_element ) # You need to explicitly return a list print ( list ( square_element )) <map object at 0x7f08c8620438> [0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100]","title":"Map Square Function to List"},{"location":"programming/python/python/#create-multiple-list","text":"lst_1 = [ 1 , 2 , 3 , 4 ] lst_2 = [ 2 , 4 , 6 , 8 ] lst_3 = [ 3 , 6 , 9 , 12 ]","title":"Create Multiple List"},{"location":"programming/python/python/#map-add-function-to-multiple-lists","text":"add_elements = map ( lambda x , y , z : x + y + z , lst_1 , lst_2 , lst_3 ) print ( list ( add_elements )) [6, 12, 18, 24]","title":"Map Add Function to Multiple Lists"},{"location":"programming/python/python/#filter","text":"","title":"Filter"},{"location":"programming/python/python/#create-list_1","text":"lst = [ i for i in range ( 10 )] print ( lst ) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]","title":"Create List"},{"location":"programming/python/python/#filter-multiples-of-3","text":"multiples_of_three = filter ( lambda x : x % 3 == 0 , lst ) print ( list ( multiples_of_three )) [0, 3, 6, 9]","title":"Filter multiples of 3"},{"location":"programming/python/python/#reduce","text":"The syntax is reduce(function, sequence) . The function is applied to the elements in the list in a sequential manner. Meaning if lst = [1, 2, 3, 4] and you have a sum function, you would arrive with ((1+2) + 3) + 4 . from functools import reduce sum_all = reduce ( lambda x , y : x + y , lst ) # Here we've 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 print ( sum_all ) print ( 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 ) 45 45","title":"Reduce"},{"location":"programming/python/python/#partial","text":"Allows us to predefine and freeze a function's argument. Combined with lambda, it allows us to have more flexibility beyond lambda's restriction of a single line. from functools import partial def display_sum_three ( a , b , c ): sum_all = a + b + c print ( f 'Sum is { sum_all } ' ) fixed_args_func = partial ( display_sum_three , b = 3 , c = 4 ) # Given fixed arguments b=3 and c=4 # We add the new variable against the fixed arguments var_int = 1 fixed_args_func ( var_int ) # More advanced mapping with partial # Add a variable from 0 to 9 to the constants print ( '-' * 50 ) _ = list ( map ( fixed_args_func , list ( range ( 10 )))) # How about using with lambda to modifying constants without # declaring your function again? print ( '-' * 50 ) _ = list ( map ( lambda x : fixed_args_func ( x , b = 2 ), list ( range ( 10 )))) Sum is 8 -------------------------------------------------- Sum is 7 Sum is 8 Sum is 9 Sum is 10 Sum is 11 Sum is 12 Sum is 13 Sum is 14 Sum is 15 Sum is 16 -------------------------------------------------- Sum is 6 Sum is 7 Sum is 8 Sum is 9 Sum is 10 Sum is 11 Sum is 12 Sum is 13 Sum is 14 Sum is 15","title":"Partial"},{"location":"programming/python/python/#generators","text":"Why: generators are typically more memory-efficient than using simple for loops Imagine wanting to sum digits 0 to 1 trillion, using a list containing those numbers and summing them would be very RAM memory-inefficient. Using a generator would allow you to sum one digit sequentially, staggering the RAM memory usage in steps. What: generator basically a function that returns an iterable object where we can iterate one bye one Types: generator functions and generator expressions Dependencies: we need to install a memory profiler, so install via pip install memory_profiler","title":"Generators"},{"location":"programming/python/python/#simple-custom-generator-function-example-sum-1-to-1000000","text":"What: let's create a simple generator, allowing us to iterate through the digits 1 to 1,000,000 (inclusive) one by one with an increment of 1 at each step and summing them How: 2 step process with a while and a yield # Load memory profiler % load_ext memory_profiler # Here we take a step from 1 def create_numbers ( end_number ): current_number = 1 # Step 1: while while current_number <= end_number : # Step 2: yield yield current_number # Add to current number current_number += 1 # Here we sum the digits 1 to 100 (inclusive) and time it % memit total = sum ( create_numbers ( 1e6 )) print ( total ) peak memory: 46.50 MiB, increment: 0.28 MiB 500000500000","title":"Simple custom generator function example: sum 1 to 1,000,000"},{"location":"programming/python/python/#without-generator-function-sum-with-list","text":"Say we don't use a generator, and have a list of digits 0 to 1,000,000 (inclusive) in memory then sum them. Notice how this is double the memory than using a generator! % memit total = sum ( list ( range ( int ( 1e6 ) + 1 ))) print ( total ) peak memory: 85.14 MiB, increment: 38.38 MiB 500000500000","title":"Without generator function: sum with list"},{"location":"programming/python/python/#without-generator-function-sum-with-for-loop","text":"Say we don't use a generator and don't put all our numbers into a list Notiice how this is much better than summing a list but still worst than a generator in terms of memory? def sum_with_loop ( end_number ): total = 0 for i in range ( end_number + 1 ): i += 1 total += i return total % memit total = sum_with_loop ( int ( 1e6 )) print ( total ) peak memory: 54.49 MiB, increment: 0.00 MiB 500001500001","title":"Without generator function: sum with for loop"},{"location":"programming/python/python/#generator-expression","text":"Like list/dictionary expressions, we can have generator expressions too We can quickly create generators this way, allowing us to make computations on the fly rather than pre-compute on a whole list/array of numbers This is more memory efficient # Define the list list_of_numbers = list ( range ( 10 )) # Find square root using the list comprehension list_of_results = [ number ** 2 for number in list_of_numbers ] print ( list_of_results ) # Use generator expression to calculate the square root generator_of_results = ( number ** 2 for number in list_of_numbers ) print ( generator_of_results ) for idx in range ( 10 ): print ( next ( generator_of_results )) [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] <generator object <genexpr> at 0x7f08c85aa4f8> 0 1 4 9 16 25 36 49 64 81","title":"Generator expression"},{"location":"programming/python/python/#decorators","text":"This allows us to to modify our original function or even entirely replace it without changing the function's code. It sounds mind-boggling, but a simple case I would like to illustrate here is using decorators for consistent logging (formatted print statements). For us to understand decorators, we'll first need to understand: first class objects *args *kwargs","title":"Decorators"},{"location":"programming/python/python/#first-class-objects","text":"def outer (): def inner (): print ( 'Inside inner() function.' ) # This returns a function. return inner # Here, we are assigning `outer()` function to the object `call_outer`. call_outer = outer () # Then we call `call_outer()` call_outer () Inside inner() function.","title":"First Class Objects"},{"location":"programming/python/python/#args","text":"This is used to indicate that positional arguments should be stored in the variable args * is for iterables and positional parameters # Define dummy function def dummy_func ( * args ): print ( args ) # * allows us to extract positional variables from an iterable when we are calling a function dummy_func ( * range ( 10 )) # If we do not use *, this would happen dummy_func ( range ( 10 )) # See how we can have varying arguments? dummy_func ( * range ( 2 )) (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) (range(0, 10),) (0, 1)","title":"*args"},{"location":"programming/python/python/#kwargs","text":"** is for dictionaries & key/value pairs # New dummy function def dummy_func_new ( ** kwargs ): print ( kwargs ) # Call function with no arguments dummy_func_new () # Call function with 2 arguments dummy_func_new ( a = 0 , b = 1 ) # Again, there's no limit to the number of arguments. dummy_func_new ( a = 0 , b = 1 , c = 2 ) # Or we can just pass the whole dictionary object if we want new_dict = { 'a' : 0 , 'b' : 1 , 'c' : 2 , 'd' : 3 } dummy_func_new ( ** new_dict ) {} {'a': 0, 'b': 1} {'a': 0, 'b': 1, 'c': 2} {'a': 0, 'b': 1, 'c': 2, 'd': 3}","title":"**kwargs"},{"location":"programming/python/python/#decorators-as-logger-and-debugging","text":"A simple way to remember the power of decorators is that the decorator (the nested function illustrated below) can (1) access the passed arguments of the decorated function and (2) access the decorated function Therefore this allows us to modify the decorated function without changing the decorated function # Create a nested function that will be our decorator def function_inspector ( func ): def inner ( * args , ** kwargs ): result = func ( * args , ** kwargs ) print ( f 'Function args: { args } ' ) print ( f 'Function kwargs: { kwargs } ' ) print ( f 'Function return result: { result } ' ) return result return inner # Decorate our multiply function with our logger for easy logging # Of arguments pass to the function and results returned @function_inspector def multiply_func ( num_one , num_two ): return num_one * num_two multiply_result = multiply_func ( num_one = 1 , num_two = 2 ) Function args: () Function kwargs: {'num_one': 1, 'num_two': 2} Function return result: 2","title":"Decorators as Logger and Debugging"},{"location":"programming/python/python/#dates","text":"","title":"Dates"},{"location":"programming/python/python/#get-current-date","text":"import datetime now = datetime . datetime . now () print ( now ) 2019-08-12 14:20:45.604849","title":"Get Current Date"},{"location":"programming/python/python/#get-clean-string-current-date","text":"# YYYY-MM-DD now . date () . strftime ( '20%y-%m- %d ' ) '2019-08-12'","title":"Get Clean String Current Date"},{"location":"programming/python/python/#count-business-days","text":"# Number of business days in a month from Jan 2019 to Feb 2019 import numpy as np days = np . busday_count ( '2019-01' , '2019-02' ) print ( days ) 23","title":"Count Business Days"},{"location":"programming/python/python/#progress-bars","text":"","title":"Progress Bars"},{"location":"programming/python/python/#tqdm","text":"Simple progress bar via pip install tqdm from tqdm import tqdm import time for i in tqdm ( range ( 100 )): time . sleep ( 0.1 ) pass 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:10<00:00, 9.91it/s]","title":"TQDM"},{"location":"programming/python/python/#check-paths","text":"","title":"Check Paths"},{"location":"programming/python/python/#check-path-exists","text":"Check if directory exists import os directory = 'new_dir' print ( os . path . exists ( directory )) # Magic function to list all folders ! ls - d */ False ls: cannot access '*/': No such file or directory","title":"Check Path Exists"},{"location":"programming/python/python/#check-path-exists-otherwise-create-folder","text":"Check if directory exists, otherwise make folder if not os . path . exists ( directory ): os . makedirs ( directory ) # Magic function to list all folders ! ls - d */ # Remove directory ! rmdir new_dir new_dir/","title":"Check Path Exists Otherwise Create Folder"},{"location":"programming/python/python/#exception-handling","text":"","title":"Exception Handling"},{"location":"programming/python/python/#try-except-finally-error","text":"This is very handy and often exploited to patch up (save) poorly written code You can use general exceptions or specific ones like ValueError , KeyboardInterrupt and MemoryError to name a few value_one = 'a' value_two = 2 # Try the following line of code try : final_sum = value_one / value_two print ( 'Code passed!' ) # If the code above fails, code nested under except will be executed except : print ( 'Code failed!' ) # This will run no matter whether the nested code in try or except is executed finally : print ( 'Ran code block regardless of error or not.' ) Code failed! Ran code block regardless of error or not.","title":"Try, Except, Finally: Error"},{"location":"programming/python/python/#try-except-finally-no-error","text":"There won't be errors because you can divide 4 with 2 value_one = 4 value_two = 2 # Try the following line of code try : final_sum = value_one / value_two print ( 'Code passed!' ) # If the code above fails, code nested under except will be executed except : print ( 'Code failed!' ) # This will run no matter whether the nested code in try or except is executed finally : print ( 'Ran code block regardless of error or not.' ) Code passed! Ran code block regardless of error or not.","title":"Try, Except, Finally: No Error"},{"location":"programming/python/python/#assertion","text":"This comes in handy when you want to enforce strict requirmenets of a certain value, shape, value type, or others for i in range ( 10 ): assert i <= 5 , 'Value is more than 5, rejected' print ( f 'Passed assertion for value { i } ' ) Passed assertion for value 0 Passed assertion for value 1 Passed assertion for value 2 Passed assertion for value 3 Passed assertion for value 4 Passed assertion for value 5 --------------------------------------------------------------------------- AssertionError Traceback ( most recent call last ) < ipython - input - 2 - d9d077e139a9 > in < module > 1 for i in range ( 10 ): ----> 2 assert i <= 5, 'Value is more than 5, rejected' 3 print ( f 'Passed assertion for value {i}' ) AssertionError : Value is more than 5 , rejected","title":"Assertion"},{"location":"programming/python/python/#asynchronous","text":"","title":"Asynchronous"},{"location":"programming/python/python/#concurrency-parallelism-asynchronous","text":"Concurrency (single CPU core): multiple threads on a single core running in sequence , only 1 thread is making progress at any point Think of 1 human, packing a box then wrapping the box Parallelism (mutliple GPU cores): multiple threads on multiple cores running in parallel , multiple threads can be making progress Think of 2 humans, one packing a box, another wrapping the box Asynchronous: concurrency but with a more dynamic system that moves amongst threads more efficiently rather than waiting for a task to finish then moving to the next task Python's asyncio allows us to code asynchronously Benefits: Scales better if you need to wait on a lot of processes Less memory (easier in this sense) to wait on thousands of co-routines than running on thousands of threads Good for IO bound uses like reading/saving from databases while subsequently running other computation Easier management than multi-thread processing like in parallel programming In the sense that everything operates sequentially in the same memory space","title":"Concurrency, Parallelism, Asynchronous"},{"location":"programming/python/python/#asynchronous-key-components","text":"The three main parts are (1) coroutines and subroutines, (2) event loops, and (3) future. Co-routine and subroutines Subroutine: the usual function Coroutine: this allows us to maintain states with memory of where things stopped so we can swap amongst subroutines async declares a function as a coroutine await to call a coroutine Event loops Future","title":"Asynchronous Key Components"},{"location":"programming/python/python/#synchronous-2-function-calls","text":"import timeit def add_numbers ( num_1 , num_2 ): print ( 'Adding' ) time . sleep ( 1 ) return num_1 + num_2 def display_sum ( num_1 , num_2 ): total_sum = add_numbers ( num_1 , num_2 ) print ( f 'Total sum { total_sum } ' ) def main (): display_sum ( 2 , 2 ) display_sum ( 2 , 2 ) start = timeit . default_timer () main () end = timeit . default_timer () total_time = end - start print ( f 'Total time { total_time : .2f } s' ) Adding Total sum 4 Adding Total sum 4 Total time 2.00s","title":"Synchronous 2 Function Calls"},{"location":"programming/python/python/#parallel-2-function-calls","text":"from multiprocessing import Pool from functools import partial start = timeit . default_timer () pool = Pool () result = pool . map ( partial ( display_sum , num_2 = 2 ), [ 2 , 2 ]) end = timeit . default_timer () total_time = end - start print ( f 'Total time { total_time : .2f } s' ) Adding Adding Total sum 4 Total sum 4 Total time 1.08s","title":"Parallel 2 Function Calls"},{"location":"programming/python/python/#asynchronous-2-function-calls","text":"For this use case, it'll take half the time compared to a synchronous application and slightly faster than parallel application (although not always true for parallel except in this case) import asyncio import timeit import time async def add_numbers ( num_1 , num_2 ): print ( 'Adding' ) await asyncio . sleep ( 1 ) return num_1 + num_2 async def display_sum ( num_1 , num_2 ): total_sum = await add_numbers ( num_1 , num_2 ) print ( f 'Total sum { total_sum } ' ) async def main (): # .gather allows us to group subroutines await asyncio . gather ( display_sum ( 2 , 2 ), display_sum ( 2 , 2 )) start = timeit . default_timer () # For .ipynb, event loop already done await main () # For .py # asyncio.run(main()) end = timeit . default_timer () total_time = end - start print ( f 'Total time { total_time : .4f } s' ) Adding Adding Total sum 4 Total sum 4 Total time 1.0021s","title":"Asynchronous 2 Function Calls"}]}