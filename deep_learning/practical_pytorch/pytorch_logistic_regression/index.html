



<!DOCTYPE html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="We deploy a top-down approach that enables you to grasp deep learning theories and code easily and quickly. Used by thousands of students and professionals from top tech companies, research institutions.">
      
      
        <link rel="canonical" href="https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_logistic_regression/">
      
      
        <meta name="author" content="Ritchie Ng">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../../../assets/images/favicon.ico">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-3.0.4">
    
    
      
        <title>Logistic Regression - Deep Learning Wizard</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/application.451f80e5.css">
      
        <link rel="stylesheet" href="../../../assets/stylesheets/application-palette.22915126.css">
      
      
        
        
        <meta name="theme-color" content="#546e7a">
      
    
    
      <script src="../../../assets/javascripts/modernizr.1aa3b519.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../../assets/fonts/material-icons.css">
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="blue-grey" data-md-color-accent="blue">
  
    <svg class="md-svg">
      <defs>
        
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="../../../#logistic-regression-with-pytorch" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://www.deeplearningwizard.com/" title="Deep Learning Wizard" class="md-header-nav__button md-logo">
          
            <i class="md-icon">whatshot</i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                Deep Learning Wizard
              </span>
              <span class="md-header-nav__topic">
                Logistic Regression
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

  

<nav class="md-tabs md-tabs--active" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  <li class="md-tabs__item">
    
      <a href="../../.." title="Home" class="md-tabs__link">
        Home
      </a>
    
  </li>

      
        
      
        
      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../intro/" title="Deep Learning Wiki" class="md-tabs__link md-tabs__link--active">
          Deep Learning Wiki
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../news/news/" title="News" class="md-tabs__link">
          News
        </a>
      
    </li>
  

      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://www.deeplearningwizard.com/" title="Deep Learning Wizard" class="md-nav__button md-logo">
      
        <i class="md-icon">whatshot</i>
      
    </a>
    Deep Learning Wizard
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../../.." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../../supporters/" title="Supporters" class="md-nav__link">
      Supporters
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../../review/" title="Reviews" class="md-nav__link">
      Reviews
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked>
    
    <label class="md-nav__link" for="nav-4">
      Deep Learning Wiki
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        Deep Learning Wiki
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/" title="Introduction" class="md-nav__link">
      Introduction
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../course_progression/" title="Course Progression" class="md-nav__link">
      Course Progression
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../pytorch_matrices/" title="Matrices" class="md-nav__link">
      Matrices
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../pytorch_gradients/" title="Gradients" class="md-nav__link">
      Gradients
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../pytorch_linear_regression/" title="Linear Regression" class="md-nav__link">
      Linear Regression
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Logistic Regression
      </label>
    
    <a href="./" title="Logistic Regression" class="md-nav__link md-nav__link--active">
      Logistic Regression
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#about-logistic-regression" title="About Logistic Regression" class="md-nav__link">
    About Logistic Regression
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#logistic-regression-basics" title="Logistic Regression Basics" class="md-nav__link">
    Logistic Regression Basics
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classification-algorithm" title="Classification algorithm" class="md-nav__link">
    Classification algorithm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#basic-comparison" title="Basic Comparison" class="md-nav__link">
    Basic Comparison
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inputoutput-comparison" title="Input/Output Comparison" class="md-nav__link">
    Input/Output Comparison
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#problems-of-linear-regression" title="Problems of Linear Regression" class="md-nav__link">
    Problems of Linear Regression
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#logistic-regression-in-depth" title="Logistic Regression In-Depth" class="md-nav__link">
    Logistic Regression In-Depth
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#predicting-probability" title="Predicting Probability" class="md-nav__link">
    Predicting Probability
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#logistic-function-g" title="Logistic Function g()" class="md-nav__link">
    Logistic Function g()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax-function-g" title="Softmax Function g()" class="md-nav__link">
    Softmax Function g()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-entropy-function-d-for-2-class" title="Cross Entropy Function D() for 2 Class" class="md-nav__link">
    Cross Entropy Function D() for 2 Class
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-entropy-function-d-for-more-than-2-class" title="Cross Entropy Function D() for More Than 2 Class" class="md-nav__link">
    Cross Entropy Function D() for More Than 2 Class
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-entropy-loss-over-n-samples" title="Cross Entropy Loss over N samples" class="md-nav__link">
    Cross Entropy Loss over N samples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#building-a-logistic-regression-model-with-pytorch" title="Building a Logistic Regression Model with PyTorch" class="md-nav__link">
    Building a Logistic Regression Model with PyTorch
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#steps" title="Steps" class="md-nav__link">
    Steps
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-1a-loading-mnist-train-dataset" title="Step 1a: Loading MNIST Train Dataset" class="md-nav__link">
    Step 1a: Loading MNIST Train Dataset
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#displaying-mnist" title="Displaying MNIST" class="md-nav__link">
    Displaying MNIST
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-1b-loading-mnist-test-dataset" title="Step 1b: Loading MNIST Test Dataset" class="md-nav__link">
    Step 1b: Loading MNIST Test Dataset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-2-make-dataset-iterable" title="Step 2: Make Dataset Iterable" class="md-nav__link">
    Step 2: Make Dataset Iterable
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-3-building-model" title="Step 3: Building Model" class="md-nav__link">
    Step 3: Building Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-4-instantiate-model-class" title="Step 4: Instantiate Model Class" class="md-nav__link">
    Step 4: Instantiate Model Class
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-5-instantiate-loss-class" title="Step 5: Instantiate Loss Class" class="md-nav__link">
    Step 5: Instantiate Loss Class
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-6-instantiate-optimizer-class" title="Step 6: Instantiate Optimizer Class" class="md-nav__link">
    Step 6: Instantiate Optimizer Class
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-7-train-model" title="Step 7: Train Model" class="md-nav__link">
    Step 7: Train Model
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#break-down-accuracy-calculation" title="Break Down Accuracy Calculation" class="md-nav__link">
    Break Down Accuracy Calculation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#saving-model" title="Saving Model" class="md-nav__link">
    Saving Model
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#building-a-logistic-regression-model-with-pytorch-gpu" title="Building a Logistic Regression Model with PyTorch (GPU)" class="md-nav__link">
    Building a Logistic Regression Model with PyTorch (GPU)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" title="Summary" class="md-nav__link">
    Summary
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#citation" title="Citation" class="md-nav__link">
    Citation
  </a>
  
</li>
      
      
      
      
      
        <li class="md-nav__item">
          <a href="#__comments" title="Comments" class="md-nav__link md-nav__link--active">
            Comments
          </a>
        </li>
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../pytorch_feedforward_neuralnetwork/" title="Feedforward Neural Networks" class="md-nav__link">
      Feedforward Neural Networks
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../pytorch_convolutional_neuralnetwork/" title="Convolutional Neural Networks" class="md-nav__link">
      Convolutional Neural Networks
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../pytorch_recurrent_neuralnetwork/" title="Recurrent Neural Networks" class="md-nav__link">
      Recurrent Neural Networks
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../pytorch_lstm_neuralnetwork/" title="Long Short Term Memory Neural Networks" class="md-nav__link">
      Long Short Term Memory Neural Networks
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../boosting_models_pytorch/derivative_gradient_jacobian/" title="Derivative, Gradient and Jacobian" class="md-nav__link">
      Derivative, Gradient and Jacobian
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5">
    
    <label class="md-nav__link" for="nav-5">
      News
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        News
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/news/" title="Welcome" class="md-nav__link">
      Welcome
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/ammi_facebook_google_recap_2018_11_21/" title="AMMI (AIMS) supported by Facebook and Google, November 2018" class="md-nav__link">
      AMMI (AIMS) supported by Facebook and Google, November 2018
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/nanjing_next_nus_tsinghua_ai_finance_healthcare_2018_11_01/" title="NExT++ AI in Healthcare and Finance, Nanjing, November 2018" class="md-nav__link">
      NExT++ AI in Healthcare and Finance, Nanjing, November 2018
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/facebook_pytorch_devcon_recap_2018_10_02/" title="Recap of Facebook PyTorch Developer Conference, San Francisco, September 2018" class="md-nav__link">
      Recap of Facebook PyTorch Developer Conference, San Francisco, September 2018
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/facebook_pytorch_developer_conference_2018_09_05/" title="Facebook PyTorch Developer Conference, San Francisco, September 2018" class="md-nav__link">
      Facebook PyTorch Developer Conference, San Francisco, September 2018
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/nvidia_nus_mit_datathon_2018_07_05/" title="NUS-MIT-NUHS NVIDIA Image Recognition Workshop, Singapore, July 2018" class="md-nav__link">
      NUS-MIT-NUHS NVIDIA Image Recognition Workshop, Singapore, July 2018
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/deep_learning_wizard_1y_2018_06_01/" title="Featured on PyTorch Website 2018" class="md-nav__link">
      Featured on PyTorch Website 2018
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/nvidia_self_driving_cars_talk_2017_06_21/" title="NVIDIA Self Driving Cars & Healthcare Talk, Singapore, June 2017" class="md-nav__link">
      NVIDIA Self Driving Cars & Healthcare Talk, Singapore, June 2017
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/deep_learning_wizard_nvidia_inception_2018_05_01/" title="NVIDIA Inception Partner Status, Singapore, May 2017" class="md-nav__link">
      NVIDIA Inception Partner Status, Singapore, May 2017
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#about-logistic-regression" title="About Logistic Regression" class="md-nav__link">
    About Logistic Regression
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#logistic-regression-basics" title="Logistic Regression Basics" class="md-nav__link">
    Logistic Regression Basics
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classification-algorithm" title="Classification algorithm" class="md-nav__link">
    Classification algorithm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#basic-comparison" title="Basic Comparison" class="md-nav__link">
    Basic Comparison
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inputoutput-comparison" title="Input/Output Comparison" class="md-nav__link">
    Input/Output Comparison
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#problems-of-linear-regression" title="Problems of Linear Regression" class="md-nav__link">
    Problems of Linear Regression
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#logistic-regression-in-depth" title="Logistic Regression In-Depth" class="md-nav__link">
    Logistic Regression In-Depth
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#predicting-probability" title="Predicting Probability" class="md-nav__link">
    Predicting Probability
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#logistic-function-g" title="Logistic Function g()" class="md-nav__link">
    Logistic Function g()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax-function-g" title="Softmax Function g()" class="md-nav__link">
    Softmax Function g()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-entropy-function-d-for-2-class" title="Cross Entropy Function D() for 2 Class" class="md-nav__link">
    Cross Entropy Function D() for 2 Class
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-entropy-function-d-for-more-than-2-class" title="Cross Entropy Function D() for More Than 2 Class" class="md-nav__link">
    Cross Entropy Function D() for More Than 2 Class
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-entropy-loss-over-n-samples" title="Cross Entropy Loss over N samples" class="md-nav__link">
    Cross Entropy Loss over N samples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#building-a-logistic-regression-model-with-pytorch" title="Building a Logistic Regression Model with PyTorch" class="md-nav__link">
    Building a Logistic Regression Model with PyTorch
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#steps" title="Steps" class="md-nav__link">
    Steps
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-1a-loading-mnist-train-dataset" title="Step 1a: Loading MNIST Train Dataset" class="md-nav__link">
    Step 1a: Loading MNIST Train Dataset
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#displaying-mnist" title="Displaying MNIST" class="md-nav__link">
    Displaying MNIST
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-1b-loading-mnist-test-dataset" title="Step 1b: Loading MNIST Test Dataset" class="md-nav__link">
    Step 1b: Loading MNIST Test Dataset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-2-make-dataset-iterable" title="Step 2: Make Dataset Iterable" class="md-nav__link">
    Step 2: Make Dataset Iterable
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-3-building-model" title="Step 3: Building Model" class="md-nav__link">
    Step 3: Building Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-4-instantiate-model-class" title="Step 4: Instantiate Model Class" class="md-nav__link">
    Step 4: Instantiate Model Class
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-5-instantiate-loss-class" title="Step 5: Instantiate Loss Class" class="md-nav__link">
    Step 5: Instantiate Loss Class
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-6-instantiate-optimizer-class" title="Step 6: Instantiate Optimizer Class" class="md-nav__link">
    Step 6: Instantiate Optimizer Class
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-7-train-model" title="Step 7: Train Model" class="md-nav__link">
    Step 7: Train Model
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#break-down-accuracy-calculation" title="Break Down Accuracy Calculation" class="md-nav__link">
    Break Down Accuracy Calculation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#saving-model" title="Saving Model" class="md-nav__link">
    Saving Model
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#building-a-logistic-regression-model-with-pytorch-gpu" title="Building a Logistic Regression Model with PyTorch (GPU)" class="md-nav__link">
    Building a Logistic Regression Model with PyTorch (GPU)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" title="Summary" class="md-nav__link">
    Summary
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#citation" title="Citation" class="md-nav__link">
    Citation
  </a>
  
</li>
      
      
      
      
      
        <li class="md-nav__item">
          <a href="#__comments" title="Comments" class="md-nav__link md-nav__link--active">
            Comments
          </a>
        </li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="logistic-regression-with-pytorch">Logistic Regression with PyTorch<a class="headerlink" href="#logistic-regression-with-pytorch" title="Permanent link">&para;</a></h1>
<h2 id="about-logistic-regression">About Logistic Regression<a class="headerlink" href="#about-logistic-regression" title="Permanent link">&para;</a></h2>
<h3 id="logistic-regression-basics">Logistic Regression Basics<a class="headerlink" href="#logistic-regression-basics" title="Permanent link">&para;</a></h3>
<h4 id="classification-algorithm">Classification algorithm<a class="headerlink" href="#classification-algorithm" title="Permanent link">&para;</a></h4>
<ul>
<li>Example: Spam vs No Spam<ul>
<li>Input: Bunch of words</li>
<li>Output: Probability spam or not</li>
</ul>
</li>
</ul>
<h4 id="basic-comparison">Basic Comparison<a class="headerlink" href="#basic-comparison" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Linear regression</strong><ul>
<li>Output: numeric value given inputs</li>
</ul>
</li>
<li><strong>Logistic regression</strong>:<ul>
<li>Output: probability [0, 1] given input belonging to a class</li>
</ul>
</li>
</ul>
<h4 id="inputoutput-comparison">Input/Output Comparison<a class="headerlink" href="#inputoutput-comparison" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Linear regression: Multiplication</strong><ul>
<li>Input: [1]<ul>
<li>Output: 2</li>
</ul>
</li>
<li>Input: [2]<ul>
<li>Output: 4</li>
</ul>
</li>
<li>Trying to model the relationship <code class="codehilite">y = 2x</code></li>
</ul>
</li>
<li><strong>Logistic regression: Spam</strong><ul>
<li>Input: "Sign up to get 1 million dollars by tonight"<ul>
<li>Output: p = 0.8</li>
</ul>
</li>
<li>Input: "This is a receipt for your recent purchase with Amazon"<ul>
<li>Output: p = 0.3</li>
</ul>
</li>
<li><strong>p: probability it is spam</strong></li>
</ul>
</li>
</ul>
<h3 id="problems-of-linear-regression">Problems of Linear Regression<a class="headerlink" href="#problems-of-linear-regression" title="Permanent link">&para;</a></h3>
<ul>
<li>Example<ul>
<li>Fever</li>
<li><strong>Input</strong>: temperature</li>
<li><strong>Output</strong>: fever or no fever</li>
</ul>
</li>
<li>Remember<ul>
<li><strong>Linear regression</strong>: minimize error between points and line</li>
</ul>
</li>
</ul>
<div class="admonition bug">
<p class="admonition-title">Linear Regression Problem 1: Fever value can go negative (below 0) and positive (above 1)</p>
<p>If you simply tried to do a simple linear regression on this fever problem, you would realize an apparent error. Fever can go beyond 1 and below 0 which does not make sense in this context.
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">75</span><span class="p">,</span> <span class="mi">100</span><span class="p">,]</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">colors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">poly1d</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">))(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Fever&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Temperature&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div></p>
</div>
<p><img alt="png" src="../pytorch_logistic_regression_files/pytorch_logistic_regression_5_1.png" /></p>
<div class="admonition bug">
<p class="admonition-title">Linear Regression Problem 2: Fever points are not predicted with the presence of outliers</p>
<p>Previously at least some points could be properly predicted. However, with the presence of outliers, everything goes wonky for simple linear regression, having no predictive capacity at all.
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">75</span><span class="p">,</span> <span class="mi">300</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">colors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">poly1d</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">))(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Fever&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Temperature&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div></p>
</div>
<p><img alt="png" src="../pytorch_logistic_regression_files/pytorch_logistic_regression_5_0.png" /></p>
<h3 id="logistic-regression-in-depth">Logistic Regression In-Depth<a class="headerlink" href="#logistic-regression-in-depth" title="Permanent link">&para;</a></h3>
<h4 id="predicting-probability">Predicting Probability<a class="headerlink" href="#predicting-probability" title="Permanent link">&para;</a></h4>
<ul>
<li>Linear regression doesn't work</li>
<li>Instead of predicting direct values: <strong>predict probability</strong></li>
</ul>
<p><img alt="" src="../images/cross_entropy_final_4.png" /></p>
<h4 id="logistic-function-g">Logistic Function g()<a class="headerlink" href="#logistic-function-g" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>"Two-class logistic regression"</strong></li>
<li><span><span class="MathJax_Preview">\boldsymbol{y} = A\boldsymbol{x} + \boldsymbol{b}</span><script type="math/tex">\boldsymbol{y} = A\boldsymbol{x} + \boldsymbol{b}</script></span><ul>
<li>Where <span><span class="MathJax_Preview">\boldsymbol{y}</span><script type="math/tex">\boldsymbol{y}</script></span> is a vector comprising the 2-class prediction <span><span class="MathJax_Preview">y_0</span><script type="math/tex">y_0</script></span> and <span><span class="MathJax_Preview">y_1</span><script type="math/tex">y_1</script></span></li>
<li>Where the labels are <span><span class="MathJax_Preview">y_0 = 0</span><script type="math/tex">y_0 = 0</script></span>  and <span><span class="MathJax_Preview">y_1 = 1</span><script type="math/tex">y_1 = 1</script></span></li>
<li>Also, it's bolded because it's a vector, not a matrix.</li>
</ul>
</li>
<li><span><span class="MathJax_Preview">g(y_1) = \frac {1} {1 + e^{-y_1}}</span><script type="math/tex">g(y_1) = \frac {1} {1 + e^{-y_1}}</script></span><ul>
<li><span><span class="MathJax_Preview">g(y_1)</span><script type="math/tex">g(y_1)</script></span> = Estimated probability that <span><span class="MathJax_Preview">y = 1</span><script type="math/tex">y = 1</script></span></li>
</ul>
</li>
<li><span><span class="MathJax_Preview">g(y_0) = 1 - g(y_1)</span><script type="math/tex">g(y_0) = 1 - g(y_1)</script></span><ul>
<li><span><span class="MathJax_Preview">g(y_0)</span><script type="math/tex">g(y_0)</script></span> = Estimated probability that <span><span class="MathJax_Preview">y = 0</span><script type="math/tex">y = 0</script></span></li>
</ul>
</li>
<li>For our illustration above, we have 4 classes, so we have to use softmax function explained below</li>
</ul>
<h4 id="softmax-function-g">Softmax Function g()<a class="headerlink" href="#softmax-function-g" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>"Multi-class logistic regression"</strong><ul>
<li>Generalization of logistic function, where you can derive back to the logistic function if you've a 2 class classification problem</li>
<li>Here, we will use a 4 class example (K = 4) as shown above to be very clear in how it relates back to that simple examaple.</li>
</ul>
</li>
<li><span><span class="MathJax_Preview">\boldsymbol{y} = A\boldsymbol{x} + \boldsymbol{b}</span><script type="math/tex">\boldsymbol{y} = A\boldsymbol{x} + \boldsymbol{b}</script></span><ul>
<li>Where <span><span class="MathJax_Preview">\boldsymbol{y}</span><script type="math/tex">\boldsymbol{y}</script></span> is a vector comprising the 4-class prediction <span><span class="MathJax_Preview">y_0, y_1, y_2, y_3</span><script type="math/tex">y_0, y_1, y_2, y_3</script></span></li>
<li>Where the 4 labels (K = 4) are <span><span class="MathJax_Preview">y_0 = 0, y_1 = 1, y_2 = 2, y_3 = 3</span><script type="math/tex">y_0 = 0, y_1 = 1, y_2 = 2, y_3 = 3</script></span></li>
</ul>
</li>
<li><span><span class="MathJax_Preview">g(y_i) = \frac {e^{y_i} } {\sum^K_i e^{y_i}}</span><script type="math/tex">g(y_i) = \frac {e^{y_i} } {\sum^K_i e^{y_i}}</script></span> where K = 4 because we have 4 classes<ul>
<li>To put numbers to this equation in relation to the illustration above where we've <span><span class="MathJax_Preview">y_0 = 1.3, y_1 = 1.2, y = 4.5, y = 4.8</span><script type="math/tex">y_0 = 1.3, y_1 = 1.2, y = 4.5, y = 4.8</script></span><ul>
<li><span><span class="MathJax_Preview">g(y_0) = \frac {e^{1.3}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.017</span><script type="math/tex">g(y_0) = \frac {e^{1.3}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.017</script></span></li>
<li><span><span class="MathJax_Preview">g(y_1) = \frac {e^{1.2}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.015</span><script type="math/tex">g(y_1) = \frac {e^{1.2}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.015</script></span></li>
<li><span><span class="MathJax_Preview">g(y_2) = \frac {e^{4.5}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.412</span><script type="math/tex">g(y_2) = \frac {e^{4.5}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.412</script></span></li>
<li><span><span class="MathJax_Preview">g(y_3) = \frac {e^{4.8}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.556</span><script type="math/tex">g(y_3) = \frac {e^{4.8}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.556</script></span></li>
<li><span><span class="MathJax_Preview">g(y_0) + g(y_1) + g(y_2) + g(y_3) = 1.0</span><script type="math/tex">g(y_0) + g(y_1) + g(y_2) + g(y_3) = 1.0</script></span></li>
<li>All softmax outputs have to sum to one as they represent a probability distribution over K classes. </li>
</ul>
</li>
</ul>
</li>
<li>Take note how these numbers are not exactly as in the illustration in the softmax box but the concept is important (intentionally made so).<ul>
<li><span><span class="MathJax_Preview">y_0</span><script type="math/tex">y_0</script></span> and <span><span class="MathJax_Preview">y_1</span><script type="math/tex">y_1</script></span> are approximately similar in values and they return similar probabilities.</li>
<li>Similarly, <span><span class="MathJax_Preview">y_2</span><script type="math/tex">y_2</script></span> and <span><span class="MathJax_Preview">y_3</span><script type="math/tex">y_3</script></span> are approximately similar in values and they return similar probabilities.</li>
</ul>
</li>
</ul>
<div class="admonition bug">
<p class="admonition-title">Softmax versus Soft(arg)max</p>
<p>Do you know many researchers and anyone in deep learning in general use the term softmax when it should be soft(arg)max.</p>
<p>This is because soft(arg)max returns the probability distribution over K classes, a vector. </p>
<p>However, softmax only returns the max! This means you will be getting a scalar value versus a probability distribution.</p>
<p>According to my friend, Alfredo Canziani (postdoc in NYU under Yann Lecun), it was actually a mistake made in the original paper previously but it was too late because the term softmax was adopted. Full credits to him for this tip.</p>
</div>
<h4 id="cross-entropy-function-d-for-2-class">Cross Entropy Function D() for 2 Class<a class="headerlink" href="#cross-entropy-function-d-for-2-class" title="Permanent link">&para;</a></h4>
<ul>
<li>Take note that here, <span><span class="MathJax_Preview">S</span><script type="math/tex">S</script></span> is our softmax outputs and <span><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span> are our labels</li>
<li><span><span class="MathJax_Preview">D(S, L) = L log S - (1-L)log(1-S)</span><script type="math/tex">D(S, L) = L log S - (1-L)log(1-S)</script></span><ul>
<li>If L = 0 (label)<ul>
<li><span><span class="MathJax_Preview">D(S, 0) = - log(1-S)</span><script type="math/tex">D(S, 0) = - log(1-S)</script></span><ul>
<li><span><span class="MathJax_Preview">- log(1-S)</span><script type="math/tex">- log(1-S)</script></span>: less positive if <span><span class="MathJax_Preview">S \longrightarrow 0</span><script type="math/tex">S \longrightarrow 0</script></span></li>
<li><span><span class="MathJax_Preview">- log(1-S)</span><script type="math/tex">- log(1-S)</script></span>: more positive if <span><span class="MathJax_Preview">S \longrightarrow 1</span><script type="math/tex">S \longrightarrow 1</script></span> (BIGGER LOSS)</li>
</ul>
</li>
</ul>
</li>
<li>If L = 1 (label)<ul>
<li><span><span class="MathJax_Preview">D(S, 1) = log S</span><script type="math/tex">D(S, 1) = log S</script></span><ul>
<li><span><span class="MathJax_Preview">logS</span><script type="math/tex">logS</script></span>: less negative if <span><span class="MathJax_Preview">S \longrightarrow 1</span><script type="math/tex">S \longrightarrow 1</script></span></li>
<li><span><span class="MathJax_Preview">logS</span><script type="math/tex">logS</script></span>: more negative if <span><span class="MathJax_Preview">S \longrightarrow 0</span><script type="math/tex">S \longrightarrow 0</script></span> (BIGGER LOSS)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Numerical example of bigger or small loss</p>
<p>You get a small error of 1e-5 if your label = 0 and your S is closer to 0 (very correct prediction).
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="k">print</span><span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mf">0.00001</span><span class="p">))</span>
</pre></div></p>
<p>You get a large error of 11.51 if your label is 0 and S is near to 1 (very wrong prediction).
<div class="codehilite"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mf">0.99999</span><span class="p">))</span> 
</pre></div></p>
<p>You get a small error of -1e-5 if your label is 1 and S is near 1 (very correct prediction).
<div class="codehilite"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">0.99999</span><span class="p">))</span>
</pre></div></p>
<p>You get a big error of -11.51 if your label is 1 and S is near 0 (very wrong prediction).
<div class="codehilite"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">0.00001</span><span class="p">))</span>
</pre></div></p>
</div>
<div class="codehilite"><pre><span></span><span class="mf">1.0000050000287824e-05</span>
<span class="mf">11.51292546497478</span>
<span class="o">-</span><span class="mf">1.0000050000287824e-05</span>
<span class="o">-</span><span class="mf">11.512925464970229</span>
</pre></div>

<h4 id="cross-entropy-function-d-for-more-than-2-class">Cross Entropy Function D() for More Than 2 Class<a class="headerlink" href="#cross-entropy-function-d-for-more-than-2-class" title="Permanent link">&para;</a></h4>
<ul>
<li>For the case where we have more than 2 class, we need a more generalized function</li>
<li><span><span class="MathJax_Preview">D(S, L) = - \sum^K_1 L_i log(S_i)</span><script type="math/tex">D(S, L) = - \sum^K_1 L_i log(S_i)</script></span><ul>
<li><span><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span>: number of classes</li>
<li><span><span class="MathJax_Preview">L_i</span><script type="math/tex">L_i</script></span>: label of i-th class, 1 if that's the class else 0</li>
<li><span><span class="MathJax_Preview">S_i</span><script type="math/tex">S_i</script></span>: output of softmax for i-th class</li>
</ul>
</li>
</ul>
<h4 id="cross-entropy-loss-over-n-samples">Cross Entropy Loss over N samples<a class="headerlink" href="#cross-entropy-loss-over-n-samples" title="Permanent link">&para;</a></h4>
<ul>
<li>Goal: Minimizing Cross Entropy Loss, L</li>
<li><span><span class="MathJax_Preview">Loss = \frac {1}{N} \sum_j^N D_j</span><script type="math/tex">Loss = \frac {1}{N} \sum_j^N D_j</script></span><ul>
<li><span><span class="MathJax_Preview">D_j</span><script type="math/tex">D_j</script></span>: j-th sample of cross entropy function <span><span class="MathJax_Preview">D(S, L)</span><script type="math/tex">D(S, L)</script></span></li>
<li><span><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span>: number of samples</li>
<li><span><span class="MathJax_Preview">Loss</span><script type="math/tex">Loss</script></span>: average cross entropy loss over N samples</li>
</ul>
</li>
</ul>
<h2 id="building-a-logistic-regression-model-with-pytorch">Building a Logistic Regression Model with PyTorch<a class="headerlink" href="#building-a-logistic-regression-model-with-pytorch" title="Permanent link">&para;</a></h2>
<p><img alt="" src="../images/lr2.png" /></p>
<h3 id="steps">Steps<a class="headerlink" href="#steps" title="Permanent link">&para;</a></h3>
<ul>
<li>Step 1: Load Dataset</li>
<li>Step 2: Make Dataset Iterable</li>
<li>Step 3: Create Model Class</li>
<li>Step 4: Instantiate Model Class</li>
<li>Step 5: Instantiate Loss Class</li>
<li>Step 6: Instantiate Optimizer Class</li>
<li>Step 7: Train Model</li>
</ul>
<h3 id="step-1a-loading-mnist-train-dataset">Step 1a: Loading MNIST Train Dataset<a class="headerlink" href="#step-1a-loading-mnist-train-dataset" title="Permanent link">&para;</a></h3>
<p><strong>Images from 1 to 9</strong></p>
<div class="admonition note">
<p class="admonition-title">Inspect length of training dataset</p>
<p>You can easily load MNIST dataset with PyTorch. Here we inspect the training set, where our algorithms will learn from, and you will discover it is made up of 60,000 images.
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="kn">as</span> <span class="nn">transforms</span>
<span class="kn">import</span> <span class="nn">torchvision.datasets</span> <span class="kn">as</span> <span class="nn">dsets</span>
</pre></div></p>
<div class="codehilite"><pre><span></span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">dsets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> 
                            <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
                            <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                            <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>

<div class="codehilite"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>
</pre></div>

</div>
<div class="codehilite"><pre><span></span><span class="mi">60000</span>
</pre></div>

<div class="admonition note">
<p class="admonition-title">Inspecting a single image</p>
<p>So this is how a single image is represented in numbers. It's actually a 28 pixel x 28 pixel image which is why you would end up with this 28x28 matrix of numbers.
<div class="codehilite"><pre><span></span><span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div></p>
</div>
<div class="codehilite"><pre><span></span><span class="p">(</span><span class="n">tensor</span><span class="p">([[[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0118</span><span class="p">,</span>  <span class="mf">0.0706</span><span class="p">,</span>
            <span class="mf">0.0706</span><span class="p">,</span>  <span class="mf">0.0706</span><span class="p">,</span>  <span class="mf">0.4941</span><span class="p">,</span>  <span class="mf">0.5333</span><span class="p">,</span>  <span class="mf">0.6863</span><span class="p">,</span>  <span class="mf">0.1020</span><span class="p">,</span>  <span class="mf">0.6510</span><span class="p">,</span>
            <span class="mf">1.0000</span><span class="p">,</span>  <span class="mf">0.9686</span><span class="p">,</span>  <span class="mf">0.4980</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.1176</span><span class="p">,</span>  <span class="mf">0.1412</span><span class="p">,</span>  <span class="mf">0.3686</span><span class="p">,</span>  <span class="mf">0.6039</span><span class="p">,</span>  <span class="mf">0.6667</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>
            <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.8824</span><span class="p">,</span>  <span class="mf">0.6745</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>
            <span class="mf">0.9490</span><span class="p">,</span>  <span class="mf">0.7647</span><span class="p">,</span>  <span class="mf">0.2510</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.1922</span><span class="p">,</span>  <span class="mf">0.9333</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>
            <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9843</span><span class="p">,</span>  <span class="mf">0.3647</span><span class="p">,</span>  <span class="mf">0.3216</span><span class="p">,</span>  <span class="mf">0.3216</span><span class="p">,</span>
            <span class="mf">0.2196</span><span class="p">,</span>  <span class="mf">0.1529</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0706</span><span class="p">,</span>  <span class="mf">0.8588</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>
            <span class="mf">0.7765</span><span class="p">,</span>  <span class="mf">0.7137</span><span class="p">,</span>  <span class="mf">0.9686</span><span class="p">,</span>  <span class="mf">0.9451</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.3137</span><span class="p">,</span>  <span class="mf">0.6118</span><span class="p">,</span>  <span class="mf">0.4196</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.8039</span><span class="p">,</span>
            <span class="mf">0.0431</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.1686</span><span class="p">,</span>  <span class="mf">0.6039</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0549</span><span class="p">,</span>  <span class="mf">0.0039</span><span class="p">,</span>  <span class="mf">0.6039</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.3529</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.5451</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.7451</span><span class="p">,</span>
            <span class="mf">0.0078</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0431</span><span class="p">,</span>  <span class="mf">0.7451</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>
            <span class="mf">0.2745</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.1373</span><span class="p">,</span>  <span class="mf">0.9451</span><span class="p">,</span>
            <span class="mf">0.8824</span><span class="p">,</span>  <span class="mf">0.6275</span><span class="p">,</span>  <span class="mf">0.4235</span><span class="p">,</span>  <span class="mf">0.0039</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.3176</span><span class="p">,</span>
            <span class="mf">0.9412</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.4667</span><span class="p">,</span>  <span class="mf">0.0980</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.1765</span><span class="p">,</span>  <span class="mf">0.7294</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.5882</span><span class="p">,</span>  <span class="mf">0.1059</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0627</span><span class="p">,</span>  <span class="mf">0.3647</span><span class="p">,</span>  <span class="mf">0.9882</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.7333</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.9765</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9765</span><span class="p">,</span>  <span class="mf">0.2510</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.1804</span><span class="p">,</span>  <span class="mf">0.5098</span><span class="p">,</span>  <span class="mf">0.7176</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.8118</span><span class="p">,</span>  <span class="mf">0.0078</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.1529</span><span class="p">,</span>  <span class="mf">0.5804</span><span class="p">,</span>
            <span class="mf">0.8980</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9804</span><span class="p">,</span>  <span class="mf">0.7137</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0941</span><span class="p">,</span>  <span class="mf">0.4471</span><span class="p">,</span>  <span class="mf">0.8667</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>
            <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.7882</span><span class="p">,</span>  <span class="mf">0.3059</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0902</span><span class="p">,</span>  <span class="mf">0.2588</span><span class="p">,</span>  <span class="mf">0.8353</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>
            <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.7765</span><span class="p">,</span>  <span class="mf">0.3176</span><span class="p">,</span>  <span class="mf">0.0078</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0706</span><span class="p">,</span>
            <span class="mf">0.6706</span><span class="p">,</span>  <span class="mf">0.8588</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.7647</span><span class="p">,</span>
            <span class="mf">0.3137</span><span class="p">,</span>  <span class="mf">0.0353</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.2157</span><span class="p">,</span>  <span class="mf">0.6745</span><span class="p">,</span>  <span class="mf">0.8863</span><span class="p">,</span>
            <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9569</span><span class="p">,</span>  <span class="mf">0.5216</span><span class="p">,</span>  <span class="mf">0.0431</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.5333</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>
            <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.8314</span><span class="p">,</span>  <span class="mf">0.5294</span><span class="p">,</span>  <span class="mf">0.5176</span><span class="p">,</span>  <span class="mf">0.0627</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">]]]),</span>
 <span class="n">tensor</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
</pre></div>

<div class="admonition note">
<p class="admonition-title">Inspecting a single data point in the training dataset</p>
<p>When you load MNIST dataset, each data point is actually a tuple containing the image matrix and the label.</p>
<div class="codehilite"><pre><span></span><span class="nb">type</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>

</div>
<div class="codehilite"><pre><span></span><span class="nb">tuple</span>
</pre></div>

<div class="admonition note">
<p class="admonition-title">Inspecting training dataset first element of tuple</p>
<p>This means to access the image, you need to access the first element in the tuple.</p>
<div class="codehilite"><pre><span></span><span class="c1"># Input Matrix</span>
<span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div>

</div>
<div class="codehilite"><pre><span></span><span class="c1"># A 28x28 sized image of a digit</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span>
</pre></div>

<div class="admonition note">
<p class="admonition-title">Inspecting training dataset second element of tuple</p>
<p>The second element actually represents the image's label. Meaning if the second element says 5, it means the 28x28 matrix of numbers represent a digit 5.</p>
<div class="codehilite"><pre><span></span><span class="c1"># Label</span>
<span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
</pre></div>

</div>
<div class="codehilite"><pre><span></span><span class="n">tensor</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>

<h4 id="displaying-mnist">Displaying MNIST<a class="headerlink" href="#displaying-mnist" title="Permanent link">&para;</a></h4>
<div class="admonition note">
<p class="admonition-title">Verifying shape of MNIST image</p>
<p>As mentioned, a single MNIST image is of the shape 28 pixel x 28 pixel.</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>  
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</pre></div>

<div class="codehilite"><pre><span></span><span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

</div>
<div class="codehilite"><pre><span></span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
</pre></div>

<div class="admonition note">
<p class="admonition-title">Plot image of MNIST image</p>
<div class="codehilite"><pre><span></span><span class="n">show_img</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
</pre></div>

<div class="codehilite"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">show_img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
</pre></div>

</div>
<p><img alt="png" src="../pytorch_logistic_regression_files/pytorch_logistic_regression_24_1.png" /></p>
<div class="admonition note">
<p class="admonition-title">Second element of tuple shows label</p>
<p>As you would expect, the label is 5.
<div class="codehilite"><pre><span></span><span class="c1"># Label</span>
<span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
</pre></div></p>
</div>
<div class="codehilite"><pre><span></span><span class="n">tensor</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>

<div class="admonition note">
<p class="admonition-title">Plot second image of MNIST image</p>
<div class="codehilite"><pre><span></span><span class="n">show_img</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
</pre></div>

<div class="codehilite"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">show_img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
</pre></div>

</div>
<p><img alt="png" src="../pytorch_logistic_regression_files/pytorch_logistic_regression_27_1.png" /></p>
<div class="admonition note">
<p class="admonition-title">Second element of tuple shows label</p>
<p>We should see 0 here as the label.
<div class="codehilite"><pre><span></span><span class="c1"># Label</span>
<span class="n">train_dataset</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
</pre></div></p>
</div>
<div class="codehilite"><pre><span></span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>

<h3 id="step-1b-loading-mnist-test-dataset">Step 1b: Loading MNIST Test Dataset<a class="headerlink" href="#step-1b-loading-mnist-test-dataset" title="Permanent link">&para;</a></h3>
<ul>
<li>Show our algorithm works beyond the data we have trained on.</li>
<li>Out-of-sample</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Load test dataset</p>
<p>Compared to the 60k images in the training set, the testing set where the model will not be trained on has 10k images to check for its out-of-sample performance.
<div class="codehilite"><pre><span></span><span class="n">test_dataset</span> <span class="o">=</span> <span class="n">dsets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> 
                           <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
                           <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>
</pre></div></p>
<div class="codehilite"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)</span>
</pre></div>

</div>
<div class="codehilite"><pre><span></span><span class="mi">10000</span>
</pre></div>

<div class="admonition note">
<p class="admonition-title">Test dataset elements</p>
<p>Exactly like the training set, the testing set has 10k tuples containing the 28x28 matrices and their respective labels.
<div class="codehilite"><pre><span></span><span class="nb">type</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div></p>
</div>
<div class="codehilite"><pre><span></span><span class="nb">tuple</span>
</pre></div>

<div class="admonition note">
<p class="admonition-title">Test dataset first element in tuple</p>
<p>This contains the image matrix, similar to the training set.
<div class="codehilite"><pre><span></span><span class="c1"># Image matrix</span>
<span class="n">test_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div></p>
</div>
<div class="codehilite"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span>
</pre></div>

<div class="admonition note">
<p class="admonition-title">Plot image sample from test dataset</p>
<div class="codehilite"><pre><span></span><span class="n">show_img</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">show_img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
</pre></div>

</div>
<p><img alt="png" src="../pytorch_logistic_regression_files/pytorch_logistic_regression_34_1.png" /></p>
<div class="admonition note">
<p class="admonition-title">Test dataset second element in tuple</p>
<div class="codehilite"><pre><span></span><span class="c1"># Label</span>
<span class="n">test_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
</pre></div>

</div>
<div class="codehilite"><pre><span></span><span class="n">tensor</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
</pre></div>

<h3 id="step-2-make-dataset-iterable">Step 2: Make Dataset Iterable<a class="headerlink" href="#step-2-make-dataset-iterable" title="Permanent link">&para;</a></h3>
<ul>
<li>Aim: make the dataset iterable</li>
<li><strong>totaldata</strong>: 60000</li>
<li><strong>minibatch</strong>: 100<ul>
<li>Number of examples in 1 iteration</li>
</ul>
</li>
<li><strong>iterations</strong>: 3000<ul>
<li>1 iteration: one mini-batch forward &amp; backward pass</li>
</ul>
</li>
<li><strong>epochs</strong><ul>
<li>1 epoch: running through the whole dataset once</li>
<li><span><span class="MathJax_Preview">epochs = iterations \div \frac{totaldata}{minibatch} = 3000 \div \frac{60000}{100} = 5</span><script type="math/tex">epochs = iterations \div \frac{totaldata}{minibatch} = 3000 \div \frac{60000}{100} = 5</script></span></li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Recap training dataset</p>
<p>Remember training dataset has 60k images and testing dataset has 10k images.
<div class="codehilite"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>
</pre></div></p>
</div>
<div class="codehilite"><pre><span></span><span class="mi">60000</span>
</pre></div>

<div class="admonition note">
<p class="admonition-title">Defining epochs</p>
<p>When the model goes through the whole 60k images once, learning how to classify 0-9, it's consider 1 epoch. </p>
<p>However, there's a concept of batch size where it means the model would look at 100 images before updating the model's weights, thereby learning. When the model updates its weights (parameters) after looking at all the images, this is considered 1 iteration.</p>
<div class="codehilite"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
</pre></div>

<p>We arbitrarily set 3000 iterations here which means the model would update 3000 times.
<div class="codehilite"><pre><span></span><span class="n">n_iters</span> <span class="o">=</span> <span class="mi">3000</span>
</pre></div></p>
<p>One epoch consists of 60,000 / 100 = 600 iterations. Because we would like to go through 3000 iterations, this implies we would have 3000 / 600 = 5 epochs as each epoch has 600 iterations. </p>
<div class="codehilite"><pre><span></span><span class="n">num_epochs</span> <span class="o">=</span> <span class="n">n_iters</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">)</span>
<span class="n">num_epochs</span>
</pre></div>

</div>
<div class="codehilite"><pre><span></span><span class="mi">5</span>
</pre></div>

<div class="admonition note">
<p class="admonition-title">Create Iterable Object: Training Dataset</p>
<div class="codehilite"><pre><span></span><span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> 
                                           <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
                                           <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>

</div>
<div class="admonition note">
<p class="admonition-title">Check Iterability</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">collections</span>
<span class="nb">isinstance</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Iterable</span><span class="p">)</span>
</pre></div>

</div>
<div class="codehilite"><pre><span></span><span class="bp">True</span>
</pre></div>

<div class="admonition note">
<p class="admonition-title">Create Iterable Object: Testing Dataset</p>
<div class="codehilite"><pre><span></span><span class="c1"># Iterable object</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">test_dataset</span><span class="p">,</span> 
                                          <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
                                          <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div>

</div>
<div class="admonition note">
<p class="admonition-title">Check iterability of testing dataset</p>
<div class="codehilite"><pre><span></span><span class="nb">isinstance</span><span class="p">(</span><span class="n">test_loader</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Iterable</span><span class="p">)</span>
</pre></div>

</div>
<div class="codehilite"><pre><span></span><span class="bp">True</span>
</pre></div>

<div class="admonition note">
<p class="admonition-title">Iterate through dataset</p>
<p>This is just a simplified example of what we're doing above where we're creating an iterable object <code class="codehilite">lst</code> to loop through so we can access all the images <code class="codehilite">img_1</code> and <code class="codehilite">img_2</code>.</p>
<p>Above, the equivalent of <code class="codehilite">lst</code> is <code class="codehilite">train_loader</code> and <code class="codehilite">test_loader</code>.</p>
<div class="codehilite"><pre><span></span><span class="n">img_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span>
<span class="n">img_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span>
<span class="n">lst</span> <span class="o">=</span> <span class="p">[</span><span class="n">img_1</span><span class="p">,</span> <span class="n">img_2</span><span class="p">]</span>
</pre></div>

<div class="codehilite"><pre><span></span><span class="c1"># Need to iterate</span>
<span class="c1"># Think of numbers as the images</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">lst</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>

</div>
<div class="codehilite"><pre><span></span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
<span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
</pre></div>

<h3 id="step-3-building-model">Step 3: Building Model<a class="headerlink" href="#step-3-building-model" title="Permanent link">&para;</a></h3>
<div class="admonition note">
<p class="admonition-title">Create model class</p>
<div class="codehilite"><pre><span></span><span class="c1"># Same as linear regression! </span>
<span class="k">class</span> <span class="nc">LogisticRegressionModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LogisticRegressionModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>

</div>
<h3 id="step-4-instantiate-model-class">Step 4: Instantiate Model Class<a class="headerlink" href="#step-4-instantiate-model-class" title="Permanent link">&para;</a></h3>
<ul>
<li>Input dimension: <ul>
<li>Size of image</li>
<li><span><span class="MathJax_Preview">28 \times 28 = 784</span><script type="math/tex">28 \times 28 = 784</script></span></li>
</ul>
</li>
<li>Output dimension: 10<ul>
<li>0, 1, 2, 3, 4, 5, 6, 7, 8, 9</li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Check size of dataset</p>
<p>This should be 28x28.
<div class="codehilite"><pre><span></span><span class="c1"># Size of images</span>
<span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div></p>
</div>
<div class="codehilite"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span>
</pre></div>

<div class="admonition note">
<p class="admonition-title">Instantiate model class based on input and out dimensions</p>
<p>As we're trying to classify digits 0-9 a total of 10 classes, our output dimension is 10. </p>
<p>And we're feeding the model with 28x28 images, hence our input dimension is 28x28.
<div class="codehilite"><pre><span></span><span class="n">input_dim</span> <span class="o">=</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span>
<span class="n">output_dim</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegressionModel</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
</pre></div></p>
</div>
<h3 id="step-5-instantiate-loss-class">Step 5: Instantiate Loss Class<a class="headerlink" href="#step-5-instantiate-loss-class" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Logistic Regression</strong>: Cross Entropy Loss<ul>
<li><em>Linear Regression: MSE</em></li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Create Cross Entry Loss Class</p>
<p>Unlike linear regression, we do not use MSE here, we need Cross Entry Loss to calculate our loss before we backpropagate and update our parameters.</p>
<div class="codehilite"><pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>  
</pre></div>

</div>
<div class="admonition alert">
<p class="admonition-title">What happens in nn.CrossEntropyLoss()?</p>
<p>It does 2 things at the same time. </p>
<p><br /> 1. Computes softmax (logistic/softmax function)
<br /> 2. Computes cross entropy</p>
</div>
<p><img alt="" src="../images/cross_entropy_final_4.png" /></p>
<h3 id="step-6-instantiate-optimizer-class">Step 6: Instantiate Optimizer Class<a class="headerlink" href="#step-6-instantiate-optimizer-class" title="Permanent link">&para;</a></h3>
<ul>
<li>Simplified equation<ul>
<li><span><span class="MathJax_Preview">\theta = \theta - \eta \cdot \nabla_\theta</span><script type="math/tex">\theta = \theta - \eta \cdot \nabla_\theta</script></span><ul>
<li><span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>: parameters (our variables)</li>
<li><span><span class="MathJax_Preview">\eta</span><script type="math/tex">\eta</script></span>: learning rate (how fast we want to learn)</li>
<li><span><span class="MathJax_Preview">\nabla_\theta</span><script type="math/tex">\nabla_\theta</script></span>: parameters' gradients</li>
</ul>
</li>
</ul>
</li>
<li>Even simplier equation<ul>
<li><code class="codehilite">parameters = parameters - learning_rate * parameters_gradients</code></li>
<li><strong>At every iteration, we update our model's parameters</strong></li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Create optimizer</p>
<p>Similar to what we've covered above, this calculates the parameters' gradients and update them subsequently.
<div class="codehilite"><pre><span></span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>  
</pre></div></p>
</div>
<div class="admonition note">
<p class="admonition-title">Parameters In-Depth</p>
<p>You'll realize we have 2 sets of parameters, 10x784 which is A and 10x1 which is b in the <span><span class="MathJax_Preview">y = AX + b</span><script type="math/tex">y = AX + b</script></span> equation where X is our input of size 784.</p>
<p>We'll go into details subsequently how these parameters interact with our input to produce our 10x1 output. </p>
<div class="codehilite"><pre><span></span><span class="c1"># Type of parameter object</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

<span class="c1"># Length of parameters</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())))</span>

<span class="c1"># FC 1 Parameters </span>
<span class="k">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>

<span class="c1"># FC 1 Bias Parameters</span>
<span class="k">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>

</div>
<div class="codehilite"><pre><span></span><span class="o">&lt;</span><span class="n">generator</span> <span class="nb">object</span> <span class="n">Module</span><span class="o">.</span><span class="n">parameters</span> <span class="n">at</span> <span class="mh">0x7ff7c884f830</span><span class="o">&gt;</span>
<span class="mi">2</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">784</span><span class="p">])</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">])</span>
</pre></div>

<div class="admonition note">
<p class="admonition-title">Quick Matrix Product Review</p>
<ul>
<li>Example 1: <strong>matrix product</strong><ul>
<li><span><span class="MathJax_Preview">A: (100, 10)</span><script type="math/tex">A: (100, 10)</script></span></li>
<li><span><span class="MathJax_Preview">B: (10, 1)</span><script type="math/tex">B: (10, 1)</script></span></li>
<li><span><span class="MathJax_Preview">A \cdot B = (100, 10) \cdot (10, 1) = (100, 1)</span><script type="math/tex">A \cdot B = (100, 10) \cdot (10, 1) = (100, 1)</script></span></li>
</ul>
</li>
<li>Example 2: <strong>matrix product</strong><ul>
<li><span><span class="MathJax_Preview">A: (50, 5)</span><script type="math/tex">A: (50, 5)</script></span></li>
<li><span><span class="MathJax_Preview">B: (5, 2)</span><script type="math/tex">B: (5, 2)</script></span></li>
<li><span><span class="MathJax_Preview">A \cdot B = (50, 5) \cdot (5, 2) = (50, 2)</span><script type="math/tex">A \cdot B = (50, 5) \cdot (5, 2) = (50, 2)</script></span></li>
</ul>
</li>
<li>Example 3: <strong>element-wise addition</strong><ul>
<li><span><span class="MathJax_Preview">A: (10, 1)</span><script type="math/tex">A: (10, 1)</script></span></li>
<li><span><span class="MathJax_Preview">B: (10, 1)</span><script type="math/tex">B: (10, 1)</script></span></li>
<li><span><span class="MathJax_Preview">A + B = (10, 1)</span><script type="math/tex">A + B = (10, 1)</script></span></li>
</ul>
</li>
</ul>
</div>
<p><img alt="" src="../images/lr_params2.png" /></p>
<h3 id="step-7-train-model">Step 7: Train Model<a class="headerlink" href="#step-7-train-model" title="Permanent link">&para;</a></h3>
<div class="admonition note">
<p class="admonition-title">7 step process for training models</p>
<ul>
<li>Process <ol>
<li>Convert inputs/labels to tensors with gradients</li>
<li>Clear gradient buffets</li>
<li>Get output given inputs</li>
<li>Get loss</li>
<li>Get gradients w.r.t. parameters</li>
<li>Update parameters using gradients<ul>
<li><code class="codehilite">parameters = parameters - learning_rate * parameters_gradients</code></li>
</ul>
</li>
<li>REPEAT</li>
</ol>
</li>
</ul>
<div class="codehilite"><pre><span></span><span class="nb">iter</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="c1"># Load images as Variable</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span>

        <span class="c1"># Clear gradients w.r.t. parameters</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># Forward pass to get output/logits</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

        <span class="c1"># Calculate Loss: softmax --&gt; cross entropy loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="c1"># Getting gradients w.r.t. parameters</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># Updating parameters</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="nb">iter</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="nb">iter</span> <span class="o">%</span> <span class="mi">500</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Calculate Accuracy         </span>
            <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="c1"># Iterate through test dataset</span>
            <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
                <span class="c1"># Load images to a Torch Variable</span>
                <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>

                <span class="c1"># Forward pass only to get logits/output</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

                <span class="c1"># Get predictions from the maximum value</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

                <span class="c1"># Total number of labels</span>
                <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

                <span class="c1"># Total correct predictions</span>
                <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

            <span class="n">accuracy</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>

            <span class="c1"># Print Loss</span>
            <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Iteration: {}. Loss: {}. Accuracy: {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">iter</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">accuracy</span><span class="p">))</span>
</pre></div>

</div>
<div class="codehilite"><pre><span></span><span class="n">Iteration</span><span class="p">:</span> <span class="mf">500.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.8513233661651611</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mi">70</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">1000.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.5732524394989014</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mi">77</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">1500.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.3840199708938599</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mi">79</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">2000.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.1711134910583496</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mi">81</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">2500.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.1094708442687988</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mi">82</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">3000.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.002761721611023</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mi">82</span>
</pre></div>

<h4 id="break-down-accuracy-calculation">Break Down Accuracy Calculation<a class="headerlink" href="#break-down-accuracy-calculation" title="Permanent link">&para;</a></h4>
<div class="admonition note">
<p class="admonition-title">Printing outputs of our model</p>
<p>As we've trained our model, we can extract the accuracy calculation portion to understand what's happening without re-training the model.</p>
<p>This would print out the output of the model's predictions on your notebook.</p>
<div class="codehilite"><pre><span></span><span class="n">iter_test</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
    <span class="n">iter_test</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">iter_test</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;OUTPUTS&#39;</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>

</div>
<div class="codehilite"><pre><span></span><span class="n">OUTPUTS</span>
<span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.4181</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0784</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4840</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0985</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2394</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1801</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1639</span><span class="p">,</span>
          <span class="mf">2.9352</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1552</span><span class="p">,</span>  <span class="mf">0.8852</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.5117</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1099</span><span class="p">,</span>  <span class="mf">1.5295</span><span class="p">,</span>  <span class="mf">0.8863</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8813</span><span class="p">,</span>  <span class="mf">0.5967</span><span class="p">,</span>  <span class="mf">1.3632</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.8977</span><span class="p">,</span>  <span class="mf">0.4183</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4990</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.0126</span><span class="p">,</span>  <span class="mf">2.4112</span><span class="p">,</span>  <span class="mf">0.2373</span><span class="p">,</span>  <span class="mf">0.0857</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7007</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2015</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3428</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.2548</span><span class="p">,</span>  <span class="mf">0.1659</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4703</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">2.8072</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.2973</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0984</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4313</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9619</span><span class="p">,</span>  <span class="mf">0.8670</span><span class="p">,</span>  <span class="mf">1.2201</span><span class="p">,</span>
          <span class="mf">0.3752</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2873</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3272</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0343</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0043</span><span class="p">,</span>  <span class="mf">0.5081</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6452</span><span class="p">,</span>  <span class="mf">1.8647</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6924</span><span class="p">,</span>  <span class="mf">0.1435</span><span class="p">,</span>
          <span class="mf">0.4330</span><span class="p">,</span>  <span class="mf">0.2958</span><span class="p">,</span>  <span class="mf">1.0339</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.5392</span><span class="p">,</span>  <span class="mf">2.9070</span><span class="p">,</span>  <span class="mf">0.2297</span><span class="p">,</span>  <span class="mf">0.3139</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6863</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2734</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8377</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.1238</span><span class="p">,</span>  <span class="mf">0.3285</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3004</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.2037</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3739</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5947</span><span class="p">,</span>  <span class="mf">0.3530</span><span class="p">,</span>  <span class="mf">1.4205</span><span class="p">,</span>  <span class="mf">0.0593</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7307</span><span class="p">,</span>
          <span class="mf">0.6642</span><span class="p">,</span>  <span class="mf">0.3937</span><span class="p">,</span>  <span class="mf">0.8004</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.4439</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3284</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7652</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0952</span><span class="p">,</span>  <span class="mf">0.9323</span><span class="p">,</span>  <span class="mf">0.3006</span><span class="p">,</span>  <span class="mf">0.0238</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.0810</span><span class="p">,</span>  <span class="mf">0.0612</span><span class="p">,</span>  <span class="mf">1.3295</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.5409</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5266</span><span class="p">,</span>  <span class="mf">0.9914</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2369</span><span class="p">,</span>  <span class="mf">0.6583</span><span class="p">,</span>  <span class="mf">0.0992</span><span class="p">,</span>  <span class="mf">0.8525</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.0562</span><span class="p">,</span>  <span class="mf">0.2013</span><span class="p">,</span>  <span class="mf">0.0462</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.6548</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7253</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9825</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1663</span><span class="p">,</span>  <span class="mf">0.9076</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0694</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3708</span><span class="p">,</span>
          <span class="mf">1.8270</span><span class="p">,</span>  <span class="mf">0.2457</span><span class="p">,</span>  <span class="mf">1.5921</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">3.2147</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7689</span><span class="p">,</span>  <span class="mf">0.8531</span><span class="p">,</span>  <span class="mf">1.2320</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8126</span><span class="p">,</span>  <span class="mf">1.1251</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2776</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.4244</span><span class="p">,</span>  <span class="mf">0.5930</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6183</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.7470</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5545</span><span class="p">,</span>  <span class="mf">1.0251</span><span class="p">,</span>  <span class="mf">0.0529</span><span class="p">,</span>  <span class="mf">0.4384</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5934</span><span class="p">,</span>  <span class="mf">0.7666</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.0084</span><span class="p">,</span>  <span class="mf">0.5313</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3465</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.7916</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7064</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7805</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1588</span><span class="p">,</span>  <span class="mf">1.3284</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1708</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2092</span><span class="p">,</span>
          <span class="mf">0.9495</span><span class="p">,</span>  <span class="mf">0.1033</span><span class="p">,</span>  <span class="mf">2.0208</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">3.0602</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.3578</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2576</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2198</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2372</span><span class="p">,</span>  <span class="mf">0.9765</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1514</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.5380</span><span class="p">,</span>  <span class="mf">0.7970</span><span class="p">,</span>  <span class="mf">0.1374</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.2613</span><span class="p">,</span>  <span class="mf">2.8594</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0874</span><span class="p">,</span>  <span class="mf">0.1974</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2018</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0064</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0923</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.2142</span><span class="p">,</span>  <span class="mf">0.2575</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3218</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.4348</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7216</span><span class="p">,</span>  <span class="mf">0.0021</span><span class="p">,</span>  <span class="mf">1.2864</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5062</span><span class="p">,</span>  <span class="mf">0.7761</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3236</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.5667</span><span class="p">,</span>  <span class="mf">0.5431</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7781</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.2157</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0200</span><span class="p">,</span>  <span class="mf">0.1829</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6882</span><span class="p">,</span>  <span class="mf">1.3815</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7609</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0902</span><span class="p">,</span>
          <span class="mf">0.8647</span><span class="p">,</span>  <span class="mf">0.3679</span><span class="p">,</span>  <span class="mf">1.8843</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.0950</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5009</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6347</span><span class="p">,</span>  <span class="mf">0.3662</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4679</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0359</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7671</span><span class="p">,</span>
          <span class="mf">2.7155</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3991</span><span class="p">,</span>  <span class="mf">0.5737</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.7005</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5366</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0434</span><span class="p">,</span>  <span class="mf">1.1289</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5873</span><span class="p">,</span>  <span class="mf">0.2555</span><span class="p">,</span>  <span class="mf">0.8187</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.6557</span><span class="p">,</span>  <span class="mf">0.1241</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4297</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.0635</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5991</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4677</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1231</span><span class="p">,</span>  <span class="mf">2.0445</span><span class="p">,</span>  <span class="mf">0.1128</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1825</span><span class="p">,</span>
          <span class="mf">0.1075</span><span class="p">,</span>  <span class="mf">0.0348</span><span class="p">,</span>  <span class="mf">1.4317</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.0319</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1595</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3415</span><span class="p">,</span>  <span class="mf">0.1095</span><span class="p">,</span>  <span class="mf">0.5339</span><span class="p">,</span>  <span class="mf">0.1973</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3272</span><span class="p">,</span>
          <span class="mf">1.5765</span><span class="p">,</span>  <span class="mf">0.4784</span><span class="p">,</span>  <span class="mf">1.4176</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.4928</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5653</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0672</span><span class="p">,</span>  <span class="mf">0.3325</span><span class="p">,</span>  <span class="mf">0.5359</span><span class="p">,</span>  <span class="mf">0.5368</span><span class="p">,</span>  <span class="mf">2.1542</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.4276</span><span class="p">,</span>  <span class="mf">0.3605</span><span class="p">,</span>  <span class="mf">0.0587</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.4761</span><span class="p">,</span>  <span class="mf">0.2958</span><span class="p">,</span>  <span class="mf">0.6597</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2658</span><span class="p">,</span>  <span class="mf">1.1279</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0676</span><span class="p">,</span>  <span class="mf">1.2506</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.2059</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1489</span><span class="p">,</span>  <span class="mf">0.1051</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0764</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9274</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6838</span><span class="p">,</span>  <span class="mf">0.3464</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2656</span><span class="p">,</span>  <span class="mf">1.4099</span><span class="p">,</span>  <span class="mf">0.4486</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.9527</span><span class="p">,</span>  <span class="mf">0.5682</span><span class="p">,</span>  <span class="mf">0.0156</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.6900</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9611</span><span class="p">,</span>  <span class="mf">0.1395</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0079</span><span class="p">,</span>  <span class="mf">1.5424</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3208</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2682</span><span class="p">,</span>
          <span class="mf">0.3586</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2771</span><span class="p">,</span>  <span class="mf">1.0389</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">4.3606</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.8621</span><span class="p">,</span>  <span class="mf">0.6310</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9657</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2486</span><span class="p">,</span>  <span class="mf">1.2009</span><span class="p">,</span>  <span class="mf">1.1873</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.8255</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2103</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2172</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.1000</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4268</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4627</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1041</span><span class="p">,</span>  <span class="mf">0.2959</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1392</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6855</span><span class="p">,</span>
          <span class="mf">1.8622</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2580</span><span class="p">,</span>  <span class="mf">1.1347</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.3625</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1323</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2224</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8754</span><span class="p">,</span>  <span class="mf">2.4684</span><span class="p">,</span>  <span class="mf">0.0295</span><span class="p">,</span>  <span class="mf">0.1161</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.2660</span><span class="p">,</span>  <span class="mf">0.3037</span><span class="p">,</span>  <span class="mf">1.4570</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">2.8688</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.4517</span><span class="p">,</span>  <span class="mf">0.1782</span><span class="p">,</span>  <span class="mf">1.1149</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0898</span><span class="p">,</span>  <span class="mf">1.1062</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0681</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.5697</span><span class="p">,</span>  <span class="mf">0.8888</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6965</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.0429</span><span class="p">,</span>  <span class="mf">1.4446</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3349</span><span class="p">,</span>  <span class="mf">0.1254</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5017</span><span class="p">,</span>  <span class="mf">0.2286</span><span class="p">,</span>  <span class="mf">0.2328</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.3290</span><span class="p">,</span>  <span class="mf">0.3949</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2586</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.8476</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0004</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1003</span><span class="p">,</span>  <span class="mf">2.2806</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2226</span><span class="p">,</span>  <span class="mf">0.9251</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3165</span><span class="p">,</span>
          <span class="mf">0.4957</span><span class="p">,</span>  <span class="mf">0.0690</span><span class="p">,</span>  <span class="mf">0.0232</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.9108</span><span class="p">,</span>  <span class="mf">1.1355</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2715</span><span class="p">,</span>  <span class="mf">0.2233</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3681</span><span class="p">,</span>  <span class="mf">0.1442</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0001</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.0174</span><span class="p">,</span>  <span class="mf">0.1454</span><span class="p">,</span>  <span class="mf">0.2286</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.0663</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8466</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7147</span><span class="p">,</span>  <span class="mf">2.5685</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2090</span><span class="p">,</span>  <span class="mf">1.2993</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3057</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.8314</span><span class="p">,</span>  <span class="mf">0.7046</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0176</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">1.7013</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8051</span><span class="p">,</span>  <span class="mf">0.7541</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5248</span><span class="p">,</span>  <span class="mf">0.8972</span><span class="p">,</span>  <span class="mf">0.1518</span><span class="p">,</span>  <span class="mf">1.4876</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.8454</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2022</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2829</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.8179</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1239</span><span class="p">,</span>  <span class="mf">0.8630</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2137</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2275</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5411</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3448</span><span class="p">,</span>
          <span class="mf">1.7354</span><span class="p">,</span>  <span class="mf">0.7751</span><span class="p">,</span>  <span class="mf">0.6234</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.6515</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0431</span><span class="p">,</span>  <span class="mf">2.7165</span><span class="p">,</span>  <span class="mf">0.1873</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0623</span><span class="p">,</span>  <span class="mf">0.1286</span><span class="p">,</span>  <span class="mf">0.3597</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.2739</span><span class="p">,</span>  <span class="mf">0.3871</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6699</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.2828</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4663</span><span class="p">,</span>  <span class="mf">0.1182</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0896</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3640</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5129</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4905</span><span class="p">,</span>
          <span class="mf">2.2914</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2227</span><span class="p">,</span>  <span class="mf">0.9463</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.2596</span><span class="p">,</span>  <span class="mf">2.0468</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4405</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0411</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8073</span><span class="p">,</span>  <span class="mf">0.0490</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0604</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.1206</span><span class="p">,</span>  <span class="mf">0.3504</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1059</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.6089</span><span class="p">,</span>  <span class="mf">0.5885</span><span class="p">,</span>  <span class="mf">0.7898</span><span class="p">,</span>  <span class="mf">1.1318</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9008</span><span class="p">,</span>  <span class="mf">0.5875</span><span class="p">,</span>  <span class="mf">0.4227</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.1815</span><span class="p">,</span>  <span class="mf">0.5652</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3590</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.4551</span><span class="p">,</span>  <span class="mf">2.9537</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2805</span><span class="p">,</span>  <span class="mf">0.2372</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4180</span><span class="p">,</span>  <span class="mf">0.0297</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1515</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.6111</span><span class="p">,</span>  <span class="mf">0.6140</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3354</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.7182</span><span class="p">,</span>  <span class="mf">1.6778</span><span class="p">,</span>  <span class="mf">0.0553</span><span class="p">,</span>  <span class="mf">0.0461</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5446</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0338</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0215</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.0881</span><span class="p">,</span>  <span class="mf">0.1506</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2107</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.8027</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7854</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1275</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3177</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1600</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1964</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6084</span><span class="p">,</span>
          <span class="mf">2.1285</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1815</span><span class="p">,</span>  <span class="mf">1.1911</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">2.0656</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4959</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1154</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1363</span><span class="p">,</span>  <span class="mf">2.2426</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7441</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8413</span><span class="p">,</span>
          <span class="mf">0.4675</span><span class="p">,</span>  <span class="mf">0.3269</span><span class="p">,</span>  <span class="mf">1.7279</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.3004</span><span class="p">,</span>  <span class="mf">1.0166</span><span class="p">,</span>  <span class="mf">1.1175</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0618</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0937</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4221</span><span class="p">,</span>  <span class="mf">0.1943</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.1020</span><span class="p">,</span>  <span class="mf">0.3670</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4683</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.0720</span><span class="p">,</span>  <span class="mf">0.2252</span><span class="p">,</span>  <span class="mf">0.0175</span><span class="p">,</span>  <span class="mf">1.3644</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7409</span><span class="p">,</span>  <span class="mf">0.4655</span><span class="p">,</span>  <span class="mf">0.5439</span><span class="p">,</span>
          <span class="mf">0.0380</span><span class="p">,</span>  <span class="mf">0.1279</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2302</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.2409</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2622</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6336</span><span class="p">,</span>  <span class="mf">1.8240</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5951</span><span class="p">,</span>  <span class="mf">1.3408</span><span class="p">,</span>  <span class="mf">0.2130</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.3789</span><span class="p">,</span>  <span class="mf">0.8363</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2101</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.3849</span><span class="p">,</span>  <span class="mf">0.3773</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0585</span><span class="p">,</span>  <span class="mf">0.6896</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0998</span><span class="p">,</span>  <span class="mf">0.2804</span><span class="p">,</span>  <span class="mf">0.0696</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.2529</span><span class="p">,</span>  <span class="mf">0.3143</span><span class="p">,</span>  <span class="mf">0.3409</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.9103</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1578</span><span class="p">,</span>  <span class="mf">1.6673</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4817</span><span class="p">,</span>  <span class="mf">0.4088</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5484</span><span class="p">,</span>  <span class="mf">0.6103</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.2287</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0665</span><span class="p">,</span>  <span class="mf">0.0055</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.1692</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.8531</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2499</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0257</span><span class="p">,</span>  <span class="mf">2.8580</span><span class="p">,</span>  <span class="mf">0.2616</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7122</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.0551</span><span class="p">,</span>  <span class="mf">0.8112</span><span class="p">,</span>  <span class="mf">2.3233</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.2790</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9494</span><span class="p">,</span>  <span class="mf">0.6096</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5653</span><span class="p">,</span>  <span class="mf">2.2792</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0687</span><span class="p">,</span>  <span class="mf">0.1634</span><span class="p">,</span>
          <span class="mf">0.3122</span><span class="p">,</span>  <span class="mf">0.1053</span><span class="p">,</span>  <span class="mf">1.0884</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.1267</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2297</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1315</span><span class="p">,</span>  <span class="mf">0.2428</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5436</span><span class="p">,</span>  <span class="mf">0.4123</span><span class="p">,</span>  <span class="mf">2.3060</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.9278</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1528</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4224</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0235</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9137</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1457</span><span class="p">,</span>  <span class="mf">1.6858</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7552</span><span class="p">,</span>  <span class="mf">0.7293</span><span class="p">,</span>  <span class="mf">0.2510</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.3955</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2187</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1505</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.5643</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2783</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4149</span><span class="p">,</span>  <span class="mf">0.0304</span><span class="p">,</span>  <span class="mf">0.8375</span><span class="p">,</span>  <span class="mf">1.5018</span><span class="p">,</span>  <span class="mf">0.0338</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.3875</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0117</span><span class="p">,</span>  <span class="mf">0.5751</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.2926</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7486</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3238</span><span class="p">,</span>  <span class="mf">1.0384</span><span class="p">,</span>  <span class="mf">0.0308</span><span class="p">,</span>  <span class="mf">0.6792</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0170</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.5797</span><span class="p">,</span>  <span class="mf">0.2819</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3510</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.1219</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5862</span><span class="p">,</span>  <span class="mf">1.5817</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1297</span><span class="p">,</span>  <span class="mf">0.4730</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9171</span><span class="p">,</span>  <span class="mf">0.7886</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.7022</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0501</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2812</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">1.7587</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.4511</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7369</span><span class="p">,</span>  <span class="mf">0.4082</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6426</span><span class="p">,</span>  <span class="mf">1.1784</span><span class="p">,</span>  <span class="mf">0.6052</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.7178</span><span class="p">,</span>  <span class="mf">1.6161</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2220</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.1267</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6719</span><span class="p">,</span>  <span class="mf">0.0505</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4972</span><span class="p">,</span>  <span class="mf">2.9027</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1461</span><span class="p">,</span>  <span class="mf">0.2807</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.2921</span><span class="p">,</span>  <span class="mf">0.2231</span><span class="p">,</span>  <span class="mf">1.1327</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.9892</span><span class="p">,</span>  <span class="mf">2.4401</span><span class="p">,</span>  <span class="mf">0.1274</span><span class="p">,</span>  <span class="mf">0.2838</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7535</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1684</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6493</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.1908</span><span class="p">,</span>  <span class="mf">0.2290</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2150</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.2071</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1351</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9191</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9309</span><span class="p">,</span>  <span class="mf">1.7747</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3046</span><span class="p">,</span>  <span class="mf">0.0183</span><span class="p">,</span>
          <span class="mf">1.0136</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1016</span><span class="p">,</span>  <span class="mf">2.1288</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0103</span><span class="p">,</span>  <span class="mf">0.3280</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6974</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2504</span><span class="p">,</span>  <span class="mf">0.3187</span><span class="p">,</span>  <span class="mf">0.4390</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1879</span><span class="p">,</span>
          <span class="mf">0.3954</span><span class="p">,</span>  <span class="mf">0.2332</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1971</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.2280</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6754</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7438</span><span class="p">,</span>  <span class="mf">0.5078</span><span class="p">,</span>  <span class="mf">0.2544</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1020</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2503</span><span class="p">,</span>
          <span class="mf">2.0799</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5033</span><span class="p">,</span>  <span class="mf">0.5890</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.3972</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9369</span><span class="p">,</span>  <span class="mf">1.2696</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6713</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4159</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0221</span><span class="p">,</span>  <span class="mf">0.6489</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.4777</span><span class="p">,</span>  <span class="mf">1.2497</span><span class="p">,</span>  <span class="mf">0.3931</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.7566</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8230</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0785</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3083</span><span class="p">,</span>  <span class="mf">0.7821</span><span class="p">,</span>  <span class="mf">0.1880</span><span class="p">,</span>  <span class="mf">0.1037</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.0956</span><span class="p">,</span>  <span class="mf">0.4219</span><span class="p">,</span>  <span class="mf">1.0798</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.0328</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1700</span><span class="p">,</span>  <span class="mf">1.3806</span><span class="p">,</span>  <span class="mf">0.5445</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2624</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0780</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3595</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.6253</span><span class="p">,</span>  <span class="mf">0.4309</span><span class="p">,</span>  <span class="mf">0.1813</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.0360</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4704</span><span class="p">,</span>  <span class="mf">0.1948</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7066</span><span class="p">,</span>  <span class="mf">0.6600</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4633</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3602</span><span class="p">,</span>
          <span class="mf">1.7494</span><span class="p">,</span>  <span class="mf">0.1522</span><span class="p">,</span>  <span class="mf">0.6086</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.2032</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7903</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5754</span><span class="p">,</span>  <span class="mf">0.4722</span><span class="p">,</span>  <span class="mf">0.6068</span><span class="p">,</span>  <span class="mf">0.5752</span><span class="p">,</span>  <span class="mf">0.2151</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.2495</span><span class="p">,</span>  <span class="mf">0.3420</span><span class="p">,</span>  <span class="mf">0.9278</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.2247</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1361</span><span class="p">,</span>  <span class="mf">0.9374</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1543</span><span class="p">,</span>  <span class="mf">0.4921</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6553</span><span class="p">,</span>  <span class="mf">0.5885</span><span class="p">,</span>
          <span class="mf">0.2617</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2216</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3736</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.2867</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4486</span><span class="p">,</span>  <span class="mf">0.6658</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8755</span><span class="p">,</span>  <span class="mf">2.3195</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7627</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2132</span><span class="p">,</span>
          <span class="mf">0.2488</span><span class="p">,</span>  <span class="mf">0.3484</span><span class="p">,</span>  <span class="mf">1.0860</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.4031</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4518</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3181</span><span class="p">,</span>  <span class="mf">2.8268</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5371</span><span class="p">,</span>  <span class="mf">1.0154</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9247</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.7385</span><span class="p">,</span>  <span class="mf">1.1031</span><span class="p">,</span>  <span class="mf">0.0422</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">2.8604</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5413</span><span class="p">,</span>  <span class="mf">0.6241</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8017</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4104</span><span class="p">,</span>  <span class="mf">0.6314</span><span class="p">,</span>  <span class="mf">0.4614</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.0218</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3411</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2609</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.2113</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2348</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8535</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1041</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2703</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1294</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7057</span><span class="p">,</span>
          <span class="mf">2.7552</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4429</span><span class="p">,</span>  <span class="mf">0.4517</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">4.5191</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.7407</span><span class="p">,</span>  <span class="mf">1.1091</span><span class="p">,</span>  <span class="mf">0.3975</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9456</span><span class="p">,</span>  <span class="mf">1.2277</span><span class="p">,</span>  <span class="mf">0.3616</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.6564</span><span class="p">,</span>  <span class="mf">0.5063</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4274</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">1.4615</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0765</span><span class="p">,</span>  <span class="mf">1.8388</span><span class="p">,</span>  <span class="mf">1.5006</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2351</span><span class="p">,</span>  <span class="mf">0.2781</span><span class="p">,</span>  <span class="mf">0.2830</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.8491</span><span class="p">,</span>  <span class="mf">0.2222</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7779</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.2160</span><span class="p">,</span>  <span class="mf">0.8502</span><span class="p">,</span>  <span class="mf">0.2413</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0798</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7880</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4286</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8060</span><span class="p">,</span>
          <span class="mf">0.7194</span><span class="p">,</span>  <span class="mf">1.2663</span><span class="p">,</span>  <span class="mf">0.6412</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.3318</span><span class="p">,</span>  <span class="mf">2.3388</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4003</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1094</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0285</span><span class="p">,</span>  <span class="mf">0.1021</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0388</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.0497</span><span class="p">,</span>  <span class="mf">0.5137</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2507</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.7853</span><span class="p">,</span>  <span class="mf">0.5884</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6108</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5557</span><span class="p">,</span>  <span class="mf">0.8696</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6226</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7983</span><span class="p">,</span>
          <span class="mf">1.7169</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0145</span><span class="p">,</span>  <span class="mf">0.8231</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.1739</span><span class="p">,</span>  <span class="mf">0.1562</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2933</span><span class="p">,</span>  <span class="mf">2.3195</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9480</span><span class="p">,</span>  <span class="mf">1.2019</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4834</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.0567</span><span class="p">,</span>  <span class="mf">0.5685</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6841</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.7920</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3339</span><span class="p">,</span>  <span class="mf">0.7452</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6529</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3307</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6092</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0950</span><span class="p">,</span>
          <span class="mf">1.7311</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3481</span><span class="p">,</span>  <span class="mf">0.3801</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.7810</span><span class="p">,</span>  <span class="mf">1.0676</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7611</span><span class="p">,</span>  <span class="mf">0.3658</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0431</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1012</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6048</span><span class="p">,</span>
          <span class="mf">0.3089</span><span class="p">,</span>  <span class="mf">0.9998</span><span class="p">,</span>  <span class="mf">0.7164</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.5856</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5261</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4859</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0551</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1838</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2144</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2599</span><span class="p">,</span>
          <span class="mf">3.3891</span><span class="p">,</span>  <span class="mf">0.4691</span><span class="p">,</span>  <span class="mf">0.7566</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.4984</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7770</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1998</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1075</span><span class="p">,</span>  <span class="mf">1.0882</span><span class="p">,</span>  <span class="mf">0.4539</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5651</span><span class="p">,</span>
          <span class="mf">1.4381</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5678</span><span class="p">,</span>  <span class="mf">1.7479</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.2938</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8536</span><span class="p">,</span>  <span class="mf">0.4259</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5429</span><span class="p">,</span>  <span class="mf">0.0066</span><span class="p">,</span>  <span class="mf">0.4120</span><span class="p">,</span>  <span class="mf">2.3793</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.3666</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2604</span><span class="p">,</span>  <span class="mf">0.0382</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.4080</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9851</span><span class="p">,</span>  <span class="mf">4.0264</span><span class="p">,</span>  <span class="mf">0.1099</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1766</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1557</span><span class="p">,</span>  <span class="mf">0.6419</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.8147</span><span class="p">,</span>  <span class="mf">0.7535</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1452</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.4636</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7323</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6433</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0274</span><span class="p">,</span>  <span class="mf">0.7227</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1799</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9336</span><span class="p">,</span>
          <span class="mf">2.1881</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2073</span><span class="p">,</span>  <span class="mf">1.6522</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.9617</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0348</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3980</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4738</span><span class="p">,</span>  <span class="mf">0.7790</span><span class="p">,</span>  <span class="mf">0.4671</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6115</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.7067</span><span class="p">,</span>  <span class="mf">1.3036</span><span class="p">,</span>  <span class="mf">0.4923</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.0151</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5385</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6072</span><span class="p">,</span>  <span class="mf">0.2902</span><span class="p">,</span>  <span class="mf">3.1570</span><span class="p">,</span>  <span class="mf">0.1062</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2169</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.4491</span><span class="p">,</span>  <span class="mf">0.6326</span><span class="p">,</span>  <span class="mf">1.6829</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.8852</span><span class="p">,</span>  <span class="mf">0.6066</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2840</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4475</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1147</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7858</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1805</span><span class="p">,</span>
          <span class="mf">3.0723</span><span class="p">,</span>  <span class="mf">0.3960</span><span class="p">,</span>  <span class="mf">0.9720</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.0344</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4878</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9675</span><span class="p">,</span>  <span class="mf">1.9649</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3146</span><span class="p">,</span>  <span class="mf">1.2183</span><span class="p">,</span>  <span class="mf">0.6730</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.3650</span><span class="p">,</span>  <span class="mf">0.0646</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0898</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.2118</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0350</span><span class="p">,</span>  <span class="mf">0.9917</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8993</span><span class="p">,</span>  <span class="mf">1.2334</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6723</span><span class="p">,</span>  <span class="mf">2.5847</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.0454</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4149</span><span class="p">,</span>  <span class="mf">0.3927</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.7365</span><span class="p">,</span>  <span class="mf">3.0447</span><span class="p">,</span>  <span class="mf">0.5115</span><span class="p">,</span>  <span class="mf">0.0786</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7544</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2158</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4876</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.2891</span><span class="p">,</span>  <span class="mf">0.5089</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6719</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.3652</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5457</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1167</span><span class="p">,</span>  <span class="mf">2.9056</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1622</span><span class="p">,</span>  <span class="mf">0.8192</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3245</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.6414</span><span class="p">,</span>  <span class="mf">0.8097</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4958</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.8755</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6983</span><span class="p">,</span>  <span class="mf">0.2208</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6463</span><span class="p">,</span>  <span class="mf">0.5276</span><span class="p">,</span>  <span class="mf">0.1145</span><span class="p">,</span>  <span class="mf">2.7229</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.0316</span><span class="p">,</span>  <span class="mf">0.1905</span><span class="p">,</span>  <span class="mf">0.2090</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.9702</span><span class="p">,</span>  <span class="mf">0.1265</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0007</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5106</span><span class="p">,</span>  <span class="mf">0.4970</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0804</span><span class="p">,</span>  <span class="mf">0.0017</span><span class="p">,</span>
          <span class="mf">0.0607</span><span class="p">,</span>  <span class="mf">0.6164</span><span class="p">,</span>  <span class="mf">0.4490</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.8271</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6822</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7434</span><span class="p">,</span>  <span class="mf">2.6457</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6143</span><span class="p">,</span>  <span class="mf">1.1486</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0705</span><span class="p">,</span>
          <span class="mf">0.5611</span><span class="p">,</span>  <span class="mf">0.6422</span><span class="p">,</span>  <span class="mf">0.1250</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.9979</span><span class="p">,</span>  <span class="mf">1.8175</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1658</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0343</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6292</span><span class="p">,</span>  <span class="mf">0.1774</span><span class="p">,</span>  <span class="mf">0.3150</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.4633</span><span class="p">,</span>  <span class="mf">0.9266</span><span class="p">,</span>  <span class="mf">0.0252</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.9039</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6030</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2173</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1768</span><span class="p">,</span>  <span class="mf">2.3198</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5072</span><span class="p">,</span>  <span class="mf">0.3418</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.1551</span><span class="p">,</span>  <span class="mf">0.1282</span><span class="p">,</span>  <span class="mf">1.4250</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.9891</span><span class="p">,</span>  <span class="mf">0.5212</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4518</span><span class="p">,</span>  <span class="mf">0.3267</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0759</span><span class="p">,</span>  <span class="mf">0.3826</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0341</span><span class="p">,</span>
          <span class="mf">0.0382</span><span class="p">,</span>  <span class="mf">0.2451</span><span class="p">,</span>  <span class="mf">0.3658</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">2.1217</span><span class="p">,</span>  <span class="mf">1.5102</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7828</span><span class="p">,</span>  <span class="mf">0.3554</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4192</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0772</span><span class="p">,</span>  <span class="mf">0.0578</span><span class="p">,</span>
          <span class="mf">0.8070</span><span class="p">,</span>  <span class="mf">0.1701</span><span class="p">,</span>  <span class="mf">0.5880</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">1.0665</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3826</span><span class="p">,</span>  <span class="mf">0.6243</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8096</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4227</span><span class="p">,</span>  <span class="mf">0.5925</span><span class="p">,</span>  <span class="mf">1.8112</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.9946</span><span class="p">,</span>  <span class="mf">0.2010</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7731</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.1263</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7484</span><span class="p">,</span>  <span class="mf">0.0041</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5439</span><span class="p">,</span>  <span class="mf">1.7242</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9475</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3835</span><span class="p">,</span>
          <span class="mf">0.8452</span><span class="p">,</span>  <span class="mf">0.3077</span><span class="p">,</span>  <span class="mf">2.2689</span><span class="p">]])</span>
</pre></div>

<div class="admonition note">
<p class="admonition-title">Printing output size</p>
<p>This produces a 100x10 matrix because each iteration has a batch size of 100 and each prediction across the 10 classes, with the largest number indicating the likely number it is predicting.
<div class="codehilite"><pre><span></span><span class="n">iter_test</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
    <span class="n">iter_test</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">iter_test</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;OUTPUTS&#39;</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div></p>
</div>
<div class="codehilite"><pre><span></span><span class="n">OUTPUTS</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
</pre></div>

<div class="admonition note">
<p class="admonition-title">Printing one output</p>
<p>This would be a 1x10 matrix where the largest number is what the model thinks the image is. Here we can see that in the tensor, position 7 has the largest number, indicating the model thinks the image is 7.</p>
<p>number 0: -0.4181
<br /> number 1: -1.0784
<br />...
<br /> number 7: 2.9352
<div class="codehilite"><pre><span></span><span class="n">iter_test</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
    <span class="n">iter_test</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">iter_test</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;OUTPUTS&#39;</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:])</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div></p>
</div>
<div class="codehilite"><pre><span></span>OUTPUTS
tensor([-0.4181, -1.0784, -0.4840, -0.0985, -0.2394, -0.1801, -1.1639,
         2.9352, -0.1552,  0.8852])
</pre></div>

<div class="admonition note">
<p class="admonition-title">Printing prediction output</p>
<p>Because our output is of size 100 (our batch size), our prediction size would also of the size 100.</p>
<div class="codehilite"><pre><span></span><span class="n">iter_test</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
    <span class="n">iter_test</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">iter_test</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;PREDICTION&#39;</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">predicted</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>

</div>
<div class="codehilite"><pre><span></span><span class="n">PREDICTION</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">100</span><span class="p">])</span>
</pre></div>

<div class="admonition note">
<p class="admonition-title">Print prediction value</p>
<p>We are printing our prediction which as verified above, should be digit 7.</p>
<div class="codehilite"><pre><span></span><span class="n">iter_test</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
    <span class="n">iter_test</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">iter_test</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;PREDICTION&#39;</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">predicted</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>

</div>
<div class="codehilite"><pre><span></span><span class="n">PREDICTION</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
</pre></div>

<div class="admonition note">
<p class="admonition-title">Print prediction, label and label size</p>
<p>We are trying to show what we are predicting and the actual values. In this case, we're predicting the right value 7!</p>
<div class="codehilite"><pre><span></span><span class="n">iter_test</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
    <span class="n">iter_test</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">iter_test</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;PREDICTION&#39;</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">predicted</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;LABEL SIZE&#39;</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>

        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;LABEL FOR IMAGE 0&#39;</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>

</div>
<div class="codehilite"><pre><span></span><span class="n">PREDICTION</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>

<span class="n">LABEL</span> <span class="n">SIZE</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">100</span><span class="p">])</span>

<span class="n">LABEL</span> <span class="n">FOR</span> <span class="n">IMAGE</span> <span class="mi">0</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
</pre></div>

<div class="admonition note">
<p class="admonition-title">Print second prediction and ground truth</p>
<p>Again, the prediction is correct. Naturally, as our model is quite competent in this simple task.</p>
<div class="codehilite"><pre><span></span><span class="n">iter_test</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
    <span class="n">iter_test</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">iter_test</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;PREDICTION&#39;</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">predicted</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;LABEL SIZE&#39;</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>

        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;LABEL FOR IMAGE 1&#39;</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>

</div>
<div class="codehilite"><pre><span></span><span class="n">PREDICTION</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="n">LABEL</span> <span class="n">SIZE</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">100</span><span class="p">])</span>

<span class="n">LABEL</span> <span class="n">FOR</span> <span class="n">IMAGE</span> <span class="mi">1</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

<div class="admonition note">
<p class="admonition-title">Print accuracy</p>
<p>Now we know what each object represents, we can understand how we arrived at our accuracy numbers.</p>
<p>One last thing to note is that <code class="codehilite">correct.item()</code> has this syntax is because <code class="codehilite">correct</code> is a PyTorch tensor and to get the value to compute with <code class="codehilite">total</code> which is an integer, we need to do this.
<div class="codehilite"><pre><span></span><span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">iter_test</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
    <span class="n">iter_test</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Total number of labels</span>
    <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Total correct predictions</span>
    <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="n">accuracy</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="n">correct</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">total</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
</pre></div></p>
</div>
<div class="codehilite"><pre><span></span><span class="mf">82.94</span>
</pre></div>

<div class="admonition note">
<p class="admonition-title">Explanation of Python's .sum() function</p>
<p>Python's .sum() function allows you to do a comparison between two matrices and sum the ones that return <code class="codehilite">True</code> or in our case, those predictions that match actual labels (correct predictions).</p>
<div class="codehilite"><pre><span></span><span class="c1"># Explaining .sum() python built-in function</span>
<span class="c1"># correct += (predicted == labels).sum()</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">10</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">10</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">a</span> <span class="o">==</span> <span class="n">b</span><span class="p">)</span>

<span class="k">print</span><span class="p">((</span><span class="n">a</span> <span class="o">==</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
</pre></div>

</div>
<div class="codehilite"><pre><span></span><span class="c1"># matrix a</span>
<span class="p">[</span><span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span><span class="p">]</span>

<span class="c1"># matrix b</span>
<span class="p">[</span><span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span><span class="p">]</span>

<span class="c1"># boolean array</span>
<span class="p">[</span> <span class="bp">True</span>  <span class="bp">True</span>  <span class="bp">True</span>  <span class="bp">True</span>  <span class="bp">True</span>  <span class="bp">True</span>  <span class="bp">True</span>  <span class="bp">True</span>  <span class="bp">True</span>  <span class="bp">True</span><span class="p">]</span>

<span class="c1"># number of elementswhere a matches b</span>
<span class="mi">10</span>
</pre></div>

<h4 id="saving-model">Saving Model<a class="headerlink" href="#saving-model" title="Permanent link">&para;</a></h4>
<div class="admonition note">
<p class="admonition-title">Saving PyTorch model</p>
<p>This is how you save your model. Feel free to just change <code class="codehilite">save_model = True</code> to save your model
<div class="codehilite"><pre><span></span><span class="n">save_model</span> <span class="o">=</span> <span class="bp">False</span>
<span class="k">if</span> <span class="n">save_model</span> <span class="ow">is</span> <span class="bp">True</span><span class="p">:</span>
    <span class="c1"># Saves only parameters</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">&#39;awesome_model.pkl&#39;</span><span class="p">)</span>
</pre></div></p>
</div>
<h2 id="building-a-logistic-regression-model-with-pytorch-gpu">Building a Logistic Regression Model with PyTorch (GPU)<a class="headerlink" href="#building-a-logistic-regression-model-with-pytorch-gpu" title="Permanent link">&para;</a></h2>
<div class="admonition note">
<p class="admonition-title">CPU version</p>
<p>The usual 7-step process, getting repetitive by now which we like. </p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="kn">as</span> <span class="nn">transforms</span>
<span class="kn">import</span> <span class="nn">torchvision.datasets</span> <span class="kn">as</span> <span class="nn">dsets</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 1: LOADING DATASET</span>
<span class="sd">&#39;&#39;&#39;</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">dsets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> 
                            <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
                            <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                            <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">dsets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> 
                           <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
                           <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 2: MAKING DATASET ITERABLE</span>
<span class="sd">&#39;&#39;&#39;</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">n_iters</span> <span class="o">=</span> <span class="mi">3000</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="n">n_iters</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">)</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> 
                                           <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
                                           <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">test_dataset</span><span class="p">,</span> 
                                          <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
                                          <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 3: CREATE MODEL CLASS</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="k">class</span> <span class="nc">LogisticRegressionModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LogisticRegressionModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 4: INSTANTIATE MODEL CLASS</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="n">input_dim</span> <span class="o">=</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span>
<span class="n">output_dim</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegressionModel</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 5: INSTANTIATE LOSS CLASS</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>


<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 6: INSTANTIATE OPTIMIZER CLASS</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 7: TRAIN THE MODEL</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="nb">iter</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="c1"># Load images as Variable</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span>

        <span class="c1"># Clear gradients w.r.t. parameters</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># Forward pass to get output/logits</span>
        <span class="c1"># 100 x 10</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

        <span class="c1"># Calculate Loss: softmax --&gt; cross entropy loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="c1"># Getting gradients w.r.t. parameters</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># Updating parameters</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="nb">iter</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="nb">iter</span> <span class="o">%</span> <span class="mi">500</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Calculate Accuracy         </span>
            <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="c1"># Iterate through test dataset</span>
            <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
                <span class="c1"># Load images to a Torch Variable</span>
                <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>

                <span class="c1"># Forward pass only to get logits/output</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

                <span class="c1"># Get predictions from the maximum value</span>
                <span class="c1"># 100 x 1</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

                <span class="c1"># Total number of labels</span>
                <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

                <span class="c1"># Total correct predictions</span>
                <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

            <span class="n">accuracy</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">total</span>

            <span class="c1"># Print Loss</span>
            <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Iteration: {}. Loss: {}. Accuracy: {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">iter</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">accuracy</span><span class="p">))</span>
</pre></div>

</div>
<div class="codehilite"><pre><span></span><span class="n">Iteration</span><span class="p">:</span> <span class="mf">500.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.876196026802063</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">64.44</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">1000.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.5153584480285645</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">75.68</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">1500.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.3521136045455933</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">78.98</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">2000.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.2136967182159424</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">80.95</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">2500.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.0934826135635376</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">81.97</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">3000.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.024120569229126</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">82.49</span>
</pre></div>

<div class="admonition note">
<p class="admonition-title">GPU version</p>
<p>2 things must be on GPU
<br />- <code class="codehilite">model</code>
<br />- <code class="codehilite">tensors</code></p>
<p>Remember step 4 and 7 will be affected and this will be the same for all model building moving forward.</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="kn">as</span> <span class="nn">transforms</span>
<span class="kn">import</span> <span class="nn">torchvision.datasets</span> <span class="kn">as</span> <span class="nn">dsets</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 1: LOADING DATASET</span>
<span class="sd">&#39;&#39;&#39;</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">dsets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> 
                            <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
                            <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                            <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">dsets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> 
                           <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
                           <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 2: MAKING DATASET ITERABLE</span>
<span class="sd">&#39;&#39;&#39;</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">n_iters</span> <span class="o">=</span> <span class="mi">3000</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="n">n_iters</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">)</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> 
                                           <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
                                           <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">test_dataset</span><span class="p">,</span> 
                                          <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
                                          <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 3: CREATE MODEL CLASS</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="k">class</span> <span class="nc">LogisticRegressionModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LogisticRegressionModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 4: INSTANTIATE MODEL CLASS</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="n">input_dim</span> <span class="o">=</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span>
<span class="n">output_dim</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegressionModel</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>

<span class="c1">#######################</span>
<span class="c1">#  USE GPU FOR MODEL  #</span>
<span class="c1">#######################</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 5: INSTANTIATE LOSS CLASS</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>


<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 6: INSTANTIATE OPTIMIZER CLASS</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 7: TRAIN THE MODEL</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="nb">iter</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>

        <span class="c1">#######################</span>
        <span class="c1">#  USE GPU FOR MODEL  #</span>
        <span class="c1">#######################</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Clear gradients w.r.t. parameters</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># Forward pass to get output/logits</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

        <span class="c1"># Calculate Loss: softmax --&gt; cross entropy loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="c1"># Getting gradients w.r.t. parameters</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># Updating parameters</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="nb">iter</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="nb">iter</span> <span class="o">%</span> <span class="mi">500</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Calculate Accuracy         </span>
            <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="c1"># Iterate through test dataset</span>
            <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
                <span class="c1">#######################</span>
                <span class="c1">#  USE GPU FOR MODEL  #</span>
                <span class="c1">#######################</span>
                <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

                <span class="c1"># Forward pass only to get logits/output</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

                <span class="c1"># Get predictions from the maximum value</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

                <span class="c1"># Total number of labels</span>
                <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

                <span class="c1">#######################</span>
                <span class="c1">#  USE GPU FOR MODEL  #</span>
                <span class="c1">#######################</span>
                <span class="c1"># Total correct predictions</span>
                <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
                    <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span> <span class="o">==</span> <span class="n">labels</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

            <span class="n">accuracy</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">total</span>

            <span class="c1"># Print Loss</span>
            <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Iteration: {}. Loss: {}. Accuracy: {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">iter</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">accuracy</span><span class="p">))</span>
</pre></div>

</div>
<div class="codehilite"><pre><span></span><span class="n">Iteration</span><span class="p">:</span> <span class="mf">500.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.8571407794952393</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">68.99</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">1000.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.5415704250335693</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">75.86</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">1500.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.2755383253097534</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">78.92</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">2000.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.2468739748001099</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">80.72</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">2500.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.0708973407745361</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">81.73</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">3000.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.0359245538711548</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">82.74</span>
</pre></div>

<h2 id="summary">Summary<a class="headerlink" href="#summary" title="Permanent link">&para;</a></h2>
<p>We've learnt to...</p>
<div class="admonition success">
<p class="admonition-title">Success</p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> <strong>Logistic regression</strong> basics</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> <strong>Problems</strong> of <strong>linear regression</strong></li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> <strong>In-depth</strong> Logistic Regression<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> Get logits</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> Get softmax</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> Get cross-entropy loss</li>
</ul>
</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> <strong>Aim</strong>: reduce cross-entropy loss</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> Built a <strong>logistic regression model</strong> in <strong>CPU and GPU</strong><ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> Step 1: Load Dataset</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> Step 2: Make Dataset Iterable</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> Step 3: Create Model Class</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> Step 4: Instantiate Model Class</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> Step 5: Instantiate Loss Class</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> Step 6: Instantiate Optimizer Class</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> Step 7: Train Model</li>
</ul>
</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> Important things to be on <strong>GPU</strong><ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> <code class="codehilite">model</code></li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> <code class="codehilite">tensors with gradients</code></li>
</ul>
</li>
</ul>
</div>
<h2 id="citation">Citation<a class="headerlink" href="#citation" title="Permanent link">&para;</a></h2>
<p>If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.</p>
<p><a href="https://zenodo.org/badge/latestdoi/139945544"><img alt="DOI" src="https://zenodo.org/badge/139945544.svg" /></a></p>
                
                  
                
              
              
                


  <h2 id="__comments">Comments</h2>
  <div id="disqus_thread"></div>
  <script>
    var disqus_config = function () {
      this.page.url = "https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_logistic_regression/";
      this.page.identifier =
        "deep_learning/practical_pytorch/pytorch_logistic_regression/";
    };
    (function() {
      var d = document, s = d.createElement("script");
      s.src = "//deep-learning-wizard.disqus.com/embed.js";
      s.setAttribute("data-timestamp", +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>

              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="deep_learning/practical_pytorch/pytorch_linear_regression/" title="Linear Regression" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Linear Regression
              </span>
            </div>
          </a>
        
        
          <a href="deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/" title="Feedforward Neural Networks" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Feedforward Neural Networks
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2018 Deep Learning Wizard
          </div>
        
        proudly an
        <a href="http://www.nvidia.com/page/home.html">NVIDIA Inception Partner</a>
        based in Singapore by
        <a href="https://www.ritchieng.com/">
          Ritchie Ng (NVIDIA Deep Learning Institute Instructor)</a>
      </div>
      
        
  <div class="md-footer-social">
    <link rel="stylesheet" href="../../../assets/fonts/font-awesome.css">
    
      <a href="https://github.com/ritchieng" class="md-footer-social__link fa fa-github"></a>
    
      <a href="https://www.facebook.com/DeepLearningWizard/" class="md-footer-social__link fa fa-facebook"></a>
    
      <a href="https://www.linkedin.com/company/deeplearningwizard/" class="md-footer-social__link fa fa-linkedin"></a>
    
  </div>

      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../../assets/javascripts/application.583bbe55.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:"../../.."}})</script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
    
      
        <script>!function(e,a,t,n,o,c,i){e.GoogleAnalyticsObject=o,e.ga=e.ga||function(){(e.ga.q=e.ga.q||[]).push(arguments)},e.ga.l=1*new Date,c=a.createElement(t),i=a.getElementsByTagName(t)[0],c.async=1,c.src="https://www.google-analytics.com/analytics.js",i.parentNode.insertBefore(c,i)}(window,document,"script",0,"ga"),ga("create","UA-122083328-1","auto"),ga("set","anonymizeIp",!0),ga("send","pageview");var links=document.getElementsByTagName("a");if(Array.prototype.map.call(links,function(e){e.host!=document.location.host&&e.addEventListener("click",function(){var a=e.getAttribute("data-md-action")||"follow";ga("send","event","outbound",a,e.href)})}),document.forms.search){var query=document.forms.search.query;query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})}</script>
      
    
  </body>
</html>