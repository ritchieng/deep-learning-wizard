


<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Learn deep learning and deep reinforcement learning math and code easily and quickly. Used by thousands of students and professionals from top tech companies and research institutions.">
      
      
        <link rel="canonical" href="https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_logistic_regression/">
      
      
        <meta name="author" content="Ritchie Ng">
      
      <link rel="shortcut icon" href="../../../docs/assets/favicon.ico">
      <meta name="generator" content="mkdocs-1.1, mkdocs-material-5.1.0">
    
    
      
        <title>Logistic Regression - Deep Learning Wizard</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.89dc9fe3.min.css">
      
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.ecd4686e.min.css">
      
      
        
        
        <meta name="theme-color" content="#546e7a">
      
    
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
    
      
        
<link rel="preconnect dns-prefetch" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-122083328-1","auto"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})}),document.addEventListener("DOMContentSwitch",function(){ga("send","pageview")})</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
  
    
    
    <body dir="ltr" data-md-color-primary="blue-grey" data-md-color-accent="indigo">
  
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#logistic-regression-with-pytorch" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href="https://www.deeplearningwizard.com/" title="Deep Learning Wizard" class="md-header-nav__button md-logo" aria-label="Deep Learning Wizard">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19.35,10.03C18.67,6.59 15.64,4 12,4C9.11,4 6.6,5.64 5.35,8.03C2.34,8.36 0,10.9 0,14A6,6 0 0,0 6,20H19A5,5 0 0,0 24,15C24,12.36 21.95,10.22 19.35,10.03Z" /></svg>

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3,6H21V8H3V6M3,11H21V13H3V11M3,16H21V18H3V16Z" /></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      
        <div class="md-header-nav__ellipsis">
          <span class="md-header-nav__topic md-ellipsis">
            Deep Learning Wizard
          </span>
          <span class="md-header-nav__topic md-ellipsis">
            
              Logistic Regression
            
          </span>
        </div>
      
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5,3A6.5,6.5 0 0,1 16,9.5C16,11.11 15.41,12.59 14.44,13.73L14.71,14H15.5L20.5,19L19,20.5L14,15.5V14.71L13.73,14.44C12.59,15.41 11.11,16 9.5,16A6.5,6.5 0 0,1 3,9.5A6.5,6.5 0 0,1 9.5,3M9.5,5C7,5 5,7 5,9.5C5,12 7,14 9.5,14C12,14 14,12 14,9.5C14,7 12,5 9.5,5Z" /></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active">
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5,3A6.5,6.5 0 0,1 16,9.5C16,11.11 15.41,12.59 14.44,13.73L14.71,14H15.5L20.5,19L19,20.5L14,15.5V14.71L13.73,14.44C12.59,15.41 11.11,16 9.5,16A6.5,6.5 0 0,1 3,9.5A6.5,6.5 0 0,1 9.5,3M9.5,5C7,5 5,7 5,9.5C5,12 7,14 9.5,14C12,14 14,12 14,9.5C14,7 12,5 9.5,5Z" /></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19,6.41L17.59,5L12,10.59L6.41,5L5,6.41L10.59,12L5,17.59L6.41,19L12,13.41L17.59,19L19,17.59L13.41,12L19,6.41Z" /></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header-nav__source">
        
<a href="https://github.com/ritchieng/deep-learning-wizard/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    ritchieng/deep-learning-wizard
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
        
      
      
        
          

  

<nav class="md-tabs md-tabs--active" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  <li class="md-tabs__item">
    
      <a href="../../.." class="md-tabs__link">
        Home
      </a>
    
  </li>

      
        
      
        
      
        
      
        
      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../intro/" class="md-tabs__link md-tabs__link--active">
          Deep Learning Tutorials (CPU/GPU)
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../machine_learning/intro/" class="md-tabs__link">
          Machine Learning Tutorials (CPU/GPU)
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../programming/intro/" class="md-tabs__link">
          Programming Tutorials
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../database/intro/" class="md-tabs__link">
          Scalable Database Tutorials
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../news/news/" class="md-tabs__link">
          News
        </a>
      
    </li>
  

      
    </ul>
  </div>
</nav>
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="https://www.deeplearningwizard.com/" title="Deep Learning Wizard" class="md-nav__button md-logo" aria-label="Deep Learning Wizard">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19.35,10.03C18.67,6.59 15.64,4 12,4C9.11,4 6.6,5.64 5.35,8.03C2.34,8.36 0,10.9 0,14A6,6 0 0,0 6,20H19A5,5 0 0,0 24,15C24,12.36 21.95,10.22 19.35,10.03Z" /></svg>

    </a>
    Deep Learning Wizard
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/ritchieng/deep-learning-wizard/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    ritchieng/deep-learning-wizard
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../../.." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../../about/" title="About Us" class="md-nav__link">
      About Us
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../../review/" title="Reviews" class="md-nav__link">
      Reviews
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../../pipeline/" title="AI Pipeline" class="md-nav__link">
      AI Pipeline
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../../consultancy/" title="Consultancy" class="md-nav__link">
      Consultancy
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-6" type="checkbox" id="nav-6" checked>
    
    <label class="md-nav__link" for="nav-6">
      Deep Learning Tutorials (CPU/GPU)
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="Deep Learning Tutorials (CPU/GPU)" data-md-level="1">
      <label class="md-nav__title" for="nav-6">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        Deep Learning Tutorials (CPU/GPU)
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/" title="Introduction" class="md-nav__link">
      Introduction
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../course_progression/" title="Course Progression" class="md-nav__link">
      Course Progression
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../pytorch_matrices/" title="Matrices" class="md-nav__link">
      Matrices
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../pytorch_gradients/" title="Gradients" class="md-nav__link">
      Gradients
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../pytorch_linear_regression/" title="Linear Regression" class="md-nav__link">
      Linear Regression
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Logistic Regression
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3,9H17V7H3V9M3,13H17V11H3V13M3,17H17V15H3V17M19,17H21V15H19V17M19,7V9H21V7H19M19,13H21V11H19V13Z" /></svg>
        </span>
      </label>
    
    <a href="./" title="Logistic Regression" class="md-nav__link md-nav__link--active">
      Logistic Regression
    </a>
    
      
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#about-logistic-regression" class="md-nav__link">
    About Logistic Regression
  </a>
  
    <nav class="md-nav" aria-label="About Logistic Regression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#logistic-regression-basics" class="md-nav__link">
    Logistic Regression Basics
  </a>
  
    <nav class="md-nav" aria-label="Logistic Regression Basics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classification-algorithm" class="md-nav__link">
    Classification algorithm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#basic-comparison" class="md-nav__link">
    Basic Comparison
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inputoutput-comparison" class="md-nav__link">
    Input/Output Comparison
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#problems-of-linear-regression" class="md-nav__link">
    Problems of Linear Regression
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#logistic-regression-in-depth" class="md-nav__link">
    Logistic Regression In-Depth
  </a>
  
    <nav class="md-nav" aria-label="Logistic Regression In-Depth">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#predicting-probability" class="md-nav__link">
    Predicting Probability
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#logistic-function-g" class="md-nav__link">
    Logistic Function g()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax-function-g" class="md-nav__link">
    Softmax Function g()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-entropy-function-d-for-2-class" class="md-nav__link">
    Cross Entropy Function D() for 2 Class
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-entropy-function-d-for-more-than-2-class" class="md-nav__link">
    Cross Entropy Function D() for More Than 2 Class
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-entropy-loss-over-n-samples" class="md-nav__link">
    Cross Entropy Loss over N samples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#building-a-logistic-regression-model-with-pytorch" class="md-nav__link">
    Building a Logistic Regression Model with PyTorch
  </a>
  
    <nav class="md-nav" aria-label="Building a Logistic Regression Model with PyTorch">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#steps" class="md-nav__link">
    Steps
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-1a-loading-mnist-train-dataset" class="md-nav__link">
    Step 1a: Loading MNIST Train Dataset
  </a>
  
    <nav class="md-nav" aria-label="Step 1a: Loading MNIST Train Dataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#displaying-mnist" class="md-nav__link">
    Displaying MNIST
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-1b-loading-mnist-test-dataset" class="md-nav__link">
    Step 1b: Loading MNIST Test Dataset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-2-make-dataset-iterable" class="md-nav__link">
    Step 2: Make Dataset Iterable
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-3-building-model" class="md-nav__link">
    Step 3: Building Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-4-instantiate-model-class" class="md-nav__link">
    Step 4: Instantiate Model Class
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-5-instantiate-loss-class" class="md-nav__link">
    Step 5: Instantiate Loss Class
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-6-instantiate-optimizer-class" class="md-nav__link">
    Step 6: Instantiate Optimizer Class
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-7-train-model" class="md-nav__link">
    Step 7: Train Model
  </a>
  
    <nav class="md-nav" aria-label="Step 7: Train Model">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#break-down-accuracy-calculation" class="md-nav__link">
    Break Down Accuracy Calculation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#saving-model" class="md-nav__link">
    Saving Model
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#building-a-logistic-regression-model-with-pytorch-gpu" class="md-nav__link">
    Building a Logistic Regression Model with PyTorch (GPU)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    Summary
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#citation" class="md-nav__link">
    Citation
  </a>
  
</li>
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../pytorch_feedforward_neuralnetwork/" title="Feedforward Neural Networks (FNN)" class="md-nav__link">
      Feedforward Neural Networks (FNN)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../pytorch_convolutional_neuralnetwork/" title="Convolutional Neural Networks (CNN)" class="md-nav__link">
      Convolutional Neural Networks (CNN)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../pytorch_recurrent_neuralnetwork/" title="Recurrent Neural Networks (RNN)" class="md-nav__link">
      Recurrent Neural Networks (RNN)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../pytorch_lstm_neuralnetwork/" title="Long Short Term Memory Neural Networks (LSTM)" class="md-nav__link">
      Long Short Term Memory Neural Networks (LSTM)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../pytorch_autoencoder/" title="Autoencoders (AE)" class="md-nav__link">
      Autoencoders (AE)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../pytorch_fc_overcomplete_ae/" title="Fully-connected Overcomplete Autoencoder (AE)" class="md-nav__link">
      Fully-connected Overcomplete Autoencoder (AE)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../boosting_models_pytorch/derivative_gradient_jacobian/" title="Derivative, Gradient and Jacobian" class="md-nav__link">
      Derivative, Gradient and Jacobian
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../boosting_models_pytorch/forwardpropagation_backpropagation_gradientdescent/" title="Forward- and Backward-propagation and Gradient Descent (From Scratch FNN Regression)" class="md-nav__link">
      Forward- and Backward-propagation and Gradient Descent (From Scratch FNN Regression)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../fromscratch/fromscratch_logistic_regression/" title="From Scratch Logistic Regression Classification" class="md-nav__link">
      From Scratch Logistic Regression Classification
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../fromscratch/fromscratch_cnn/" title="From Scratch CNN Classification" class="md-nav__link">
      From Scratch CNN Classification
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../boosting_models_pytorch/lr_scheduling/" title="Learning Rate Scheduling" class="md-nav__link">
      Learning Rate Scheduling
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../boosting_models_pytorch/optimizers/" title="Optimization Algorithms" class="md-nav__link">
      Optimization Algorithms
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../boosting_models_pytorch/weight_initialization_activation_functions/" title="Weight Initialization and Activation Functions" class="md-nav__link">
      Weight Initialization and Activation Functions
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../deep_reinforcement_learning_pytorch/supervised_to_rl/" title="Supervised Learning to Reinforcement Learning (RL)" class="md-nav__link">
      Supervised Learning to Reinforcement Learning (RL)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../deep_reinforcement_learning_pytorch/bellman_mdp/" title="Markov Decision Processes (MDP) and Bellman Equations" class="md-nav__link">
      Markov Decision Processes (MDP) and Bellman Equations
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/" title="Dynamic Programming" class="md-nav__link">
      Dynamic Programming
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../production_pytorch/speed_optimization_basics_numba/" title="Speed Optimization Basics Numba" class="md-nav__link">
      Speed Optimization Basics Numba
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../readings/" title="Additional Readings" class="md-nav__link">
      Additional Readings
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-7" type="checkbox" id="nav-7">
    
    <label class="md-nav__link" for="nav-7">
      Machine Learning Tutorials (CPU/GPU)
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="Machine Learning Tutorials (CPU/GPU)" data-md-level="1">
      <label class="md-nav__title" for="nav-7">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        Machine Learning Tutorials (CPU/GPU)
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../machine_learning/intro/" title="Introduction" class="md-nav__link">
      Introduction
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../machine_learning/gpu/rapids_cudf/" title="GPU DataFrames" class="md-nav__link">
      GPU DataFrames
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../machine_learning/gpu/gpu_fractional_differencing/" title="GPU/CPU Fractional Differencing" class="md-nav__link">
      GPU/CPU Fractional Differencing
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-8" type="checkbox" id="nav-8">
    
    <label class="md-nav__link" for="nav-8">
      Programming Tutorials
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="Programming Tutorials" data-md-level="1">
      <label class="md-nav__title" for="nav-8">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        Programming Tutorials
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../programming/intro/" title="Introduction" class="md-nav__link">
      Introduction
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../programming/cpp/cpp/" title="C++" class="md-nav__link">
      C++
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../programming/bash/bash/" title="Bash" class="md-nav__link">
      Bash
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../programming/python/python/" title="Python" class="md-nav__link">
      Python
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../programming/javascript/javascript/" title="Javascript" class="md-nav__link">
      Javascript
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../programming/electron/electron/" title="Electron" class="md-nav__link">
      Electron
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-9" type="checkbox" id="nav-9">
    
    <label class="md-nav__link" for="nav-9">
      Scalable Database Tutorials
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="Scalable Database Tutorials" data-md-level="1">
      <label class="md-nav__title" for="nav-9">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        Scalable Database Tutorials
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../database/intro/" title="Introduction" class="md-nav__link">
      Introduction
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../database/setting_up_cluster/" title="Cassandra Cluster Setup" class="md-nav__link">
      Cassandra Cluster Setup
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-10" type="checkbox" id="nav-10">
    
    <label class="md-nav__link" for="nav-10">
      News
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="News" data-md-level="1">
      <label class="md-nav__title" for="nav-10">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        News
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/news/" title="Welcome" class="md-nav__link">
      Welcome
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/dbs_gpu_rapids_nvidia_ensemble_frac_diff/" title="Fractional Differencing with GPU (GFD), DBS and NVIDIA, September 2019" class="md-nav__link">
      Fractional Differencing with GPU (GFD), DBS and NVIDIA, September 2019
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/defence_and_science_technology_agency_dsta_nvidia_talk_2016_06/" title="Deep Learning Introduction, Defence and Science Technology Agency (DSTA) and NVIDIA, June 2019" class="md-nav__link">
      Deep Learning Introduction, Defence and Science Technology Agency (DSTA) and NVIDIA, June 2019
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/detect_waterbone_debris_ai_for_social_good_icml_2019_06/" title="Oral Presentation for AI for Social Good Workshop ICML, June 2019" class="md-nav__link">
      Oral Presentation for AI for Social Good Workshop ICML, June 2019
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/it_youth_leader_2019_03_11/" title="IT Youth Leader of The Year 2019, March 2019" class="md-nav__link">
      IT Youth Leader of The Year 2019, March 2019
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/ammi_facebook_google_recap_2018_11_21/" title="AMMI (AIMS) supported by Facebook and Google, November 2018" class="md-nav__link">
      AMMI (AIMS) supported by Facebook and Google, November 2018
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/nanjing_next_nus_tsinghua_ai_finance_healthcare_2018_11_01/" title="NExT++ AI in Healthcare and Finance, Nanjing, November 2018" class="md-nav__link">
      NExT++ AI in Healthcare and Finance, Nanjing, November 2018
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/facebook_pytorch_devcon_recap_2018_10_02/" title="Recap of Facebook PyTorch Developer Conference, San Francisco, September 2018" class="md-nav__link">
      Recap of Facebook PyTorch Developer Conference, San Francisco, September 2018
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/facebook_pytorch_developer_conference_2018_09_05/" title="Facebook PyTorch Developer Conference, San Francisco, September 2018" class="md-nav__link">
      Facebook PyTorch Developer Conference, San Francisco, September 2018
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/nvidia_nus_mit_datathon_2018_07_05/" title="NUS-MIT-NUHS NVIDIA Image Recognition Workshop, Singapore, July 2018" class="md-nav__link">
      NUS-MIT-NUHS NVIDIA Image Recognition Workshop, Singapore, July 2018
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/deep_learning_wizard_1y_2018_06_01/" title="Featured on PyTorch Website 2018" class="md-nav__link">
      Featured on PyTorch Website 2018
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/nvidia_self_driving_cars_talk_2017_06_21/" title="NVIDIA Self Driving Cars & Healthcare Talk, Singapore, June 2017" class="md-nav__link">
      NVIDIA Self Driving Cars & Healthcare Talk, Singapore, June 2017
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/deep_learning_wizard_nvidia_inception_2018_05_01/" title="NVIDIA Inception Partner Status, Singapore, May 2017" class="md-nav__link">
      NVIDIA Inception Partner Status, Singapore, May 2017
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#about-logistic-regression" class="md-nav__link">
    About Logistic Regression
  </a>
  
    <nav class="md-nav" aria-label="About Logistic Regression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#logistic-regression-basics" class="md-nav__link">
    Logistic Regression Basics
  </a>
  
    <nav class="md-nav" aria-label="Logistic Regression Basics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classification-algorithm" class="md-nav__link">
    Classification algorithm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#basic-comparison" class="md-nav__link">
    Basic Comparison
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inputoutput-comparison" class="md-nav__link">
    Input/Output Comparison
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#problems-of-linear-regression" class="md-nav__link">
    Problems of Linear Regression
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#logistic-regression-in-depth" class="md-nav__link">
    Logistic Regression In-Depth
  </a>
  
    <nav class="md-nav" aria-label="Logistic Regression In-Depth">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#predicting-probability" class="md-nav__link">
    Predicting Probability
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#logistic-function-g" class="md-nav__link">
    Logistic Function g()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax-function-g" class="md-nav__link">
    Softmax Function g()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-entropy-function-d-for-2-class" class="md-nav__link">
    Cross Entropy Function D() for 2 Class
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-entropy-function-d-for-more-than-2-class" class="md-nav__link">
    Cross Entropy Function D() for More Than 2 Class
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-entropy-loss-over-n-samples" class="md-nav__link">
    Cross Entropy Loss over N samples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#building-a-logistic-regression-model-with-pytorch" class="md-nav__link">
    Building a Logistic Regression Model with PyTorch
  </a>
  
    <nav class="md-nav" aria-label="Building a Logistic Regression Model with PyTorch">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#steps" class="md-nav__link">
    Steps
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-1a-loading-mnist-train-dataset" class="md-nav__link">
    Step 1a: Loading MNIST Train Dataset
  </a>
  
    <nav class="md-nav" aria-label="Step 1a: Loading MNIST Train Dataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#displaying-mnist" class="md-nav__link">
    Displaying MNIST
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-1b-loading-mnist-test-dataset" class="md-nav__link">
    Step 1b: Loading MNIST Test Dataset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-2-make-dataset-iterable" class="md-nav__link">
    Step 2: Make Dataset Iterable
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-3-building-model" class="md-nav__link">
    Step 3: Building Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-4-instantiate-model-class" class="md-nav__link">
    Step 4: Instantiate Model Class
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-5-instantiate-loss-class" class="md-nav__link">
    Step 5: Instantiate Loss Class
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-6-instantiate-optimizer-class" class="md-nav__link">
    Step 6: Instantiate Optimizer Class
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-7-train-model" class="md-nav__link">
    Step 7: Train Model
  </a>
  
    <nav class="md-nav" aria-label="Step 7: Train Model">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#break-down-accuracy-calculation" class="md-nav__link">
    Break Down Accuracy Calculation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#saving-model" class="md-nav__link">
    Saving Model
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#building-a-logistic-regression-model-with-pytorch-gpu" class="md-nav__link">
    Building a Logistic Regression Model with PyTorch (GPU)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    Summary
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#citation" class="md-nav__link">
    Citation
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/ritchieng/deep-learning-wizard/edit/master/docs/deep_learning/practical_pytorch/pytorch_logistic_regression.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71,7.04C21.1,6.65 21.1,6 20.71,5.63L18.37,3.29C18,2.9 17.35,2.9 16.96,3.29L15.12,5.12L18.87,8.87M3,17.25V21H6.75L17.81,9.93L14.06,6.18L3,17.25Z" /></svg>
                  </a>
                
                
                  
                
                
                <h1 id="logistic-regression-with-pytorch">Logistic Regression with PyTorch<a class="headerlink" href="#logistic-regression-with-pytorch" title="Permanent link">&para;</a></h1>
<div class="admonition tip">
<p class="admonition-title">Run Jupyter Notebook</p>
<p>You can run the code for this section in this <a href="https://github.com/ritchieng/deep-learning-wizard/blob/master/docs/deep_learning/practical_pytorch/pytorch_logistic_regression.ipynb">jupyter notebook link</a>.</p>
</div>
<h2 id="about-logistic-regression">About Logistic Regression<a class="headerlink" href="#about-logistic-regression" title="Permanent link">&para;</a></h2>
<h3 id="logistic-regression-basics">Logistic Regression Basics<a class="headerlink" href="#logistic-regression-basics" title="Permanent link">&para;</a></h3>
<h4 id="classification-algorithm">Classification algorithm<a class="headerlink" href="#classification-algorithm" title="Permanent link">&para;</a></h4>
<ul>
<li>Example: Spam vs No Spam<ul>
<li>Input: Bunch of words</li>
<li>Output: Probability spam or not</li>
</ul>
</li>
</ul>
<h4 id="basic-comparison">Basic Comparison<a class="headerlink" href="#basic-comparison" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Linear regression</strong><ul>
<li>Output: numeric value given inputs</li>
</ul>
</li>
<li><strong>Logistic regression</strong>:<ul>
<li>Output: probability [0, 1] given input belonging to a class</li>
</ul>
</li>
</ul>
<h4 id="inputoutput-comparison">Input/Output Comparison<a class="headerlink" href="#inputoutput-comparison" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Linear regression: Multiplication</strong><ul>
<li>Input: [1]<ul>
<li>Output: 2</li>
</ul>
</li>
<li>Input: [2]<ul>
<li>Output: 4</li>
</ul>
</li>
<li>Trying to model the relationship <code>y = 2x</code></li>
</ul>
</li>
<li><strong>Logistic regression: Spam</strong><ul>
<li>Input: "Sign up to get 1 million dollars by tonight"<ul>
<li>Output: p = 0.8</li>
</ul>
</li>
<li>Input: "This is a receipt for your recent purchase with Amazon"<ul>
<li>Output: p = 0.3</li>
</ul>
</li>
<li><strong>p: probability it is spam</strong></li>
</ul>
</li>
</ul>
<h3 id="problems-of-linear-regression">Problems of Linear Regression<a class="headerlink" href="#problems-of-linear-regression" title="Permanent link">&para;</a></h3>
<ul>
<li>Example<ul>
<li>Fever</li>
<li><strong>Input</strong>: temperature</li>
<li><strong>Output</strong>: fever or no fever</li>
</ul>
</li>
<li>Remember<ul>
<li><strong>Linear regression</strong>: minimize error between points and line</li>
</ul>
</li>
</ul>
<div class="admonition bug">
<p class="admonition-title">Linear Regression Problem 1: Fever value can go negative (below 0) and positive (above 1)</p>
<p>If you simply tried to do a simple linear regression on this fever problem, you would realize an apparent error. Fever can go beyond 1 and below 0 which does not make sense in this context.
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">75</span><span class="p">,</span> <span class="mi">100</span><span class="p">,]</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">colors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">poly1d</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">))(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Fever&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Temperature&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></p>
</div>
<p><img alt="png" src="../pytorch_logistic_regression_files/pytorch_logistic_regression_5_1.png" /></p>
<div class="admonition bug">
<p class="admonition-title">Linear Regression Problem 2: Fever points are not predicted with the presence of outliers</p>
<p>Previously at least some points could be properly predicted. However, with the presence of outliers, everything goes wonky for simple linear regression, having no predictive capacity at all.
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">75</span><span class="p">,</span> <span class="mi">300</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">colors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">poly1d</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">))(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Fever&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Temperature&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></p>
</div>
<p><img alt="png" src="../pytorch_logistic_regression_files/pytorch_logistic_regression_5_0.png" /></p>
<h3 id="logistic-regression-in-depth">Logistic Regression In-Depth<a class="headerlink" href="#logistic-regression-in-depth" title="Permanent link">&para;</a></h3>
<h4 id="predicting-probability">Predicting Probability<a class="headerlink" href="#predicting-probability" title="Permanent link">&para;</a></h4>
<ul>
<li>Linear regression doesn't work</li>
<li>Instead of predicting direct values: <strong>predict probability</strong></li>
</ul>
<p><img alt="" src="../images/cross_entropy_final_4.png" /></p>
<h4 id="logistic-function-g">Logistic Function g()<a class="headerlink" href="#logistic-function-g" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>"Two-class logistic regression"</strong></li>
<li><span><span class="MathJax_Preview">\boldsymbol{y} = A\boldsymbol{x} + \boldsymbol{b}</span><script type="math/tex">\boldsymbol{y} = A\boldsymbol{x} + \boldsymbol{b}</script></span><ul>
<li>Where <span><span class="MathJax_Preview">\boldsymbol{y}</span><script type="math/tex">\boldsymbol{y}</script></span> is a vector comprising the 2-class prediction <span><span class="MathJax_Preview">y_0</span><script type="math/tex">y_0</script></span> and <span><span class="MathJax_Preview">y_1</span><script type="math/tex">y_1</script></span></li>
<li>Where the labels are <span><span class="MathJax_Preview">y_0 = 0</span><script type="math/tex">y_0 = 0</script></span>  and <span><span class="MathJax_Preview">y_1 = 1</span><script type="math/tex">y_1 = 1</script></span></li>
<li>Also, it's bolded because it's a vector, not a matrix.</li>
</ul>
</li>
<li><span><span class="MathJax_Preview">g(y_1) = \frac {1} {1 + e^{-y_1}}</span><script type="math/tex">g(y_1) = \frac {1} {1 + e^{-y_1}}</script></span><ul>
<li><span><span class="MathJax_Preview">g(y_1)</span><script type="math/tex">g(y_1)</script></span> = Estimated probability that <span><span class="MathJax_Preview">y = 1</span><script type="math/tex">y = 1</script></span></li>
</ul>
</li>
<li><span><span class="MathJax_Preview">g(y_0) = 1 - g(y_1)</span><script type="math/tex">g(y_0) = 1 - g(y_1)</script></span><ul>
<li><span><span class="MathJax_Preview">g(y_0)</span><script type="math/tex">g(y_0)</script></span> = Estimated probability that <span><span class="MathJax_Preview">y = 0</span><script type="math/tex">y = 0</script></span></li>
</ul>
</li>
<li>For our illustration above, we have 4 classes, so we have to use softmax function explained below</li>
</ul>
<h4 id="softmax-function-g">Softmax Function g()<a class="headerlink" href="#softmax-function-g" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>"Multi-class logistic regression"</strong><ul>
<li>Generalization of logistic function, where you can derive back to the logistic function if you've a 2 class classification problem</li>
<li>Here, we will use a 4 class example (K = 4) as shown above to be very clear in how it relates back to that simple examaple.</li>
</ul>
</li>
<li><span><span class="MathJax_Preview">\boldsymbol{y} = A\boldsymbol{x} + \boldsymbol{b}</span><script type="math/tex">\boldsymbol{y} = A\boldsymbol{x} + \boldsymbol{b}</script></span><ul>
<li>Where <span><span class="MathJax_Preview">\boldsymbol{y}</span><script type="math/tex">\boldsymbol{y}</script></span> is a vector comprising the 4-class prediction <span><span class="MathJax_Preview">y_0, y_1, y_2, y_3</span><script type="math/tex">y_0, y_1, y_2, y_3</script></span></li>
<li>Where the 4 labels (K = 4) are <span><span class="MathJax_Preview">y_0 = 0, y_1 = 1, y_2 = 2, y_3 = 3</span><script type="math/tex">y_0 = 0, y_1 = 1, y_2 = 2, y_3 = 3</script></span></li>
</ul>
</li>
<li><span><span class="MathJax_Preview">g(y_i) = \frac {e^{y_i} } {\sum^K_i e^{y_i}}</span><script type="math/tex">g(y_i) = \frac {e^{y_i} } {\sum^K_i e^{y_i}}</script></span> where K = 4 because we have 4 classes<ul>
<li>To put numbers to this equation in relation to the illustration above where we've <span><span class="MathJax_Preview">y_0 = 1.3, y_1 = 1.2, y = 4.5, y = 4.8</span><script type="math/tex">y_0 = 1.3, y_1 = 1.2, y = 4.5, y = 4.8</script></span><ul>
<li><span><span class="MathJax_Preview">g(y_0) = \frac {e^{1.3}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.017</span><script type="math/tex">g(y_0) = \frac {e^{1.3}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.017</script></span></li>
<li><span><span class="MathJax_Preview">g(y_1) = \frac {e^{1.2}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.015</span><script type="math/tex">g(y_1) = \frac {e^{1.2}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.015</script></span></li>
<li><span><span class="MathJax_Preview">g(y_2) = \frac {e^{4.5}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.412</span><script type="math/tex">g(y_2) = \frac {e^{4.5}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.412</script></span></li>
<li><span><span class="MathJax_Preview">g(y_3) = \frac {e^{4.8}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.556</span><script type="math/tex">g(y_3) = \frac {e^{4.8}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.556</script></span></li>
<li><span><span class="MathJax_Preview">g(y_0) + g(y_1) + g(y_2) + g(y_3) = 1.0</span><script type="math/tex">g(y_0) + g(y_1) + g(y_2) + g(y_3) = 1.0</script></span></li>
<li>All softmax outputs have to sum to one as they represent a probability distribution over K classes. </li>
</ul>
</li>
</ul>
</li>
<li>Take note how these numbers are not exactly as in the illustration in the softmax box but the concept is important (intentionally made so).<ul>
<li><span><span class="MathJax_Preview">y_0</span><script type="math/tex">y_0</script></span> and <span><span class="MathJax_Preview">y_1</span><script type="math/tex">y_1</script></span> are approximately similar in values and they return similar probabilities.</li>
<li>Similarly, <span><span class="MathJax_Preview">y_2</span><script type="math/tex">y_2</script></span> and <span><span class="MathJax_Preview">y_3</span><script type="math/tex">y_3</script></span> are approximately similar in values and they return similar probabilities.</li>
</ul>
</li>
</ul>
<div class="admonition bug">
<p class="admonition-title">Softmax versus Soft(arg)max</p>
<p>Do you know many researchers and anyone in deep learning in general use the term softmax when it should be soft(arg)max.</p>
<p>This is because soft(arg)max returns the probability distribution over K classes, a vector. </p>
<p>However, softmax only returns the max! This means you will be getting a scalar value versus a probability distribution.</p>
<p>According to my friend, Alfredo Canziani (postdoc in NYU under Yann Lecun), it was actually a mistake made in the original paper previously but it was too late because the term softmax was adopted. Full credits to him for this tip.</p>
</div>
<h4 id="cross-entropy-function-d-for-2-class">Cross Entropy Function D() for 2 Class<a class="headerlink" href="#cross-entropy-function-d-for-2-class" title="Permanent link">&para;</a></h4>
<ul>
<li>Take note that here, <span><span class="MathJax_Preview">S</span><script type="math/tex">S</script></span> is our softmax outputs and <span><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span> are our labels</li>
<li><span><span class="MathJax_Preview">D(S, L) = -(L log S + (1-L)log(1-S))</span><script type="math/tex">D(S, L) = -(L log S + (1-L)log(1-S))</script></span><ul>
<li>If L = 0 (label)<ul>
<li><span><span class="MathJax_Preview">D(S, 0) = - log(1-S)</span><script type="math/tex">D(S, 0) = - log(1-S)</script></span><ul>
<li><span><span class="MathJax_Preview">- log(1-S)</span><script type="math/tex">- log(1-S)</script></span>: less positive if <span><span class="MathJax_Preview">S \longrightarrow 0</span><script type="math/tex">S \longrightarrow 0</script></span></li>
<li><span><span class="MathJax_Preview">- log(1-S)</span><script type="math/tex">- log(1-S)</script></span>: more positive if <span><span class="MathJax_Preview">S \longrightarrow 1</span><script type="math/tex">S \longrightarrow 1</script></span> (BIGGER LOSS)</li>
</ul>
</li>
</ul>
</li>
<li>If L = 1 (label)<ul>
<li><span><span class="MathJax_Preview">D(S, 1) = - log S</span><script type="math/tex">D(S, 1) = - log S</script></span><ul>
<li><span><span class="MathJax_Preview">-log(S)</span><script type="math/tex">-log(S)</script></span>: less positive if <span><span class="MathJax_Preview">S \longrightarrow 1</span><script type="math/tex">S \longrightarrow 1</script></span></li>
<li><span><span class="MathJax_Preview">-log(S)</span><script type="math/tex">-log(S)</script></span>: more positive if <span><span class="MathJax_Preview">S \longrightarrow 0</span><script type="math/tex">S \longrightarrow 0</script></span> (BIGGER LOSS)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Numerical example of bigger or small loss</p>
<p>You get a small error of 1e-5 if your label = 0 and your S is closer to 0 (very correct prediction).
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">math</span>
<span class="nb">print</span><span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mf">0.00001</span><span class="p">))</span>
</code></pre></div></p>
<p>You get a large error of 11.51 if your label is 0 and S is near to 1 (very wrong prediction).
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mf">0.99999</span><span class="p">))</span> 
</code></pre></div></p>
<p>You get a small error of -1e-5 if your label is 1 and S is near 1 (very correct prediction).
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">0.99999</span><span class="p">))</span>
</code></pre></div></p>
<p>You get a big error of -11.51 if your label is 1 and S is near 0 (very wrong prediction).
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">0.00001</span><span class="p">))</span>
</code></pre></div></p>
</div>
<div class="highlight"><pre><span></span><code><span class="mf">1.0000050000287824e-05</span>
<span class="mf">11.51292546497478</span>
<span class="mf">1.0000050000287824e-05</span>
<span class="mf">11.512925464970229</span>
</code></pre></div>

<h4 id="cross-entropy-function-d-for-more-than-2-class">Cross Entropy Function D() for More Than 2 Class<a class="headerlink" href="#cross-entropy-function-d-for-more-than-2-class" title="Permanent link">&para;</a></h4>
<ul>
<li>For the case where we have more than 2 class, we need a more generalized function</li>
<li><span><span class="MathJax_Preview">D(S, L) = - \sum^K_1 L_i log(S_i)</span><script type="math/tex">D(S, L) = - \sum^K_1 L_i log(S_i)</script></span><ul>
<li><span><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span>: number of classes</li>
<li><span><span class="MathJax_Preview">L_i</span><script type="math/tex">L_i</script></span>: label of i-th class, 1 if that's the class else 0</li>
<li><span><span class="MathJax_Preview">S_i</span><script type="math/tex">S_i</script></span>: output of softmax for i-th class</li>
</ul>
</li>
</ul>
<h4 id="cross-entropy-loss-over-n-samples">Cross Entropy Loss over N samples<a class="headerlink" href="#cross-entropy-loss-over-n-samples" title="Permanent link">&para;</a></h4>
<ul>
<li>Goal: Minimizing Cross Entropy Loss, L</li>
<li><span><span class="MathJax_Preview">Loss = \frac {1}{N} \sum_j^N D_j</span><script type="math/tex">Loss = \frac {1}{N} \sum_j^N D_j</script></span><ul>
<li><span><span class="MathJax_Preview">D_j</span><script type="math/tex">D_j</script></span>: j-th sample of cross entropy function <span><span class="MathJax_Preview">D(S, L)</span><script type="math/tex">D(S, L)</script></span></li>
<li><span><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span>: number of samples</li>
<li><span><span class="MathJax_Preview">Loss</span><script type="math/tex">Loss</script></span>: average cross entropy loss over N samples</li>
</ul>
</li>
</ul>
<h2 id="building-a-logistic-regression-model-with-pytorch">Building a Logistic Regression Model with PyTorch<a class="headerlink" href="#building-a-logistic-regression-model-with-pytorch" title="Permanent link">&para;</a></h2>
<p><img alt="" src="../images/lr2.png" /></p>
<h3 id="steps">Steps<a class="headerlink" href="#steps" title="Permanent link">&para;</a></h3>
<ul>
<li>Step 1: Load Dataset</li>
<li>Step 2: Make Dataset Iterable</li>
<li>Step 3: Create Model Class</li>
<li>Step 4: Instantiate Model Class</li>
<li>Step 5: Instantiate Loss Class</li>
<li>Step 6: Instantiate Optimizer Class</li>
<li>Step 7: Train Model</li>
</ul>
<h3 id="step-1a-loading-mnist-train-dataset">Step 1a: Loading MNIST Train Dataset<a class="headerlink" href="#step-1a-loading-mnist-train-dataset" title="Permanent link">&para;</a></h3>
<p><strong>Images from 1 to 9</strong></p>
<div class="admonition note">
<p class="admonition-title">Inspect length of training dataset</p>
<p>You can easily load MNIST dataset with PyTorch. Here we inspect the training set, where our algorithms will learn from, and you will discover it is made up of 60,000 images.
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">import</span> <span class="nn">torchvision.datasets</span> <span class="k">as</span> <span class="nn">dsets</span>
</code></pre></div></p>
<div class="highlight"><pre><span></span><code><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">dsets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> 
                            <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                            <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                            <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="highlight"><pre><span></span><code><span class="mi">60000</span>
</code></pre></div>

<div class="admonition note">
<p class="admonition-title">Inspecting a single image</p>
<p>So this is how a single image is represented in numbers. It's actually a 28 pixel x 28 pixel image which is why you would end up with this 28x28 matrix of numbers.
<div class="highlight"><pre><span></span><code><span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></p>
</div>
<div class="highlight"><pre><span></span><code><span class="p">(</span><span class="n">tensor</span><span class="p">([[[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0118</span><span class="p">,</span>  <span class="mf">0.0706</span><span class="p">,</span>
            <span class="mf">0.0706</span><span class="p">,</span>  <span class="mf">0.0706</span><span class="p">,</span>  <span class="mf">0.4941</span><span class="p">,</span>  <span class="mf">0.5333</span><span class="p">,</span>  <span class="mf">0.6863</span><span class="p">,</span>  <span class="mf">0.1020</span><span class="p">,</span>  <span class="mf">0.6510</span><span class="p">,</span>
            <span class="mf">1.0000</span><span class="p">,</span>  <span class="mf">0.9686</span><span class="p">,</span>  <span class="mf">0.4980</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.1176</span><span class="p">,</span>  <span class="mf">0.1412</span><span class="p">,</span>  <span class="mf">0.3686</span><span class="p">,</span>  <span class="mf">0.6039</span><span class="p">,</span>  <span class="mf">0.6667</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>
            <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.8824</span><span class="p">,</span>  <span class="mf">0.6745</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>
            <span class="mf">0.9490</span><span class="p">,</span>  <span class="mf">0.7647</span><span class="p">,</span>  <span class="mf">0.2510</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.1922</span><span class="p">,</span>  <span class="mf">0.9333</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>
            <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9843</span><span class="p">,</span>  <span class="mf">0.3647</span><span class="p">,</span>  <span class="mf">0.3216</span><span class="p">,</span>  <span class="mf">0.3216</span><span class="p">,</span>
            <span class="mf">0.2196</span><span class="p">,</span>  <span class="mf">0.1529</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0706</span><span class="p">,</span>  <span class="mf">0.8588</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>
            <span class="mf">0.7765</span><span class="p">,</span>  <span class="mf">0.7137</span><span class="p">,</span>  <span class="mf">0.9686</span><span class="p">,</span>  <span class="mf">0.9451</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.3137</span><span class="p">,</span>  <span class="mf">0.6118</span><span class="p">,</span>  <span class="mf">0.4196</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.8039</span><span class="p">,</span>
            <span class="mf">0.0431</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.1686</span><span class="p">,</span>  <span class="mf">0.6039</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0549</span><span class="p">,</span>  <span class="mf">0.0039</span><span class="p">,</span>  <span class="mf">0.6039</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.3529</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.5451</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.7451</span><span class="p">,</span>
            <span class="mf">0.0078</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0431</span><span class="p">,</span>  <span class="mf">0.7451</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>
            <span class="mf">0.2745</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.1373</span><span class="p">,</span>  <span class="mf">0.9451</span><span class="p">,</span>
            <span class="mf">0.8824</span><span class="p">,</span>  <span class="mf">0.6275</span><span class="p">,</span>  <span class="mf">0.4235</span><span class="p">,</span>  <span class="mf">0.0039</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.3176</span><span class="p">,</span>
            <span class="mf">0.9412</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.4667</span><span class="p">,</span>  <span class="mf">0.0980</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.1765</span><span class="p">,</span>  <span class="mf">0.7294</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.5882</span><span class="p">,</span>  <span class="mf">0.1059</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0627</span><span class="p">,</span>  <span class="mf">0.3647</span><span class="p">,</span>  <span class="mf">0.9882</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.7333</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.9765</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9765</span><span class="p">,</span>  <span class="mf">0.2510</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.1804</span><span class="p">,</span>  <span class="mf">0.5098</span><span class="p">,</span>  <span class="mf">0.7176</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.8118</span><span class="p">,</span>  <span class="mf">0.0078</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.1529</span><span class="p">,</span>  <span class="mf">0.5804</span><span class="p">,</span>
            <span class="mf">0.8980</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9804</span><span class="p">,</span>  <span class="mf">0.7137</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0941</span><span class="p">,</span>  <span class="mf">0.4471</span><span class="p">,</span>  <span class="mf">0.8667</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>
            <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.7882</span><span class="p">,</span>  <span class="mf">0.3059</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0902</span><span class="p">,</span>  <span class="mf">0.2588</span><span class="p">,</span>  <span class="mf">0.8353</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>
            <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.7765</span><span class="p">,</span>  <span class="mf">0.3176</span><span class="p">,</span>  <span class="mf">0.0078</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0706</span><span class="p">,</span>
            <span class="mf">0.6706</span><span class="p">,</span>  <span class="mf">0.8588</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.7647</span><span class="p">,</span>
            <span class="mf">0.3137</span><span class="p">,</span>  <span class="mf">0.0353</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.2157</span><span class="p">,</span>  <span class="mf">0.6745</span><span class="p">,</span>  <span class="mf">0.8863</span><span class="p">,</span>
            <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9569</span><span class="p">,</span>  <span class="mf">0.5216</span><span class="p">,</span>  <span class="mf">0.0431</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.5333</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.9922</span><span class="p">,</span>
            <span class="mf">0.9922</span><span class="p">,</span>  <span class="mf">0.8314</span><span class="p">,</span>  <span class="mf">0.5294</span><span class="p">,</span>  <span class="mf">0.5176</span><span class="p">,</span>  <span class="mf">0.0627</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
            <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">]]]),</span>
 <span class="n">tensor</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
</code></pre></div>

<div class="admonition note">
<p class="admonition-title">Inspecting a single data point in the training dataset</p>
<p>When you load MNIST dataset, each data point is actually a tuple containing the image matrix and the label.</p>
<div class="highlight"><pre><span></span><code><span class="nb">type</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>

</div>
<div class="highlight"><pre><span></span><code><span class="nb">tuple</span>
</code></pre></div>

<div class="admonition note">
<p class="admonition-title">Inspecting training dataset first element of tuple</p>
<p>This means to access the image, you need to access the first element in the tuple.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Input Matrix</span>
<span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</code></pre></div>

</div>
<div class="highlight"><pre><span></span><code><span class="c1"># A 28x28 sized image of a digit</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span>
</code></pre></div>

<div class="admonition note">
<p class="admonition-title">Inspecting training dataset second element of tuple</p>
<p>The second element actually represents the image's label. Meaning if the second element says 5, it means the 28x28 matrix of numbers represent a digit 5.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Label</span>
<span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div>

</div>
<div class="highlight"><pre><span></span><code><span class="n">tensor</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div>

<h4 id="displaying-mnist">Displaying MNIST<a class="headerlink" href="#displaying-mnist" title="Permanent link">&para;</a></h4>
<div class="admonition note">
<p class="admonition-title">Verifying shape of MNIST image</p>
<p>As mentioned, a single MNIST image is of the shape 28 pixel x 28 pixel.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>  
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>

</div>
<div class="highlight"><pre><span></span><code><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
</code></pre></div>

<div class="admonition note">
<p class="admonition-title">Plot image of MNIST image</p>
<div class="highlight"><pre><span></span><code><span class="n">show_img</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">show_img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
</code></pre></div>

</div>
<p><img alt="png" src="../pytorch_logistic_regression_files/pytorch_logistic_regression_24_1.png" /></p>
<div class="admonition note">
<p class="admonition-title">Second element of tuple shows label</p>
<p>As you would expect, the label is 5.
<div class="highlight"><pre><span></span><code><span class="c1"># Label</span>
<span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></p>
</div>
<div class="highlight"><pre><span></span><code><span class="n">tensor</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div>

<div class="admonition note">
<p class="admonition-title">Plot second image of MNIST image</p>
<div class="highlight"><pre><span></span><code><span class="n">show_img</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">show_img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
</code></pre></div>

</div>
<p><img alt="png" src="../pytorch_logistic_regression_files/pytorch_logistic_regression_27_1.png" /></p>
<div class="admonition note">
<p class="admonition-title">Second element of tuple shows label</p>
<p>We should see 0 here as the label.
<div class="highlight"><pre><span></span><code><span class="c1"># Label</span>
<span class="n">train_dataset</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></p>
</div>
<div class="highlight"><pre><span></span><code><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>

<h3 id="step-1b-loading-mnist-test-dataset">Step 1b: Loading MNIST Test Dataset<a class="headerlink" href="#step-1b-loading-mnist-test-dataset" title="Permanent link">&para;</a></h3>
<ul>
<li>Show our algorithm works beyond the data we have trained on.</li>
<li>Out-of-sample</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Load test dataset</p>
<p>Compared to the 60k images in the training set, the testing set where the model will not be trained on has 10k images to check for its out-of-sample performance.
<div class="highlight"><pre><span></span><code><span class="n">test_dataset</span> <span class="o">=</span> <span class="n">dsets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> 
                           <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                           <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>
</code></pre></div></p>
<div class="highlight"><pre><span></span><code><span class="nb">len</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="highlight"><pre><span></span><code><span class="mi">10000</span>
</code></pre></div>

<div class="admonition note">
<p class="admonition-title">Test dataset elements</p>
<p>Exactly like the training set, the testing set has 10k tuples containing the 28x28 matrices and their respective labels.
<div class="highlight"><pre><span></span><code><span class="nb">type</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></p>
</div>
<div class="highlight"><pre><span></span><code><span class="nb">tuple</span>
</code></pre></div>

<div class="admonition note">
<p class="admonition-title">Test dataset first element in tuple</p>
<p>This contains the image matrix, similar to the training set.
<div class="highlight"><pre><span></span><code><span class="c1"># Image matrix</span>
<span class="n">test_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</code></pre></div></p>
</div>
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span>
</code></pre></div>

<div class="admonition note">
<p class="admonition-title">Plot image sample from test dataset</p>
<div class="highlight"><pre><span></span><code><span class="n">show_img</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">show_img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
</code></pre></div>

</div>
<p><img alt="png" src="../pytorch_logistic_regression_files/pytorch_logistic_regression_34_1.png" /></p>
<div class="admonition note">
<p class="admonition-title">Test dataset second element in tuple</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Label</span>
<span class="n">test_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div>

</div>
<div class="highlight"><pre><span></span><code><span class="n">tensor</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
</code></pre></div>

<h3 id="step-2-make-dataset-iterable">Step 2: Make Dataset Iterable<a class="headerlink" href="#step-2-make-dataset-iterable" title="Permanent link">&para;</a></h3>
<ul>
<li>Aim: make the dataset iterable</li>
<li><strong>totaldata</strong>: 60000</li>
<li><strong>minibatch</strong>: 100<ul>
<li>Number of examples in 1 iteration</li>
</ul>
</li>
<li><strong>iterations</strong>: 3000<ul>
<li>1 iteration: one mini-batch forward &amp; backward pass</li>
</ul>
</li>
<li><strong>epochs</strong><ul>
<li>1 epoch: running through the whole dataset once</li>
<li><span><span class="MathJax_Preview">epochs = iterations \div \frac{totaldata}{minibatch} = 3000 \div \frac{60000}{100} = 5</span><script type="math/tex">epochs = iterations \div \frac{totaldata}{minibatch} = 3000 \div \frac{60000}{100} = 5</script></span></li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Recap training dataset</p>
<p>Remember training dataset has 60k images and testing dataset has 10k images.
<div class="highlight"><pre><span></span><code><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>
</code></pre></div></p>
</div>
<div class="highlight"><pre><span></span><code><span class="mi">60000</span>
</code></pre></div>

<div class="admonition note">
<p class="admonition-title">Defining epochs</p>
<p>When the model goes through the whole 60k images once, learning how to classify 0-9, it's consider 1 epoch. </p>
<p>However, there's a concept of batch size where it means the model would look at 100 images before updating the model's weights, thereby learning. When the model updates its weights (parameters) after looking at all the images, this is considered 1 iteration.</p>
<div class="highlight"><pre><span></span><code><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
</code></pre></div>

<p>We arbitrarily set 3000 iterations here which means the model would update 3000 times.
<div class="highlight"><pre><span></span><code><span class="n">n_iters</span> <span class="o">=</span> <span class="mi">3000</span>
</code></pre></div></p>
<p>One epoch consists of 60,000 / 100 = 600 iterations. Because we would like to go through 3000 iterations, this implies we would have 3000 / 600 = 5 epochs as each epoch has 600 iterations. </p>
<div class="highlight"><pre><span></span><code><span class="n">num_epochs</span> <span class="o">=</span> <span class="n">n_iters</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">)</span>
<span class="n">num_epochs</span>
</code></pre></div>

</div>
<div class="highlight"><pre><span></span><code><span class="mi">5</span>
</code></pre></div>

<div class="admonition note">
<p class="admonition-title">Create Iterable Object: Training Dataset</p>
<div class="highlight"><pre><span></span><code><span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> 
                                           <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
                                           <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="admonition note">
<p class="admonition-title">Check Iterability</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">collections</span>
<span class="nb">isinstance</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Iterable</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="highlight"><pre><span></span><code><span class="kc">True</span>
</code></pre></div>

<div class="admonition note">
<p class="admonition-title">Create Iterable Object: Testing Dataset</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Iterable object</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">test_dataset</span><span class="p">,</span> 
                                          <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
                                          <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="admonition note">
<p class="admonition-title">Check iterability of testing dataset</p>
<div class="highlight"><pre><span></span><code><span class="nb">isinstance</span><span class="p">(</span><span class="n">test_loader</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Iterable</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="highlight"><pre><span></span><code><span class="kc">True</span>
</code></pre></div>

<div class="admonition note">
<p class="admonition-title">Iterate through dataset</p>
<p>This is just a simplified example of what we're doing above where we're creating an iterable object <code>lst</code> to loop through so we can access all the images <code>img_1</code> and <code>img_2</code>.</p>
<p>Above, the equivalent of <code>lst</code> is <code>train_loader</code> and <code>test_loader</code>.</p>
<div class="highlight"><pre><span></span><code><span class="n">img_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span>
<span class="n">img_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span>
<span class="n">lst</span> <span class="o">=</span> <span class="p">[</span><span class="n">img_1</span><span class="p">,</span> <span class="n">img_2</span><span class="p">]</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="c1"># Need to iterate</span>
<span class="c1"># Think of numbers as the images</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">lst</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="highlight"><pre><span></span><code><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
<span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
</code></pre></div>

<h3 id="step-3-building-model">Step 3: Building Model<a class="headerlink" href="#step-3-building-model" title="Permanent link">&para;</a></h3>
<div class="admonition note">
<p class="admonition-title">Create model class</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Same as linear regression! </span>
<span class="k">class</span> <span class="nc">LogisticRegressionModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LogisticRegressionModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div>

</div>
<h3 id="step-4-instantiate-model-class">Step 4: Instantiate Model Class<a class="headerlink" href="#step-4-instantiate-model-class" title="Permanent link">&para;</a></h3>
<ul>
<li>Input dimension: <ul>
<li>Size of image</li>
<li><span><span class="MathJax_Preview">28 \times 28 = 784</span><script type="math/tex">28 \times 28 = 784</script></span></li>
</ul>
</li>
<li>Output dimension: 10<ul>
<li>0, 1, 2, 3, 4, 5, 6, 7, 8, 9</li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Check size of dataset</p>
<p>This should be 28x28.
<div class="highlight"><pre><span></span><code><span class="c1"># Size of images</span>
<span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</code></pre></div></p>
</div>
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span>
</code></pre></div>

<div class="admonition note">
<p class="admonition-title">Instantiate model class based on input and out dimensions</p>
<p>As we're trying to classify digits 0-9 a total of 10 classes, our output dimension is 10. </p>
<p>And we're feeding the model with 28x28 images, hence our input dimension is 28x28.
<div class="highlight"><pre><span></span><code><span class="n">input_dim</span> <span class="o">=</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span>
<span class="n">output_dim</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegressionModel</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
</code></pre></div></p>
</div>
<h3 id="step-5-instantiate-loss-class">Step 5: Instantiate Loss Class<a class="headerlink" href="#step-5-instantiate-loss-class" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Logistic Regression</strong>: Cross Entropy Loss<ul>
<li><em>Linear Regression: MSE</em></li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Create Cross Entry Loss Class</p>
<p>Unlike linear regression, we do not use MSE here, we need Cross Entry Loss to calculate our loss before we backpropagate and update our parameters.</p>
<div class="highlight"><pre><span></span><code><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>  
</code></pre></div>

</div>
<div class="admonition alert">
<p class="admonition-title">What happens in nn.CrossEntropyLoss()?</p>
<p>It does 2 things at the same time. </p>
<p><br /> 1. Computes softmax (logistic/softmax function)
<br /> 2. Computes cross entropy</p>
</div>
<p><img alt="" src="../images/cross_entropy_final_4.png" /></p>
<h3 id="step-6-instantiate-optimizer-class">Step 6: Instantiate Optimizer Class<a class="headerlink" href="#step-6-instantiate-optimizer-class" title="Permanent link">&para;</a></h3>
<ul>
<li>Simplified equation<ul>
<li><span><span class="MathJax_Preview">\theta = \theta - \eta \cdot \nabla_\theta</span><script type="math/tex">\theta = \theta - \eta \cdot \nabla_\theta</script></span><ul>
<li><span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>: parameters (our variables)</li>
<li><span><span class="MathJax_Preview">\eta</span><script type="math/tex">\eta</script></span>: learning rate (how fast we want to learn)</li>
<li><span><span class="MathJax_Preview">\nabla_\theta</span><script type="math/tex">\nabla_\theta</script></span>: parameters' gradients</li>
</ul>
</li>
</ul>
</li>
<li>Even simplier equation<ul>
<li><code>parameters = parameters - learning_rate * parameters_gradients</code></li>
<li><strong>At every iteration, we update our model's parameters</strong></li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Create optimizer</p>
<p>Similar to what we've covered above, this calculates the parameters' gradients and update them subsequently.
<div class="highlight"><pre><span></span><code><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>  
</code></pre></div></p>
</div>
<div class="admonition note">
<p class="admonition-title">Parameters In-Depth</p>
<p>You'll realize we have 2 sets of parameters, 10x784 which is A and 10x1 which is b in the <span><span class="MathJax_Preview">y = AX + b</span><script type="math/tex">y = AX + b</script></span> equation where X is our input of size 784.</p>
<p>We'll go into details subsequently how these parameters interact with our input to produce our 10x1 output. </p>
<div class="highlight"><pre><span></span><code><span class="c1"># Type of parameter object</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

<span class="c1"># Length of parameters</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())))</span>

<span class="c1"># FC 1 Parameters </span>
<span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>

<span class="c1"># FC 1 Bias Parameters</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</code></pre></div>

</div>
<div class="highlight"><pre><span></span><code><span class="o">&lt;</span><span class="n">generator</span> <span class="nb">object</span> <span class="n">Module</span><span class="o">.</span><span class="n">parameters</span> <span class="n">at</span> <span class="mh">0x7ff7c884f830</span><span class="o">&gt;</span>
<span class="mi">2</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">784</span><span class="p">])</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">])</span>
</code></pre></div>

<div class="admonition note">
<p class="admonition-title">Quick Matrix Product Review</p>
<ul>
<li>Example 1: <strong>matrix product</strong><ul>
<li><span><span class="MathJax_Preview">A: (100, 10)</span><script type="math/tex">A: (100, 10)</script></span></li>
<li><span><span class="MathJax_Preview">B: (10, 1)</span><script type="math/tex">B: (10, 1)</script></span></li>
<li><span><span class="MathJax_Preview">A \cdot B = (100, 10) \cdot (10, 1) = (100, 1)</span><script type="math/tex">A \cdot B = (100, 10) \cdot (10, 1) = (100, 1)</script></span></li>
</ul>
</li>
<li>Example 2: <strong>matrix product</strong><ul>
<li><span><span class="MathJax_Preview">A: (50, 5)</span><script type="math/tex">A: (50, 5)</script></span></li>
<li><span><span class="MathJax_Preview">B: (5, 2)</span><script type="math/tex">B: (5, 2)</script></span></li>
<li><span><span class="MathJax_Preview">A \cdot B = (50, 5) \cdot (5, 2) = (50, 2)</span><script type="math/tex">A \cdot B = (50, 5) \cdot (5, 2) = (50, 2)</script></span></li>
</ul>
</li>
<li>Example 3: <strong>element-wise addition</strong><ul>
<li><span><span class="MathJax_Preview">A: (10, 1)</span><script type="math/tex">A: (10, 1)</script></span></li>
<li><span><span class="MathJax_Preview">B: (10, 1)</span><script type="math/tex">B: (10, 1)</script></span></li>
<li><span><span class="MathJax_Preview">A + B = (10, 1)</span><script type="math/tex">A + B = (10, 1)</script></span></li>
</ul>
</li>
</ul>
</div>
<p><img alt="" src="../images/lr_params2.png" /></p>
<h3 id="step-7-train-model">Step 7: Train Model<a class="headerlink" href="#step-7-train-model" title="Permanent link">&para;</a></h3>
<div class="admonition note">
<p class="admonition-title">7 step process for training models</p>
<ul>
<li>Process <ol>
<li>Convert inputs/labels to tensors with gradients</li>
<li>Clear gradient buffets</li>
<li>Get output given inputs</li>
<li>Get loss</li>
<li>Get gradients w.r.t. parameters</li>
<li>Update parameters using gradients<ul>
<li><code>parameters = parameters - learning_rate * parameters_gradients</code></li>
</ul>
</li>
<li>REPEAT</li>
</ol>
</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="nb">iter</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="c1"># Load images as Variable</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span>

        <span class="c1"># Clear gradients w.r.t. parameters</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># Forward pass to get output/logits</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

        <span class="c1"># Calculate Loss: softmax --&gt; cross entropy loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="c1"># Getting gradients w.r.t. parameters</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># Updating parameters</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="nb">iter</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="nb">iter</span> <span class="o">%</span> <span class="mi">500</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Calculate Accuracy         </span>
            <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="c1"># Iterate through test dataset</span>
            <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
                <span class="c1"># Load images to a Torch Variable</span>
                <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>

                <span class="c1"># Forward pass only to get logits/output</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

                <span class="c1"># Get predictions from the maximum value</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

                <span class="c1"># Total number of labels</span>
                <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

                <span class="c1"># Total correct predictions</span>
                <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

            <span class="n">accuracy</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>

            <span class="c1"># Print Loss</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Iteration: </span><span class="si">{}</span><span class="s1">. Loss: </span><span class="si">{}</span><span class="s1">. Accuracy: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">iter</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">accuracy</span><span class="p">))</span>
</code></pre></div>

</div>
<div class="highlight"><pre><span></span><code><span class="n">Iteration</span><span class="p">:</span> <span class="mf">500.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.8513233661651611</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mi">70</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">1000.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.5732524394989014</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mi">77</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">1500.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.3840199708938599</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mi">79</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">2000.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.1711134910583496</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mi">81</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">2500.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.1094708442687988</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mi">82</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">3000.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.002761721611023</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mi">82</span>
</code></pre></div>

<h4 id="break-down-accuracy-calculation">Break Down Accuracy Calculation<a class="headerlink" href="#break-down-accuracy-calculation" title="Permanent link">&para;</a></h4>
<div class="admonition note">
<p class="admonition-title">Printing outputs of our model</p>
<p>As we've trained our model, we can extract the accuracy calculation portion to understand what's happening without re-training the model.</p>
<p>This would print out the output of the model's predictions on your notebook.</p>
<div class="highlight"><pre><span></span><code><span class="n">iter_test</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
    <span class="n">iter_test</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">iter_test</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;OUTPUTS&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="highlight"><pre><span></span><code><span class="n">OUTPUTS</span>
<span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.4181</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0784</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4840</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0985</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2394</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1801</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1639</span><span class="p">,</span>
          <span class="mf">2.9352</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1552</span><span class="p">,</span>  <span class="mf">0.8852</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.5117</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1099</span><span class="p">,</span>  <span class="mf">1.5295</span><span class="p">,</span>  <span class="mf">0.8863</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8813</span><span class="p">,</span>  <span class="mf">0.5967</span><span class="p">,</span>  <span class="mf">1.3632</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.8977</span><span class="p">,</span>  <span class="mf">0.4183</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4990</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.0126</span><span class="p">,</span>  <span class="mf">2.4112</span><span class="p">,</span>  <span class="mf">0.2373</span><span class="p">,</span>  <span class="mf">0.0857</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7007</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2015</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3428</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.2548</span><span class="p">,</span>  <span class="mf">0.1659</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4703</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">2.8072</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.2973</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0984</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4313</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9619</span><span class="p">,</span>  <span class="mf">0.8670</span><span class="p">,</span>  <span class="mf">1.2201</span><span class="p">,</span>
          <span class="mf">0.3752</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2873</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3272</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0343</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0043</span><span class="p">,</span>  <span class="mf">0.5081</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6452</span><span class="p">,</span>  <span class="mf">1.8647</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6924</span><span class="p">,</span>  <span class="mf">0.1435</span><span class="p">,</span>
          <span class="mf">0.4330</span><span class="p">,</span>  <span class="mf">0.2958</span><span class="p">,</span>  <span class="mf">1.0339</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.5392</span><span class="p">,</span>  <span class="mf">2.9070</span><span class="p">,</span>  <span class="mf">0.2297</span><span class="p">,</span>  <span class="mf">0.3139</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6863</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2734</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8377</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.1238</span><span class="p">,</span>  <span class="mf">0.3285</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3004</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.2037</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3739</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5947</span><span class="p">,</span>  <span class="mf">0.3530</span><span class="p">,</span>  <span class="mf">1.4205</span><span class="p">,</span>  <span class="mf">0.0593</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7307</span><span class="p">,</span>
          <span class="mf">0.6642</span><span class="p">,</span>  <span class="mf">0.3937</span><span class="p">,</span>  <span class="mf">0.8004</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.4439</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3284</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7652</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0952</span><span class="p">,</span>  <span class="mf">0.9323</span><span class="p">,</span>  <span class="mf">0.3006</span><span class="p">,</span>  <span class="mf">0.0238</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.0810</span><span class="p">,</span>  <span class="mf">0.0612</span><span class="p">,</span>  <span class="mf">1.3295</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.5409</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5266</span><span class="p">,</span>  <span class="mf">0.9914</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2369</span><span class="p">,</span>  <span class="mf">0.6583</span><span class="p">,</span>  <span class="mf">0.0992</span><span class="p">,</span>  <span class="mf">0.8525</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.0562</span><span class="p">,</span>  <span class="mf">0.2013</span><span class="p">,</span>  <span class="mf">0.0462</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.6548</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7253</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9825</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1663</span><span class="p">,</span>  <span class="mf">0.9076</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0694</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3708</span><span class="p">,</span>
          <span class="mf">1.8270</span><span class="p">,</span>  <span class="mf">0.2457</span><span class="p">,</span>  <span class="mf">1.5921</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">3.2147</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7689</span><span class="p">,</span>  <span class="mf">0.8531</span><span class="p">,</span>  <span class="mf">1.2320</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8126</span><span class="p">,</span>  <span class="mf">1.1251</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2776</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.4244</span><span class="p">,</span>  <span class="mf">0.5930</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6183</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.7470</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5545</span><span class="p">,</span>  <span class="mf">1.0251</span><span class="p">,</span>  <span class="mf">0.0529</span><span class="p">,</span>  <span class="mf">0.4384</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5934</span><span class="p">,</span>  <span class="mf">0.7666</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.0084</span><span class="p">,</span>  <span class="mf">0.5313</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3465</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.7916</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7064</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7805</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1588</span><span class="p">,</span>  <span class="mf">1.3284</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1708</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2092</span><span class="p">,</span>
          <span class="mf">0.9495</span><span class="p">,</span>  <span class="mf">0.1033</span><span class="p">,</span>  <span class="mf">2.0208</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">3.0602</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.3578</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2576</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2198</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2372</span><span class="p">,</span>  <span class="mf">0.9765</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1514</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.5380</span><span class="p">,</span>  <span class="mf">0.7970</span><span class="p">,</span>  <span class="mf">0.1374</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.2613</span><span class="p">,</span>  <span class="mf">2.8594</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0874</span><span class="p">,</span>  <span class="mf">0.1974</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2018</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0064</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0923</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.2142</span><span class="p">,</span>  <span class="mf">0.2575</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3218</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.4348</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7216</span><span class="p">,</span>  <span class="mf">0.0021</span><span class="p">,</span>  <span class="mf">1.2864</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5062</span><span class="p">,</span>  <span class="mf">0.7761</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3236</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.5667</span><span class="p">,</span>  <span class="mf">0.5431</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7781</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.2157</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0200</span><span class="p">,</span>  <span class="mf">0.1829</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6882</span><span class="p">,</span>  <span class="mf">1.3815</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7609</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0902</span><span class="p">,</span>
          <span class="mf">0.8647</span><span class="p">,</span>  <span class="mf">0.3679</span><span class="p">,</span>  <span class="mf">1.8843</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.0950</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5009</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6347</span><span class="p">,</span>  <span class="mf">0.3662</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4679</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0359</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7671</span><span class="p">,</span>
          <span class="mf">2.7155</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3991</span><span class="p">,</span>  <span class="mf">0.5737</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.7005</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5366</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0434</span><span class="p">,</span>  <span class="mf">1.1289</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5873</span><span class="p">,</span>  <span class="mf">0.2555</span><span class="p">,</span>  <span class="mf">0.8187</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.6557</span><span class="p">,</span>  <span class="mf">0.1241</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4297</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.0635</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5991</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4677</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1231</span><span class="p">,</span>  <span class="mf">2.0445</span><span class="p">,</span>  <span class="mf">0.1128</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1825</span><span class="p">,</span>
          <span class="mf">0.1075</span><span class="p">,</span>  <span class="mf">0.0348</span><span class="p">,</span>  <span class="mf">1.4317</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.0319</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1595</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3415</span><span class="p">,</span>  <span class="mf">0.1095</span><span class="p">,</span>  <span class="mf">0.5339</span><span class="p">,</span>  <span class="mf">0.1973</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3272</span><span class="p">,</span>
          <span class="mf">1.5765</span><span class="p">,</span>  <span class="mf">0.4784</span><span class="p">,</span>  <span class="mf">1.4176</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.4928</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5653</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0672</span><span class="p">,</span>  <span class="mf">0.3325</span><span class="p">,</span>  <span class="mf">0.5359</span><span class="p">,</span>  <span class="mf">0.5368</span><span class="p">,</span>  <span class="mf">2.1542</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.4276</span><span class="p">,</span>  <span class="mf">0.3605</span><span class="p">,</span>  <span class="mf">0.0587</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.4761</span><span class="p">,</span>  <span class="mf">0.2958</span><span class="p">,</span>  <span class="mf">0.6597</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2658</span><span class="p">,</span>  <span class="mf">1.1279</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0676</span><span class="p">,</span>  <span class="mf">1.2506</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.2059</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1489</span><span class="p">,</span>  <span class="mf">0.1051</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0764</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9274</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6838</span><span class="p">,</span>  <span class="mf">0.3464</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2656</span><span class="p">,</span>  <span class="mf">1.4099</span><span class="p">,</span>  <span class="mf">0.4486</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.9527</span><span class="p">,</span>  <span class="mf">0.5682</span><span class="p">,</span>  <span class="mf">0.0156</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.6900</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9611</span><span class="p">,</span>  <span class="mf">0.1395</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0079</span><span class="p">,</span>  <span class="mf">1.5424</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3208</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2682</span><span class="p">,</span>
          <span class="mf">0.3586</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2771</span><span class="p">,</span>  <span class="mf">1.0389</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">4.3606</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.8621</span><span class="p">,</span>  <span class="mf">0.6310</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9657</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2486</span><span class="p">,</span>  <span class="mf">1.2009</span><span class="p">,</span>  <span class="mf">1.1873</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.8255</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2103</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2172</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.1000</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4268</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4627</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1041</span><span class="p">,</span>  <span class="mf">0.2959</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1392</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6855</span><span class="p">,</span>
          <span class="mf">1.8622</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2580</span><span class="p">,</span>  <span class="mf">1.1347</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.3625</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1323</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2224</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8754</span><span class="p">,</span>  <span class="mf">2.4684</span><span class="p">,</span>  <span class="mf">0.0295</span><span class="p">,</span>  <span class="mf">0.1161</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.2660</span><span class="p">,</span>  <span class="mf">0.3037</span><span class="p">,</span>  <span class="mf">1.4570</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">2.8688</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.4517</span><span class="p">,</span>  <span class="mf">0.1782</span><span class="p">,</span>  <span class="mf">1.1149</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0898</span><span class="p">,</span>  <span class="mf">1.1062</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0681</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.5697</span><span class="p">,</span>  <span class="mf">0.8888</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6965</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.0429</span><span class="p">,</span>  <span class="mf">1.4446</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3349</span><span class="p">,</span>  <span class="mf">0.1254</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5017</span><span class="p">,</span>  <span class="mf">0.2286</span><span class="p">,</span>  <span class="mf">0.2328</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.3290</span><span class="p">,</span>  <span class="mf">0.3949</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2586</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.8476</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0004</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1003</span><span class="p">,</span>  <span class="mf">2.2806</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2226</span><span class="p">,</span>  <span class="mf">0.9251</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3165</span><span class="p">,</span>
          <span class="mf">0.4957</span><span class="p">,</span>  <span class="mf">0.0690</span><span class="p">,</span>  <span class="mf">0.0232</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.9108</span><span class="p">,</span>  <span class="mf">1.1355</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2715</span><span class="p">,</span>  <span class="mf">0.2233</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3681</span><span class="p">,</span>  <span class="mf">0.1442</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0001</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.0174</span><span class="p">,</span>  <span class="mf">0.1454</span><span class="p">,</span>  <span class="mf">0.2286</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.0663</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8466</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7147</span><span class="p">,</span>  <span class="mf">2.5685</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2090</span><span class="p">,</span>  <span class="mf">1.2993</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3057</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.8314</span><span class="p">,</span>  <span class="mf">0.7046</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0176</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">1.7013</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8051</span><span class="p">,</span>  <span class="mf">0.7541</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5248</span><span class="p">,</span>  <span class="mf">0.8972</span><span class="p">,</span>  <span class="mf">0.1518</span><span class="p">,</span>  <span class="mf">1.4876</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.8454</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2022</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2829</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.8179</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1239</span><span class="p">,</span>  <span class="mf">0.8630</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2137</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2275</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5411</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3448</span><span class="p">,</span>
          <span class="mf">1.7354</span><span class="p">,</span>  <span class="mf">0.7751</span><span class="p">,</span>  <span class="mf">0.6234</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.6515</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0431</span><span class="p">,</span>  <span class="mf">2.7165</span><span class="p">,</span>  <span class="mf">0.1873</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0623</span><span class="p">,</span>  <span class="mf">0.1286</span><span class="p">,</span>  <span class="mf">0.3597</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.2739</span><span class="p">,</span>  <span class="mf">0.3871</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6699</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.2828</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4663</span><span class="p">,</span>  <span class="mf">0.1182</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0896</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3640</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5129</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4905</span><span class="p">,</span>
          <span class="mf">2.2914</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2227</span><span class="p">,</span>  <span class="mf">0.9463</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.2596</span><span class="p">,</span>  <span class="mf">2.0468</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4405</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0411</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8073</span><span class="p">,</span>  <span class="mf">0.0490</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0604</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.1206</span><span class="p">,</span>  <span class="mf">0.3504</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1059</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.6089</span><span class="p">,</span>  <span class="mf">0.5885</span><span class="p">,</span>  <span class="mf">0.7898</span><span class="p">,</span>  <span class="mf">1.1318</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9008</span><span class="p">,</span>  <span class="mf">0.5875</span><span class="p">,</span>  <span class="mf">0.4227</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.1815</span><span class="p">,</span>  <span class="mf">0.5652</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3590</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.4551</span><span class="p">,</span>  <span class="mf">2.9537</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2805</span><span class="p">,</span>  <span class="mf">0.2372</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4180</span><span class="p">,</span>  <span class="mf">0.0297</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1515</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.6111</span><span class="p">,</span>  <span class="mf">0.6140</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3354</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.7182</span><span class="p">,</span>  <span class="mf">1.6778</span><span class="p">,</span>  <span class="mf">0.0553</span><span class="p">,</span>  <span class="mf">0.0461</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5446</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0338</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0215</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.0881</span><span class="p">,</span>  <span class="mf">0.1506</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2107</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.8027</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7854</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1275</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3177</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1600</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1964</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6084</span><span class="p">,</span>
          <span class="mf">2.1285</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1815</span><span class="p">,</span>  <span class="mf">1.1911</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">2.0656</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4959</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1154</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1363</span><span class="p">,</span>  <span class="mf">2.2426</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7441</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8413</span><span class="p">,</span>
          <span class="mf">0.4675</span><span class="p">,</span>  <span class="mf">0.3269</span><span class="p">,</span>  <span class="mf">1.7279</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.3004</span><span class="p">,</span>  <span class="mf">1.0166</span><span class="p">,</span>  <span class="mf">1.1175</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0618</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0937</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4221</span><span class="p">,</span>  <span class="mf">0.1943</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.1020</span><span class="p">,</span>  <span class="mf">0.3670</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4683</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.0720</span><span class="p">,</span>  <span class="mf">0.2252</span><span class="p">,</span>  <span class="mf">0.0175</span><span class="p">,</span>  <span class="mf">1.3644</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7409</span><span class="p">,</span>  <span class="mf">0.4655</span><span class="p">,</span>  <span class="mf">0.5439</span><span class="p">,</span>
          <span class="mf">0.0380</span><span class="p">,</span>  <span class="mf">0.1279</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2302</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.2409</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2622</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6336</span><span class="p">,</span>  <span class="mf">1.8240</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5951</span><span class="p">,</span>  <span class="mf">1.3408</span><span class="p">,</span>  <span class="mf">0.2130</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.3789</span><span class="p">,</span>  <span class="mf">0.8363</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2101</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.3849</span><span class="p">,</span>  <span class="mf">0.3773</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0585</span><span class="p">,</span>  <span class="mf">0.6896</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0998</span><span class="p">,</span>  <span class="mf">0.2804</span><span class="p">,</span>  <span class="mf">0.0696</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.2529</span><span class="p">,</span>  <span class="mf">0.3143</span><span class="p">,</span>  <span class="mf">0.3409</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.9103</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1578</span><span class="p">,</span>  <span class="mf">1.6673</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4817</span><span class="p">,</span>  <span class="mf">0.4088</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5484</span><span class="p">,</span>  <span class="mf">0.6103</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.2287</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0665</span><span class="p">,</span>  <span class="mf">0.0055</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.1692</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.8531</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2499</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0257</span><span class="p">,</span>  <span class="mf">2.8580</span><span class="p">,</span>  <span class="mf">0.2616</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7122</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.0551</span><span class="p">,</span>  <span class="mf">0.8112</span><span class="p">,</span>  <span class="mf">2.3233</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.2790</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9494</span><span class="p">,</span>  <span class="mf">0.6096</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5653</span><span class="p">,</span>  <span class="mf">2.2792</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0687</span><span class="p">,</span>  <span class="mf">0.1634</span><span class="p">,</span>
          <span class="mf">0.3122</span><span class="p">,</span>  <span class="mf">0.1053</span><span class="p">,</span>  <span class="mf">1.0884</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.1267</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2297</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1315</span><span class="p">,</span>  <span class="mf">0.2428</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5436</span><span class="p">,</span>  <span class="mf">0.4123</span><span class="p">,</span>  <span class="mf">2.3060</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.9278</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1528</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4224</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0235</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9137</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1457</span><span class="p">,</span>  <span class="mf">1.6858</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7552</span><span class="p">,</span>  <span class="mf">0.7293</span><span class="p">,</span>  <span class="mf">0.2510</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.3955</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2187</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1505</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.5643</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2783</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4149</span><span class="p">,</span>  <span class="mf">0.0304</span><span class="p">,</span>  <span class="mf">0.8375</span><span class="p">,</span>  <span class="mf">1.5018</span><span class="p">,</span>  <span class="mf">0.0338</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.3875</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0117</span><span class="p">,</span>  <span class="mf">0.5751</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.2926</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7486</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3238</span><span class="p">,</span>  <span class="mf">1.0384</span><span class="p">,</span>  <span class="mf">0.0308</span><span class="p">,</span>  <span class="mf">0.6792</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0170</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.5797</span><span class="p">,</span>  <span class="mf">0.2819</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3510</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.1219</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5862</span><span class="p">,</span>  <span class="mf">1.5817</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1297</span><span class="p">,</span>  <span class="mf">0.4730</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9171</span><span class="p">,</span>  <span class="mf">0.7886</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.7022</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0501</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2812</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">1.7587</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.4511</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7369</span><span class="p">,</span>  <span class="mf">0.4082</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6426</span><span class="p">,</span>  <span class="mf">1.1784</span><span class="p">,</span>  <span class="mf">0.6052</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.7178</span><span class="p">,</span>  <span class="mf">1.6161</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2220</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.1267</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6719</span><span class="p">,</span>  <span class="mf">0.0505</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4972</span><span class="p">,</span>  <span class="mf">2.9027</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1461</span><span class="p">,</span>  <span class="mf">0.2807</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.2921</span><span class="p">,</span>  <span class="mf">0.2231</span><span class="p">,</span>  <span class="mf">1.1327</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.9892</span><span class="p">,</span>  <span class="mf">2.4401</span><span class="p">,</span>  <span class="mf">0.1274</span><span class="p">,</span>  <span class="mf">0.2838</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7535</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1684</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6493</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.1908</span><span class="p">,</span>  <span class="mf">0.2290</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2150</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.2071</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1351</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9191</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9309</span><span class="p">,</span>  <span class="mf">1.7747</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3046</span><span class="p">,</span>  <span class="mf">0.0183</span><span class="p">,</span>
          <span class="mf">1.0136</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1016</span><span class="p">,</span>  <span class="mf">2.1288</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0103</span><span class="p">,</span>  <span class="mf">0.3280</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6974</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2504</span><span class="p">,</span>  <span class="mf">0.3187</span><span class="p">,</span>  <span class="mf">0.4390</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1879</span><span class="p">,</span>
          <span class="mf">0.3954</span><span class="p">,</span>  <span class="mf">0.2332</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1971</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.2280</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6754</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7438</span><span class="p">,</span>  <span class="mf">0.5078</span><span class="p">,</span>  <span class="mf">0.2544</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1020</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2503</span><span class="p">,</span>
          <span class="mf">2.0799</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5033</span><span class="p">,</span>  <span class="mf">0.5890</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.3972</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9369</span><span class="p">,</span>  <span class="mf">1.2696</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6713</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4159</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0221</span><span class="p">,</span>  <span class="mf">0.6489</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.4777</span><span class="p">,</span>  <span class="mf">1.2497</span><span class="p">,</span>  <span class="mf">0.3931</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.7566</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8230</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0785</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3083</span><span class="p">,</span>  <span class="mf">0.7821</span><span class="p">,</span>  <span class="mf">0.1880</span><span class="p">,</span>  <span class="mf">0.1037</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.0956</span><span class="p">,</span>  <span class="mf">0.4219</span><span class="p">,</span>  <span class="mf">1.0798</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.0328</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1700</span><span class="p">,</span>  <span class="mf">1.3806</span><span class="p">,</span>  <span class="mf">0.5445</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2624</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0780</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3595</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.6253</span><span class="p">,</span>  <span class="mf">0.4309</span><span class="p">,</span>  <span class="mf">0.1813</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.0360</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4704</span><span class="p">,</span>  <span class="mf">0.1948</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7066</span><span class="p">,</span>  <span class="mf">0.6600</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4633</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3602</span><span class="p">,</span>
          <span class="mf">1.7494</span><span class="p">,</span>  <span class="mf">0.1522</span><span class="p">,</span>  <span class="mf">0.6086</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.2032</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7903</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5754</span><span class="p">,</span>  <span class="mf">0.4722</span><span class="p">,</span>  <span class="mf">0.6068</span><span class="p">,</span>  <span class="mf">0.5752</span><span class="p">,</span>  <span class="mf">0.2151</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.2495</span><span class="p">,</span>  <span class="mf">0.3420</span><span class="p">,</span>  <span class="mf">0.9278</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.2247</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1361</span><span class="p">,</span>  <span class="mf">0.9374</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1543</span><span class="p">,</span>  <span class="mf">0.4921</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6553</span><span class="p">,</span>  <span class="mf">0.5885</span><span class="p">,</span>
          <span class="mf">0.2617</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2216</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3736</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.2867</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4486</span><span class="p">,</span>  <span class="mf">0.6658</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8755</span><span class="p">,</span>  <span class="mf">2.3195</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7627</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2132</span><span class="p">,</span>
          <span class="mf">0.2488</span><span class="p">,</span>  <span class="mf">0.3484</span><span class="p">,</span>  <span class="mf">1.0860</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.4031</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4518</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3181</span><span class="p">,</span>  <span class="mf">2.8268</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5371</span><span class="p">,</span>  <span class="mf">1.0154</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9247</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.7385</span><span class="p">,</span>  <span class="mf">1.1031</span><span class="p">,</span>  <span class="mf">0.0422</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">2.8604</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5413</span><span class="p">,</span>  <span class="mf">0.6241</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8017</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4104</span><span class="p">,</span>  <span class="mf">0.6314</span><span class="p">,</span>  <span class="mf">0.4614</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.0218</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3411</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2609</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.2113</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2348</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8535</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1041</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2703</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1294</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7057</span><span class="p">,</span>
          <span class="mf">2.7552</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4429</span><span class="p">,</span>  <span class="mf">0.4517</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">4.5191</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.7407</span><span class="p">,</span>  <span class="mf">1.1091</span><span class="p">,</span>  <span class="mf">0.3975</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9456</span><span class="p">,</span>  <span class="mf">1.2277</span><span class="p">,</span>  <span class="mf">0.3616</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.6564</span><span class="p">,</span>  <span class="mf">0.5063</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4274</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">1.4615</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0765</span><span class="p">,</span>  <span class="mf">1.8388</span><span class="p">,</span>  <span class="mf">1.5006</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2351</span><span class="p">,</span>  <span class="mf">0.2781</span><span class="p">,</span>  <span class="mf">0.2830</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.8491</span><span class="p">,</span>  <span class="mf">0.2222</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7779</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.2160</span><span class="p">,</span>  <span class="mf">0.8502</span><span class="p">,</span>  <span class="mf">0.2413</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0798</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7880</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4286</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8060</span><span class="p">,</span>
          <span class="mf">0.7194</span><span class="p">,</span>  <span class="mf">1.2663</span><span class="p">,</span>  <span class="mf">0.6412</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.3318</span><span class="p">,</span>  <span class="mf">2.3388</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4003</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1094</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0285</span><span class="p">,</span>  <span class="mf">0.1021</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0388</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.0497</span><span class="p">,</span>  <span class="mf">0.5137</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2507</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.7853</span><span class="p">,</span>  <span class="mf">0.5884</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6108</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5557</span><span class="p">,</span>  <span class="mf">0.8696</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6226</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7983</span><span class="p">,</span>
          <span class="mf">1.7169</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0145</span><span class="p">,</span>  <span class="mf">0.8231</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.1739</span><span class="p">,</span>  <span class="mf">0.1562</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2933</span><span class="p">,</span>  <span class="mf">2.3195</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9480</span><span class="p">,</span>  <span class="mf">1.2019</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4834</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.0567</span><span class="p">,</span>  <span class="mf">0.5685</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6841</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.7920</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3339</span><span class="p">,</span>  <span class="mf">0.7452</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6529</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3307</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6092</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0950</span><span class="p">,</span>
          <span class="mf">1.7311</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3481</span><span class="p">,</span>  <span class="mf">0.3801</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.7810</span><span class="p">,</span>  <span class="mf">1.0676</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7611</span><span class="p">,</span>  <span class="mf">0.3658</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0431</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1012</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6048</span><span class="p">,</span>
          <span class="mf">0.3089</span><span class="p">,</span>  <span class="mf">0.9998</span><span class="p">,</span>  <span class="mf">0.7164</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.5856</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5261</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4859</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0551</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1838</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2144</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2599</span><span class="p">,</span>
          <span class="mf">3.3891</span><span class="p">,</span>  <span class="mf">0.4691</span><span class="p">,</span>  <span class="mf">0.7566</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.4984</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7770</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1998</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1075</span><span class="p">,</span>  <span class="mf">1.0882</span><span class="p">,</span>  <span class="mf">0.4539</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5651</span><span class="p">,</span>
          <span class="mf">1.4381</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5678</span><span class="p">,</span>  <span class="mf">1.7479</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.2938</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8536</span><span class="p">,</span>  <span class="mf">0.4259</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5429</span><span class="p">,</span>  <span class="mf">0.0066</span><span class="p">,</span>  <span class="mf">0.4120</span><span class="p">,</span>  <span class="mf">2.3793</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.3666</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2604</span><span class="p">,</span>  <span class="mf">0.0382</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.4080</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9851</span><span class="p">,</span>  <span class="mf">4.0264</span><span class="p">,</span>  <span class="mf">0.1099</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1766</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1557</span><span class="p">,</span>  <span class="mf">0.6419</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.8147</span><span class="p">,</span>  <span class="mf">0.7535</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1452</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.4636</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7323</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6433</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0274</span><span class="p">,</span>  <span class="mf">0.7227</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1799</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9336</span><span class="p">,</span>
          <span class="mf">2.1881</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2073</span><span class="p">,</span>  <span class="mf">1.6522</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.9617</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0348</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3980</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4738</span><span class="p">,</span>  <span class="mf">0.7790</span><span class="p">,</span>  <span class="mf">0.4671</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6115</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.7067</span><span class="p">,</span>  <span class="mf">1.3036</span><span class="p">,</span>  <span class="mf">0.4923</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.0151</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5385</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6072</span><span class="p">,</span>  <span class="mf">0.2902</span><span class="p">,</span>  <span class="mf">3.1570</span><span class="p">,</span>  <span class="mf">0.1062</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2169</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.4491</span><span class="p">,</span>  <span class="mf">0.6326</span><span class="p">,</span>  <span class="mf">1.6829</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.8852</span><span class="p">,</span>  <span class="mf">0.6066</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2840</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4475</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1147</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7858</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1805</span><span class="p">,</span>
          <span class="mf">3.0723</span><span class="p">,</span>  <span class="mf">0.3960</span><span class="p">,</span>  <span class="mf">0.9720</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.0344</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4878</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9675</span><span class="p">,</span>  <span class="mf">1.9649</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3146</span><span class="p">,</span>  <span class="mf">1.2183</span><span class="p">,</span>  <span class="mf">0.6730</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.3650</span><span class="p">,</span>  <span class="mf">0.0646</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0898</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.2118</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0350</span><span class="p">,</span>  <span class="mf">0.9917</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8993</span><span class="p">,</span>  <span class="mf">1.2334</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6723</span><span class="p">,</span>  <span class="mf">2.5847</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.0454</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4149</span><span class="p">,</span>  <span class="mf">0.3927</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.7365</span><span class="p">,</span>  <span class="mf">3.0447</span><span class="p">,</span>  <span class="mf">0.5115</span><span class="p">,</span>  <span class="mf">0.0786</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7544</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2158</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4876</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.2891</span><span class="p">,</span>  <span class="mf">0.5089</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6719</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.3652</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5457</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1167</span><span class="p">,</span>  <span class="mf">2.9056</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1622</span><span class="p">,</span>  <span class="mf">0.8192</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3245</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.6414</span><span class="p">,</span>  <span class="mf">0.8097</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4958</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.8755</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6983</span><span class="p">,</span>  <span class="mf">0.2208</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6463</span><span class="p">,</span>  <span class="mf">0.5276</span><span class="p">,</span>  <span class="mf">0.1145</span><span class="p">,</span>  <span class="mf">2.7229</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.0316</span><span class="p">,</span>  <span class="mf">0.1905</span><span class="p">,</span>  <span class="mf">0.2090</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.9702</span><span class="p">,</span>  <span class="mf">0.1265</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0007</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5106</span><span class="p">,</span>  <span class="mf">0.4970</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0804</span><span class="p">,</span>  <span class="mf">0.0017</span><span class="p">,</span>
          <span class="mf">0.0607</span><span class="p">,</span>  <span class="mf">0.6164</span><span class="p">,</span>  <span class="mf">0.4490</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.8271</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6822</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7434</span><span class="p">,</span>  <span class="mf">2.6457</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6143</span><span class="p">,</span>  <span class="mf">1.1486</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0705</span><span class="p">,</span>
          <span class="mf">0.5611</span><span class="p">,</span>  <span class="mf">0.6422</span><span class="p">,</span>  <span class="mf">0.1250</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.9979</span><span class="p">,</span>  <span class="mf">1.8175</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1658</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0343</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6292</span><span class="p">,</span>  <span class="mf">0.1774</span><span class="p">,</span>  <span class="mf">0.3150</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.4633</span><span class="p">,</span>  <span class="mf">0.9266</span><span class="p">,</span>  <span class="mf">0.0252</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.9039</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6030</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2173</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1768</span><span class="p">,</span>  <span class="mf">2.3198</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5072</span><span class="p">,</span>  <span class="mf">0.3418</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.1551</span><span class="p">,</span>  <span class="mf">0.1282</span><span class="p">,</span>  <span class="mf">1.4250</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.9891</span><span class="p">,</span>  <span class="mf">0.5212</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4518</span><span class="p">,</span>  <span class="mf">0.3267</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0759</span><span class="p">,</span>  <span class="mf">0.3826</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0341</span><span class="p">,</span>
          <span class="mf">0.0382</span><span class="p">,</span>  <span class="mf">0.2451</span><span class="p">,</span>  <span class="mf">0.3658</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">2.1217</span><span class="p">,</span>  <span class="mf">1.5102</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7828</span><span class="p">,</span>  <span class="mf">0.3554</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4192</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0772</span><span class="p">,</span>  <span class="mf">0.0578</span><span class="p">,</span>
          <span class="mf">0.8070</span><span class="p">,</span>  <span class="mf">0.1701</span><span class="p">,</span>  <span class="mf">0.5880</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">1.0665</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3826</span><span class="p">,</span>  <span class="mf">0.6243</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8096</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4227</span><span class="p">,</span>  <span class="mf">0.5925</span><span class="p">,</span>  <span class="mf">1.8112</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.9946</span><span class="p">,</span>  <span class="mf">0.2010</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7731</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.1263</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7484</span><span class="p">,</span>  <span class="mf">0.0041</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5439</span><span class="p">,</span>  <span class="mf">1.7242</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9475</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3835</span><span class="p">,</span>
          <span class="mf">0.8452</span><span class="p">,</span>  <span class="mf">0.3077</span><span class="p">,</span>  <span class="mf">2.2689</span><span class="p">]])</span>
</code></pre></div>

<div class="admonition note">
<p class="admonition-title">Printing output size</p>
<p>This produces a 100x10 matrix because each iteration has a batch size of 100 and each prediction across the 10 classes, with the largest number indicating the likely number it is predicting.
<div class="highlight"><pre><span></span><code><span class="n">iter_test</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
    <span class="n">iter_test</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">iter_test</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;OUTPUTS&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></p>
</div>
<div class="highlight"><pre><span></span><code><span class="n">OUTPUTS</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
</code></pre></div>

<div class="admonition note">
<p class="admonition-title">Printing one output</p>
<p>This would be a 1x10 matrix where the largest number is what the model thinks the image is. Here we can see that in the tensor, position 7 has the largest number, indicating the model thinks the image is 7.</p>
<p>number 0: -0.4181
<br /> number 1: -1.0784
<br />...
<br /> number 7: 2.9352
<div class="highlight"><pre><span></span><code><span class="n">iter_test</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
    <span class="n">iter_test</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">iter_test</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;OUTPUTS&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:])</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></p>
</div>
<div class="highlight"><pre><span></span><code>OUTPUTS
tensor([-0.4181, -1.0784, -0.4840, -0.0985, -0.2394, -0.1801, -1.1639,
         2.9352, -0.1552,  0.8852])
</code></pre></div>

<div class="admonition note">
<p class="admonition-title">Printing prediction output</p>
<p>Because our output is of size 100 (our batch size), our prediction size would also of the size 100.</p>
<div class="highlight"><pre><span></span><code><span class="n">iter_test</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
    <span class="n">iter_test</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">iter_test</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;PREDICTION&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">predicted</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</code></pre></div>

</div>
<div class="highlight"><pre><span></span><code><span class="n">PREDICTION</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">100</span><span class="p">])</span>
</code></pre></div>

<div class="admonition note">
<p class="admonition-title">Print prediction value</p>
<p>We are printing our prediction which as verified above, should be digit 7.</p>
<div class="highlight"><pre><span></span><code><span class="n">iter_test</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
    <span class="n">iter_test</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">iter_test</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;PREDICTION&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">predicted</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>

</div>
<div class="highlight"><pre><span></span><code><span class="n">PREDICTION</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
</code></pre></div>

<div class="admonition note">
<p class="admonition-title">Print prediction, label and label size</p>
<p>We are trying to show what we are predicting and the actual values. In this case, we're predicting the right value 7!</p>
<div class="highlight"><pre><span></span><code><span class="n">iter_test</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
    <span class="n">iter_test</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">iter_test</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;PREDICTION&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">predicted</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;LABEL SIZE&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>

        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;LABEL FOR IMAGE 0&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>

</div>
<div class="highlight"><pre><span></span><code><span class="n">PREDICTION</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>

<span class="n">LABEL</span> <span class="n">SIZE</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">100</span><span class="p">])</span>

<span class="n">LABEL</span> <span class="n">FOR</span> <span class="n">IMAGE</span> <span class="mi">0</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
</code></pre></div>

<div class="admonition note">
<p class="admonition-title">Print second prediction and ground truth</p>
<p>Again, the prediction is correct. Naturally, as our model is quite competent in this simple task.</p>
<div class="highlight"><pre><span></span><code><span class="n">iter_test</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
    <span class="n">iter_test</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">iter_test</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;PREDICTION&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">predicted</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;LABEL SIZE&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>

        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;LABEL FOR IMAGE 1&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div>

</div>
<div class="highlight"><pre><span></span><code><span class="n">PREDICTION</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="n">LABEL</span> <span class="n">SIZE</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">100</span><span class="p">])</span>

<span class="n">LABEL</span> <span class="n">FOR</span> <span class="n">IMAGE</span> <span class="mi">1</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div>

<div class="admonition note">
<p class="admonition-title">Print accuracy</p>
<p>Now we know what each object represents, we can understand how we arrived at our accuracy numbers.</p>
<p>One last thing to note is that <code>correct.item()</code> has this syntax is because <code>correct</code> is a PyTorch tensor and to get the value to compute with <code>total</code> which is an integer, we need to do this.
<div class="highlight"><pre><span></span><code><span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">iter_test</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
    <span class="n">iter_test</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Total number of labels</span>
    <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Total correct predictions</span>
    <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="n">accuracy</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="n">correct</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">total</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
</code></pre></div></p>
</div>
<div class="highlight"><pre><span></span><code><span class="mf">82.94</span>
</code></pre></div>

<div class="admonition note">
<p class="admonition-title">Explanation of Python's .sum() function</p>
<p>Python's .sum() function allows you to do a comparison between two matrices and sum the ones that return <code>True</code> or in our case, those predictions that match actual labels (correct predictions).</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Explaining .sum() python built-in function</span>
<span class="c1"># correct += (predicted == labels).sum()</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">10</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">10</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">a</span> <span class="o">==</span> <span class="n">b</span><span class="p">)</span>

<span class="nb">print</span><span class="p">((</span><span class="n">a</span> <span class="o">==</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
</code></pre></div>

</div>
<div class="highlight"><pre><span></span><code><span class="c1"># matrix a</span>
<span class="p">[</span><span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span><span class="p">]</span>

<span class="c1"># matrix b</span>
<span class="p">[</span><span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span><span class="p">]</span>

<span class="c1"># boolean array</span>
<span class="p">[</span> <span class="kc">True</span>  <span class="kc">True</span>  <span class="kc">True</span>  <span class="kc">True</span>  <span class="kc">True</span>  <span class="kc">True</span>  <span class="kc">True</span>  <span class="kc">True</span>  <span class="kc">True</span>  <span class="kc">True</span><span class="p">]</span>

<span class="c1"># number of elementswhere a matches b</span>
<span class="mi">10</span>
</code></pre></div>

<h4 id="saving-model">Saving Model<a class="headerlink" href="#saving-model" title="Permanent link">&para;</a></h4>
<div class="admonition note">
<p class="admonition-title">Saving PyTorch model</p>
<p>This is how you save your model. Feel free to just change <code>save_model = True</code> to save your model
<div class="highlight"><pre><span></span><code><span class="n">save_model</span> <span class="o">=</span> <span class="kc">False</span>
<span class="k">if</span> <span class="n">save_model</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
    <span class="c1"># Saves only parameters</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">&#39;awesome_model.pkl&#39;</span><span class="p">)</span>
</code></pre></div></p>
</div>
<h2 id="building-a-logistic-regression-model-with-pytorch-gpu">Building a Logistic Regression Model with PyTorch (GPU)<a class="headerlink" href="#building-a-logistic-regression-model-with-pytorch-gpu" title="Permanent link">&para;</a></h2>
<div class="admonition note">
<p class="admonition-title">CPU version</p>
<p>The usual 7-step process, getting repetitive by now which we like. </p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">import</span> <span class="nn">torchvision.datasets</span> <span class="k">as</span> <span class="nn">dsets</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 1: LOADING DATASET</span>
<span class="sd">&#39;&#39;&#39;</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">dsets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> 
                            <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                            <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                            <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">dsets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> 
                           <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                           <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 2: MAKING DATASET ITERABLE</span>
<span class="sd">&#39;&#39;&#39;</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">n_iters</span> <span class="o">=</span> <span class="mi">3000</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="n">n_iters</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">)</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> 
                                           <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
                                           <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">test_dataset</span><span class="p">,</span> 
                                          <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
                                          <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 3: CREATE MODEL CLASS</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="k">class</span> <span class="nc">LogisticRegressionModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LogisticRegressionModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 4: INSTANTIATE MODEL CLASS</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="n">input_dim</span> <span class="o">=</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span>
<span class="n">output_dim</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegressionModel</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 5: INSTANTIATE LOSS CLASS</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>


<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 6: INSTANTIATE OPTIMIZER CLASS</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 7: TRAIN THE MODEL</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="nb">iter</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="c1"># Load images as Variable</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span>

        <span class="c1"># Clear gradients w.r.t. parameters</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># Forward pass to get output/logits</span>
        <span class="c1"># 100 x 10</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

        <span class="c1"># Calculate Loss: softmax --&gt; cross entropy loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="c1"># Getting gradients w.r.t. parameters</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># Updating parameters</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="nb">iter</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="nb">iter</span> <span class="o">%</span> <span class="mi">500</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Calculate Accuracy         </span>
            <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="c1"># Iterate through test dataset</span>
            <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
                <span class="c1"># Load images to a Torch Variable</span>
                <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>

                <span class="c1"># Forward pass only to get logits/output</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

                <span class="c1"># Get predictions from the maximum value</span>
                <span class="c1"># 100 x 1</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

                <span class="c1"># Total number of labels</span>
                <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

                <span class="c1"># Total correct predictions</span>
                <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

            <span class="n">accuracy</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">total</span>

            <span class="c1"># Print Loss</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Iteration: </span><span class="si">{}</span><span class="s1">. Loss: </span><span class="si">{}</span><span class="s1">. Accuracy: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">iter</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">accuracy</span><span class="p">))</span>
</code></pre></div>

</div>
<div class="highlight"><pre><span></span><code><span class="n">Iteration</span><span class="p">:</span> <span class="mf">500.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.876196026802063</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">64.44</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">1000.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.5153584480285645</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">75.68</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">1500.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.3521136045455933</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">78.98</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">2000.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.2136967182159424</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">80.95</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">2500.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.0934826135635376</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">81.97</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">3000.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.024120569229126</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">82.49</span>
</code></pre></div>

<div class="admonition note">
<p class="admonition-title">GPU version</p>
<p>2 things must be on GPU
<br />- <code>model</code>
<br />- <code>tensors</code></p>
<p>Remember step 4 and 7 will be affected and this will be the same for all model building moving forward.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">import</span> <span class="nn">torchvision.datasets</span> <span class="k">as</span> <span class="nn">dsets</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 1: LOADING DATASET</span>
<span class="sd">&#39;&#39;&#39;</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">dsets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> 
                            <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                            <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                            <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">dsets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> 
                           <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                           <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 2: MAKING DATASET ITERABLE</span>
<span class="sd">&#39;&#39;&#39;</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">n_iters</span> <span class="o">=</span> <span class="mi">3000</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="n">n_iters</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">)</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> 
                                           <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
                                           <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">test_dataset</span><span class="p">,</span> 
                                          <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
                                          <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 3: CREATE MODEL CLASS</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="k">class</span> <span class="nc">LogisticRegressionModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LogisticRegressionModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 4: INSTANTIATE MODEL CLASS</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="n">input_dim</span> <span class="o">=</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span>
<span class="n">output_dim</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegressionModel</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>

<span class="c1">#######################</span>
<span class="c1">#  USE GPU FOR MODEL  #</span>
<span class="c1">#######################</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 5: INSTANTIATE LOSS CLASS</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>


<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 6: INSTANTIATE OPTIMIZER CLASS</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">STEP 7: TRAIN THE MODEL</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="nb">iter</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>

        <span class="c1">#######################</span>
        <span class="c1">#  USE GPU FOR MODEL  #</span>
        <span class="c1">#######################</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Clear gradients w.r.t. parameters</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># Forward pass to get output/logits</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

        <span class="c1"># Calculate Loss: softmax --&gt; cross entropy loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="c1"># Getting gradients w.r.t. parameters</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># Updating parameters</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="nb">iter</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="nb">iter</span> <span class="o">%</span> <span class="mi">500</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Calculate Accuracy         </span>
            <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="c1"># Iterate through test dataset</span>
            <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
                <span class="c1">#######################</span>
                <span class="c1">#  USE GPU FOR MODEL  #</span>
                <span class="c1">#######################</span>
                <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

                <span class="c1"># Forward pass only to get logits/output</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

                <span class="c1"># Get predictions from the maximum value</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

                <span class="c1"># Total number of labels</span>
                <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

                <span class="c1">#######################</span>
                <span class="c1">#  USE GPU FOR MODEL  #</span>
                <span class="c1">#######################</span>
                <span class="c1"># Total correct predictions</span>
                <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
                    <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span> <span class="o">==</span> <span class="n">labels</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

            <span class="n">accuracy</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">total</span>

            <span class="c1"># Print Loss</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Iteration: </span><span class="si">{}</span><span class="s1">. Loss: </span><span class="si">{}</span><span class="s1">. Accuracy: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">iter</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">accuracy</span><span class="p">))</span>
</code></pre></div>

</div>
<div class="highlight"><pre><span></span><code><span class="n">Iteration</span><span class="p">:</span> <span class="mf">500.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.8571407794952393</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">68.99</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">1000.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.5415704250335693</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">75.86</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">1500.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.2755383253097534</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">78.92</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">2000.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.2468739748001099</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">80.72</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">2500.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.0708973407745361</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">81.73</span>
<span class="n">Iteration</span><span class="p">:</span> <span class="mf">3000.</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.0359245538711548</span><span class="o">.</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">82.74</span>
</code></pre></div>

<h2 id="summary">Summary<a class="headerlink" href="#summary" title="Permanent link">&para;</a></h2>
<p>We've learnt to...</p>
<div class="admonition success">
<p class="admonition-title">Success</p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> <strong>Logistic regression</strong> basics</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> <strong>Problems</strong> of <strong>linear regression</strong></li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> <strong>In-depth</strong> Logistic Regression<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> Get logits</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> Get softmax</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> Get cross-entropy loss</li>
</ul>
</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> <strong>Aim</strong>: reduce cross-entropy loss</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> Built a <strong>logistic regression model</strong> in <strong>CPU and GPU</strong><ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> Step 1: Load Dataset</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> Step 2: Make Dataset Iterable</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> Step 3: Create Model Class</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> Step 4: Instantiate Model Class</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> Step 5: Instantiate Loss Class</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> Step 6: Instantiate Optimizer Class</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> Step 7: Train Model</li>
</ul>
</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> Important things to be on <strong>GPU</strong><ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> <code>model</code></li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> <code>tensors with gradients</code></li>
</ul>
</li>
</ul>
</div>
<h2 id="citation">Citation<a class="headerlink" href="#citation" title="Permanent link">&para;</a></h2>
<p>If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.</p>
<p><a href="https://zenodo.org/badge/latestdoi/139945544"><img alt="DOI" src="https://zenodo.org/badge/139945544.svg" /></a></p>
                
              
              
                


  <h2 id="__comments">Comments</h2>
  <div id="disqus_thread"></div>
  <script>var disqus_config=function(){this.page.url="https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_logistic_regression/",this.page.identifier="deep_learning/practical_pytorch/pytorch_logistic_regression/"};!function(){var e=document,i=e.createElement("script");i.src="//deep-learning-wizard.disqus.com/embed.js",i.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(i)}()</script>

              
            </article>
          </div>
        </div>
      </main>
      
        

<!-- Application footer -->
<footer class="md-footer">

  <!-- Link to previous and/or next page -->
  
    <div class="md-footer-nav">
      <nav
        class="md-footer-nav__inner md-grid"
        aria-label="Footer"
      >

        <!-- Link to previous page -->
        
          <a
            href="../pytorch_linear_regression/"
            title="Linear Regression"
            class="md-footer-nav__link md-footer-nav__link--prev"
            rel="prev"
          >
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Linear Regression
              </div>
            </div>
          </a>
        

        <!-- Link to next page -->
        
          <a
            href="../pytorch_feedforward_neuralnetwork/"
            title="Feedforward Neural Networks (FNN)"
            class="md-footer-nav__link md-footer-nav__link--next"
            rel="next"
          >
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Feedforward Neural Networks (FNN)
              </div>
            </div>
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4,11V13H16L10.5,18.5L11.92,19.92L19.84,12L11.92,4.08L10.5,5.5L16,11H4Z" /></svg>
            </div>
          </a>
        
      </nav>
    </div>
  

  <!-- Further information -->
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">

      <!-- Copyright and theme information -->
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2020 Deep Learning Wizard by Ritchie Ng
          </div>
        
        proudly an
        <a href="http://www.nvidia.com/page/home.html">NVIDIA Inception Partner</a> and supported by the
        <a href="https://aws.amazon.com/activate/">Amazon AWS Activate Programme</a> by
        <a href="https://www.ritchieng.com/">
          Ritchie Ng</a>
        </a>
      </div>

      <!-- Social links -->
      
  <div class="md-footer-social">
    
      
      
      <a href="https://github.com/ritchieng" target="_blank" rel="noopener" title="github.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
      </a>
    
      
      
      <a href="https://www.facebook.com/DeepLearningWizard/" target="_blank" rel="noopener" title="www.facebook.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14 0 55.52 4.84 55.52 4.84v61h-31.28c-30.8 0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"/></svg>
      </a>
    
      
      
      <a href="https://www.linkedin.com/company/deeplearningwizard/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
      </a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../../assets/javascripts/vendor.36cbf620.min.js"></script>
      <script src="../../../assets/javascripts/bundle.00c583dd.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents"}</script>
      
      <script>
        app = initialize({
          base: "../../..",
          features: ["tabs", "instant"],
          search: Object.assign({
            worker: "../../../assets/javascripts/worker/search.7f7c8775.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>